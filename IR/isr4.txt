20846321	Comparison of the efficacy of three PubMed search filters in finding randomized controlled trials to answer clinical questions.
J Eval Clin Pract 20100916 2013Oct
The aim of this study was to compare the performance of three search methods in the retrieval of relevant clinical trials from PubMed to answer specific clinical questions. Included studies of a sample of 100 Cochrane reviews which recorded in PubMed were considered as the reference standard. The search queries were formulated based on the systematic review titles. Precision, recall and number of retrieved records for limiting the results to clinical trial publication type, and using sensitive and specific clinical queries filters were compared. The number of keywords, presence of specific names of intervention and syndrome in the search keywords were used in a model to predict the recalls and precisions. The Clinical queries-sensitive search strategy retrieved the largest number of records (33) and had the highest recall (41.6%) and lowest precision (4.8%). The presence of specific intervention name was the only significant predictor of all recalls and precisions (P = 0.016). The recall and precision of combination of simple clinical search queries and methodological search filters to find clinical trials in various subjects were considerably low. The limit field strategy yielded in higher precision and fewer retrieved records and approximately similar recall, compared with the clinical queries-sensitive strategy. Presence of specific intervention name in the search keywords increased both recall and precision.
24384709	Text categorization of biomedical data sets using graph kernels and a controlled vocabulary.
IEEE/ACM Trans Comput Biol Bioinform  2013 Sep-Oct
Recently, graph representations of text have been showing improved performance over conventional bag-of-words representations in text categorization applications. In this paper, we present a graph-based representation for biomedical articles and use graph kernels to classify those articles into high-level categories. In our representation, common biomedical concepts and semantic relationships are identified with the help of an existing ontology and are used to build a rich graph structure that provides a consistent feature set and preserves additional semantic information that could improve a classifier's performance. We attempt to classify the graphs using both a set-based graph kernel that is capable of dealing with the disconnected nature of the graphs and a simple linear kernel. Finally, we report the results comparing the classification performance of the kernel classifiers to common text-based classifiers.
24384710	Chemical name extraction based on automatic training data generation and rich feature set.
IEEE/ACM Trans Comput Biol Bioinform  2013 Sep-Oct
The automation of extracting chemical names from text has significant value to biomedical and life science research. A major barrier in this task is the difficulty of getting a sizable and good quality data to train a reliable entity extraction model. Another difficulty is the selection of informative features of chemical names, since comprehensive domain knowledge on chemistry nomenclature is required. Leveraging random text generation techniques, we explore the idea of automatically creating training sets for the task of chemical name extraction. Assuming the availability of an incomplete list of chemical names, called a dictionary, we are able to generate well-controlled, random, yet realistic chemical-like training documents. We statistically analyze the construction of chemical names based on the incomplete dictionary, and propose a series of new features, without relying on any domain knowledge. Compared to state-of-the-art models learned from manually labeled data and domain knowledge, our solution shows better or comparable results in annotating real-world data with less human effort. Moreover, we report an interesting observation about the language for chemical names. That is, both the structural and semantic components of chemical names follow a Zipfian distribution, which resembles many natural languages.
24579196	Unsupervised deep feature learning for deformable registration of MR brain images.
Med Image Comput Comput Assist Interv  2013
Establishing accurate anatomical correspondences is critical for medical image registration. Although many hand-engineered features have been proposed for correspondence detection in various registration applications, no features are general enough to work well for all image data. Although many learning-based methods have been developed to help selection of best features for guiding correspondence detection across subjects with large anatomical variations, they are often limited by requiring the known correspondences (often presumably estimated by certain registration methods) as the ground truth for training. To address this limitation, we propose using an unsupervised deep learning approach to directly learn the basis filters that can effectively represent all observed image patches. Then, the coefficients by these learnt basis filters in representing the particular image patch can be regarded as the morphological signature for correspondence detection during image registration. Specifically, a stacked two-layer convolutional network is constructed to seek for the hierarchical representations for each image patch, where the high-level features are inferred from the responses of the low-level network. By replacing the hand-engineered features with our learnt data-adaptive features for image registration, we achieve promising registration results, which demonstrates that a general approach can be built to improve image registration by using data-adaptive features through unsupervised deep learning.
22014476	Directed graph based image registration.
Comput Med Imaging Graph 20111019 2012Mar
In this paper, a novel image registration method is proposed to achieve accurate registration between images having large shape differences with the help of a set of appropriate intermediate templates. We first demonstrate that directionality is a key factor in both pairwise image registration and groupwise registration, which is defined in this paper to describe the influence of the registration direction and paths on the registration performance. In our solution, the intermediate template selection and intermediate template guided registration are two coherent steps with directionality being considered. To take advantage of the directionality, a directed graph is built based on the asymmetric distance defined on all ordered image pairs in the image population, which is fundamentally different from the undirected graph with symmetric distance metrics in all previous methods, and the shortest distance between template and subject on the directed graph is calculated. The allocated directed path can be thus utilized to better guide the registration by successively registering the subject through the intermediate templates one by one on the path towards the template. The proposed directed graph based solution can also be used in groupwise registration. Specifically, by building a minimum spanning arborescence (MSA) on the directed graph, the population center, i.e., a selected template, as well as the directed registration paths from all the rest of images to the population center, is determined simultaneously. The performance of directed graph based registration algorithm is demonstrated by the spatial normalization on both synthetic dataset and real brain MR images. It is shown that our method can achieve more accurate registration results than both the undirected graph based solution and the direct pairwise registration.
24803509	RAID: a comprehensive resource for human RNA-associated (RNA-RNA/RNA-protein) interaction.
RNA 20140506 2014Jul
Transcriptomic analyses have revealed an unexpected complexity in the eukaryote transcriptome, which includes not only protein-coding transcripts but also an expanding catalog of noncoding RNAs (ncRNAs). Diverse coding and noncoding RNAs (ncRNAs) perform functions through interaction with each other in various cellular processes. In this project, we have developed RAID (http://www.rna-society.org/raid), an RNA-associated (RNA-RNA/RNA-protein) interaction database. RAID intends to provide the scientific community with all-in-one resources for efficient browsing and extraction of the RNA-associated interactions in human. This version of RAID contains more than 6100 RNA-associated interactions obtained by manually reviewing more than 2100 published papers, including 4493 RNA-RNA interactions and 1619 RNA-protein interactions. Each entry contains detailed information on an RNA-associated interaction, including RAID ID, RNA/protein symbol, RNA/protein categories, validated method, expressing tissue, literature references (Pubmed IDs), and detailed functional description. Users can query, browse, analyze, and manipulate RNA-associated (RNA-RNA/RNA-protein) interaction. RAID provides a comprehensive resource of human RNA-associated (RNA-RNA/RNA-protein) interaction network. Furthermore, this resource will help in uncovering the generic organizing principles of cellular function network.
23053906	Automatic retrieval of bone fracture knowledge using natural language processing.
J Digit Imaging  2013Aug
Natural language processing (NLP) techniques to extract data from unstructured text into formal computer representations are valuable for creating robust, scalable methods to mine data in medical documents and radiology reports. As voice recognition (VR) becomes more prevalent in radiology practice, there is opportunity for implementing NLP in real time for decision-support applications such as context-aware information retrieval. For example, as the radiologist dictates a report, an NLP algorithm can extract concepts from the text and retrieve relevant classification or diagnosis criteria or calculate disease probability. NLP can work in parallel with VR to potentially facilitate evidence-based reporting (for example, automatically retrieving the Bosniak classification when the radiologist describes a kidney cyst). For these reasons, we developed and validated an NLP system which extracts fracture and anatomy concepts from unstructured text and retrieves relevant bone fracture knowledge. We implement our NLP in an HTML5 web application to demonstrate a proof-of-concept feedback NLP system which retrieves bone fracture knowledge in real time.
23268488	A sequence labeling approach to link medications and their attributes in clinical notes and clinical trial announcements for information extraction.
J Am Med Inform Assoc 20121225 2013 Sep-Oct
The goal of this work was to evaluate machine learning methods, binary classification and sequence labeling, for medication-attribute linkage detection in two clinical corpora. We double annotated 3000 clinical trial announcements (CTA) and 1655 clinical notes (CN) for medication named entities and their attributes. A binary support vector machine (SVM) classification method with parsimonious feature sets, and a conditional random fields (CRF)-based multi-layered sequence labeling (MLSL) model were proposed to identify the linkages between the entities and their corresponding attributes. We evaluated the system's performance against the human-generated gold standard. The experiments showed that the two machine learning approaches performed statistically significantly better than the baseline rule-based approach. The binary SVM classification achieved 0.94 F-measure with individual tokens as features. The SVM model trained on a parsimonious feature set achieved 0.81 F-measure for CN and 0.87 for CTA. The CRF MLSL method achieved 0.80 F-measure on both corpora. We compared the novel MLSL method with a binary classification and a rule-based method. The MLSL method performed statistically significantly better than the rule-based method. However, the SVM-based binary classification method was statistically significantly better than the MLSL method for both the CTA and CN corpora. Using parsimonious feature sets both the SVM-based binary classification and CRF-based MLSL methods achieved high performance in detecting medication name and attribute linkages in CTA and CN.
23447055	In vivo investigation of restricted diffusion in the human brain with optimized oscillating diffusion gradient encoding.
Magn Reson Med 20130227 2014Jan
Previous studies in phantoms and animals using animal MR systems have shown promising results in using oscillating gradient spin echo (OGSE) diffusion acquisition to depict microstructure information. The OGSE approach has also been shown to be a sensitive biomarker of tumor treatment response and white matter-related diseases. Translating these studies to a human MR scanner faces multiple challenges due to the much weaker gradient system. The goals of this study are to optimize the OGSE acquisition for a human MR system and investigate its applicability in the in vivo human brain. An analytical analysis of the OGSE modulation spectrum was provided. Based on this analysis and thorough simulation experiments, the OGSE acquisition was optimized in terms of diffusion waveform shape, waveform timing, and sequence timing-to achieve higher diffusion sensitivity and better sampling of the diffusion spectrum. The trapezoid-cosine waveform was found to be the optimal OGSE waveform. At the three employed peak encoding frequencies of 18 Hz, 44 Hz, and 63 Hz, the waveform polarity for the least blurry sampling of the diffusion spectrum was 90+/180-, 90+/180+, and 90+/180+, respectively. For the highest diffusion-to-noise ratio at 63 Hz, the b-value was 200 s/mm(2) and the echo time was 116 ms. Using the optimized sequence, a frequency dependence of the measured apparent diffusion coefficients was observed in white matter-dominant regions such as the corpus callosum. The obtained results demonstrate, for the first time, the potential of using an OGSE acquisition for investigating microstructure information on a human MR system.
24356351	Learning multimodal latent attributes.
IEEE Trans Pattern Anal Mach Intell  2014Feb
The rapid development of social media sharing has created a huge demand for automatic media classification and annotation techniques. Attribute learning has emerged as a promising paradigm for bridging the semantic gap and addressing data sparsity via transferring attribute knowledge in object recognition and relatively simple action classification. In this paper, we address the task of attribute learning for understanding multimedia data with sparse and incomplete labels. In particular, we focus on videos of social group activities, which are particularly challenging and topical examples of this task because of their multimodal content and complex and unstructured nature relative to the density of annotations. To solve this problem, we 1) introduce a concept of semilatent attribute space, expressing user-defined and latent attributes in a unified framework, and 2) propose a novel scalable probabilistic topic model for learning multimodal semilatent attributes, which dramatically reduces requirements for an exhaustive accurate attribute ontology and expensive annotation effort. We show that our framework is able to exploit latent attributes to outperform contemporary approaches for addressing a variety of realistic multimedia sparse data learning tasks including: multitask learning, learning with label noise, N-shot transfer learning, and importantly zero-shot learning.
24076749	Sharing behavioral data through a grid infrastructure using data standards.
J Am Med Inform Assoc 20130927 2014 Jul-Aug
In an effort to standardize behavioral measures and their data representation, the present study develops a methodology for incorporating measures found in the National Cancer Institute's (NCI) grid-enabled measures (GEM) portal, a repository for behavioral and social measures, into the cancer data standards registry and repository (caDSR). The methodology consists of four parts for curating GEM measures into the caDSR: (1) develop unified modeling language (UML) models for behavioral measures; (2) create common data elements (CDE) for UML components; (3) bind CDE with concepts from the NCI thesaurus; and (4) register CDE in the caDSR. UML models have been developed for four GEM measures, which have been registered in the caDSR as CDE. New behavioral concepts related to these measures have been created and incorporated into the NCI thesaurus. Best practices for representing measures using UML models have been utilized in the practice (eg, caDSR). One dataset based on a GEM-curated measure is available for use by other systems and users connected to the grid. Behavioral and population science data can be standardized by using and extending current standards. A new branch of CDE for behavioral science was developed for the caDSR. It expands the caDSR domain coverage beyond the clinical and biological areas. In addition, missing terms and concepts specific to the behavioral measures addressed in this paper were added to the NCI thesaurus. A methodology was developed and refined for curation of behavioral and population science data.
24296908	N-CANDA data integration: anatomy of an asynchronous infrastructure for multi-site, multi-instrument longitudinal data capture.
J Am Med Inform Assoc 20131202 2014 Jul-Aug
The infrastructure for data collection implemented by the National Consortium on Alcohol and NeuroDevelopment in Adolescence (N-CANDA) for data collection comprises several innovative features: (a) secure, asynchronous transfer and persistent storage of collected data via a revision control system; (b) two-stage import into a longitudinal database; and (c) use of a script-controlled web browser for data retrieval from a third-party, web-based neuropsychological test battery. The asynchronous operation of data transmission and import is of particular benefit, as it has allowed the consortium sites to begin data collection before the receiving database infrastructure had been deployed. Records were collected within 86 days of funding, 35 days after finalizing the collected instruments. Final instruments were added to the database import 225 days after instrument selection, with up to 173 records already collected at that time. Thus, the concepts implemented in N-CANDA's data collection system helped reduce project start-up time by several months.
23404629	Multi-series DICOM: an extension of DICOM that stores a whole study in a single object.
J Digit Imaging  2013Aug
Today, most medical images are stored as a set of single-frame composite Digital Imaging and Communications in Medicine (DICOM) objects that contain the four levels of the DICOM information model-patient, study, series, and instance. Although DICOM addresses most of the issues related to medical image archiving, it has some limitations. Replicating the header information with each DICOM object increases the study size and the parsing overhead. Multi-frame DICOM (MFD) was developed to address this, among other issues. The MFD combines all DICOM objects belonging to a series into a single DICOM object. Hence, the series-level attributes are normalized, and the amount of header data repetition is reduced. In this paper, multi-series DICOM (MSD) is introduced as a potential extension to the DICOM standard that allows faster parsing, transmission, and storage of studies. MSD extends the MFD de-duplication of series-level attributes to study-level attributes. A single DICOM object that stores the whole study is proposed. An efficient algorithm, called the one-pass de-duplication algorithm, was developed to find and eliminate the replicated data elements within the study. A group of experiments were done that evaluate MSD and the one-pass de-duplication algorithm performance. The experiments show that MSD significantly reduces the amount of data repetition and decreases the time required to read and parse DICOM studies. MSD is one possible solution that addresses the DICOM limitations regarding header information repetition.
24222671	ICD-10 codes used to identify adverse drug events in administrative data: a systematic review.
J Am Med Inform Assoc 20131112 2014 May-Jun
Adverse drug events, the unintended and harmful effects of medications, are important outcome measures in health services research. Yet no universally accepted set of International Classification of Diseases (ICD) revision 10 codes or coding algorithms exists to ensure their consistent identification in administrative data. Our objective was to synthesize a comprehensive set of ICD-10 codes used to identify adverse drug events. We developed a systematic search strategy and applied it to five electronic reference databases. We searched relevant medical journals, conference proceedings, electronic grey literature and bibliographies of relevant studies, and contacted content experts for unpublished studies. One author reviewed the titles and abstracts for inclusion and exclusion criteria. Two authors reviewed eligible full-text articles and abstracted data in duplicate. Data were synthesized in a qualitative manner. Of 4241 titles identified, 41 were included. We found a total of 827 ICD-10 codes that have been used in the medical literature to identify adverse drug events. The median number of codes used to search for adverse drug events was 190 (IQR 156-289) with a large degree of variability between studies in the numbers and types of codes used. Authors commonly used external injury (Y40.0-59.9) and disease manifestation codes. Only two papers reported on the sensitivity of their code set. Substantial variability exists in the methods used to identify adverse drug events in administrative data. Our work may serve as a point of reference for future research and consensus building in this area.
22874281	Isolation of patients with vancomycin resistant enterococci (VRE): efficacy of an electronic alert system.
Stud Health Technol Inform  2012
This study investigates the implementation of an alert system for the isolation of vancomycin resistant enterococci (VRE) colonized patients. Given the risk of admitting a patient colonized by VRE it is necessary to implement efficient isolation measures. An electronic alert system integrated into a health information system (HIS) could help with the detection of these patients and their isolation in proper units. Determine the efficacy of an electronic alert system in improving the rate of properly isolation of patients colonized with VRE. two consecutive series of admission in adults units of 67 patients that were infected or colonized with VRE were compared. The time period of the study was six months before the implementation of the alert system and six months post-implementation of the system. The proportion of admission with proper isolation of the patient in correct units increased 44% after the alert system implementation. The implementation of an alert system improved the proportion of properly isolated patients with VRE.
23892907	Is mutual information adequate for feature selection in regression?
Neural Netw 20130711 2013Dec
Feature selection is an important preprocessing step for many high-dimensional regression problems. One of the most common strategies is to select a relevant feature subset based on the mutual information criterion. However, no connection has been established yet between the use of mutual information and a regression error criterion in the machine learning literature. This is obviously an important lack, since minimising such a criterion is eventually the objective one is interested in. This paper demonstrates that under some reasonable assumptions, features selected with the mutual information criterion are the ones minimising the mean squared error and the mean absolute error. On the contrary, it is also shown that the mutual information criterion can fail in selecting optimal features in some situations that we characterise. The theoretical developments presented in this work are expected to lead in practice to a critical and efficient use of the mutual information for feature selection.
20135080	Putting biomedical ontologies to work.
Methods Inf Med 20100205 2010
Biomedical ontologies exist to serve integration of clinical and experimental data, and it is critical to their success that they be put to widespread use in the annotation of data. How, then, can ontologies achieve the sort of user-friendliness, reliability, cost-effectiveness, and breadth of coverage that is necessary to ensure extensive usage? Our focus here is on two different sets of answers to these questions that have been proposed, on the one hand in medicine, by the SNOMED CT community, and on the other hand in biology, by the OBO Foundry. We address more specifically the issue as to how adherence to certain development principles can advance the usability and effectiveness of an ontology or terminology resource, for example by allowing more accurate maintenance, more reliable application, and more efficient interoperation with other ontologies and information resources. SNOMED CT and the OBO Foundry differ considerably in their general approach. Nevertheless, a general trend towards more formal rigor and cross-domain interoperability can be seen in both and we argue that this trend should be accepted by all similar initiatives in the future. Future efforts in ontology development have to address the need for harmonization and integration of ontologies across disciplinary borders, and for this, coherent formalization of ontologies is a pre-requisite.
20371497	Payao: a community platform for SBML pathway model curation.
Bioinformatics 20100405 2010May15
Payao is a community-based, collaborative web service platform for gene-regulatory and biochemical pathway model curation. The system combines Web 2.0 technologies and online model visualization functions to enable a collaborative community to annotate and curate biological models. Payao reads the models in Systems Biology Markup Language format, displays them with CellDesigner, a process diagram editor, which complies with the Systems Biology Graphical Notation, and provides an interface for model enrichment (adding tags and comments to the models) for the access-controlled community members. Freely available for model curation service at http://www.payaologue.org. Web site implemented in Seaser Framework 2.0 with S2Flex2, MySQL 5.0 and Tomcat 5.5, with all major browsers supported. kitano@sbi.jp
20712718	Caption-based topical descriptors for microscopic images as published in academic papers.
Health Info Libr J  2010Sep
Visual findings summarized in the figures and tables of academic papers are invaluable sources for biomedical researchers. Captions associated with the visual findings are often neglected while retrieving biomedical images in published academic papers. This study is to assess caption-based topical descriptors for microscopic images of breast neoplasms, as published in academic papers retrieved through the PubMed Central database. Human indexers as well as an automatic keyword finder called TAPoR generated the topical descriptors from collected captions. The study then compared the human-generated descriptors to machine-generated descriptors. Finally, a set of core descriptors was developed from both sets and automatically mapped into the Unified Medical Language System's (UMLS) Metathesaurus through a MetaMap Transfer engine. Major topical descriptors included histologic disease names, laboratory procedures, genetic functions and components. Human indexers provided more relevant descriptors than TAPoR. The UMLS Metathesaurus identified several semantic types including Indicator, Reagent, or Diagnostic Aid; Organic Chemical; Laboratory Procedure; Spatial Concept; Qualitative Concept; and Quantitative Concept. The findings suggest that caption-based descriptors can complement title or abstract-based literature indexing for figure image retrieval in articles. With respect to forming a metadata framework for online microscopic image description, the semantic types can be used as a core metadata set. In this regard, this finding can be used for standardising a microscopic image description protocol to train medical students. It is incumbent upon libraries and other information agencies to promote and maintain an interest in the opportunities and challenges associated with biomedical imaging.
20841844	A unified framework for biomedical terminologies and ontologies.
Stud Health Technol Inform  2010
The goal of the OBO (Open Biomedical Ontologies) Foundry initiative is to create and maintain an evolving collection of non-overlapping interoperable ontologies that will offer unambiguous representations of the types of entities in biological and biomedical reality. These ontologies are designed to serve non-redundant annotation of data and scientific text. To achieve these ends, the Foundry imposes strict requirements upon the ontologies eligible for inclusion. While these requirements are not met by most existing biomedical terminologies, the latter may nonetheless support the Foundry's goal of consistent and non-redundant annotation if appropriate mappings of data annotated with their aid can be achieved. To construct such mappings in reliable fashion, however, it is necessary to analyze terminological resources from an ontologically realistic perspective in such a way as to identify the exact import of the 'concepts' and associated terms which they contain. We propose a framework for such analysis that is designed to maximize the degree to which legacy terminologies and the data coded with their aid can be successfully used for information-driven clinical and translational research.
20841870	Finding knowledge translation articles in CINAHL.
Stud Health Technol Inform  2010
The process of moving research into practice has a number of names including knowledge translation (KT). Researchers and decision makers need to be able to readily access the literature on KT for the field to grow and to evaluate the existing evidence. To develop and validate search filters for finding KT articles in the database Cumulative Index to Nursing and Allied Health (CINAHL). A gold standard database was constructed by hand searching and classifying articles from 12 journals as KT Content, KT Applications and KT Theory. Sensitivity, specificity, precision, and accuracy of the search filters. Optimized search filters had fairly low sensitivity and specificity for KT Content (58.4% and 64.9% respectively), while sensitivity and specificity increased for retrieving KT Application (67.5% and 70.2%) and KT Theory articles (70.4% and 77.8%). Search filter performance was suboptimal marking the broad base of disciplines and vocabularies used by KT researchers. Such diversity makes retrieval of KT studies in CINAHL difficult.
20807837	A sample storage management system for biobanks.
Bioinformatics 20100831 2010Nov1
Establishment of large-scale biobanks of human specimens is essential to conduct molecular pathological or epidemiological studies. This requires automation of procedures for specimen cataloguing and tracking through complex analytical processes. The International Agency for Research on Cancer (IARC) develops a large portfolio of studies broadly aimed at cancer prevention and including cohort, case-control and case-only studies in various parts of the world. This diversity of study designs, structure, annotations and specimen collections is extremely difficult to accommodate into a single sample management system (SMS). Current commercial or academic SMS are often restricted to a few sample types and tailored to a limited number of analytic workflows [Voegele et al. (2007) A laboratory information management system (LIMS) for a high throughput genetic platform aimed at candidate gene mutation screening. Bioinformatics, 23, 2504-2506]. Thus, we developed a system based on a three-tier architecture and relying on an Oracle database and an Oracle Forms web application. Data are imported through forms or csv files, and information retrieval is enabled via multi-criteria queries that can generate different types of reports including tables, Excel files, trees, pictures and graphs. The system is easy to install, flexible, expandable and implemented with a high degree of data security and confidentiality. Both the database and the interface have been modeled to be compatible with and adaptable to almost all types of biobanks. The SMS source codes, which are under the GNU General Public License, and supplementary data are freely available at 'http://www-gcs.iarc.fr/sms.php' Supplementary data are available at Bioinformatics online.
21138947	Toward an automatic method for extracting cancer- and other disease-related point mutations from the biomedical literature.
Bioinformatics 20101207 2011Feb1
A major goal of biomedical research in personalized medicine is to find relationships between mutations and their corresponding disease phenotypes. However, most of the disease-related mutational data are currently buried in the biomedical literature in textual form and lack the necessary structure to allow easy retrieval and visualization. We introduce a high-throughput computational method for the identification of relevant disease mutations in PubMed abstracts applied to prostate (PCa) and breast cancer (BCa) mutations. We developed the extractor of mutations (EMU) tool to identify mutations and their associated genes. We benchmarked EMU against MutationFinder--a tool to extract point mutations from text. Our results show that both methods achieve comparable performance on two manually curated datasets. We also benchmarked EMU's performance for extracting the complete mutational information and phenotype. Remarkably, we show that one of the steps in our approach, a filter based on sequence analysis, increases the precision for that task from 0.34 to 0.59 (PCa) and from 0.39 to 0.61 (BCa). We also show that this high-throughput approach can be extended to other diseases. Our method improves the current status of disease-mutation databases by significantly increasing the number of annotated mutations. We found 51 and 128 mutations manually verified to be related to PCa and Bca, respectively, that are not currently annotated for these cancer types in the OMIM or Swiss-Prot databases. EMU's retrieval performance represents a 2-fold improvement in the number of annotated mutations for PCa and BCa. We further show that our method can benefit from full-text analysis once there is an increase in Open Access availability of full-text articles. Freely available at: http://bioinf.umbc.edu/EMU/ftp.
21278185	Quality control and preprocessing of metagenomic datasets.
Bioinformatics 20110128 2011Mar15
Here, we present PRINSEQ for easy and rapid quality control and data preprocessing of genomic and metagenomic datasets. Summary statistics of FASTA (and QUAL) or FASTQ files are generated in tabular and graphical form and sequences can be filtered, reformatted and trimmed by a variety of options to improve downstream analysis. This open-source application was implemented in Perl and can be used as a stand alone version or accessed online through a user-friendly web interface. The source code, user help and additional information are available at http://prinseq.sourceforge.net/.
22168572	Selected national pharmacovigilance websites: an analysis of contents.
Drug Saf  2012Feb1
Pharmacovigilance involves the detection, assessment, understanding, and prevention of adverse drug reactions (ADRs), nationally and internationally. Effective communication, which relies increasingly on the Internet, is a crucial aspect of pharmacovigilance activities. The aim of this study was to perform an exploratory survey of national pharmacovigilance websites and compare their contents. Of 99 international pharmacovigilance organizations known to us (listed in the Side Effects of Drugs Annual 30), 45 included website addresses and 35 provided some or all of the information in English. We reviewed 10 of these 35 websites in order to identify their contents. The 10 sites that we selected contained the most extensive information on pharmacovigilance of those that we were able to access. Reviewing these sites, we identified 32 items of information that we used to assess the scope of each website systematically, using a scoring system based on the presence or absence of those items. All the websites gave clear descriptions of national pharmacovigilance requirements and the reporting systems for ADRs, and all included devices. Beyond this, there was great variability in content from site to site. Few websites allowed access to raw pharmacovigilance data, such as individual case reports. Online drug safety communication from the selected national websites we examined is highly variable from site to site, although a wider study is needed to confirm this. Agreement on the key components of pharmacovigilance websites would facilitate the development of a standardized format to improve online communication.
24071798	Practical challenges in integrating genomic data into the electronic health record.
Genet. Med. 20130926 2013Oct
Genetic testing has had limited impact on routine clinical care. Widespread adoption of electronic health records presents a promising means of disseminating genetic testing into diverse care settings. Practical challenges to integration of genomic data into electronic health records include size and complexity of genetic test results, inadequate use of standards for clinical and genetic data, and limitations in electronic health record capacity to store and analyze genetic data. Related challenges include uncertainty in the interpretation of regulatory requirements for return of results, and privacy concerns specific to genetic testing. Successful integration of genomic data may require significant redesign of existing electronic health record systems.
24505805	Minimizing joint risk of mislabeling for iterative Patch-based label fusion.
Med Image Comput Comput Assist Interv  2013
Automated labeling of anatomical structures in medical images is very important in many neuroscience studies. Recently, patch-based labeling in the non-local manner has been widely investigated to alleviate the possible misalignment when registering atlases to the target image. However, the weights used for label fusion from the registered atlases in conventional methods are generally computed independently and thus lack the capability of preventing the ambiguous atlas patches from contributing to the label fusion. More critically, these weights are often calculated based only on the simple patch similarity, thus not necessarily providing optimal solution for label fusion. To address these issues, we present a novel patch-based label fusion method in multi-atlas scenario, for the goal of labeling each voxel in the target image by the best representative atlas patches that also have the lowest joint risk of mislabeling. Specifically, sparse coding is used to select a small number of atlas patches which best represent the underlying patch at each point of the target image, thus minimizing the chance of including the misleading atlas patches for labeling. Furthermore, we examine the joint risk of any pair of atlas patches in making similar labeling error, by analyzing the correlation of their morphological error patterns and also the labeling consensus among atlases. This joint risk will be further recursively updated based on the latest labeling results to correct the possible labeling errors. To demonstrate the performance of our proposed method, we have evaluated it on both whole brain parcellation and hippocampus segmentation, and achieved promising labeling results, compared with the state-of-the-art methods.
24376084	HTS navigator: freely accessible cheminformatics software for analyzing high-throughput screening data.
Bioinformatics 20131228 2014Feb15
We report on the development of the high-throughput screening (HTS) Navigator software to analyze and visualize the results of HTS of chemical libraries. The HTS Navigator processes output files from different plate readers' formats, computes the overall HTS matrix, automatically detects hits and has different types of baseline navigation and correction features. The software incorporates advanced cheminformatics capabilities such as chemical structure storage and visualization, fast similarity search and chemical neighborhood analysis for retrieved hits. The software is freely available for academic laboratories. http://fourches.web.unc.edu/
25011278	[Acquisition and storage methods for image data collected in Chinese medicine resources survey].
Zhongguo Zhong Yao Za Zhi  2014Apr
The acquisition and storage of the image data are important in the Chinese medicine resources survey, and it is important data and evidence for the process and the results. The image data of the Chinese medicinal materials' habitat, original plant or animal, processing in habitat, commodity form, the relative contents and workshop scenarios in the investigation are important for the compiling of the Color Atlas of National Chinese Medicine Resources, mapping the digital scattergram of the Chinese medicine resources, establishing the digital Chinese medicine plant herbarium and acquiring the documentary of the Chinese medicine resource survey. The content, procedures and methods of the video data collecting have been related and analyzed in this article to provide reference for the Chinese medicine resources survey.
24265724	Exploratory search on Twitter utilizing user feedback and multi-perspective microblog analysis.
PLoS ONE 20131112 2013
In recent years, besides typical information retrieval, a broader concept of information exploration--exploratory search--is emerging into the foreground. In addition, more and more valuable information is presented in microblogs on social networks. We propose a new method for supporting the exploratory search on the Twitter social network. The method copes with several challenges, namely brevity of microblogs called tweets, limited number of available ratings and the need to process the recommendations online. In order to tackle the first challenge, the representation of microblogs is enriched by information from referenced links, topic summarization and affect analysis. The small number of available ratings is raised by interpreting implicit feedback trained by feedback model during browsing. Recommendations are made by a preference model that models user's preferences over tweets. The evaluation shows promising results even when navigating in the space of brief pieces of information, making recommendations based only on a small number of ratings, and by optimizing the models to process in real time.
24197398	How to improve your PubMed/MEDLINE searches: 1. background and basic searching.
J Telemed Telecare 20131106 2013Dec
PubMed provides free access via the Internet to more than 23 million records, of which over 19 million are from the MEDLINE database of journal articles. PubMed also provides access to other databases, such as the NCBI Bookshelf. To perform a basic search, you can simply enter the search terms or the concept that you are looking for in the search box. However, taking care to clarify your key concepts may save much time later on, because a non-specific search is likely to produce an overwhelming number of result hits. One way to make your search more specific is to specify which field you want to search using field tags. By default, the results of a search are sorted by the date added to PubMed and displayed in summary format with 20 result hits (records) on each page. In summary format, the title of the article, list of authors, source of information (e.g., journal name followed by date of publication, volume, issue, pages) and the unique PubMed record number called the PubMed identifier (PMID) are shown. Although information is stored about the articles, PubMed/MEDLINE does not store the full text of the papers themselves. However, PubMedCentral (PMC) stores more than 2.8 million articles (roughly 10% of the articles in PubMed) and provides access to them for free to the users.
22672429	Information-searching behaviors of main and allied health professionals: a nationwide survey in Taiwan.
J Eval Clin Pract 20120603 2013Oct
There are a variety of resources to obtain health information, but few studies have examined if main and allied health professionals prefer different methods. The current study was to investigate their information-searching behaviours. A constructed questionnaire survey was conducted from January through April 2011 in nationwide regional hospitals of Taiwan. Questionnaires were mailed to main professionals (physicians and nurses) and allied professionals (pharmacists, physical therapists, technicians and others), with 6160 valid returns collected. Among all professional groups, the most commonly used resource for seeking health information was a Web portal, followed by colleague consultations and continuing education. Physicians more often accessed Internet-based professional resources (online databases, electronic journals and electronic books) than the other groups (P &lt; 0.05). In contrast, physical therapists more often accessed printed resources (printed journals and textbooks) than the other specialists (P &lt; 0.05). And nurses, physical therapists and technicians more often asked colleagues and used continuing education than the other groups (P &lt; 0.01). The most commonly used online database was Micromedex for pharmacists and MEDLINE for physicians, technicians and physical therapists. Nurses more often accessed Chinese-language databases rather than English-language databases (P &lt; 0.001). This national survey depicts the information-searching pattern of various health professionals. There were significant differences between and within main and allied health professionals in their information searching. The data provide clinical implications for strategies to promote the accessing of evidence-based information.
23677906	Crushed rephased orthogonal slice selection (CROSS) for simultaneous acquisition of two orthogonal proton resonance frequency temperature maps.
J Magn Reson Imaging 20130515 2013Dec
To evaluate a novel imaging sequence termed crushed rephased orthogonal slice selection (CROSS) that uses the available time in long echo time (TE) gradient echo (GRE) imaging-as employed for proton resonance frequency (PRF) shift thermometry-to simultaneously acquire two orthogonal magnetic resonance imaging (MRI) temperature maps around the target region. The CROSS sequence encodes a second orthogonal slice between excitation and data readout in long-TE imaging and applies dedicated crusher (CR) gradients to separate the signals from the two slices. Numerical simulations of the Bloch equations and phantom experiments were performed to analyze the MR signal. In phantom and in vivo experiments with two domestic pigs, the applicability of the CROSS sequence for temperature mapping of thermal therapies with focused ultrasound and laser was studied. A successful separation of the signals from the two slices was achieved for CR dephasing lengths approaching the in-plane resolution. In the two animal experiments, CROSS temperature mapping could be successfully demonstrated at a temporal resolution of 2-3 seconds and a temperature uncertainty of 3-4K. At the expense of a reduced signal in the overlap of the two slices, the CROSS sequence achieves an improvement of temporal resolution by 50%, without requiring further acceleration techniques such as parallel imaging, over conventional sequential GRE sequences employing the same repetition time as the CROSS sequence acquires two slices within one repetition interval.
20513764	Metadata matters: access to image data in the real world.
J. Cell Biol.  2010May31
Data sharing is important in the biological sciences to prevent duplication of effort, to promote scientific integrity, and to facilitate and disseminate scientific discovery. Sharing requires centralized repositories, and submission to and utility of these resources require common data formats. This is particularly challenging for multidimensional microscopy image data, which are acquired from a variety of platforms with a myriad of proprietary file formats (PFFs). In this paper, we describe an open standard format that we have developed for microscopy image data. We call on the community to use open image data standards and to insist that all imaging platforms support these file formats. This will build the foundation for an open image data repository.
22743775	Biological imaging software tools.
Nat. Methods 20120628 2012Jul
Few technologies are more widespread in modern biological laboratories than imaging. Recent advances in optical technologies and instrumentation are providing hitherto unimagined capabilities. Almost all these advances have required the development of software to enable the acquisition, management, analysis and visualization of the imaging data. We review each computational step that biologists encounter when dealing with digital images, the inherent challenges and the overall status of available software for bioimage informatics, focusing on open-source options.
24271252	Development of a standard fall data format for signals from body-worn sensors : the FARSEEING consensus.
Z Gerontol Geriatr  2013Dec
Objective measurement of real-world fall events by using body-worn sensor devices can improve the understanding of falls in older people and enable new technology to prevent, predict, and automatically recognize falls. However, these events are rare and hence challenging to capture. The FARSEEING (FAll Repository for the design of Smart and sElf-adapaive Environments prolonging INdependent livinG) consortium and associated partners strongly argue that a sufficient dataset of real-world falls can only be acquired through a collaboration of many research groups. Therefore, the major aim of the FARSEEING project is to build a meta-database of real-world falls. To establish this meta-database, standardization of data is necessary to make it possible to combine different sources for analysis and to guarantee data quality. A consensus process was started in January 2012 to propose a standard fall data format, involving 40 experts from different countries and different disciplines working in the field of fall recording and fall prevention. During a web-based Delphi process, possible variables to describe participants, falls, and fall signals were collected and rated by the experts. The summarized results were presented and finally discussed during a workshop at the 20th Conference of the International Society of Posture and Gait Research 2012, in Trondheim, Norway. The consensus includes recommendations for a fall definition, fall reporting (including fall reporting frequency, and fall reporting variables), a minimum clinical dataset, a sensor configuration, and variables to describe the signal characteristics.
24318832	Using the Textpresso Site-Specific Recombinases Web server to identify Cre expressing mouse strains and floxed alleles.
Methods Mol. Biol.  2014
Effective tools for searching the biomedical literature are essential for identifying reagents or mouse strains as well as for effective experimental design and informed interpretation of experimental results. We have built the Textpresso Site Specific Recombinases (Textpresso SSR) Web server to enable researchers who use mice to perform in-depth searches of a rapidly growing and complex part of the mouse literature. Our Textpresso Web server provides an interface for searching the full text of most of the peer-reviewed publications that report the characterization or use of mouse strains that express Cre or Flp recombinase. The database also contains most of the publications that describe the characterization or analysis of strains carrying conditional alleles or transgenes that can be inactivated or activated by site-specific recombinases such as Cre or Flp. Textpresso SSR complements the existing online databases that catalog Cre and Flp expression patterns by providing a unique online interface for the in-depth text mining of the site specific recombinase literature.
24377417	Global catalogue of microorganisms (gcm): a comprehensive database and information retrieval, analysis, and visualization system for microbial resources.
BMC Genomics 20131230 2013
Throughout the long history of industrial and academic research, many microbes have been isolated, characterized and preserved (whenever possible) in culture collections. With the steady accumulation in observational data of biodiversity as well as microbial sequencing data, bio-resource centers have to function as data and information repositories to serve academia, industry, and regulators on behalf of and for the general public. Hence, the World Data Centre for Microorganisms (WDCM) started to take its responsibility for constructing an effective information environment that would promote and sustain microbial research data activities, and bridge the gaps currently present within and outside the microbiology communities. Strain catalogue information was collected from collections by online submission. We developed tools for automatic extraction of strain numbers and species names from various sources, including Genbank, Pubmed, and SwissProt. These new tools connect strain catalogue information with the corresponding nucleotide and protein sequences, as well as to genome sequence and references citing a particular strain. All information has been processed and compiled in order to create a comprehensive database of microbial resources, and was named Global Catalogue of Microorganisms (GCM). The current version of GCM contains information of over 273,933 strains, which includes 43,436 bacterial, fungal and archaea species from 52 collections in 25 countries and regions.A number of online analysis and statistical tools have been integrated, together with advanced search functions, which should greatly facilitate the exploration of the content of GCM. A comprehensive dynamic database of microbial resources has been created, which unveils the resources preserved in culture collections especially for those whose informatics infrastructures are still under development, which should foster cumulative research, facilitating the activities of microbiologists world-wide, who work in both public and industrial research centres. This database is available from http://gcm.wfcc.info.
24086311	CoCiter: an efficient tool to infer gene function by assessing the significance of literature co-citation.
PLoS ONE 20130923 2013
A routine approach to inferring functions for a gene set is by using function enrichment analysis based on GO, KEGG or other curated terms and pathways. However, such analysis requires the existence of overlapping genes between the query gene set and those annotated by GO/KEGG. Furthermore, GO/KEGG databases only maintain a very restricted vocabulary. Here, we have developed a tool called "CoCiter" based on literature co-citations to address the limitations in conventional function enrichment analysis. Co-citation analysis is widely used in ranking articles and predicting protein-protein interactions (PPIs). Our algorithm can further assess the co-citation significance of a gene set with any other user-defined gene sets, or with free terms. We show that compared with the traditional approaches, CoCiter is a more accurate and flexible function enrichment analysis method. CoCiter is freely available at www.picb.ac.cn/hanlab/cociter/.
23846532	Content-based medical image retrieval: a survey of applications to multidimensional and multimodality data.
J Digit Imaging  2013Dec
Medical imaging is fundamental to modern healthcare, and its widespread use has resulted in the creation of image databases, as well as picture archiving and communication systems. These repositories now contain images from a diverse range of modalities, multidimensional (three-dimensional or time-varying) images, as well as co-aligned multimodality images. These image collections offer the opportunity for evidence-based diagnosis, teaching, and research; for these applications, there is a requirement for appropriate methods to search the collections for images that have characteristics similar to the case(s) of interest. Content-based image retrieval (CBIR) is an image search technique that complements the conventional text-based retrieval of images by using visual features, such as color, texture, and shape, as search criteria. Medical CBIR is an established field of study that is beginning to realize promise when applied to multidimensional and multimodality medical data. In this paper, we present a review of state-of-the-art medical CBIR approaches in five main categories: two-dimensional image retrieval, retrieval of images with three or more dimensions, the use of nonimage data to enhance the retrieval, multimodality image retrieval, and retrieval from diverse datasets. We use these categories as a framework for discussing the state of the art, focusing on the characteristics and modalities of the information used during medical image retrieval.
23884657	The Cancer Imaging Archive (TCIA): maintaining and operating a public information repository.
J Digit Imaging  2013Dec
The National Institutes of Health have placed significant emphasis on sharing of research data to support secondary research. Investigators have been encouraged to publish their clinical and imaging data as part of fulfilling their grant obligations. Realizing it was not sufficient to merely ask investigators to publish their collection of imaging and clinical data, the National Cancer Institute (NCI) created the open source National Biomedical Image Archive software package as a mechanism for centralized hosting of cancer related imaging. NCI has contracted with Washington University in Saint Louis to create The Cancer Imaging Archive (TCIA)-an open-source, open-access information resource to support research, development, and educational initiatives utilizing advanced medical imaging of cancer. In its first year of operation, TCIA accumulated 23 collections (3.3 million images). Operating and maintaining a high-availability image archive is a complex challenge involving varied archive-specific resources and driven by the needs of both image submitters and image consumers. Quality archives of any type (traditional library, PubMed, refereed journals) require management and customer service. This paper describes the management tasks and user support model for TCIA.
23782905	Radiation internal exposure measurements archiving system (REMAS).
Radiat Prot Dosimetry 20130619 2013Dec
This paper describes a personal-computer-based software, REMAS, which helps users to estimate intake activity and resulting internal doses for all radionuclides existing in (International Commission on Radiological Protection) ICRP 78 and other important elements. In addition to its use in internal dose calculations, it facilitates management of data of monitored persons who are occupationally exposed to unsealed radioactive substances. Furthermore, REMAS offers the possibility to generate different reports of results. The program is suitable for laboratories working in the field of assessment of occupational intake and also for users of radioactive material who are routinely monitored. REMAS, which is bilingual program (English and Arabic), was built with GUI environment and was developed using Microsoft FoxPro. It runs on Microsoft Windows XP operating systems.
24725396	Shifting ground for big data researchers.
Cell  2014Apr10
To harness the strength of data sets growing by leaps and bounds every day, cultural norms in biomedical research are under pressure to change with the times.
23315831	Model-based Acceleration of Parameter mapping (MAP) for saturation prepared radially acquired data.
Magn Reson Med 20130111 2013Dec
A reconstruction technique called Model-based Acceleration of Parameter mapping (MAP) is presented allowing for quantification of longitudinal relaxation time and proton density from radial single-shot measurements after saturation recovery magnetization preparation. Using a mono-exponential model in image space, an iterative fitting algorithm is used to reconstruct one well resolved and consistent image for each of the projections acquired during the saturation recovery relaxation process. The functionality of the algorithm is examined in numerical simulations, phantom experiments, and in-vivo studies. MAP reconstructions of single-shot acquisitions feature the same image quality and resolution as fully sampled reference images in phantom and in-vivo studies. The longitudinal relaxation times obtained from the MAP reconstructions are in very good agreement with the reference values in numerical simulations as well as phantom and in-vivo measurements. Compared to available contrast manipulation techniques, no averaging of projections acquired at different time points of the relaxation process is required in MAP imaging. The proposed technique offers new ways of extracting quantitative information from single-shot measurements acquired after magnetization preparation. The reconstruction simultaneously yields images with high spatiotemporal resolution fully consistent with the acquired data as well as maps of the effective longitudinal relaxation parameter and the relative proton density.
23364759	Accelerated MRI by SPEED with generalized sampling schemes.
Magn Reson Med 20130130 2013Dec
To enhance the fast imaging technique of skipped phase encoding (PE) and edge deghosting (SPEED) for more general sampling options, and thus more flexibility in implementations and applications. SPEED uses skipped PE steps to accelerate MRI scan. Previously, the PE skip size was chosen from prime numbers only. This restriction has been relaxed in this study to allow choice of any integers rather than merely prime numbers. Various sampling patterns were studied under all possible combinations of PE skip size and PE shifts. A criterion based on the rank values of ghost phasor matrices was introduced to evaluate SPEED reconstruction. The reconstruction quality was found to correlate with the rank value of the ghost phasor matrix and the skipped PE size N. A low-rank value indicates a singular matrix that causes failure of the SPEED reconstruction. Composite numbers combined with appropriately chosen PE shifts yielded satisfactory reconstruction results. With properly chosen PE shifts, it was found that any integers, including both prime numbers and composite numbers, could be used as PE skip size for SPEED. This finding allows much more flexible data acquisition options that may lead to more freedom in practical implementations and applications.
23973272	An innovative portal for rare genetic diseases research: the semantic Diseasecard.
J Biomed Inform 20130821 2013Dec
Advances in "omics" hardware and software technologies are bringing rare diseases research back from the sidelines. Whereas in the past these disorders were seldom considered relevant, in the era of whole genome sequencing the direct connections between rare phenotypes and a reduced set of genes are of vital relevance. This increased interest in rare genetic diseases research is pushing forward investment and effort towards the creation of software in the field, and leveraging the wealth of available life sciences data. Alas, most of these tools target one or more rare diseases, are focused solely on a single type of user, or are limited to the most relevant scientific breakthroughs for a specific niche. Furthermore, despite some high quality efforts, the ever-growing number of resources, databases, services and applications is still a burden to this area. Hence, there is a clear interest in new strategies to deliver a holistic perspective over the entire rare genetic diseases research domain. This is Diseasecard's reasoning, to build a true lightweight knowledge base covering rare genetic diseases. Developed with the latest semantic web technologies, this portal delivers unified access to a comprehensive network for researchers, clinicians, patients and bioinformatics developers. With in-context access covering over 20 distinct heterogeneous resources, Diseasecard's workspace provides access to the most relevant scientific knowledge regarding a given disorder, whether through direct common identifiers or through full-text search over all connected resources. In addition to its user-oriented features, Diseasecard's semantic knowledge base is also available for direct querying, enabling everyone to include rare genetic diseases knowledge in new or existing information systems. Diseasecard is publicly available at http://bioinformatics.ua.pt/diseasecard/.
23973273	A methodology for extending domain coverage in SemRep.
J Biomed Inform 20130821 2013Dec
We describe a domain-independent methodology to extend SemRep coverage beyond the biomedical domain. SemRep, a natural language processing application originally designed for biomedical texts, uses the knowledge sources provided by the Unified Medical Language System (UMLS©). Ontological and terminological extensions to the system are needed in order to support other areas of knowledge. We extended SemRep's application by developing a semantic representation of a previously unsupported domain. This was achieved by adapting well-known ontology engineering phases and integrating them with the UMLS knowledge sources on which SemRep crucially depends. While the process to extend SemRep coverage has been successfully applied in earlier projects, this paper presents in detail the step-wise approach we followed and the mechanisms implemented. A case study in the field of medical informatics illustrates how the ontology engineering phases have been adapted for optimal integration with the UMLS. We provide qualitative and quantitative results, which indicate the validity and usefulness of our methodology.
23999002	The EHR-ARCHE project: satisfying clinical information needs in a Shared Electronic Health Record system based on IHE XDS and Archetypes.
Int J Med Inform 20130814 2013Dec
While contributing to an improved continuity of care, Shared Electronic Health Record (EHR) systems may also lead to information overload of healthcare providers. Document-oriented architectures, such as the commonly employed IHE XDS profile, which only support information retrieval at the level of documents, are particularly susceptible for this problem. The objective of the EHR-ARCHE project was to develop a methodology and a prototype to efficiently satisfy healthcare providers' information needs when accessing a patient's Shared EHR during a treatment situation. We especially aimed to investigate whether this objective can be reached by integrating EHR Archetypes into an IHE XDS environment. Using methodical triangulation, we first analysed the information needs of healthcare providers, focusing on the treatment of diabetes patients as an exemplary application domain. We then designed ISO/EN 13606 Archetypes covering the identified information needs. To support a content-based search for fine-grained information items within EHR documents, we extended the IHE XDS environment with two additional actors. Finally, we conducted a formative and summative evaluation of our approach within a controlled study. We identified 446 frequently needed diabetes-specific information items, representing typical information needs of healthcare providers. We then created 128 Archetypes and 120 EHR documents for two fictive patients. All seven diabetes experts, who evaluated our approach, preferred the content-based search to a conventional XDS search. Success rates of finding relevant information was higher for the content-based search (100% versus 80%) and the latter was also more time-efficient (8-14min versus 20min or more). Our results show that for an efficient satisfaction of health care providers' information needs, a content-based search that rests upon the integration of Archetypes into an IHE XDS-based Shared EHR system is superior to a conventional metadata-based XDS search.
24120407	Contrasting temporal trend discovery for large healthcare databases.
Comput Methods Programs Biomed 20130916 2014
With the increased acceptance of electronic health records, we can observe the increasing interest in the application of data mining approaches within this field. This study introduces a novel approach for exploring and comparing temporal trends within different in-patient subgroups, which is based on associated rule mining using Apriori algorithm and linear model-based recursive partitioning. The Nationwide Inpatient Sample (NIS), Healthcare Cost and Utilization Project (HCUP), Agency for Healthcare Research and Quality was used to evaluate the proposed approach. This study presents a novel approach where visual analytics on big data is used for trend discovery in form of a regression tree with scatter plots in the leaves of the tree. The trend lines are used for directly comparing linear trends within a specified time frame. Our results demonstrate the existence of opposite trends in relation to age and sex based subgroups that would be impossible to discover using traditional trend-tracking techniques. Such an approach can be employed regarding decision support applications for policy makers when organizing campaigns or by hospital management for observing trends that cannot be directly discovered using traditional analytical techniques.
24183386	MIAPS: a web-based system for remotely accessing and presenting medical images.
Comput Methods Programs Biomed 20130920 2014
MIAPS (medical image access and presentation system) is a web-based system designed for remotely accessing and presenting DICOM image. MIAPS is accessed with web browser through the Internet. MIAPS provides four features: DICOM image retrieval, maintenance, presentation and output. MIAPS does not intent to replace sophisticated commercial and open source packages, but it provides a web-based solution for teleradiology and medical image sharing. The system has been evaluated by 39 hospitals in China for 10 months.
24160892	Autonomic care platform for optimizing query performance.
BMC Med Inform Decis Mak 20131027 2013
As the amount of information in electronic health care systems increases, data operations get more complicated and time-consuming. Intensive Care platforms require a timely processing of data retrievals to guarantee the continuous display of recent data of patients. Physicians and nurses rely on this data for their decision making. Manual optimization of query executions has become difficult to handle due to the increased amount of queries across multiple sources. Hence, a more automated management is necessary to increase the performance of database queries. The autonomic computing paradigm promises an approach in which the system adapts itself and acts as self-managing entity, thereby limiting human interventions and taking actions. Despite the usage of autonomic control loops in network and software systems, this approach has not been applied so far for health information systems. We extend the COSARA architecture, an infection surveillance and antibiotic management service platform for the Intensive Care Unit (ICU), with self-managed components to increase the performance of data retrievals. We used real-life ICU COSARA queries to analyse slow performance and measure the impact of optimizations. Each day more than 2 million COSARA queries are executed. Three control loops, which monitor the executions and take action, have been proposed: reactive, deliberative and reflective control loops. We focus on improvements of the execution time of microbiology queries directly related to the visual displays of patients' data on the bedside screens. The results show that autonomic control loops are beneficial for the optimizations in the data executions in the ICU. The application of reactive control loop results in a reduction of 8.61% of the average execution time of microbiology results. The combined application of the reactive and deliberative control loop results in an average query time reduction of 10.92% and the combination of reactive, deliberative and reflective control loops provides a reduction of 13.04%. We found that by controlled reduction of queries' executions the performance for the end-user can be improved. The implementation of autonomic control loops in an existing health platform, COSARA, has a positive effect on the timely data visualization for the physician and nurse.
23026995	Digital images are data: and should be treated as such.
Methods Mol. Biol.  2013
The scientific community has become very concerned about inappropriate image manipulation. In journals that check figures after acceptance, 20-25% of the papers contained at least one figure that did not comply with the journal's instructions to authors. The scientific press continues to report a small, but steady stream of cases of fraudulent image manipulation. Inappropriate image manipulation taints the scientific record, damages trust within science, and degrades science's reputation with the general public. Scientists can learn from historians and photojournalists, who have provided a number of examples of attempts to alter or misrepresent the historical record. Scientists must remember that digital images are numerically sampled data that represent the state of a specific sample when examined with a specific instrument. These data should be carefully managed. Changes made to the original data need to be tracked like the protocols used for other experimental procedures. To avoid pitfalls, unexpected artifacts, and unintentional misrepresentation of the image data, a number of image processing guidelines are offered.
24216942	Information-based analysis of X-ray in-line phase tomography with application to the detection of iron oxide nanoparticles in the brain.
Opt Express  2013Nov4
The study analyzes noise in X-ray in-line phase tomography in a biomedical context. The impact of noise on detection of iron oxide nanoparticles in mouse brain is assessed. The part of the noise due to the imaging system and the part due to biology are quantitatively expressed in a Neyman Pearson detection strategy with two models of noise. This represents a practical extension of previous work on noise in phase-contrast X-ray imaging which focused on the theoretical expression of the signal-to-noise ratio in mono-dimensional phantoms, taking account of the statistical noise of the imaging system only. We also report the impact of the phase retrieval step on detection performance. Taken together, this constitutes a general methodology of practical interest for quantitative extraction of information from X-ray in-line phase tomography, and is also relevant to assessment of contrast agents with a blob-like signature in high resolution imaging.
24738976	Matched molecular pair analysis: significance and the impact of experimental uncertainty.
J. Med. Chem. 20140416 2014May8
Matched molecular pair analysis (MMPA) has become a major tool for analyzing large chemistry data sets for promising chemical transformations. However, the dependence of MMPA predictions on data constraints such as the number of pairs involved, experimental uncertainty, source of the experiments, and variability of the true physical effect has not yet been described. In this contribution the statistical basics for judging MMPA are analyzed. We illustrate the connection between overall MMPA statistics and individual pairs with a detailed comparison of average CHEMBL hERG MMPA results versus pairs with extreme transformation effects. Comparing the CHEMBL results to Novartis data, we find that significant transformation effects agree very well if the experimental uncertainty is considered. This indicates that caution must be exercised for predictions from insignificant MMPAs, yet highlights the robustness of statistically validated MMPA and shows that MMPA on public databases can yield results that are very useful for medicinal chemistry.
22692275	An overview of biomedical literature search on the World Wide Web in the third millennium.
Oral Health Dent Manag  2012Jun
Complete access to the existing pool of biomedical literature and the ability to "hit" upon the exact information of the relevant specialty are becoming essential elements of academic and clinical expertise. With the rapid expansion of the literature database, it is almost impossible to keep up to date with every innovation. Using the Internet, however, most people can freely access this literature at any time, from almost anywhere. This paper highlights the use of the Internet in obtaining valuable biomedical research information, which is mostly available from journals, databases, textbooks and e-journals in the form of web pages, text materials, images, and so on. The authors present an overview of web-based resources for biomedical researchers, providing information about Internet search engines (e.g., Google), web-based bibliographic databases (e.g., PubMed, IndMed) and how to use them, and other online biomedical resources that can assist clinicians in reaching well-informed clinical decisions.
24229773	Intelligent platforms for disease assessment: novel approaches in functional echocardiography.
JACC Cardiovasc Imaging  2013Nov
Accelerating trends in the dynamic digital era (from 2004 onward) has resulted in the emergence of novel parametric imaging tools that allow easy and accurate extraction of quantitative information from cardiac images. This review principally attempts to heighten the awareness of newer emerging paradigms that may advance acquisition, visualization and interpretation of the large functional data sets obtained during cardiac ultrasound imaging. Incorporation of innovative cognitive software that allow advanced pattern recognition and disease forecasting will likely transform the human-machine interface and interpretation process to achieve a more efficient and effective work environment. Novel technologies for automation and big data analytics that are already active in other fields need to be rapidly adapted to the health care environment with new academic-industry collaborations to enrich and accelerate the delivery of newer decision making tools for enhancing patient care.
21417260	Residue preference mapping of ligand fragments in the Protein Data Bank.
J Chem Inf Model 20110318 2011Apr25
The interaction between small molecules and proteins is one of the major concerns for structure-based drug design because the principles of protein-ligand interactions and molecular recognition are not thoroughly understood. Fortunately, the analysis of protein-ligand complexes in the Protein Data Bank (PDB) enables unprecedented possibilities for new insights. Herein, we applied molecule-fragmentation algorithms to split the ligands extracted from PDB crystal structures into small fragments. Subsequently, we have developed a ligand fragment and residue preference mapping (LigFrag-RPM) algorithm to map the profiles of the interactions between these fragments and the 20 proteinogenic amino acid residues. A total of 4032 fragments were generated from 71 798 PDB ligands by a ring cleavage (RC) algorithm. Among these ligand fragments, 315 unique fragments were characterized with the corresponding fragment-residue interaction profiles by counting residues close to these fragments. The interaction profiles revealed that these fragments have specific preferences for certain types of residues. The applications of these interaction profiles were also explored and evaluated in case studies, showing great potential for the study of protein-ligand interactions and drug design. Our studies demonstrated that the fragment-residue interaction profiles generated from the PDB ligand fragments can be used to detect whether these fragments are in their favorable or unfavorable environments. The algorithm for a ligand fragment and residue preference mapping (LigFrag-RPM) developed here also has the potential to guide lead chemistry modifications as well as binding residues predictions.
21282667	The Mouse Tumor Biology Database (MTB): a central electronic resource for locating and integrating mouse tumor pathology data.
Vet. Pathol. 20110131 2012Jan
The Mouse Tumor Biology Database (MTB) is designed to provide an electronic data storage, search, and analysis system for information on mouse models of human cancer. The MTB includes data on tumor frequency and latency, strain, germ line, and somatic genetics, pathologic notations, and photomicrographs. The MTB collects data from the primary literature, other public databases, and direct submissions from the scientific community. The MTB is a community resource that provides integrated access to mouse tumor data from different scientific research areas and facilitates integration of molecular, genetic, and pathologic data. Current status of MTB, search capabilities, data types, and future enhancements are described in this article.
23429244	Data protection among junior medical staff: a questionnaire study.
J Patient Saf  2013Jun
There have been numerous reports of loss of confidential information amongst UK public agencies. The aim of the study was to examine current standards of practice and knowledge of junior medical staff with respect to management of patient identifiable information. An anonymous multiple choice questionnaire was completed by 50 junior medical staff in each of 2 separate district general hospitals in the UK. Sixty-two percent of physicians surveyed held patient identifiable information electronically, outside of normal NHS use. Thirty percent of physicians used portable memory sticks, of which, 68% were not password protected. Ninety percent of physicians used patient ward lists in paper format with 18% frequently using a domestic waste bin for disposal. Thirty-five percent of physicians were aware of the Caldicott principles, and 58% were aware of the Data Protection Act as applied to their duties. Despite having statutory duties toward the management of patient identifiable information, many physicians are not aware of their responsibilities and obligations. This is unlikely to be an isolated local issue. More emphasis needs to be placed on data management in hospital induction procedures for new employees, and security measures, such as encryption software, should be made more widely available.
24218380	Number needed to benefit from information (NNBI): proposal from a mixed methods research study with practicing family physicians.
Ann Fam Med  2013 Nov-Dec
We wanted to describe family physicians' use of information from an electronic knowledge resource for answering clinical questions, and their perception of subsequent patient health outcomes; and to estimate the number needed to benefit from information (NNBI), defined as the number of patients for whom clinical information was retrieved for 1 to benefit. We undertook a mixed methods research study, combining quantitative longitudinal and qualitative research studies. Participants were 41 family physicians from primary care clinics across Canada. Physicians were given access to 1 electronic knowledge resource on handheld computer in 2008-2009. For the outcome assessment, participants rated their searches using a validated method. Rated searches were examined during interviews guided by log reports that included ratings. Cases were defined as clearly described searches where clinical information was used for a specific patient. For each case, interviewees described information-related patient health outcomes. For the mixed methods data analysis, quantitative and qualitative data were merged into clinical vignettes (each vignette describing a case). We then estimated the NNBI. In 715 of 1,193 searches for information conducted during an average of 86 days, the search objective was directly linked to a patient. Of those searches, 188 were considered to be cases. In 53 cases, participants associated the use of information with at least 1 patient health benefit. This finding suggested an NNBI of 14 (715/53). The NNBI may be used in further experimental research to compare electronic knowledge resources. A low NNBI can encourage clinicians to search for information more frequently. If all searches had benefits, the NNBI would be 1. In addition to patient benefits, learning and knowledge reinforcement outcomes are frequently reported.
24941546	Virtualization and cloud computing in dentistry.
J Mass Dent Soc  2014Spring
The use of virtualization and cloud computing has changed the way we use computers. Virtualization is a method of placing software called a hypervisor on the hardware of a computer or a host operating system. It allows a guest operating system to run on top of the physical computer with a virtual machine (i.e., virtual computer). Virtualization allows multiple virtual computers to run on top of one physical computer and to share its hardware resources, such as printers, scanners, and modems. This increases the efficient use of the computer by decreasing costs (e.g., hardware, electricity administration, and management) since only one physical computer is needed and running. This virtualization platform is the basis for cloud computing. It has expanded into areas of server and storage virtualization. One of the commonly used dental storage systems is cloud storage. Patient information is encrypted as required by the Health Insurance Portability and Accountability Act (HIPAA) and stored on off-site private cloud services for a monthly service fee. As computer costs continue to increase, so too will the need for more storage and processing power. Virtual and cloud computing will be a method for dentists to minimize costs and maximize computer efficiency in the near future. This article will provide some useful information on current uses of cloud computing.
22325770	Outcome of the first electron microscopy validation task force meeting.
Structure  2012Feb8
This Meeting Review describes the proceedings and conclusions from the inaugural meeting of the Electron Microscopy Validation Task Force organized by the Unified Data Resource for 3DEM (http://www.emdatabank.org) and held at Rutgers University in New Brunswick, NJ on September 28 and 29, 2010. At the workshop, a group of scientists involved in collecting electron microscopy data, using the data to determine three-dimensional electron microscopy (3DEM) density maps, and building molecular models into the maps explored how to assess maps, models, and other data that are deposited into the Electron Microscopy Data Bank and Protein Data Bank public data archives. The specific recommendations resulting from the workshop aim to increase the impact of 3DEM in biology and medicine.
23059729	SHARE: system design and case studies for statistical health information release.
J Am Med Inform Assoc 20121011 2013Jan1
We present SHARE, a new system for statistical health information release with differential privacy. We present two case studies that evaluate the software on real medical datasets and demonstrate the feasibility and utility of applying the differential privacy framework on biomedical data. SHARE releases statistical information in electronic health records with differential privacy, a strong privacy framework for statistical data release. It includes a number of state-of-the-art methods for releasing multidimensional histograms and longitudinal patterns. We performed a variety of experiments on two real datasets, the surveillance, epidemiology and end results (SEER) breast cancer dataset and the Emory electronic medical record (EeMR) dataset, to demonstrate the feasibility and utility of SHARE. Experimental results indicate that SHARE can deal with heterogeneous data present in medical data, and that the released statistics are useful. The Kullback-Leibler divergence between the released multidimensional histograms and the original data distribution is below 0.5 and 0.01 for seven-dimensional and three-dimensional data cubes generated from the SEER dataset, respectively. The relative error for longitudinal pattern queries on the EeMR dataset varies between 0 and 0.3. While the results are promising, they also suggest that challenges remain in applying statistical data release using the differential privacy framework for higher dimensional data. SHARE is one of the first systems to provide a mechanism for custodians to release differentially private aggregate statistics for a variety of use cases in the medical domain. This proof-of-concept system is intended to be applied to large-scale medical data warehouses.
23646026	Measures of health sciences journal use: a comparison of vendor, link-resolver, and local citation statistics.
J Med Libr Assoc  2013Apr
Libraries require efficient and reliable methods to assess journal use. Vendors provide complete counts of articles retrieved from their platforms. However, if a journal is available on multiple platforms, several sets of statistics must be merged. Link-resolver reports merge data from all platforms into one report but only record partial use because users can access library subscriptions from other paths. Citation data are limited to publication use. Vendor, link-resolver, and local citation data were examined to determine correlation. Because link-resolver statistics are easy to obtain, the study library especially wanted to know if they correlate highly with the other measures. Vendor, link-resolver, and local citation statistics for the study institution were gathered for health sciences journals. Spearman rank-order correlation coefficients were calculated. There was a high positive correlation between all three data sets, with vendor data commonly showing the highest use. However, a small percentage of titles showed anomalous results. Link-resolver data correlate well with vendor and citation data, but due to anomalies, low link-resolver data would best be used to suggest titles for further evaluation using vendor data. Citation data may not be needed as it correlates highly with other measures.
24200479	A retrieval-based computer-aided diagnosis system for the characterization of liver lesions in CT scans.
Acad Radiol  2013Dec
To evaluate a computer-aided diagnosis (CADx) system for the characterization of liver lesions in computed tomography (CT) scans. The stand-alone predictive performance of the CADx system was assessed and compared to that of three radiologists who were provided with the same amount of image information to which the CADx system had access. The CADx system operates as an image search engine exploiting texture analysis of liver lesion image data for the lesion in question and lesions from a database. A region of interest drawn around an indeterminate liver lesion is used as input query. The CADx system retrieves lesions of similar histology (benign/malignant), density (hypodense/hyperdense), or type (cyst/hemangioma/metastasis). The system's performance was evaluated with leave-one-patient-out receiver operating characteristic area under the curve on 685 CT scans from 372 patients that contained 2325 liver lesions (193 &lt;1 cm(3)). Sensitivity, specificity, and positive and negative predictive values were evaluated separately for subcentimeter lesions. Results were compared to those of three radiologists who rated 83 liver lesions (20 hemangiomas, 20 metastases, 20 cysts, 20 hepatocellular carcinomas, and 3 focal nodular hyperplasias) displaying only the liver. The CADx system's leave-one-patient-out receiver operating characteristic area under the curve was 97.1% for density, 91.4% for histology, and 95.5% for lesion type. For subcentimeter lesions, input of additional semantic information improved the system's performance. The CADx system has been proved to significantly outperform radiologists in discriminating lesion histology and type, provided the radiologists have no access to information other than the image. The radiologists were most reliable in diagnosing hemangioma given the limited image data. The CADx system under study discriminated reliably between various liver lesions, even outperforming radiologists when accessing the same image information and demonstrated promising performance in classifying subcentimeter lesions in particular.
24941613	Working data together: the accountability and reflexivity of digital astronomical practice.
Soc Stud Sci  2014Apr
Drawing on ethnomethodology, this article considers the sequential work of astronomers who combine observations from telescopes at two observatories in making a data set for scientific analyses. By witnessing the induction of a graduate student into this work, it aims at revealing the backgrounded assumptions that enter it. I find that these researchers achieved a consistent data set by engaging diverse evidential contexts as contexts of accountability. Employing graphs that visualize data in conventional representational formats of observational astronomy, experienced practitioners held each other accountable by using an 'implicit cosmology', a shared (but sometimes negotiable) characterization of 'what the universe looks like' through these formats. They oriented to data as malleable, that is, as containing artifacts of the observing situation which are unspecified initially but can be defined and subsequently removed. Alternating between reducing data and deducing astronomical phenomena, they ascribed artifacts to local observing conditions or computational procedures, thus maintaining previously stabilized phenomena reflexively. As researchers in data-intensive sciences are often removed from the instruments that generated the data they use, this example demonstrates how scientists can achieve agreement by engaging stable 'global' data sets and diverse contexts of accountability, allowing them to bypass troubling features and limitations of data generators.
23134701	Public health interventions in midwifery: a systematic review of systematic reviews.
BMC Public Health 20121108 2012
Maternity care providers, particularly midwives, have a window of opportunity to influence pregnant women about positive health choices. This aim of this paper is to identify evidence of effective public health interventions from good quality systematic reviews that could be conducted by midwives. Relevant databases including MEDLINE, Pubmed, EBSCO, CRD, MIDIRS, Web of Science, The Cochrane Library and Econlit were searched to identify systematic reviews in October 2010. Quality assessment of all reviews was conducted. Thirty-six good quality systematic reviews were identified which reported on effective interventions. The reviews were conducted on a diverse range of interventions across the reproductive continuum and were categorised under: screening; supplementation; support; education; mental health; birthing environment; clinical care in labour and breast feeding. The scope and strength of the review findings are discussed in relation to current practice. A logic model was developed to provide an overarching framework of midwifery public health roles to inform research policy and practice. This review provides a broad scope of high quality systematic review evidence and definitively highlights the challenge of knowledge transfer from research into practice. The review also identified gaps in knowledge around the impact of core midwifery practice on public health outcomes and the value of this contribution. This review provides evidence for researchers and funders as to the gaps in current knowledge and should be used to inform the strategic direction of the role of midwifery in public health in policy and practice.
23178860	The relationship between commercial website ratings and traditional hospital performance measures in the USA.
BMJ Qual Saf 20121123 2013Mar
Our goal was to compare hospital scores from the most widely used commercial website in the USA to hospital scores from more systematic measures of patient experience and outcomes, and to assess what drives variation in the commercial website scores. For a national sample of US hospitals, we compared scores on Yelp.com, which aggregates website visitor ratings (1-5 stars), with traditional measures of hospital quality. We calculated correlations between hospital Yelp scores and the following: hospital percent high ratings (9 or 10, scale 0-10) on the 'Overall' item on the Hospital Consumer Assessment of Healthcare Providers and Systems (HCAHPS) survey; hospital individual HCAHPS domain scores (eg, nurse communication, pain control); hospital 30-day mortality; and hospital 30-day readmission rates. Of hospitals reporting HCAHPS (n=3796), 962 (25%) had scores on Yelp. Among hospitals with &gt;5 Yelp ratings, the correlation of percent high ratings between Yelp and HCAHPS was 0.49 (p&lt;0.001). The percent high ratings within each HCAHPS domain increased monotonically with increasing Yelp scores (p≤0.001 for all domains). Percent high ratings in Yelp and HCAHPS were statistically significantly correlated with lower mortality for myocardial infarction (MI; -0.19 for Yelp and -0.13 for HCAHPS) and pneumonia (-0.14 and -0.18), and fewer readmissions for MI (-0.17 and -0.39), heart failure (-0.31 and -0.39), and pneumonia (-0.18 and -0.27). These data suggest that rater experiences for Yelp and HCAHPS may be similar, and that consumers posting ratings on Yelp may observe aspects of care related to important patient outcomes.
23515788	External phenome analysis enables a rational federated query strategy to detect changing rates of treatment-related complications associated with multiple myeloma.
J Am Med Inform Assoc 20130320 2013 Jul-Aug
Electronic health records (EHRs) are increasingly useful for health services research. For relatively uncommon conditions, such as multiple myeloma (MM) and its treatment-related complications, a combination of multiple EHR sources is essential for such research. The Shared Health Research Information Network (SHRINE) enables queries for aggregate results across participating institutions. Development of a rational search strategy in SHRINE may be augmented through analysis of pre-existing databases. We developed a SHRINE query for likely non-infectious treatment-related complications of MM, based upon an analysis of the Multiparameter Intelligent Monitoring in Intensive Care (MIMIC II) database. Using this query strategy, we found that the rate of likely treatment-related complications significantly increased from 2001 to 2007, by an average of 6% a year (p=0.01), across the participating SHRINE institutions. This finding is in keeping with increasingly aggressive strategies in the treatment of MM. This proof of concept demonstrates that a staged approach to federated queries, using external EHR data, can yield potentially clinically meaningful results.
24273241	Pathway Commons at virtual cell: use of pathway data for mathematical modeling.
Bioinformatics 20131122 2014Jan15
Pathway Commons is a resource permitting simultaneous queries of multiple pathway databases. However, there is no standard mechanism for using these data (stored in BioPAX format) to annotate and build quantitative mathematical models. Therefore, we developed a new module within the virtual cell modeling and simulation software. It provides pathway data retrieval and visualization and enables automatic creation of executable network models directly from qualitative connections between pathway nodes. Available at Virtual Cell (http://vcell.org/). Application runs on all major platforms and does not require registration for use on the user’s computer. Tutorials and video are available at user guide page.
23183117	Validation and repeatability of a shoulder biomechanics data collection methodology and instrumentation.
J Appl Biomech 20121121 2013Oct
The purpose of our study was to establish criterion-related validity and repeatability of a shoulder biomechanics testing protocol involving an electromagnetic tracking system (Flock of Birds [FoB]). Eleven subjects completed humeral elevation tasks in the sagittal, scapular, and frontal planes on two occasions. Shoulder kinematics were assessed with a digital inclinometer and the FoB. Intrasession and intersession repeatability for orthopedic angles, and humeral and scapular kinematics ranged from moderate to excellent. Correlation analyses revealed strong relationships between inclinometer and FoB measures of humeral motion, yet considerable mean differences were noted between the measurement devices. Our results validate use of the FoB for measuring humeral kinematics and establish our testing protocol as reliable. We must continue to consider factors that can impact system accuracy and the effects they may have on kinematic descriptions and how data are reported.
23337870	Trendspotting in the Protein Data Bank.
FEBS Lett. 20130118 2013Apr17
The Protein Data Bank (PDB) was established in 1971 as a repository for the three dimensional structures of biological macromolecules. Since then, more than 85000 biological macromolecule structures have been determined and made available in the PDB archive. Through analysis of the corpus of data, it is possible to identify trends that can be used to inform us abou the future of structural biology and to plan the best ways to improve the management of the ever-growing amount of PDB data.
23819887	Compensating for literature annotation bias when predicting novel drug-disease relationships through Medical Subject Heading Over-representation Profile (MeSHOP) similarity.
BMC Med Genomics 20130507 2013
Using annotations to the articles in MEDLINE®/PubMed®, over six thousand chemical compounds with pharmacological actions have been tracked since 1996. Medical Subject Heading Over-representation Profiles (MeSHOPs) quantitatively leverage the literature associated with biological entities such as diseases or drugs, providing the opportunity to reposition known compounds towards novel disease applications. A MeSHOP is constructed by counting the number of times each medical subject term is assigned to an entity-related research publication in the MEDLINE database and calculating the significance of the count by comparing against the count of the term in a background set of publications. Based on the expectation that drugs suitable for treatment of a disease (or disease symptom) will have similar annotation properties to the disease, we successfully predict drug-disease associations by comparing MeSHOPs of diseases and drugs. The MeSHOP comparison approach delivers an 11% improvement over bibliometric baselines. However, novel drug-disease associations are observed to be biased towards drugs and diseases with more publications. To account for the annotation biases, a correction procedure is introduced and evaluated. By explicitly accounting for the annotation bias, unexpectedly similar drug-disease pairs are highlighted as candidates for drug repositioning research. MeSHOPs are shown to provide a literature-supported perspective for discovery of new links between drugs and diseases based on pre-existing knowledge.
24551389	Automatically extracting clinically useful sentences from UpToDate to support clinicians' information needs.
AMIA Annu Symp Proc 20131116 2013
Clinicians raise several information needs in the course of care. Most of these needs can be met by online health knowledge resources such as UpToDate. However, finding relevant information in these resources often requires significant time and cognitive effort. To design and assess algorithms for extracting from UpToDate the sentences that represent the most clinically useful information for patient care decision making. We developed algorithms based on semantic predications extracted with SemRep, a semantic natural language processing parser. Two algorithms were compared against a gold standard composed of UpToDate sentences rated in terms of clinical usefulness. Clinically useful sentences were strongly correlated with predication frequency (correlation= 0.95). The two algorithms did not differ in terms of top ten precision (53% vs. 49%; p=0.06). Semantic predications may serve as the basis for extracting clinically useful sentences. Future research is needed to improve the algorithms.
23838899	Survey of muscle relaxant effects management with a kinemyographic-based data archiving system: a retrospective quantitative and contextual quality control approach.
J Clin Monit Comput 20130710 2013Dec
In a retrospective quality control study of muscle relaxant management, we assessed unbiased files provided by an automatic archiving system using quantitative monitoring generated by a kinemyographic transducer and suggest improvements for a possible future design. 200 randomly selected files were double checked to collect the values of twitch height ratio (THr), train of four ratio (TOFr) and TOF count in four periods: references values acquisition (REF), maximal level of paralysis, paralysis maintenance, pre-tracheal extubation residual paralysis assessment (RPA). The parameter values were selected according to period-specific predefined rules. A quantitative quality control was based upon standardized cut-offs values. A contextual quality control was based upon the detection of "difficult-to-interpret" episodes. Results were expressed on a descriptive basis only. For the REF period, THrs and TOFrs were lacking in, respectively, 47 and 18 of the 200 recordings analysed. A starting TOFr above 0.90 existed in 119 files. Concomitant THrs and TOFrs &gt;0.90 were evidenced 93 times. During RPA period, TOFr &gt;0.90 was recorded on 82 occasions. The optimal combination of THr &gt;0.80 and TOFr &gt;0.90 was detected in 30 files only. Presence of "difficult to interpret" episodes started with 18 files for the REF period and increased to 42, 86 and 52 in the subsequent ones most of them probably related to the absence of initial calibration procedure. In the real life conditions, a near to optimal quality control is not always observable with the quantitative neuromuscular monitoring studied. To improve the NMT monitoring, the calibration of the sensor should be performed vigorously by the anaesthesia provider and the quality of this calibration must be displayed on the screen of the monitor.
24363621	A two-level cache for distributed information retrieval in search engines.
ScientificWorldJournal 20131128 2013
To improve the performance of distributed information retrieval in search engines, we propose a two-level cache structure based on the queries of the users' logs. We extract the highest rank queries of users from the static cache, in which the queries are the most popular. We adopt the dynamic cache as an auxiliary to optimize the distribution of the cache data. We propose a distribution strategy of the cache data. The experiments prove that the hit rate, the efficiency, and the time consumption of the two-level cache have advantages compared with other structures of cache.
23038162	Snomed CT implementation. Mapping guidelines facilitating reuse of data.
Methods Inf Med 20121001 2012
Clinical practice as well as research and quality-assurance benefit from unambiguous clinical information resulting from the use of a common terminology like the Systematized Nomenclature of Medicine - Clinical Terms (SNOMED CT). A common terminology is a necessity to enable consistent reuse of data, and supporting semantic interoperability. Managing use of terminology for large cross specialty Electronic Health Record systems (EHR systems) or just beyond the level of single EHR systems requires that mappings are kept consistent. The objective of this study is to provide a clear methodology for SNOMED CT mapping to enhance applicability of SNOMED CT despite incompleteness and redundancy. Such mapping guidelines are presented based on an in depth analysis of 14 different EHR templates retrieved from five Danish and Swedish EHR systems. Each mapping is assessed against defined quality criteria and mapping guidelines are specified. Future work will include guideline validation.
23154618	Semantic localization-driven partial image retrieval in CT series.
Methods Inf Med 20121116 2012
Picture archiving and communication systems (PACS) contain very large amounts of computed tomography (CT) data. When querying a PACS for a particular series, the user is often not interested in the complete series but in a certain region of interest (ROI), described e.g. by an example view in another series or an anatomical concept. Restricting a retrieval query to such an ROI saves both loading time and navigational effort. In this paper, we propose an efficient method for defining and retrieving ROIs. We employ interpolation and regression techniques for mapping the slices of a series to a newly generated standardized height atlas of the human body. Examinations of the accuracy and the saved input/output (I/O) costs of our new method on a repository of 1,360 CT series demonstrate the advantages of our system. Depending on the scope of the retrieval query, we can economize up to 99% of the total loading time. Our proposed method for flexible, context-based, partial image retrieval enables the user to directly focus on the relevant portion of the image material and it targets the high potential of I/O cost reduction of a common PACS.
23212781	Health information search to deal with the exploding amount of health information produced.
Methods Inf Med  2012
This focus theme deals with the various aspects of health information search that are necessary to cope with the challenges of an increasing amount and complexity of medical information currently produced. This editorial reviews the main challenges of health information search and summarizes the five papers of this focus theme. The five papers of the focus theme cover a large part of the current challenges in health information search such as coding standards, information extraction from complex data, user requirements analysis, multimedia data analysis and the access to big data. Several future challenges are identified such as the combination of visual and textual data for information search and the difficulty to scale when analyzing big data.
24237169	A new patent-based approach for technology mapping in the pharmaceutical domain.
Pharm Pat Anal  2013Sep
The key factor in decision-making is the quality of information collected and processed in the problem analysis. In most cases, patents represent a very important source of information. The main problem is how to extract such information from the huge corpus of documents with a high recall and precision, and in a short time. This article demonstrates a patent search and classification method, called Knowledge Organizing Module, which consists of creating, almost automatically, a pool of patents based on polysemy expansion and homonymy disambiguation. Since the pool is done, an automatic patent technology landscaping is provided for fixing the state of the art of our product, and exploring competing alternative treatments and/or possible technological opportunities. An exemplary case study is provided, it deals with a patent analysis in the field of verruca treatments.
24800426	HIPAA, dermatology images, and the law.
Semin Cutan Med Surg  2013Dec
From smart phones to iPads, the world has grown increasingly reliant on new technology. In this ever-expanding digital age, medicine is at the forefront of these new technologies. In the field of dermatology and general medicine, digital images have become an important tool used in patient management. Today, one can even find physicians who use their cellular phone cameras to take patient images and transmit them to other physicians. However, as digital imaging technology has become more prevalent so too have concerns about the impact of this technology on the electronic medical record, quality of patient care, and medicolegal issues. This article will discuss the advent of digital imaging technology in dermatology and the legal ramifications digital images have on medical care, abiding by HIPAA, the use of digital images as evidence, and the possible abuses digital images can pose in a health care setting.
24457906	On the syntactic monoids associated with a class of synchronized codes.
ScientificWorldJournal 20131224 2013
A complete code C over an alphabet A is called synchronized if there exist x, y ∈ C* such that xA*∩A*y⊆C*. In this paper we describe the syntactic monoid Syn(C (+)) of C (+) for a complete synchronized code C over A such that C (+), the semigroup generated by C, is a single class of its syntactic congruence P C (+) . In particular, we prove that, for such a code C, either C = A or Syn(C (+)) is isomorphic to a special submonoid of &#x1d4af; (l) (I) × &#x1d4af; (r) (Λ), where &#x1d4af; (l) (I) and &#x1d4af; (r) (Λ) are the full transformation semigroups on the nonempty sets I and Λ, respectively.
24459444	CMOS: efficient clustered data monitoring in sensor networks.
ScientificWorldJournal 20131225 2013
Tiny and smart sensors enable applications that access a network of hundreds or thousands of sensors. Thus, recently, many researchers have paid attention to wireless sensor networks (WSNs). The limitation of energy is critical since most sensors are battery-powered and it is very difficult to replace batteries in cases that sensor networks are utilized outdoors. Data transmission between sensor nodes needs more energy than computation in a sensor node. In order to reduce the energy consumption of sensors, we present an approximate data gathering technique, called CMOS, based on the Kalman filter. The goal of CMOS is to efficiently obtain the sensor readings within a certain error bound. In our approach, spatially close sensors are grouped as a cluster. Since a cluster header generates approximate readings of member nodes, a user query can be answered efficiently using the cluster headers. In addition, we suggest an energy efficient clustering method to distribute the energy consumption of cluster headers. Our simulation results with synthetic data demonstrate the efficiency and accuracy of our proposed technique.
22314879	Function biomedical informatics research network recommendations for prospective multicenter functional MRI studies.
J Magn Reson Imaging 20120207 2012Jul
This report provides practical recommendations for the design and execution of multicenter functional MRI (MC-fMRI) studies based on the collective experience of the Function Biomedical Informatics Research Network (FBIRN). The study was inspired by many requests from the fMRI community to FBIRN group members for advice on how to conduct MC-fMRI studies. The introduction briefly discusses the advantages and complexities of MC-fMRI studies. Prerequisites for MC-fMRI studies are addressed before delving into the practical aspects of carefully and efficiently setting up a MC-fMRI study. Practical multisite aspects include: (i) establishing and verifying scan parameters including scanner types and magnetic fields, (ii) establishing and monitoring of a scanner quality program, (iii) developing task paradigms and scan session documentation, (iv) establishing clinical and scanner training to ensure consistency over time, (v) developing means for uploading, storing, and monitoring of imaging and other data, (vi) the use of a traveling fMRI expert, and (vii) collectively analyzing imaging data and disseminating results. We conclude that when MC-fMRI studies are organized well with careful attention to unification of hardware, software and procedural aspects, the process can be a highly effective means for accessing a desired participant demographics while accelerating scientific discovery.
22822042	Challenges to nurses' efforts of retrieving, documenting, and communicating patient care information.
J Am Med Inform Assoc 20120721 2013 Mar-Apr
To examine information flow, a vital component of a patient's care and outcomes, in a sample of multiple hospital nursing units to uncover potential sources of error and opportunities for systematic improvement. This was a qualitative study of a sample of eight medical-surgical nursing units from four diverse hospitals in one US state. We conducted direct work observations of nursing staff's communication patterns for entire shifts (8 or 12 h) for a total of 200 h and gathered related documentation artifacts for analyses. Data were coded using qualitative content analysis procedures and then synthesized and organized thematically to characterize current practices. Three major themes emerged from the analyses, which represent serious vulnerabilities in the flow of patient care information during nurse hand-offs and to the entire interdisciplinary team across time and settings. The three themes are: (1) variation in nurse documentation and communication; (2) the absence of a centralized care overview in the patient's electronic health record, ie, easily accessible by the entire care team; and (3) rarity of interdisciplinary communication. The care information flow vulnerabilities are a catalyst for multiple types of serious and undetectable clinical errors. We have two major recommendations to address the gaps: (1) to standardize the format, content, and words used to document core information, such as the plan of care, and make this easily accessible to all team members; (2) to conduct extensive usability testing to ensure that tools in the electronic health record help the disconnected interdisciplinary team members to maintain a shared understanding of the patient's plan.
24143170	Can inferred provenance and its visualisation be used to detect erroneous annotation? A case study using UniProtKB.
PLoS ONE 20131015 2013
A constant influx of new data poses a challenge in keeping the annotation in biological databases current. Most biological databases contain significant quantities of textual annotation, which often contains the richest source of knowledge. Many databases reuse existing knowledge; during the curation process annotations are often propagated between entries. However, this is often not made explicit. Therefore, it can be hard, potentially impossible, for a reader to identify where an annotation originated from. Within this work we attempt to identify annotation provenance and track its subsequent propagation. Specifically, we exploit annotation reuse within the UniProt Knowledgebase (UniProtKB), at the level of individual sentences. We describe a visualisation approach for the provenance and propagation of sentences in UniProtKB which enables a large-scale statistical analysis. Initially levels of sentence reuse within UniProtKB were analysed, showing that reuse is heavily prevalent, which enables the tracking of provenance and propagation. By analysing sentences throughout UniProtKB, a number of interesting propagation patterns were identified, covering over [Formula: see text] sentences. Over [Formula: see text] sentences remain in the database after they have been removed from the entries where they originally occurred. Analysing a subset of these sentences suggest that approximately [Formula: see text] are erroneous, whilst [Formula: see text] appear to be inconsistent. These results suggest that being able to visualise sentence propagation and provenance can aid in the determination of the accuracy and quality of textual annotation. Source code and supplementary data are available from the authors website at http://homepages.cs.ncl.ac.uk/m.j.bell1/sentence_analysis/.
24028824	Phased searching with NEAT in a time-scaled framework: experiments on a computer-aided detection system for lung nodules.
Artif Intell Med 20130812 2013Nov
In the field of computer-aided detection (CAD) systems for lung nodules in computed tomography (CT) scans, many image features are presented and many artificial neural network (ANN) classifiers with various structural topologies are analyzed; frequently, the classifier topologies are selected by trial-and-error experiments. To avoid these trial and error approaches, we present a novel classifier that evolves ANNs using genetic algorithms, called "Phased Searching with NEAT in a Time or Generation-Scaled Framework", integrating feature selection with the classification task. We analyzed our method's performance on 360 CT scans from the public Lung Image Database Consortium database. We compare our method's performance with other more-established classifiers, namely regular NEAT, Feature-Deselective NEAT (FD-NEAT), fixed-topology ANNs, and support vector machines (SVMs) using ten-fold cross-validation experiments of all 360 scans. The results show that the proposed "Phased Searching" method performs better and faster than regular NEAT, better than FD-NEAT, and achieves sensitivities at 3 and 4 false positives (FP) per scan that are comparable with the fixed-topology ANN and SVM classifiers, but with fewer input features. It achieves a detection sensitivity of 83.0±9.7% with an average of 4FP/scan, for nodules with a diameter greater than or equal to 3mm. It also evolves networks with shorter evolution times and with lower complexities than regular NEAT (p=0.026 and p&lt;0.001, respectively). Analysis on the average and best network complexities evolved by regular NEAT and by our approach shows that our approach searches for good solutions in lower dimensional search spaces, and evolves networks without superfluous structure. We have presented a novel approach that combines feature selection with the evolution of ANN topology and weights. Compared with the original threshold-based Phased Searching method of Green, our method requires fewer parameters and converges to the optimal network complexity required for the classification task at hand. The results of the ten-fold cross-validation experiments also show that our proposed CAD system for lung nodule detection performs well with respect to other methods in the literature.
24088321	Online drug user-led harm reduction in Hungary: a review of "Daath".
Harm Reduct J 20131002 2013
Harm reduction has been increasingly finding its way into public drug policies and healthcare practices worldwide, with successful intervention measures justifiably focussing on the highest-risk groups, such as injecting drug users. However, there are also other types of drug users in need for harm reduction, even though they pose less, low, or no public health risk. Occasionally, drug users may autonomously organise themselves into groups to provide advocacy, harm reduction, and peer-help services, sometimes online. The http://www.daath.hu website has been operated since 2001 by the "Hungarian Psychedelic Community", an unorganised drug user group with a special interest in hallucinogenic and related substances. As of today, the website serves about 1200 visitors daily, and the online community comprises of more than 8000 registered members. The Daath community is driven by a strong commitment to the policy of harm reduction in the form of various peer-help activities that aim to expand harm reduction without promoting drug use. Our review comprehensively summarises Daath's user-led harm reduction services and activities from the last ten years, firstly outlining the history and growth phases of Daath, along with its self-set guidelines and policies. Online services (such as a discussion board, and an Ecstasy pill database) and offline activities (such as Ecstasy pill field testing, and a documentary film about psychedelics) are described. In order to extend its harm reduction services and activities in the future, Daath has several social, commercial, and legislative challenges to face. Starting with a need to realign its focus, outlooks for the upcoming operation of Daath are pondered. Future trends in harm reduction, such as separating harm-decreasing from benefit-increasing, are also discussed. We aim to share these innovative harm reduction measures and good practices in order to be critically assessed, and--if found useful--adapted and applied elsewhere.
24311982	Design and analysis of a dynamic mobility management scheme for wireless mesh network.
ScientificWorldJournal 20131107 2013
Seamless mobility management of the mesh clients (MCs) in wireless mesh network (WMN) has drawn a lot of attention from the research community. A number of mobility management schemes such as mesh network with mobility management (MEMO), mesh mobility management (M(3)), and wireless mesh mobility management (WMM) have been proposed. The common problem with these schemes is that they impose uniform criteria on all the MCs for sending route update message irrespective of their distinct characteristics. This paper proposes a session-to-mobility ratio (SMR) based dynamic mobility management scheme for handling both internet and intranet traffic. To reduce the total communication cost, this scheme considers each MC's session and mobility characteristics by dynamically determining optimal threshold SMR value for each MC. A numerical analysis of the proposed scheme has been carried out. Comparison with other schemes shows that the proposed scheme outperforms MEMO, M(3), and WMM with respect to total cost.
24279829	Weeding out the information: an ethnographic approach to exploring how young people make sense of the evidence on cannabis.
Harm Reduct J 20131127 2013
Contradictory evidence on cannabis adds to the climate of confusion regarding the health harms related to use. This is particularly true for young people as they encounter and make sense of opposing information on cannabis. Knowledge translation (KT) is in part focused on ensuring that knowledge users have access to and understand best evidence; yet, little attention has focused on the processes youth use to weigh scientific evidence. There is growing interest in how KT efforts can involve knowledge users in shaping the delivery of youth-focused public health messages. To date, the youth voice has been largely absent from the creation of public health messages on cannabis. This ethnographic study describes a knowledge translation project that focused on engaging young people in a review of evidence on cannabis that concluded with the creation of public health messages generated by youth participants. We facilitated two groups with a total of 18 youth participants. Data included transcribed segments of weekly sessions, researcher field notes, participant research logs, and transcribed follow-up interviews. Qualitative, thematic analysis was conducted. Group dynamics were influential in terms of how participants made sense of the evidence. The processes by which participants came to understand the current evidence on cannabis are described, followed by the manner in which they engaged with the literature for the purpose of creating an individual public health message to share with the group. At project end, youth created collaborative public health messages based on their understanding of the evidence illustrating their capacity to "weed out" the information. The content of these messages reflect a youth-informed harm reduction approach to cannabis use. This study demonstrates the feasibility of involving young people in knowledge translation initiatives that target peers. Youth participants demonstrated that they were capable of reading scientific literature and had the capacity to engage in the creation of evidence-informed public health messages on cannabis that resonate with young people. Rather than simply being the target of KT messages, they embraced the opportunity to engage in dialogue focused on cannabis.
24459440	A novel coordinated edge caching with request filtration in radio access network.
ScientificWorldJournal 20131229 2013
Content caching at the base station of the Radio Access Network (RAN) is a way to reduce backhaul transmission and improve the quality of experience. So it is crucial to manage such massive microcaches to store the contents in a coordinated manner, in order to increase the overall mobile network capacity to support more number of requests. We achieve this goal in this paper with a novel caching scheme, which reduces the repeating traffic by request filtration and asynchronous multicast in a RAN. Request filtration can make the best use of the limited bandwidth and in turn ensure the good performance of the coordinated caching. Moreover, the storage at the mobile devices is also considered to be used to further reduce the backhaul traffic and improve the users' experience. In addition, we drive the optimal cache division in this paper with the aim of reducing the average latency user perceived. The simulation results show that the proposed scheme outperforms existing algorithms.
24034775	Assessing the feasibility of extracting clinical information to create quality indicators from primary healthcare practice EMRs.
Healthc Q  2013
In 2011, a panel of primary healthcare (PHC) providers in Nova Scotia rated 19 of 35 selected Canadian Institute for Health Information (2006) clinical quality indicators (QIs) as "acceptable." In this study, the authors explored the feasibility of extracting electronic medical record (EMR) data required to create these PHC QI measures.
24217329	Net improvement of correct answers to therapy questions after pubmed searches: pre/post comparison.
J. Med. Internet Res. 20131108 2013
Clinicians search PubMed for answers to clinical questions although it is time consuming and not always successful. To determine if PubMed used with its Clinical Queries feature to filter results based on study quality would improve search success (more correct answers to clinical questions related to therapy). We invited 528 primary care physicians to participate, 143 (27.1%) consented, and 111 (21.0% of the total and 77.6% of those who consented) completed the study. Participants answered 14 yes/no therapy questions and were given 4 of these (2 originally answered correctly and 2 originally answered incorrectly) to search using either the PubMed main screen or PubMed Clinical Queries narrow therapy filter via a purpose-built system with identical search screens. Participants also picked 3 of the first 20 retrieved citations that best addressed each question. They were then asked to re-answer the original 14 questions. We found no statistically significant differences in the rates of correct or incorrect answers using the PubMed main screen or PubMed Clinical Queries. The rate of correct answers increased from 50.0% to 61.4% (95% CI 55.0%-67.8%) for the PubMed main screen searches and from 50.0% to 59.1% (95% CI 52.6%-65.6%) for Clinical Queries searches. These net absolute increases of 11.4% and 9.1%, respectively, included previously correct answers changing to incorrect at a rate of 9.5% (95% CI 5.6%-13.4%) for PubMed main screen searches and 9.1% (95% CI 5.3%-12.9%) for Clinical Queries searches, combined with increases in the rate of being correct of 20.5% (95% CI 15.2%-25.8%) for PubMed main screen searches and 17.7% (95% CI 12.7%-22.7%) for Clinical Queries searches. PubMed can assist clinicians answering clinical questions with an approximately 10% absolute rate of improvement in correct answers. This small increase includes more correct answers partially offset by a decrease in previously correct answers.
24284061	The effects of preference for information on consumers' online health information search behavior.
J. Med. Internet Res. 20131126 2013
Preference for information is a personality trait that affects people's tendency to seek information in health-related situations. Prior studies have focused primarily on investigating its impact on patient-provider communication and on the implications for designing information interventions that prepare patients for medical procedures. Few studies have examined its impact on general consumers' interactions with Web-based search engines for health information or the implications for designing more effective health information search systems. This study intends to fill this gap by investigating the impact of preference for information on the search behavior of general consumers seeking health information, their perceptions of search tasks (representing information needs), and user experience with search systems. Forty general consumers who had previously searched for health information online participated in the study in our usability lab. Preference for information was measured using Miller's Monitor-Blunter Style Scale (MBSS) and the Krantz Health Opinion Survey-Information Scale (KHOS-I). Each participant completed four simulated health information search tasks: two look-up (fact-finding) and two exploratory. Their behaviors while interacting with the search systems were automatically logged and ratings of their perceptions of tasks and user experience with the systems were collected using Likert-scale questionnaires. The MBSS showed low reliability with the participants (Monitoring subscale: Cronbach alpha=.53; Blunting subscale: Cronbach alpha=.35). Thus, no further analyses were performed based on the scale. KHOS-I had sufficient reliability (Cronbach alpha=.77). Participants were classified into low- and high-preference groups based on their KHOS-I scores. The high-preference group submitted significantly shorter queries when completing the look-up tasks (P=.02). The high-preference group made a significantly higher percentage of parallel movements in query reformulation than did the low-preference group (P=.04), whereas the low-preference group made a significantly higher percentage of new concept movements than the high-preference group when completing the exploratory tasks (P=.01). The high-preference group found the exploratory tasks to be significantly more difficult (P=.05) and the systems to be less useful (P=.04) than did the low-preference group. Preference for information has an impact on the search behavior of general consumers seeking health information. Those with a high preference were more likely to use more general queries when searching for specific factual information and to develop more complex mental representations of health concerns of an exploratory nature and try different combinations of concepts to explore these concerns. High-preference users were also more demanding on the system. Health information search systems should be tailored to fit individuals' information preferences.
24319361	Multi-objective approach for energy-aware workflow scheduling in cloud computing environments.
ScientificWorldJournal 20131104 2013
We address the problem of scheduling workflow applications on heterogeneous computing systems like cloud computing infrastructures. In general, the cloud workflow scheduling is a complex optimization problem which requires considering different criteria so as to meet a large number of QoS (Quality of Service) requirements. Traditional research in workflow scheduling mainly focuses on the optimization constrained by time or cost without paying attention to energy consumption. The main contribution of this study is to propose a new approach for multi-objective workflow scheduling in clouds, and present the hybrid PSO algorithm to optimize the scheduling performance. Our method is based on the Dynamic Voltage and Frequency Scaling (DVFS) technique to minimize energy consumption. This technique allows processors to operate in different voltage supply levels by sacrificing clock frequencies. This multiple voltage involves a compromise between the quality of schedules and energy. Simulation results on synthetic and real-world scientific applications highlight the robust performance of the proposed approach.
24319383	A relation routing scheme for distributed semantic media query.
ScientificWorldJournal 20131111 2013
Performing complex semantic queries over large-scale distributed media contents is a challenging task for rich media applications. The dynamics and openness of data sources make it uneasy to realize a query scheme that simultaneously achieves precision, scalability, and reliability. In this paper, a novel relation routing scheme (RRS) is proposed by renovating the routing model of Content Centric Network (CCN) for directly querying large-scale semantic media content. By using proper query model and routing mechanism, semantic queries with complex relation constrains from users can be guided towards potential media sources through semantic guider nodes. The scattered and fragmented query results can be integrated on their way back for semantic needs or to avoid duplication. Several new techniques, such as semantic-based naming, incomplete response avoidance, timeout checking, and semantic integration, are developed in this paper to improve the accuracy, efficiency, and practicality of the proposed approach. Both analytical and experimental results show that the proposed scheme is a promising and effective solution for complex semantic queries and integration over large-scale networks.
24385877	A novel artificial bee colony approach of live virtual machine migration policy using Bayes theorem.
ScientificWorldJournal 20131209 2013
Green cloud data center has become a research hotspot of virtualized cloud computing architecture. Since live virtual machine (VM) migration technology is widely used and studied in cloud computing, we have focused on the VM placement selection of live migration for power saving. We present a novel heuristic approach which is called PS-ABC. Its algorithm includes two parts. One is that it combines the artificial bee colony (ABC) idea with the uniform random initialization idea, the binary search idea, and Boltzmann selection policy to achieve an improved ABC-based approach with better global exploration's ability and local exploitation's ability. The other one is that it uses the Bayes theorem to further optimize the improved ABC-based process to faster get the final optimal solution. As a result, the whole approach achieves a longer-term efficient optimization for power saving. The experimental results demonstrate that PS-ABC evidently reduces the total incremental power consumption and better protects the performance of VM running and migrating compared with the existing research. It makes the result of live VM migration more high-effective and meaningful.
24495746	DRUMS: Disk Repository with Update Management and Select option for high throughput sequencing data.
BMC Bioinformatics 20140204 2014
New technologies for analyzing biological samples, like next generation sequencing, are producing a growing amount of data together with quality scores. Moreover, software tools (e.g., for mapping sequence reads), calculating transcription factor binding probabilities, estimating epigenetic modification enriched regions or determining single nucleotide polymorphism increase this amount of position-specific DNA-related data even further. Hence, requesting data becomes challenging and expensive and is often implemented using specialised hardware. In addition, picking specific data as fast as possible becomes increasingly important in many fields of science. The general problem of handling big data sets was addressed by developing specialized databases like HBase, HyperTable or Cassandra. However, these database solutions require also specialized or distributed hardware leading to expensive investments. To the best of our knowledge, there is no database capable of (i) storing billions of position-specific DNA-related records, (ii) performing fast and resource saving requests, and (iii) running on a single standard computer hardware. Here, we present DRUMS (Disk Repository with Update Management and Select option), satisfying demands (i)-(iii). It tackles the weaknesses of traditional databases while handling position-specific DNA-related data in an efficient manner. DRUMS is capable of storing up to billions of records. Moreover, it focuses on optimizing relating single lookups as range request, which are needed permanently for computations in bioinformatics. To validate the power of DRUMS, we compare it to the widely used MySQL database. The test setting considers two biological data sets. We use standard desktop hardware as test environment. DRUMS outperforms MySQL in writing and reading records by a factor of two up to a factor of 10000. Furthermore, it can work with significantly larger data sets. Our work focuses on mid-sized data sets up to several billion records without requiring cluster technology. Storing position-specific data is a general problem and the concept we present here is a generalized approach. Hence, it can be easily applied to other fields of bioinformatics.
22366294	Evaluating the state of the art in coreference resolution for electronic medical records.
J Am Med Inform Assoc 20120224 2012 Sep-Oct
The fifth i2b2/VA Workshop on Natural Language Processing Challenges for Clinical Records conducted a systematic review on resolution of noun phrase coreference in medical records. Informatics for Integrating Biology and the Bedside (i2b2) and the Veterans Affair (VA) Consortium for Healthcare Informatics Research (CHIR) partnered to organize the coreference challenge. They provided the research community with two corpora of medical records for the development and evaluation of the coreference resolution systems. These corpora contained various record types (ie, discharge summaries, pathology reports) from multiple institutions. The coreference challenge provided the community with two annotated ground truth corpora and evaluated systems on coreference resolution in two ways: first, it evaluated systems for their ability to identify mentions of concepts and to link together those mentions. Second, it evaluated the ability of the systems to link together ground truth mentions that refer to the same entity. Twenty teams representing 29 organizations and nine countries participated in the coreference challenge. The teams' system submissions showed that machine-learning and rule-based approaches worked best when augmented with external knowledge sources and coreference clues extracted from document structure. The systems performed better in coreference resolution when provided with ground truth mentions. Overall, the systems struggled in solving coreference resolution for cases that required domain knowledge.
23467472	An end-to-end system to identify temporal relation in discharge summaries: 2012 i2b2 challenge.
J Am Med Inform Assoc 20130306 2013 Sep-Oct
To create an end-to-end system to identify temporal relation in discharge summaries for the 2012 i2b2 challenge. The challenge includes event extraction, timex extraction, and temporal relation identification. An end-to-end temporal relation system was developed. It includes three subsystems: an event extraction system (conditional random fields (CRF) name entity extraction and their corresponding attribute classifiers), a temporal extraction system (CRF name entity extraction, their corresponding attribute classifiers, and context-free grammar based normalization system), and a temporal relation system (10 multi-support vector machine (SVM) classifiers and a Markov logic networks inference system) using labeled sequential pattern mining, syntactic structures based on parse trees, and results from a coordination classifier. Micro-averaged precision (P), recall (R), averaged P&amp;R (P&amp;R), and F measure (F) were used to evaluate results. For event extraction, the system achieved 0.9415 (P), 0.8930 (R), 0.9166 (P&amp;R), and 0.9166 (F). The accuracies of their type, polarity, and modality were 0.8574, 0.8585, and 0.8560, respectively. For timex extraction, the system achieved 0.8818, 0.9489, 0.9141, and 0.9141, respectively. The accuracies of their type, value, and modifier were 0.8929, 0.7170, and 0.8907, respectively. For temporal relation, the system achieved 0.6589, 0.7129, 0.6767, and 0.6849, respectively. For end-to-end temporal relation, it achieved 0.5904, 0.5944, 0.5921, and 0.5924, respectively. With the F measure used for evaluation, we were ranked first out of 14 competing teams (event extraction), first out of 14 teams (timex extraction), third out of 12 teams (temporal relation), and second out of seven teams (end-to-end temporal relation). The system achieved encouraging results, demonstrating the feasibility of the tasks defined by the i2b2 organizers. The experiment result demonstrates that both global and local information is useful in the 2012 challenge.
23523875	A la Recherche du Temps Perdu: extracting temporal relations from medical text in the 2012 i2b2 NLP challenge.
J Am Med Inform Assoc 20130323 2013 Sep-Oct
An analysis of the timing of events is critical for a deeper understanding of the course of events within a patient record. The 2012 i2b2 NLP challenge focused on the extraction of temporal relationships between concepts within textual hospital discharge summaries. The team from the National Research Council Canada (NRC) submitted three system runs to the second track of the challenge: typifying the time-relationship between pre-annotated entities. The NRC system was designed around four specialist modules containing statistical machine learning classifiers. Each specialist targeted distinct sets of relationships: local relationships, 'sectime'-type relationships, non-local overlap-type relationships, and non-local causal relationships. The best NRC submission achieved a precision of 0.7499, a recall of 0.6431, and an F1 score of 0.6924, resulting in a statistical tie for first place. Post hoc improvements led to a precision of 0.7537, a recall of 0.6455, and an F1 score of 0.6954, giving the highest scores reported on this task to date. Methods for general relation extraction extended well to temporal relations, and gave top-ranked state-of-the-art results. Careful ordering of predictions within result sets proved critical to this success.
24150319	Partially scanned interferogram methodology applied to IASI for the retrieval of CO, CO₂, CH₄ and N₂O.
Opt Express  2013Oct21
The technique of partially scanned interferograms is applied to the retrieval of trace gases from Infrared Atmospheric Sounding Interferometer (IASI) observations. For the specific case of CO, CO₂, CH₄ and N₂O, we show that this methodology allows us to retrieve trace gases column abundances at an unprecedented accuracy at the level of the single IASI footprint. The technique consists in transforming the IASI spectra back to the interferogram domain where we identify small regions that are mostly sensitive to single gas species. The retrieval is then performed by directly applying Least Squares estimation to these small segments of interferometric radiances. One of the main advantages of the technique is that it allows the efficient use of the information contained in all the IASI channels that are available in the absorption bands of a specific gas species. The retrieval technique has been applied to IASI radiances measured over the Mediterranean sea during the month of July 2010, one of the hottest months on record. Results have been validated against ground-based measurements. We have also carried out a comparison with Atmospheric Infrared Radiometer Sounder data and IASI retrievals obtained with usual variational approaches in the spectral domain.
24063607	A modular framework for biomedical concept recognition.
BMC Bioinformatics 20130924 2013
Concept recognition is an essential task in biomedical information extraction, presenting several complex and unsolved challenges. The development of such solutions is typically performed in an ad-hoc manner or using general information extraction frameworks, which are not optimized for the biomedical domain and normally require the integration of complex external libraries and/or the development of custom tools. This article presents Neji, an open source framework optimized for biomedical concept recognition built around four key characteristics: modularity, scalability, speed, and usability. It integrates modules for biomedical natural language processing, such as sentence splitting, tokenization, lemmatization, part-of-speech tagging, chunking and dependency parsing. Concept recognition is provided through dictionary matching and machine learning with normalization methods. Neji also integrates an innovative concept tree implementation, supporting overlapped concept names and respective disambiguation techniques. The most popular input and output formats, namely Pubmed XML, IeXML, CoNLL and A1, are also supported. On top of the built-in functionalities, developers and researchers can implement new processing modules or pipelines, or use the provided command-line interface tool to build their own solutions, applying the most appropriate techniques to identify heterogeneous biomedical concepts. Neji was evaluated against three gold standard corpora with heterogeneous biomedical concepts (CRAFT, AnEM and NCBI disease corpus), achieving high performance results on named entity recognition (F1-measure for overlap matching: species 95%, cell 92%, cellular components 83%, gene and proteins 76%, chemicals 65%, biological processes and molecular functions 63%, disorders 85%, and anatomical entities 82%) and on entity normalization (F1-measure for overlap name matching and correct identifier included in the returned list of identifiers: species 88%, cell 71%, cellular components 72%, gene and proteins 64%, chemicals 53%, and biological processes and molecular functions 40%). Neji provides fast and multi-threaded data processing, annotating up to 1200 sentences/second when using dictionary-based concept identification. Considering the provided features and underlying characteristics, we believe that Neji is an important contribution to the biomedical community, streamlining the development of complex concept recognition solutions. Neji is freely available at http://bioinformatics.ua.pt/neji.
24759479	Constructing a search strategy and searching for evidence. A guide to the literature search for a systematic review.
Am J Nurs  2014May
This article is the third in a new series on the systematic review from the Joanna Briggs Institute, an international collaborative supporting evidence-based practice in nursing, medicine, and allied health fields. The purpose of the series is to show nurses how to conduct a systematic review-one step at a time. This article details the major considerations surrounding search strategies and presents an example of a search using the PubMed platform (pubmed.gov).
21316559	Three-dimensional high-frequency backscatter and envelope quantification of cancerous human lymph nodes.
Ultrasound Med Biol  2011Mar
Quantitative imaging methods using high-frequency ultrasound (HFU) offer a means of characterizing biological tissue at the microscopic level. Previously, high-frequency, 3-D quantitative ultrasound (QUS) methods were developed to characterize 46 freshly-dissected lymph nodes of colorectal-cancer patients. 3-D ultrasound radiofrequency data were acquired using a 25.6 MHz center-frequency transducer and each node was inked before tissue fixation to recover orientation after sectioning for 3-D histological evaluation. Backscattered echo signals were processed using 3-D cylindrical regions-of-interest (ROIs) to yield four QUS estimates associated with tissue microstructure (i.e., effective scatterer size, acoustic concentration, intercept and slope). These QUS estimates, obtained by parameterizing the backscatter spectrum, showed great potential for cancer detection. In the present study, these QUS methods were applied to 112 lymph nodes from 77 colorectal and gastric cancer patients. Novel QUS methods parameterizing the envelope statistics of the ROIs using Nakagami and homodyned-K distributions were also developed; they yielded four additional QUS estimates. The ability of these eight QUS estimates to classify lymph nodes and detect cancer was evaluated using receiver operating characteristics (ROC) curves. An area under the ROC curve of 0.996 with specificity and sensitivity of 95% were obtained by combining effective scatterer size and one envelope parameter based on the homodyned-K distribution. Therefore, these advanced 3-D QUS methods potentially can be valuable for detecting small metastatic foci in dissected lymph nodes.
21653522	The variant call format and VCFtools.
Bioinformatics 20110607 2011Aug1
The variant call format (VCF) is a generic format for storing DNA polymorphism data such as SNPs, insertions, deletions and structural variants, together with rich annotations. VCF is usually stored in a compressed manner and can be indexed for fast data retrieval of variants from a range of positions on the reference genome. The format was developed for the 1000 Genomes Project, and has also been adopted by other projects such as UK10K, dbSNP and the NHLBI Exome Project. VCFtools is a software suite that implements various utilities for processing VCF files, including validation, merging, comparing and also provides a general Perl API. http://vcftools.sourceforge.net
24225647	IAServ: an intelligent home care web services platform in a cloud for aging-in-place.
Int J Environ Res Public Health 20131112 2013Nov
As the elderly population has been rapidly expanding and the core tax-paying population has been shrinking, the need for adequate elderly health and housing services continues to grow while the resources to provide such services are becoming increasingly scarce. Thus, increasing the efficiency of the delivery of healthcare services through the use of modern technology is a pressing issue. The seamless integration of such enabling technologies as ontology, intelligent agents, web services, and cloud computing is transforming healthcare from hospital-based treatments to home-based self-care and preventive care. A ubiquitous healthcare platform based on this technological integration, which synergizes service providers with patients' needs to be developed to provide personalized healthcare services at the right time, in the right place, and the right manner. This paper presents the development and overall architecture of IAServ (the Intelligent Aging-in-place Home care Web Services Platform) to provide personalized healthcare service ubiquitously in a cloud computing setting to support the most desirable and cost-efficient method of care for the aged-aging in place. The IAServ is expected to offer intelligent, pervasive, accurate and contextually-aware personal care services. Architecturally the implemented IAServ leverages web services and cloud computing to provide economic, scalable, and robust healthcare services over the Internet.
24232290	Mobile, cloud, and big data computing: contributions, challenges, and new directions in telecardiology.
Int J Environ Res Public Health 20131113 2013Nov
Many studies have indicated that computing technology can enable off-site cardiologists to read patients' electrocardiograph (ECG), echocardiography (ECHO), and relevant images via smart phones during pre-hospital, in-hospital, and post-hospital teleconsultation, which not only identifies emergency cases in need of immediate treatment, but also prevents the unnecessary re-hospitalizations. Meanwhile, several studies have combined cloud computing and mobile computing to facilitate better storage, delivery, retrieval, and management of medical files for telecardiology. In the future, the aggregated ECG and images from hospitals worldwide will become big data, which should be used to develop an e-consultation program helping on-site practitioners deliver appropriate treatment. With information technology, real-time tele-consultation and tele-diagnosis of ECG and images can be practiced via an e-platform for clinical, research, and educational purposes. While being devoted to promote the application of information technology onto telecardiology, we need to resolve several issues: (1) data confidentiality in the cloud, (2) data interoperability among hospitals, and (3) network latency and accessibility. If these challenges are overcome, tele-consultation will be ubiquitous, easy to perform, inexpensive, and beneficial. Most importantly, these services will increase global collaboration and advance clinical practice, education, and scientific research in cardiology.
24227678	CheNER: chemical named entity recognizer.
Bioinformatics 20131113 2014Apr1
Chemical named entity recognition is used to automatically identify mentions to chemical compounds in text and is the basis for more elaborate information extraction. However, only a small number of applications are freely available to identify such mentions. Particularly challenging and useful is the identification of International Union of Pure and Applied Chemistry (IUPAC) chemical compounds, which due to the complex morphology of IUPAC names requires more advanced techniques than that of brand names. We present CheNER, a tool for automated identification of systematic IUPAC chemical mentions. We evaluated different systems using an established literature corpus to show that CheNER has a superior performance in identifying IUPAC names specifically, and that it makes better use of computational resources. http://metres.udl.cat/index.php/9-download/4-chener, http://chener.bioinfo.cnio.es/
24001514	Evaluating the impact of pre-annotation on annotation speed and potential bias: natural language processing gold standard development for clinical named entity recognition in clinical trial announcements.
J Am Med Inform Assoc 20130903 2014 May-Jun
To present a series of experiments: (1) to evaluate the impact of pre-annotation on the speed of manual annotation of clinical trial announcements; and (2) to test for potential bias, if pre-annotation is utilized. To build the gold standard, 1400 clinical trial announcements from the clinicaltrials.gov website were randomly selected and double annotated for diagnoses, signs, symptoms, Unified Medical Language System (UMLS) Concept Unique Identifiers, and SNOMED CT codes. We used two dictionary-based methods to pre-annotate the text. We evaluated the annotation time and potential bias through F-measures and ANOVA tests and implemented Bonferroni correction. Time savings ranged from 13.85% to 21.5% per entity. Inter-annotator agreement (IAA) ranged from 93.4% to 95.5%. There was no statistically significant difference for IAA and annotator performance in pre-annotations. On every experiment pair, the annotator with the pre-annotated text needed less time to annotate than the annotator with non-labeled text. The time savings were statistically significant. Moreover, the pre-annotation did not reduce the IAA or annotator performance. Dictionary-based pre-annotation is a feasible and practical method to reduce the cost of annotation of clinical named entity recognition in the eligibility sections of clinical trial announcements without introducing bias in the annotation process.
23349080	Federated queries of clinical data repositories: the sum of the parts does not equal the whole.
J Am Med Inform Assoc 20130124 2013Jun
In 2008 we developed a shared health research information network (SHRINE), which for the first time enabled research queries across the full patient populations of four Boston hospitals. It uses a federated architecture, where each hospital returns only the aggregate count of the number of patients who match a query. This allows hospitals to retain control over their local databases and comply with federal and state privacy laws. However, because patients may receive care from multiple hospitals, the result of a federated query might differ from what the result would be if the query were run against a single central repository. This paper describes the situations when this happens and presents a technique for correcting these errors. We use a one-time process of identifying which patients have data in multiple repositories by comparing one-way hash values of patient demographics. This enables us to partition the local databases such that all patients within a given partition have data at the same subset of hospitals. Federated queries are then run separately on each partition independently, and the combined results are presented to the user. Using theoretical bounds and simulated hospital networks, we demonstrate that once the partitions are made, SHRINE can produce more precise estimates of the number of patients matching a query. Uncertainty in the overlap of patient populations across hospitals limits the effectiveness of SHRINE and other federated query tools. Our technique reduces this uncertainty while retaining an aggregate federated architecture.
23557711	An analysis of FMA using structural self-bisimilarity.
J Biomed Inform 20130402 2013Jun
As ontologies are mostly manually created, they tend to contain errors and inconsistencies. In this paper, we present an automated computational method to audit symmetric concepts in ontologies by leveraging self-bisimilarity and linguistic structure in the concept names. Two concepts A and B are symmetric if concept B can be obtained from concept A by replacing a single modifier such as "left" with its symmetric modifier such as "right." All possible local structural types for symmetric concept pairs are enumerated according to their local subsumption hierarchy, and the pairs are further classified into Non-Matches and Matches. To test the feasibility and validate the benefits of this method, we computed all the symmetric modifier pairs in the Foundational Model of Anatomy (FMA) and selected six of them for experimentation. 9893 Non-Matches and 221 abnormal Matches with potential errors were discovered by our algorithm. Manual evaluation by FMA domain experts on 176 selected Non-Matches and all the 221 abnormal Matches found 102 missing concepts and 40 misaligned concepts. Corrections for them have currently been implemented in the latest version of FMA. Our result demonstrates that self-bisimilarity can be a valuable method for ontology quality assurance, particularly in uncovering missing concepts and misaligned concepts. Our approach is computationally scalable and can be applied to other ontologies that are rich in symmetric concepts.
23876611	How life changes itself: the Read-Write (RW) genome.
Phys Life Rev 20130708 2013Sep
The genome has traditionally been treated as a Read-Only Memory (ROM) subject to change by copying errors and accidents. In this review, I propose that we need to change that perspective and understand the genome as an intricately formatted Read-Write (RW) data storage system constantly subject to cellular modifications and inscriptions. Cells operate under changing conditions and are continually modifying themselves by genome inscriptions. These inscriptions occur over three distinct time-scales (cell reproduction, multicellular development and evolutionary change) and involve a variety of different processes at each time scale (forming nucleoprotein complexes, epigenetic formatting and changes in DNA sequence structure). Research dating back to the 1930s has shown that genetic change is the result of cell-mediated processes, not simply accidents or damage to the DNA. This cell-active view of genome change applies to all scales of DNA sequence variation, from point mutations to large-scale genome rearrangements and whole genome duplications (WGDs). This conceptual change to active cell inscriptions controlling RW genome functions has profound implications for all areas of the life sciences.
23715988	Controlled annotations for systems biology.
Methods Mol. Biol.  2013
The aim of this chapter is to provide sufficient information to enable a reader, new to the subject of Systems Biology, to create and use effectively controlled annotations, using resolvable Identifiers.org Uniform Resource Identifiers (URIs). The text details the underlying requirements that have led to the development of such an identification scheme and infrastructure, the principles that underpin its syntax and the benefits derived through its use. It also places into context the relationship with other standardization efforts, how it differs from other pre-existing identification schemes, recent improvements to the system, as well as those that are planned in the future. Throughout, the reader is provided with explicit examples of use and directed to supplementary information where necessary.
24131049	When cloud computing meets bioinformatics: a review.
J Bioinform Comput Biol 20131010 2013Oct
In the past decades, with the rapid development of high-throughput technologies, biology research has generated an unprecedented amount of data. In order to store and process such a great amount of data, cloud computing and MapReduce were applied to many fields of bioinformatics. In this paper, we first introduce the basic concepts of cloud computing and MapReduce, and their applications in bioinformatics. We then highlight some problems challenging the applications of cloud computing and MapReduce to bioinformatics. Finally, we give a brief guideline for using cloud computing in biology research.
24151586	CADe system integrated within the electronic health record.
Biomed Res Int 20130917 2013
The latest technological advances and information support systems for clinics and hospitals produce a wide range of possibilities in the storage and retrieval of an ever-growing amount of clinical information as well as in detection and diagnosis. In this work, an Electronic Health Record (EHR) combined with a Computer Aided Detection (CADe) system for breast cancer diagnosis has been implemented. Our objective is to provide to radiologists a comprehensive working environment that facilitates the integration, the image visualization, and the use of aided tools within the EHR. For this reason, a development methodology based on hardware and software system features in addition to system requirements must be present during the whole development process. This will lead to a complete environment for displaying, editing, and reporting results not only for the patient information but also for their medical images in standardised formats such as DICOM and DICOM-SR. As a result, we obtain a CADe system which helps in detecting breast cancer using mammograms and is completely integrated into an EHR.
23499924	Basophile: accurate fragment charge state prediction improves peptide identification rates.
Genomics Proteomics Bioinformatics 20130308 2013Apr
In shotgun proteomics, database search algorithms rely on fragmentation models to predict fragment ions that should be observed for a given peptide sequence. The most widely used strategy (Naive model) is oversimplified, cleaving all peptide bonds with equal probability to produce fragments of all charges below that of the precursor ion. More accurate models, based on fragmentation simulation, are too computationally intensive for on-the-fly use in database search algorithms. We have created an ordinal-regression-based model called Basophile that takes fragment size and basic residue distribution into account when determining the charge retention during CID/higher-energy collision induced dissociation (HCD) of charged peptides. This model improves the accuracy of predictions by reducing the number of unnecessary fragments that are routinely predicted for highly-charged precursors. Basophile increased the identification rates by 26% (on average) over the Naive model, when analyzing triply-charged precursors from ion trap data. Basophile achieves simplicity and speed by solving the prediction problem with an ordinal regression equation, which can be incorporated into any database search software for shotgun proteomic identification.
23373974	Robustness, evolvability, and the logic of genetic regulation.
Artif. Life 20130201 2014Winter
In gene regulatory circuits, the expression of individual genes is commonly modulated by a set of regulating gene products, which bind to a gene's cis-regulatory region. This region encodes an input-output function, referred to as signal-integration logic, that maps a specific combination of regulatory signals (inputs) to a particular expression state (output) of a gene. The space of all possible signal-integration functions is vast and the mapping from input to output is many-to-one: For the same set of inputs, many functions (genotypes) yield the same expression output (phenotype). Here, we exhaustively enumerate the set of signal-integration functions that yield identical gene expression patterns within a computational model of gene regulatory circuits. Our goal is to characterize the relationship between robustness and evolvability in the signal-integration space of regulatory circuits, and to understand how these properties vary between the genotypic and phenotypic scales. Among other results, we find that the distributions of genotypic robustness are skewed, so that the majority of signal-integration functions are robust to perturbation. We show that the connected set of genotypes that make up a given phenotype are constrained to specific regions of the space of all possible signal-integration functions, but that as the distance between genotypes increases, so does their capacity for unique innovations. In addition, we find that robust phenotypes are (i) evolvable, (ii) easily identified by random mutation, and (iii) mutationally biased toward other robust phenotypes. We explore the implications of these latter observations for mutation-based evolution by conducting random walks between randomly chosen source and target phenotypes. We demonstrate that the time required to identify the target phenotype is independent of the properties of the source phenotype.
24113802	Database queries for hospitalizations for acute congestive heart failure: flexible methods and validation based on set theory.
J Am Med Inform Assoc 20131010 2014 Mar-Apr
Electronic health records databases are increasingly used for identifying cohort populations, covariates, or outcomes, but discerning such clinical 'phenotypes' accurately is an ongoing challenge. We developed a flexible method using overlapping (Venn diagram) queries. Here we describe this approach to find patients hospitalized with acute congestive heart failure (CHF), a sampling strategy for one-by-one 'gold standard' chart review, and calculation of positive predictive value (PPV) and sensitivities, with SEs, across different definitions. We used retrospective queries of hospitalizations (2002-2011) in the Indiana Network for Patient Care with any CHF ICD-9 diagnoses, a primary diagnosis, an echocardiogram performed, a B-natriuretic peptide (BNP) drawn, or BNP &gt;500 pg/mL. We used a hybrid between proportional sampling by Venn zone and over-sampling non-overlapping zones. The acute CHF (presence/absence) outcome was based on expert chart review using a priori criteria. Among 79,091 hospitalizations, we reviewed 908. A query for any ICD-9 code for CHF had PPV 42.8% (SE 1.5%) for acute CHF and sensitivity 94.3% (1.3%). Primary diagnosis of 428 and BNP &gt;500 pg/mL had PPV 90.4% (SE 2.4%) and sensitivity 28.8% (1.1%). PPV was &lt;10% when there was no echocardiogram, no BNP, and no primary diagnosis. 'False positive' hospitalizations were for other heart disease, lung disease, or other reasons. This novel method successfully allowed flexible application and validation of queries for patients hospitalized with acute CHF.
24272162	Exploiting the potential of large databases of electronic health records for research using rapid search algorithms and an intuitive query interface.
J Am Med Inform Assoc 20131122 2014 Mar-Apr
UK primary care databases, which contain diagnostic, demographic and prescribing information for millions of patients geographically representative of the UK, represent a significant resource for health services and clinical research. They can be used to identify patients with a specified disease or condition (phenotyping) and to investigate patterns of diagnosis and symptoms. Currently, extracting such information manually is time-consuming and requires considerable expertise. In order to exploit more fully the potential of these large and complex databases, our interdisciplinary team developed generic methods allowing access to different types of user. Using the Clinical Practice Research Datalink database, we have developed an online user-focused system (TrialViz), which enables users interactively to select suitable medical general practices based on two criteria: suitability of the patient base for the intended study (phenotyping) and measures of data quality. An end-to-end system, underpinned by an innovative search algorithm, allows the user to extract information in near real-time via an intuitive query interface and to explore this information using interactive visualization tools. A usability evaluation of this system produced positive results. We present the challenges and results in the development of TrialViz and our plans for its extension for wider applications of clinical research. Our fast search algorithms and simple query algorithms represent a significant advance for users of clinical research databases.
23981021	Integrating information literacy in health sciences curricula: a case study from Québec.
Health Info Libr J 20130417 2013Sep
To portray an information literacy programme demonstrating a high level of integration in health sciences curricula and a teaching orientation aiming towards the development of lifelong learning skills. The setting is a French-speaking North American university. The offering includes standard workshops such as MEDLINE searching and specialised sessions such as pharmaceutical patents searching. A contribution to an international teaching collaboration in Haiti where workshops had to be thoroughly adapted to the clientele is also presented. Online guides addressing information literacy topics complement the programme. A small team of librarians and technicians taught 276 hours of library instruction (LI) during the 2011-2012 academic year. Methods used for evaluating information skills include scoring features of literature searches and user satisfaction surveys. Privileged contacts between librarians and faculty resulting from embedded LI as well as from active participation in library committees result in a growing reputation of library services across academic departments and bring forth collaboration opportunities. Sustainability and evolution of the LI programme is warranted by frequent communication with partners in the clinical field, active involvement in academic networks and health library associations, and reflective professional strategies.
24268183	VarRanker: rapid prioritization of sequence variations associated with human disease.
BMC Bioinformatics 20131001 2013
Identification of the genetic alterations responsible for human disease is a central challenge facing medical genetics. While many algorithms have been developed to predict the degree of damage caused by a given sequence alteration, few tools are able to incorporate information about a given phenotype of interest. Here, we describe an algorithm and web-based application which take into account both the probability that a variant damages the function of a gene as well as the relevance of the gene to a given phenotype. Phenotypes are described by a list of scored terms supplied by the user. These terms are then used to search a variety of public databases including NCBI gene summaries, PubMed abstracts, and Gene Ontology terms, and protein-protein interactions in String-DB to determine a relevance score. The overall ranking is determined by the product of the functional damage score and the relevance score, such that highly ranked variants are likely to be damaging and in genes of interest. We demonstrate the method on several test cases including samples with Hereditary Hemorrhagic Telangiectasia (HHT) and Diamond-Blackfan Anemia (DBA). We have also implemented a web-based application which allows public access to the VarRanker algorithm. Automated searching of public literature and online databases may substantially decrease the amount of time required to identify the mutations underlying human disease. However, several ad-hoc and subjective decisions must be made, and the results of such analyses are likely to depend on the researcher and the state of the literature and databases involved.
24564220	Development and tuning of an original search engine for patent libraries in medicinal chemistry.
BMC Bioinformatics 20140110 2014
The large increase in the size of patent collections has led to the need of efficient search strategies. But the development of advanced text-mining applications dedicated to patents of the biomedical field remains rare, in particular to address the needs of the pharmaceutical &amp; biotech industry, which intensively uses patent libraries for competitive intelligence and drug development. We describe here the development of an advanced retrieval engine to search information in patent collections in the field of medicinal chemistry. We investigate and combine different strategies and evaluate their respective impact on the performance of the search engine applied to various search tasks, which covers the putatively most frequent search behaviours of intellectual property officers in medical chemistry: 1) a prior art search task; 2) a technical survey task; and 3) a variant of the technical survey task, sometimes called known-item search task, where a single patent is targeted. The optimal tuning of our engine resulted in a top-precision of 6.76% for the prior art search task, 23.28% for the technical survey task and 46.02% for the variant of the technical survey task. We observed that co-citation boosting was an appropriate strategy to improve prior art search tasks, while IPC classification of queries was improving retrieval effectiveness for technical survey tasks. Surprisingly, the use of the full body of the patent was always detrimental for search effectiveness. It was also observed that normalizing biomedical entities using curated dictionaries had simply no impact on the search tasks we evaluate. The search engine was finally implemented as a web-application within Novartis Pharma. The application is briefly described in the report. We have presented the development of a search engine dedicated to patent search, based on state of the art methods applied to patent corpora. We have shown that a proper tuning of the system to adapt to the various search tasks clearly increases the effectiveness of the system. We conclude that different search tasks demand different information retrieval engines' settings in order to yield optimal end-user retrieval.
24043847	Making open data work for plant scientists.
J. Exp. Bot. 20130916 2013Nov
Despite the clear demand for open data sharing, its implementation within plant science is still limited. This is, at least in part, because open data-sharing raises several unanswered questions and challenges to current research practices. In this commentary, some of the challenges encountered by plant researchers at the bench when generating, interpreting, and attempting to disseminate their data have been highlighted. The difficulties involved in sharing sequencing, transcriptomics, proteomics, and metabolomics data are reviewed. The benefits and drawbacks of three data-sharing venues currently available to plant scientists are identified and assessed: (i) journal publication; (ii) university repositories; and (iii) community and project-specific databases. It is concluded that community and project-specific databases are the most useful to researchers interested in effective data sharing, since these databases are explicitly created to meet the researchers' needs, support extensive curation, and embody a heightened awareness of what it takes to make data reuseable by others. Such bottom-up and community-driven approaches need to be valued by the research community, supported by publishers, and provided with long-term sustainable support by funding bodies and government. At the same time, these databases need to be linked to generic databases where possible, in order to be discoverable to the majority of researchers and thus promote effective and efficient data sharing. As we look forward to a future that embraces open access to data and publications, it is essential that data policies, data curation, data integration, data infrastructure, and data funding are linked together so as to foster data access and research productivity.
23941928	Contextualisation of clinical information from fragmented health records.
Stud Health Technol Inform  2013
Traditionally health records have a source-oriented structure, as opposed to a clinically logical structure. The aim of the current study is to record and depict the clinically logical information structure built by doctors as part of their contextualisation of clinical information, to compare this structure to that of a given health record, and to assess whether a structural difference may have negative consequences. Eight doctors in a medical department were observed during contextualisation of clinical information. The method of observation included simultaneous interviews and with the technique used it was possible to depict the clinically logical information structure and compare it to the structure of the health record. The doctors' information structure was found to differ widely from the structure of the health record, causing split-attention and stress. It is concluded that the present health record is suboptimal. Further research in information support based on clinically logical structure is recommended.
23941941	Information gaps in reporting patient falls: the challenges and technical solutions.
Stud Health Technol Inform  2013
The emerging computerized system for patient safety event reporting eases the course of learning from medical errors and adverse events for a safer healthcare environment. To a medical event like patient falls, the course usually involves pre, during and post stages for the prediction, reporting and solution of the event. However, the reporting stage often separates from the other two stages for risk assessment and cause analysis. As this iterative flow of actions falls apart and becomes unintelligible or intangible due to information gaps, it is dubious for users to join and complete the task at all three stages in a high quality. Therefore, in this paper, by referencing studies in aspects of Norman' s task action theory and fall management programs, we proposed a gap-bridging model to describe the process of assisting users in proceeding along the stages by user-centered design approaches. Based upon the model, we also developed a series of interface artifacts served as gap-bridging features, which hold promise in improving the quality of reporting and reporter engagement of the system.
23941949	Digital communication to support clinical supervision: considering the human factors.
Stud Health Technol Inform  2013
During the last three years the School of Nursing and Midwifery at the University of Tasmania has used a needs assessment survey to explore the needs of organizations and nursing professionals that facilitate and clinically supervise Bachelor of Nursing students in the workplace. Findings from the survey indicated that staff at healthcare organizations wanted a communication strategy that was easily accessible by clinicians who supervised students during work integrated learning placements. In particular they wanted to receive timely information related to the role and function of supervisors in practice. The development of the digital strategy to strengthen the development of a community of practice between the University, organizations, facilities and clinical supervisors was identified as the key method of improving communication. Blogging and micro blogging were selected as methods of choice for the implementation of the digital strategy because they were easy to set up, use and enable equity of access to geographically dispersed practitioners in urban and rural areas. Change champions were identified to disseminate information about the strategy within their workplaces. Although clinicians indicated electronic communication as their preferred method, there were a number of human factors at a systems and individual level identified to be challenges when communicating with clinical supervisors who were based off-campus. Information communication technology policies and embedded culture towards social presence were impediments to using this approach in some organizations. Additionally, it was found that it is necessary for this group of clinicians to be educated about using digital methods to undertake their role as clinical supervisors in their varied clinical practice environments.
23941955	Using personas as an intermediate construct in the development of tailored messages: a case study.
Stud Health Technol Inform  2013
Tailoring health education messages to patients' preferences for message style is believed to increase patients' susceptibility to the given advice. This paper presents a persona-centered approach towards creating tailored health messages for chronically ill patients. A case study of tailoring messages to the preferences of patients with coronary heart disease illustrates the approach and shows the need for patient-centered data collection so that personas reflect patients' preferences. Based on these personas, a manageable set of tailored messages can be created in a step-by-step approach.
23941956	Collecting data in real time with postcards.
Stud Health Technol Inform  2013
The success of information technology (IT) in transforming healthcare is often limited by the lack of clear understanding of the context at which the technology is used. Various methods have been proposed to understand healthcare context better in designing and implementing Health Information Systems. These methods often involve cross-sectional, retrospective data collection. This paper describes the postcard method for prospective real-time data collection, both in paper format and electronic format. This paper then describes the results obtained using postcard techniques in Denmark and Australia. The benefits of this technique are illustrated. There are limitations in using postcard techniques and this paper provides a detail discussion about these limitations. Postcard techniques provide unique advantages in understanding real time healthcare context and it is an important technique to consider in IT design and implementation in healthcare.
24505722	On describing human white matter anatomy: the white matter query language.
Med Image Comput Comput Assist Interv  2013
The main contribution of this work is the careful syntactical definition of major white matter tracts in the human brain based on a neuroanatomist's expert knowledge. We present a technique to formally describe white matter tracts and to automatically extract them from diffusion MRI data. The framework is based on a novel query language with a near-to-English textual syntax. This query language allows us to construct a dictionary of anatomical definitions describing white matter tracts. The definitions include adjacent gray and white matter regions, and rules for spatial relations. This enables automated coherent labeling of white matter anatomy across subjects. We use our method to encode anatomical knowledge in human white matter describing 10 association and 8 projection tracts per hemisphere and 7 commissural tracts. The technique is shown to be comparable in accuracy to manual labeling. We present results applying this framework to create a white matter atlas from 77 healthy subjects, and we use this atlas in a proof-of-concept study to detect tract changes specific to schizophrenia.
24551325	On-time clinical phenotype prediction based on narrative reports.
AMIA Annu Symp Proc 20131116 2013
In this paper we describe a natural language processing system which is able to predict whether or not a patient exhibits a specific phenotype using the information extracted from the narrative reports associated with the patient. Furthermore, the phenotypic annotations from our report dataset were performed at the report level which allows us to perform the prediction of the clinical phenotype at any point in time during the patient hospitalization period. Our experiments indicate that an important factor in achieving better results for this problem is to determine how much information to extract from the patient reports in the time interval between the patient admission time and the current prediction time.
24551326	Applying Distance Histogram to retrieve 3D cardiac medical models.
AMIA Annu Symp Proc 20131116 2013
Three-dimensional models are being extensively used in the medical area in order to improve clinical medical examinations and diagnosis. The Cardiology field handles with several types of image slices to compose the diagnosis. MRI (Magnetic Resonance Imaging) is a non-invasive technique to detect anomalies from internal images of the human body that generates hundreds of images, which takes long for the specialist to analyze frame by frame and the diagnosis precision can be affected. Many cardiac diseases could be identified through shape deformation, but systems aimed to aid diagnosis usually identify shapes in two-dimensional (2D) images. Our aim is to apply a shape descriptor to retrieve three-dimensional cardiac models, obtained from a set of 2D slices, which were segmented and reconstructed from MRI images using their geometry information. Preliminary results show that the shape deformation in 3D models can be a good indicator to detect Congestive Heart Failure, a very common heart disease.
24551329	Semantic MEDLINE for discovery browsing: using semantic predications and the literature-based discovery paradigm to elucidate a mechanism for the obesity paradox.
AMIA Annu Symp Proc 20131116 2013
Applying the principles of literature-based discovery (LBD), we elucidate the paradox that obesity is beneficial in critical care despite contributing to disease generally. Our approach enhances a previous extension to LBD, called "discovery browsing," and is implemented using Semantic MEDLINE, which summarizes the results of a PubMed search into an interactive graph of semantic predications. The methodology allows a user to construct argumentation underpinning an answer to a biomedical question by engaging the user in an iterative process between system output and user knowledge. Components of the Semantic MEDLINE output graph identified as "interesting" by the user both contribute to subsequent searches and are constructed into a logical chain of relationships constituting an explanatory network in answer to the initial question. Based on this methodology we suggest that phthalates leached from plastic in critical care interventions activate PPAR gamma, which is anti-inflammatory and abundant in obese patients.
24551332	Leveraging user query sessions to improve searching of medical literature.
AMIA Annu Symp Proc 20131116 2013
Published reports about searching medical literature do not refer to leveraging the query context, as expressed by previous queries in a session. We aimed to assess novel strategies for context-aware searching, hypothesizing that this would be better than baseline. Building upon methods using term frequency-inverse document frequency, we added extensions such as a function incorporating search results and terms of previous queries, with higher weights for more recent queries. Among 60 medical students generating queries against the TREC 9 benchmark dataset, we assessed recall and mean average precision. For difficult queries, we achieved improvement (27%) in average precision over baseline. Improvements in recall were also seen. Our methods outperformed baseline by 4% to 14% on average. Furthermore, the effectiveness of context-aware search was greater for longer query sessions, which are typically more challenging. In conclusion, leveraging the previous queries in a session improved overall search quality with this biomedical database.
24551334	Practical choices for infobutton customization: experience from four sites.
AMIA Annu Symp Proc 20131116 2013
Context-aware links between electronic health records (EHRs) and online knowledge resources, commonly called "infobuttons" are being used increasingly as part of EHR "meaningful use" requirements. While an HL7 standard exists for specifying how the links should be constructed, there is no guidance on what links to construct. Collectively, the authors manage four infobutton systems that serve 16 institutions. The purpose of this paper is to publish our experience with linking various resources and specifying particular criteria that can be used by infobutton managers to select resources that are most relevant for a given situation. This experience can be used directly by those wishing to customize their own EHRs, for example by using the OpenInfobutton infobutton manager and its configuration tool, the Librarian Infobutton Tailoring Environment.
24551337	Does access modality matter? Evaluation of validity in reusing clinical care data.
AMIA Annu Symp Proc 20131116 2013
Self-service database portals may improve access to institutional data resources for clinical research or quality improvement, but questions remain about the validity of this approach. We tested the accuracy of data extracted from a clinical data repository using a self-service portal by comparing three approaches to measuring medication use among patients with coronary disease: (1) automated extraction using a portal, (2) extraction by an experienced data architect, and (3) manual chart abstraction. Outcomes included medications and diagnoses (e.g., myocardial infarction, heart failure). Charts were manually reviewed for 200 patients. Using matched criteria, self-service query identified 7327 of 7358 patients identified by the data analyst. For patients in both cohorts, agreement rates ranged from 0.99 for demographic data to 0.94 for laboratory data. Based on chart review, the self-service portal and the analyst had similar sensitivities and specificities for comorbid diagnoses and statin use.
24551345	A large-scale analysis of the reasons given for excluding articles that are retrieved by literature search during systematic review.
AMIA Annu Symp Proc 20131116 2013
A literature search to identify relevant studies is one of the first steps in performing a systematic review (SR) in support of evidence-based medicine. To maximize efficiency, the search must find practically all relevant studies and retrieve few that are irrelevant; however, this level of precision is seldom attained. Therefore, many articles must be manually examined for relevance. To better understand the limitations of current search tools as applied to SR, we characterized the most common reasons that papers retrieved by SR searches were excluded from the review. The textual reasons given for retrieved but excluded articles were extracted from 6,743 SRs performed by 54 Cochrane Collaboration review groups. The frequencies of different exclusion reasons were analyzed, and we developed a taxonomy summarizing these reasons. Almost 65% of articles were excluded because the means of comparison were inappropriate. Of these, about 72% were due to the randomized controlled trial (RCT) design being required but not employed by the excluded study. Mismatching interventions and outcomes and incorrect population characteristics were also common reasons for exclusion. Currently available search methods do not adequately address the most common exclusion reasons for systematic review, even those based primarily on study design.
24551354	Building and evaluating an ontology-based tool for reasoning about consent permission.
AMIA Annu Symp Proc 20131116 2013
Given the lack of mechanisms for specifying, sharing and checking the compliance of consent permissions, we focus on building and testing novel approaches to address this gap. In our previous work, we introduced a "permission ontology" to capture in a precise, machine-interpretable form informed consent permissions in research studies. Here we explain how we built and evaluated a framework for specifying subject's permissions and checking researcher's resource request in compliance with those permissions. The framework is proposed as an extension of an existing policy engine based on the eXtensible Access Control Markup Language (XACML), incorporating ontology-based reasoning. The framework is evaluated in the context of the UCSD Moores Cancer Center biorepository, modeling permissions from an informed consent and a HIPAA form. The resulting permission ontology and mechanisms to check subject's permission are implementation and institution independent, and therefore offer the potential to be reusable in other biorepositories and data warehouses.
24551366	Desiderata for healthcare integrated data repositories based on architectural comparison of three public repositories.
AMIA Annu Symp Proc 20131116 2013
Integrated data repositories (IDRs) are indispensable tools for numerous biomedical research studies. We compare three large IDRs (Informatics for Integrating Biology and the Bedside (i2b2), HMO Research Network's Virtual Data Warehouse (VDW) and Observational Medical Outcomes Partnership (OMOP) repository) in order to identify common architectural features that enable efficient storage and organization of large amounts of clinical data. We define three high-level classes of underlying data storage models and we analyze each repository using this classification. We look at how a set of sample facts is represented in each repository and conclude with a list of desiderata for IDRs that deal with the information storage model, terminology model, data integration and value-sets management.
24551382	Using image references in radiology reports to support enhanced report-to-image navigation.
AMIA Annu Symp Proc 20131116 2013
Radiology reports frequently contain references to image slices that are illustrative of described findings, for instance, "Neurofibroma in superior right extraconal space (series 5, image 104)". In the current workflow, if a report consumer wants to view a referenced image, he or she needs to (1) open prior study, (2) open the series of interest (series 5 in this example), and (3) navigate to the corresponding image slice (image 104). This research aims to improve this report-to-image navigation process by providing hyperlinks to images. We develop and evaluate a regular expressions-based algorithm that recognizes image references at a sentence level. Validation on 314 image references from general radiology reports shows precision of 99.35%, recall of 98.08% and F-measure of 98.71%, suggesting this is a viable approach for image reference extraction. We demonstrate how recognized image references can be hyperlinked in a PACS report viewer allowing one-click access to the images.
24551386	Predicting clicks of PubMed articles.
AMIA Annu Symp Proc 20131116 2013
Predicting the popularity or access usage of an article has the potential to improve the quality of PubMed searches. We can model the click trend of each article as its access changes over time by mining the PubMed query logs, which contain the previous access history for all articles. In this article, we examine the access patterns produced by PubMed users in two years (July 2009 to July 2011). We explore the time series of accesses for each article in the query logs, model the trends with regression approaches, and subsequently use the models for prediction. We show that the click trends of PubMed articles are best fitted with a log-normal regression model. This model allows the number of accesses an article receives and the time since it first becomes available in PubMed to be related via quadratic and logistic functions, with the model parameters to be estimated via maximum likelihood. Our experiments predicting the number of accesses for an article based on its past usage demonstrate that the mean absolute error and mean absolute percentage error of our model are 4.0% and 8.1% lower than the power-law regression model, respectively. The log-normal distribution is also shown to perform significantly better than a previous prediction method based on a human memory theory in cognitive science. This work warrants further investigation on the utility of such a log-normal regression approach towards improving information access in PubMed.
24551388	A natural language processing algorithm to define a venous thromboembolism phenotype.
AMIA Annu Symp Proc 20131116 2013
Deep venous thrombosis and pulmonary embolism are diseases associated with significant morbidity and mortality. Known risk factors are attributed for only slight majority of venous thromboembolic disease (VTE) with the remainder of risk presumably related to unidentified genetic factors. We designed a general purpose Natural Language (NLP) algorithm to retrospectively capture both acute and historical cases of thromboembolic disease in a de-identified electronic health record. Applying the NLP algorithm to a separate evaluation set found a positive predictive value of 84.7% and sensitivity of 95.3% for an F-measure of 0.897, which was similar to the training set of 0.925. Use of the same algorithm on problem lists only in patients without VTE ICD-9s was found to be the best means of capturing historical cases with a PPV of 83%. NLP of VTE ICD-9 positive cases and non-ICD-9 positive problem lists provides an effective means for capture of both acute and historical cases of venous thromboembolic disease.
24551392	Semantic annotation of clinical events for generating a problem list.
AMIA Annu Symp Proc 20131116 2013
We present a pilot study of an annotation schema representing problems and their attributes, along with their relationship to temporal modifiers. We evaluated the ability for humans to annotate clinical reports using the schema and assessed the contribution of semantic annotations in determining the status of a problem mention as active, inactive, proposed, resolved, negated, or other. Our hypothesis is that the schema captures semantic information useful for generating an accurate problem list. Clinical named entities such as reference events, time points, time durations, aspectual phase, ordering words and their relationships including modifications and ordering relations can be annotated by humans with low to moderate recall. Once identified, most attributes can be annotated with low to moderate agreement. Some attributes - Experiencer, Existence, and Certainty - are more informative than other attributes - Intermittency and Generalized/Conditional - for predicting a problem mention's status. Support vector machine outperformed Naïve Bayes and Decision Tree for predicting a problem's status.
24551407	Optimizing the txt2MEDLINE search portal for low-resource clinical decision support.
AMIA Annu Symp Proc 20131116 2013
txt2MEDLINE provides access to high-quality medical evidence via text-messaging in settings with inadequate Internet access. We optimized the txt2MEDLINE search technique by parsing queries for MeSH (Medical Subject Heading) terms and searching MEDLINE for articles containing these terms in their titles or abstracts. We compared our results to the existing txt2MEDLINE tool by compiling benchmark queries from low-income and low-middle-income countries, and asking doctors and nurses with practice experience in low-resource areas to evaluate them. The median scores on a 5-point Likert scale were 2.9 for the existing txt2MEDLINE vs. 3.8 for the modified version (p=0.015). This reached our predefined criterion for clinical significance, a difference of 0.5 standard deviations. Improving this technology could improve clinical information resources in the world's most medically underserved communities.
24551423	A literature-based assessment of concept pairs as a measure of semantic relatedness.
AMIA Annu Symp Proc 20131116 2013
The semantic relatedness between two concepts, according to human perception, is domain-rooted and reflects prior knowledge. We developed a new method for semantic relatedness assessment that reflects human judgment, utilizing semantic predications extracted from PubMed citations by SemRep. We compared the new method to other approaches utilizing path-based, statistical, and context vector methods, using a gold standard for evaluation. The new method outperformed all others, except one variation of the context vector technique. These findings have implications in several natural language processing applications, such as serendipitous knowledge discovery.
24551429	ASLForm: an adaptive self learning medical form generating system.
AMIA Annu Symp Proc 20131116 2013
To facilitate the process of extracting information from narrative medical reports and transforming extracted data into standardized structured forms, we present an interactive, incrementally learning based information extraction system - ASLForm. ASLForm provides users a convenient interface that can be used as a simple data extraction and data entry system. It is unique, however, in its ability to transparently analyze and quickly learn, from users' interactions with a small number of reports, the desired values for the data fields. Additional user feedback (through acceptance decision or edits on the generated values) can incrementally refine the decision model in real-time, which further reduces users' interaction effort thereafter. The system eventually achieves high accuracy on data extraction with minimal effort from users. ASLForm requires no special configuration or training sets, and is not constrained to specific domains, thus it is easy to use and highly portable. Our experiments demonstrate the effectiveness of the system.
24683956	Bayesian estimation of regularization and atlas building in diffeomorphic image registration.
Inf Process Med Imaging  2013
This paper presents a generative Bayesian model for diffeomorphic image registration and atlas building. We develop an atlas estimation procedure that simultaneously estimates the parameters controlling the smoothness of the diffeomorphic transformations. To achieve this, we introduce a Monte Carlo Expectation Maximization algorithm, where the expectation step is approximated via Hamiltonian Monte Carlo sampling on the manifold of diffeomorphisms. An added benefit of this stochastic approach is that it can successfully solve difficult registration problems involving large deformations, where direct geodesic optimization fails. Using synthetic data generated from the forward model with known parameters, we demonstrate the ability of our model to successfully recover the atlas and regularization parameters. We also demonstrate the effectiveness of the proposed method in the atlas estimation problem for 3D brain images.
24683987	Learning from M/EEG data with variable brain activation delays.
Inf Process Med Imaging  2013
Magneto- and electroencephalography (M/EEG) measure the electromagnetic signals produced by brain activity. In order to address the issue of limited signal-to-noise ratio (SNR) with raw data, acquisitions consist of multiple repetitions of the same experiment. An important challenge arising from such data is the variability of brain activations over the repetitions. It hinders statistical analysis such as prediction performance in a supervised learning setup. One such confounding variability is the time offset of the peak of the activation, which varies across repetitions. We propose to address this misalignment issue by explicitly modeling time shifts of different brain responses in a classification setup. To this end, we use the latent support vector machine (LSVM) formulation, where the latent shifts are inferred while learning the classifier parameters. The inferred shifts are further used to improve the SNR of the M/EEG data, and to infer the chronometry and the sequence of activations across the brain regions that are involved in the experimental task. Results are validated on a long-term memory retrieval task, showing significant improvement using the proposed latent discriminative method.
24683992	Multimodal surface matching: fast and generalisable cortical registration using discrete optimisation.
Inf Process Med Imaging  2013
Group neuroimaging studies of the cerebral cortex benefit from accurate, surface-based, cross-subject alignment for investigating brain architecture, function and connectivity. There is an increasing amount of high quality data available. However, establishing how different modalities correlate across groups remains an open research question. One reason for this is that the current methods for registration, based on cortical folding, provide sub-optimal alignment of some functional subregions of the brain. A more flexible framework is needed that will allow robust alignment of multiple modalities. We adapt the Fast Primal-Dual (Fast-PD) approach for discrete Markov Random Field (MRF) optimisation to spherical registration by reframing the deformation labels as a discrete set of rotations and propose a novel regularisation term, derived from the geodesic distance between rotation matrices. This formulation allows significant flexibility in the choice of similarity metric. To this end we propose a new multivariate cost function based on the discretisation of a graph-based mutual information measure. Results are presented for alignment driven by scalar metrics of curvature and myelination, and multivariate features derived from functional task performance. These experiments demonstrate the potential of this approach for improving the integration of complementary brain data sets in the future.
24683996	Moving frames for heart fiber geometry.
Inf Process Med Imaging  2013
Elongated cardiac muscle cells named cardiomyocytes are densely packed in an intercellular collagen matrix and are aligned to helical segments in a manner which facilitates pumping via alternate contraction and relaxation. Characterizing the geometrical variation of their groupings as cardiac fibers is central to our understanding of normal heart function. Motivated by a recent abstraction by Savadjiev et al. of heart wall fibers into generalized helicoid minimal surfaces, this paper develops an extension based on differential forms. The key idea is to use Maurer-Cartan's method of moving frames to study the rotations of a frame field attached to the local fiber direction. This approach provides a new set of parameters that are complimentary to those of Savadjiev et al. and offers a framework for developing new models of the cardiac fiber architecture. This framework is used to compute the generalized helicoid parameters directly, without the need to formulate an optimization problem. The framework admits a straightforward numerical implementation that provides statistical measurements consistent with those previously reported. Using Diffusion MRI we demonstrate that one such specialization, the homeoid, constrains fibers to lie locally within ellipsoidal shells and yields improved fits in the rat, the dog and the human to those obtained using generalized helicoids.
24684000	Active testing search for point cloud matching.
Inf Process Med Imaging  2013
We present a general approach for solving the point-cloud matching problem for the case of mildly nonlinear transformations. Our method quickly finds a coarse approximation of the solution by exploring a reduced set of partial matches using an approach to which we refer to as Active Testing Search (ATS). We apply the method to registration of graph structures by branching point matching. It is based solely on the geometric position of the points, no additional information is used nor the knowledge of an initial alignment. In the second stage, we use dynamic programming to refine the solution. We tested our algorithm on angiography, retinal fundus, and neuronal data gathered using electron and light microscopy. We show that our method solves cases not solved by most approaches, and is faster than the remaining ones.
24684001	Relating Fisher information to detectability of changes in nodule characteristics with CT.
Inf Process Med Imaging  2013
Fisher information provides a bound on the variance of any unbiased estimate for estimation tasks involving nonrandom parameters. In addition, a Fisher information approximation for ideal-observer detectability has been derived. We adopt and generalize such an approximation to establish a method to assess a system's ability to detect small changes in lesion characteristics. By representing the lesion by a size parameter, the ability to detect small changes can be approximated by a function involving the size difference and the Fisher information. A concept, termed the approximated least required difference (ALRD), is introduced and evaluated as an upper bound for assessing a system's power in size discrimination. We present a simulation study for lung nodules as an example to illustrate such a framework, where the image model incorporates a simulated CT imaging system, a thorax background and parameterized nodules. The noise is assumed to be multivariate Gaussian and the noise power spectrum (NPS) method is used to estimate the covariance matrix for the Fisher information calculation. In addition to bounding performance, our results also provide insights into factors, including nodule characteristics and acquisition parameters, that influence ALRD performance. This framework can be extended to connect other discrimination and estimation tasks, facilitating objective assessment and optimization of quantitative imaging systems.
20813090	[The importance of case reports in current pediatric endocrinology and metabolism literature--the analysis of publications indexed in Medline in the years 2004-2009].
Pediatr Endocrinol Diabetes Metab  2010
Case reports have been used as an educational tool in medicine for a long time, as they are usually the first kind of publications prepared by new authors. In compliance with the rules of the EBM, the evidence-based medicine, a clinical case report (which describes and analyses the manner of diagnosis and treatment) appears at the bottom of the hierarchy of scientific evidences. As a result, medical periodicals limit the publishing of case reports. The aim of the study is to assess how important case reports are in the modern pediatric literature, with special interests in endocrine and metabolic disorders. Searching Medline over the last 5 years (October 2004-October 2009) using key words "case report" was performed. The limitations: "human", age "all children: 0-18 years" and the kind of publication: "case report" were used. For the huge number of publications found, the number and language of the publication was analyzed, and the first 100 free full-text publications in field of endocrine and metabolic disorders in English (from 2007-2009) were rated depending on the kind of case and aim of the publication. A total of 55379 publications classified by Pubmed as case reports were found, including 48805 English, 1592 Spanish, 1538 French, 794 German, 370 Polish, 356 Chinese, 265 Russian and 135 Italian. The published case reports were more often informative than educational in character. The content of the published case reports deals more often with the notification of a new mutation, less frequently presentation of an interesting, rare, unusual case or a new illness or the description of a novel therapy. Less often the published case reports appear to be of educational character or those concerning diagnostic problems or treatment failure. Case reports remain an important contribution to the pediatric literature, mainly fulfilling an essential role in providing information about new medical problems. Medical literature published in Polish is relatively rich in clinical case reports. Publishing a case report in field of endocrinology and metabolism in Pubmed indexed journal is considered reasonable in situations where a new medical condition is being presented (most often a new mutation), a novel therapy has been discovered, the case is interesting or where the publishing of such reports fulfils didactic/educational needs.
23869631	Building and evaluating an informatics tool to facilitate analysis of a biomedical literature search service in an academic medical center library.
Med Ref Serv Q  2013
This study utilizes an informatics tool to analyze a robust literature search service in an academic medical center library. Structured interviews with librarians were conducted focusing on the benefits of such a tool, expectations for performance, and visual layout preferences. The resulting application utilizes Microsoft SQL Server and .Net Framework 3.5 technologies, allowing for the use of a web interface. Customer tables and MeSH terms are included. The National Library of Medicine MeSH database and entry terms for each heading are incorporated, resulting in functionality similar to searching the MeSH database through PubMed. Data reports will facilitate analysis of the search service.
23869634	Using NLM exhibits and events to engage library users and reach the community.
Med Ref Serv Q  2013
In an effort to reach out to library users and make the library a more relevant, welcoming place, the University of Florida's Health Science Center Library hosted exhibits from the National Library of Medicine's (NLM) Traveling Exhibition Program. From 2010 through 2012, the library hosted four NLM exhibits and created event series for each. Through reflection and use of a participant survey, lessons were learned concerning creating relevant programs, marketing events, and forming new partnerships. Each successive exhibit added events and activities to address different audiences. A survey of libraries that have hosted NLM exhibits highlights lessons learned at those institutions.
23869635	Lessons learned: year-by-year improvement of a required information competency course.
Med Ref Serv Q  2013
At the Ohio State University, a health sciences librarian is the co-instructor in a required information competencies course for first-year undergraduate students in the Honors Biomedical Science Major. This article discusses the creation and development of the credit-bearing, in-person course from the curriculum planning phase in 2005 to present. Improvements to the course are described by year. Student feedback, student performance, and reflection by the co-instructors influenced the course improvements, including changes in content, delivery, student feedback mechanisms, and assessment of student learning. The course teaches students to access, organize, read, and analyze the biomedical research literature.
23869636	Efficacy of screen-capture tutorials in literature search training: a pilot study of a research method.
Med Ref Serv Q  2013
This pilot study evaluated a research method for examining the efficacy of screen capture tutorials in teaching database search skills. This is not a results-oriented paper but rather describes the facets and testing of a mixed methods protocol. The lessons learned can be applied to a result-oriented study with a larger sample size and to the development of methods for similar studies. The protocol tried to balance control of variables with observing behavior in the natural setting. A combination of concept maps, an information stage questionnaire, and screen recordings provided rich information about health practitioners' research questions and search strategies.
24745248	Electrochemical-signal enhanced information storage device composed of cytochrome c/SNP bilayer.
J Nanosci Nanotechnol  2014Mar
The films organized with biomolecules and organic materials are important elements for developing bioelectronic devices according to their electron transfer property. Until now, several concepts of techniques have been accomplished to be used for developing biomemory devices. However it is difficult to detect the current signal from the electron transfer between biomolecules and the substrate in these fabricated films. To enhance the current signal, the silver nanoparticle was introduced to the cytochrome c in this present study. The surface morphology of the fabricated film was investigated by atomic force microscopy. The current signal enhancement was investigated by cyclic voltammetry. As a result, we could obtain the redox potentials. Moreover, by chronoamperometry, we validated that this proposed layer showed the signal-enhanced memory property for biomemory devices. This new film composed of the cytochrome c and the silver nanoparticle showed the signal enhancement. Using chronoamperometry, the areas under the graphs between 0 s and 50 ms were calculated. The calculated result showed that the areas under the cytochrome c/SNP graph and cytochrome c graph were 6.93 x 10(-7) C and 4.54 x 10(-7) C, respectively. This numerical value verified that the cytochrome c/silver nanoparticle hetero-layer film showed better electron charged biomemory performance compared to the cytochrome c monolayer. This signal-enhanced film can be applied to the bioelectronic devices which are able to replace existing electronic devices in the near future.
23418540	canEvolve: a web portal for integrative oncogenomics.
PLoS ONE 20130213 2013
Genome-wide profiles of tumors obtained using functional genomics platforms are being deposited to the public repositories at an astronomical scale, as a result of focused efforts by individual laboratories and large projects such as the Cancer Genome Atlas (TCGA) and the International Cancer Genome Consortium. Consequently, there is an urgent need for reliable tools that integrate and interpret these data in light of current knowledge and disseminate results to biomedical researchers in a user-friendly manner. We have built the canEvolve web portal to meet this need. canEvolve query functionalities are designed to fulfill most frequent analysis needs of cancer researchers with a view to generate novel hypotheses. canEvolve stores gene, microRNA (miRNA) and protein expression profiles, copy number alterations for multiple cancer types, and protein-protein interaction information. canEvolve allows querying of results of primary analysis, integrative analysis and network analysis of oncogenomics data. The querying for primary analysis includes differential gene and miRNA expression as well as changes in gene copy number measured with SNP microarrays. canEvolve provides results of integrative analysis of gene expression profiles with copy number alterations and with miRNA profiles as well as generalized integrative analysis using gene set enrichment analysis. The network analysis capability includes storage and visualization of gene co-expression, inferred gene regulatory networks and protein-protein interaction information. Finally, canEvolve provides correlations between gene expression and clinical outcomes in terms of univariate survival analysis. At present canEvolve provides different types of information extracted from 90 cancer genomics studies comprising of more than 10,000 patients. The presence of multiple data types, novel integrative analysis for identifying regulators of oncogenesis, network analysis and ability to query gene lists/pathways are distinctive features of canEvolve. canEvolve will facilitate integrative and meta-analysis of oncogenomics datasets. The canEvolve web portal is available at http://www.canevolve.org/.
23508969	CistromeFinder for ChIP-seq and DNase-seq data reuse.
Bioinformatics 20130318 2013May15
Chromatin immunoprecipitation and DNase I hypersensitivity assays with high-throughput sequencing have greatly accelerated the understanding of transcriptional and epigenetic regulation, although data reuse for the community of experimental biologists has been challenging. We created a data portal CistromeFinder that can help query, evaluate and visualize publicly available Chromatin immunoprecipitation and DNase I hypersensitivity assays with high-throughput sequencing data in human and mouse. The database currently contains 6378 samples over 4391 datasets, 313 factors and 102 cell lines or cell populations. Each dataset has gone through a consistent analysis and quality control pipeline; therefore, users could evaluate the overall quality of each dataset before examining binding sites near their genes of interest. CistromeFinder is integrated with UCSC genome browser for visualization, Primer3Plus for ChIP-qPCR primer design and CistromeMap for submitting newly available datasets. It also allows users to leave comments to facilitate data evaluation and update. http://cistrome.org/finder. xsliu@jimmy.harvard.edu or henry_long@dfci.harvard.edu.
24064416	BioServices: a common Python package to access biological Web Services programmatically.
Bioinformatics 20130923 2013Dec15
Web interfaces provide access to numerous biological databases. Many can be accessed to in a programmatic way thanks to Web Services. Building applications that combine several of them would benefit from a single framework. BioServices is a comprehensive Python framework that provides programmatic access to major bioinformatics Web Services (e.g. KEGG, UniProt, BioModels, ChEMBLdb). Wrapping additional Web Services based either on Representational State Transfer or Simple Object Access Protocol/Web Services Description Language technologies is eased by the usage of object-oriented programming. BioServices releases and documentation are available at http://pypi.python.org/pypi/bioservices under a GPL-v3 license.
24698297	A population search filter for hard-to-reach populations increased search efficiency for a systematic review.
J Clin Epidemiol  2014May
This article discusses how hard-to-reach population groups were conceptualized into a search filter. The objectives of this article were to (1) discuss how the authors designed a multistranded population search filter and (2) retrospectively test the effectiveness of the search filter in capturing all relevant populations (eg, homeless people, immigrants, substance misusers) in a public health systematic review. Systematic and retrospective analysis via a case study. Retrospective analysis of the search filter was conducted by comparing the MEDLINE search results retrieved without using the search filter against those retrieved with the search filter. A total of 5,465 additional results from the unfiltered search were screened to the same criteria as the filtered search. No additional populations were identified in the unfiltered sample. The search filter reduced the volume of MEDLINE hits to screen by 64%, with no impact on inclusion of populations. The results demonstrate the effectiveness of the filter in capturing all relevant UK populations for the review. This suggests that well-planned search filters can be written for reviews that analyze imprecisely defined population groups. This filter could be used in topic areas of associated comorbidities, for rapid clinical searches, or for investigating hard-to-reach populations.
23330918	Computer use in educational activities by students with ADHD.
Scand J Occup Ther 20130121 2013Sep
The aim of this study was to investigate computer use in educational activities by students with attention deficit hyperactivity disorder (ADHD) in comparison with that of students with physical disabilities and students from the general population. The design of the study was cross-sectional with group comparison. Students with ADHD (n = 102) were pair-matched in terms of age and sex with students with physical disabilities and students from the general population (n = 940) were used as a reference group. The study showed that less than half of the students with ADHD had access to a computer in the classroom. Students with ADHD reported significantly less frequent use of computers for almost all educational activities compared with students with physical disabilities and students from the general population. Students with ADHD reported low satisfaction with computer use in school. In addition, students with ADHD reported a desire to use computers more often and for more activities in school compared with students with physical disabilities. These results indicate that occupational therapists should place more emphasize on how to enable students with ADHD to use computers in educational activities in school.
20936065	Biomedical text summarization to support genetic database curation: using Semantic MEDLINE to create a secondary database of genetic information.
J Med Libr Assoc  2010Oct
This paper examines the development and evaluation of an automatic summarization system in the domain of molecular genetics. The system is a potential component of an advanced biomedical information management application called Semantic MEDLINE and could assist librarians in developing secondary databases of genetic information extracted from the primary literature. An existing summarization system was modified for identifying biomedical text relevant to the genetic etiology of disease. The summarization system was evaluated on the task of identifying data describing genes associated with bladder cancer in MEDLINE citations. A gold standard was produced using records from Genetics Home Reference and Online Mendelian Inheritance in Man. Genes in text found by the system were compared to the gold standard. Recall, precision, and F-measure were calculated. The system achieved recall of 46%, and precision of 88% (F-measure=0.61) by taking Gene References into Function (GeneRIFs) into account. The new summarization schema for genetic etiology has potential as a component in Semantic MEDLINE to support the work of data curators.
21284871	Dynamic summarization of bibliographic-based data.
BMC Med Inform Decis Mak 20110201 2011
Traditional information retrieval techniques typically return excessive output when directed at large bibliographic databases. Natural Language Processing applications strive to extract salient content from the excessive data. Semantic MEDLINE, a National Library of Medicine (NLM) natural language processing application, highlights relevant information in PubMed data. However, Semantic MEDLINE implements manually coded schemas, accommodating few information needs. Currently, there are only five such schemas, while many more would be needed to realistically accommodate all potential users. The aim of this project was to develop and evaluate a statistical algorithm that automatically identifies relevant bibliographic data; the new algorithm could be incorporated into a dynamic schema to accommodate various information needs in Semantic MEDLINE, and eliminate the need for multiple schemas. We developed a flexible algorithm named Combo that combines three statistical metrics, the Kullback-Leibler Divergence (KLD), Riloff's RlogF metric (RlogF), and a new metric called PredScal, to automatically identify salient data in bibliographic text. We downloaded citations from a PubMed search query addressing the genetic etiology of bladder cancer. The citations were processed with SemRep, an NLM rule-based application that produces semantic predications. SemRep output was processed by Combo, in addition to the standard Semantic MEDLINE genetics schema and independently by the two individual KLD and RlogF metrics. We evaluated each summarization method using an existing reference standard within the task-based context of genetic database curation. Combo asserted 74 genetic entities implicated in bladder cancer development, whereas the traditional schema asserted 10 genetic entities; the KLD and RlogF metrics individually asserted 77 and 69 genetic entities, respectively. Combo achieved 61% recall and 81% precision, with an F-score of 0.69. The traditional schema achieved 23% recall and 100% precision, with an F-score of 0.37. The KLD metric achieved 61% recall, 70% precision, with an F-score of 0.65. The RlogF metric achieved 61% recall, 72% precision, with an F-score of 0.66. Semantic MEDLINE summarization using the new Combo algorithm outperformed a conventional summarization schema in a genetic database curation task. It potentially could streamline information acquisition for other needs without having to hand-build multiple saliency schemas.
22621674	Text summarization as a decision support aid.
BMC Med Inform Decis Mak 20120523 2012
PubMed data potentially can provide decision support information, but PubMed was not exclusively designed to be a point-of-care tool. Natural language processing applications that summarize PubMed citations hold promise for extracting decision support information. The objective of this study was to evaluate the efficiency of a text summarization application called Semantic MEDLINE, enhanced with a novel dynamic summarization method, in identifying decision support data. We downloaded PubMed citations addressing the prevention and drug treatment of four disease topics. We then processed the citations with Semantic MEDLINE, enhanced with the dynamic summarization method. We also processed the citations with a conventional summarization method, as well as with a baseline procedure. We evaluated the results using clinician-vetted reference standards built from recommendations in a commercial decision support product, DynaMed. For the drug treatment data, Semantic MEDLINE enhanced with dynamic summarization achieved average recall and precision scores of 0.848 and 0.377, while conventional summarization produced 0.583 average recall and 0.712 average precision, and the baseline method yielded average recall and precision values of 0.252 and 0.277. For the prevention data, Semantic MEDLINE enhanced with dynamic summarization achieved average recall and precision scores of 0.655 and 0.329. The baseline technique resulted in recall and precision scores of 0.269 and 0.247. No conventional Semantic MEDLINE method accommodating summarization for prevention exists. Semantic MEDLINE with dynamic summarization outperformed conventional summarization in terms of recall, and outperformed the baseline method in both recall and precision. This new approach to text summarization demonstrates potential in identifying decision support data for multiple needs.
23059733	Identifying clinical/translational research cohorts: ascertainment via querying an integrated multi-source database.
J Am Med Inform Assoc 20121011 2013Jan1
Ascertainment of potential subjects has been a longstanding problem in clinical research. Various methods have been proposed, including using data in electronic health records. However, these methods typically suffer from scaling effects-some methods work well for large cohorts; others work for small cohorts only. We propose a method that provides a simple identification of pre-research cohorts and relies on data available in most states in the USA: merged public health data sources. The Utah Population Database Limited query tool allows users to build complex queries that may span several types of health records, such as cancer registries, inpatient hospital discharges, and death certificates; in addition, these can be combined with family history information. The architectural approach incorporates several coding systems for medical information. It provides a front-end graphical user interface and enables researchers to build and run queries and view aggregate results. Multiple strategies have been incorporated to maintain confidentiality. This tool was rapidly adopted; since its release, 241 users representing a wide range of disciplines from 17 institutions have signed the user agreement and used the query tool. Three examples are discussed: pregnancy complications co-occurring with cardiovascular disease; spondyloarthritis; and breast cancer. This query tool was designed to provide results as pre-research so that institutional review board approval would not be required. This architecture uses well-described technologies that should be within the reach of most institutions.
23286165	Non-local robust detection of DTI white matter differences with small databases.
Med Image Comput Comput Assist Interv  2012
Diffusion imaging, through the study of water diffusion, allows for the characterization of brain white matter, both at the population and individual level. In recent years, it has been employed to detect brain abnormalities in patients suffering from a disease, e.g., from multiple sclerosis (MS). State-of-the-art methods usually utilize a database of matched (age, sex, ...) controls, registered onto a template, to test for differences in the patient white matter. Such approaches however suffer from two main drawbacks. First, registration algorithms are prone to local errors, thereby degrading the comparison results. Second, the database needs to be large enough to obtain reliable results. However, in medical imaging, such large databases are hardly available. In this paper, we propose a new method that addresses these two issues. It relies on the search for samples in a local neighborhood of each pixel to increase the size of the database. Then, we propose a new test based on these samples to perform a voxelwise comparison of a patient image with respect to a population of controls. We demonstrate on simulated and real MS patient data how such a framework allows for an improve detection power and a better robustness and reproducibility, even with a small database.
23195749	Design and validation of an automated method to detect known adverse drug reactions in MEDLINE: a contribution from the EU-ADR project.
J Am Med Inform Assoc 20121129 2013May1
The aim of this research was to automate the search of publications concerning adverse drug reactions (ADR) by defining the queries used to search MEDLINE and by determining the required threshold for the number of extracted publications to confirm the drug/event association in the literature. We defined an approach based on the medical subject headings (MeSH) 'descriptor records' and 'supplementary concept records' thesaurus, using the subheadings 'chemically induced' and 'adverse effects' with the 'pharmacological action' knowledge. An expert-built validation set of true positive and true negative drug/adverse event associations (n=61) was used to validate our method. Using a threshold of three of more extracted publications, the automated search method presented a sensitivity of 90% and a specificity of 100%. For nine different drug/event pairs selected, the recall of the automated search ranged from 24% to 64% and the precision from 93% to 48%. This work presents a method to find previously established relationships between drugs and adverse events in the literature. Using MEDLINE, following a MeSH approach to filter the signals, is a valid option. Our contribution is available as a web service that will be integrated in the final European EU-ADR project (Exploring and Understanding Adverse Drug Reactions by integrative mining of clinical records and biomedical knowledge) automated system.
23467469	Web-scale pharmacovigilance: listening to signals from the crowd.
J Am Med Inform Assoc 20130306 2013May1
Adverse drug events cause substantial morbidity and mortality and are often discovered after a drug comes to market. We hypothesized that Internet users may provide early clues about adverse drug events via their online information-seeking. We conducted a large-scale study of Web search log data gathered during 2010. We pay particular attention to the specific drug pairing of paroxetine and pravastatin, whose interaction was reported to cause hyperglycemia after the time period of the online logs used in the analysis. We also examine sets of drug pairs known to be associated with hyperglycemia and those not associated with hyperglycemia. We find that anonymized signals on drug interactions can be mined from search logs. Compared to analyses of other sources such as electronic health records (EHR), logs are inexpensive to collect and mine. The results demonstrate that logs of the search activities of populations of computer users can contribute to drug safety surveillance.
23565985	The eroticism of Internet cruising as a self-contained behaviour: a multivariate analysis of men seeking men demographics and getting off online.
Cult Health Sex 20130409 2013
Most studies on men seeking men and who use the Internet for sexual purposes have focused on the epidemiological outcomes of Internet cruising. Other research has only focused on online sexual behaviours such as cybersex. The present study examines men who find the acts of Internet cruising and emailing to be erotic as self-contained behaviours. We surveyed 499 men who used craigslist.org for sexually-oriented purposes, and ran an ordinary least squares multiple regression model to determine the demographic characteristics of men seeking men who found Internet cruising erotic. Our results showed that younger compared to older men seeking men found the acts erotic. Likewise, men seeking men from mid-sized cities and large cities compared to men from smaller cities found Internet cruising and emailing to be erotic. Most notably, bisexual- and heterosexual-identifying men seeking men compared to gay-identifying men found these acts to be more erotic. Our results suggested that self-contained Internet cruising might provide dual functions. For some men (e.g., heterosexual-identifying men), the behaviour provides a sexual outlet in which fantasy and experimentation may be explored without risking stigmatization. For other men (e.g., those from large cities), the behaviour may be an alternative to offset sexual risk while still being able to 'get off'.
23886922	Examining clinical decision support integrity: is clinician self-reported data entry accurate?
J Am Med Inform Assoc 20130725 2014 Jan-Feb
The aim of this study was to assess the accuracy of clinician-entered data in imaging clinical decision support (CDS). We used CDS-guided CT angiography (CTA) for pulmonary embolus (PE) in the emergency department as a case example because it required clinician entry of d-dimer results which could be unambiguously compared with actual laboratory values. Of 1296 patients with CTA orders for suspected PE during 2011, 1175 (90.7%) had accurate d-dimer values entered. In 55 orders (4.2%), incorrectly entered data shielded clinicians from intrusive computer alerts, resulting in potential CTA overuse. Remaining data entry errors did not affect user workflow. We found no missed PEs in our cohort. The majority of data entered by clinicians into imaging CDS are accurate. A small proportion may be intentionally erroneous to avoid intrusive computer alerts. Quality improvement methods, including academic detailing and improved integration between electronic medical record and CDS to minimize redundant data entry, may be necessary to optimize adoption of evidence presented through CDS.
22084150	A new efficient data structure for storage and retrieval of multiple biosequences.
IEEE/ACM Trans Comput Biol Bioinform 20111110 2012
Today's genome analysis applications require sequence representations allowing for fast access to their contents while also being memory-efficient enough to facilitate analyses of large-scale data. While a wide variety of sequence representations exist, lack of a generic implementation of efficient sequence storage has led to a plethora of poorly reusable or programming language-specific implementations. We present a novel, space-efficient data structure (GtEncseq) for storing multiple biological sequences of variable alphabet size, with customizable character transformations, wildcard support and an assortment of internal representations optimized for different distributions of wildcards and sequence lengths. For the human genome (3.1 gigabases, including 237 million wildcard characters) our representation requires only 2 + 8 &amp;#x00D7; 10^-6bits per character. Implemented in C, our portable software implementation provides a variety of methods for random and sequential access to characters and substrings (including different reading directions) using an object-oriented interface. In addition, it includes access to metadata like sequence descriptions or character distributions. The library is extensible to be used from various scripting languages. GtEncseq is much more versatile than previous solutions, adding features that were previously unavailable. Benchmarks show that it is competitive with respect to space and time requirements.
23981113	Mechanical control of electroresistive switching.
Nano Lett. 20130829 2013Sep11
Hysteretic metal-insulator transitions (MIT) mediated by ionic dynamics or ferroic phase transitions underpin emergent applications for nonvolatile memories and logic devices. The vast majority of applications and studies have explored the MIT coupled to the electric field or temperarture. Here, we argue that MIT coupled to ionic dynamics should be controlled by mechanical stimuli, the behavior we refer to as the piezochemical effect. We verify this effect experimentally and demonstrate that it allows both studying materials physics and enabling novel data storage technologies with mechanical writing and current-based readout.
23823371	Comparison of automated and manual vital sign collection at hospital wards.
Stud Health Technol Inform  2013
Using a cross-over study design, vital signs were collected from 60 patients by 6 nurses. Each nurse was randomly assigned for manual vital sign collection in 5 patients and for automated data collection in other 5 patients. The mean time taken for vital signs information to be available in EMR was significantly (p &lt;0.004) lower after automated data collection (158.7±67.0) than after the manual collection (4079.8±7091.8 s). The nursing satisfaction score of collecting vital signs was significantly lower (p&lt;0.007) for the manual way (10.3±3.9) than for the automated way (16.5±3.4). We found that 30% of vital sign records were transmitted to EMR with at least one error after manual data collection whereas there wasno transmission error with automated data collection. Allparticipating nurses stated that the automated vital sign collection can improve their efficiency and save their time for direct patient care.
23823373	Medical data analysis and coding using natural language processing techniques in order to derive structured data information.
Stud Health Technol Inform  2013
Medical data are, most of the times, very complex both in form and content. One of the greatest challenges for the IT community in healthcare is to enable the full utilization of these data by information systems. This explicit variety combined with the fact that data usually derives from diverse systems are great obstacles to this task. The result is that data stored in medical information systems usually do not accurately represent reality. In order to eliminate the fallacy between stored and real data, specialized applications that facilitate and accelerate data import into information systems must be developed. This is the goal of Natural Language Processing, the scientific field that combines computer science and linguistics. As a result NLP systems use applications for the coding and standardization of information, known as controlled medical vocabularies. The result of these processes is data that can be used by various technologies, such as clinical data warehouses and decision support systems, the functionality of which is fully dependable on the completeness and accuracy of the data on which their analysis is imposed.
23823382	Evaluation of health professionals in the use of internet information retrieval systems in health: a literature review.
Stud Health Technol Inform  2013
This paper presents a literature review on how health professionals are seeking health information using internet retrieval systems, databases. Publications present many attitude scales which evaluate the behavior of the users and the barriers that they face through the information research. On the following review are mentioned the characteristics that health professionals encounter on the use of computing. Also, is mentioned a number of problems which are associated with the information recourses such as reliability that were elicited and reviewed.
23823388	Terminology-based documentation systems: a systemantic comparison of four different approaches.
Stud Health Technol Inform  2013
The supplementation of documentation systems with controlled vocabularies is a prerequisite for improved and consistent communication, high data quality and efficient data exchange. To outline the requirements of an application system which supports the terminology-based development of documentation systems, existing documentation systems offering a terminological support in the definition of item collections were retrieved in the literature and analyzed. The analysis of four selected documentation systems caused us to define four main criteria for a terminology-based documentation system, which are: use of a controlled vocabulary, definition of a characteristic level, the definition of a value domain level and definition of roles and entities to combine vocabulary, characteristic and value domain levels together.
23823393	Functional requirements regarding medical registries--preliminary results.
Stud Health Technol Inform  2013
The term medical registry is used to reference tools and processes to support clinical or epidemiologic research or provide a data basis for decisions regarding health care policies. In spite of this wide range of applications the term registry and the functional requirements which a registry should support are not clearly defined. This work presents preliminary results of a literature review to discover functional requirements which form a registry. To extract these requirements a set of peer reviewed articles was collected. These set of articles was screened by using methods from qualitative research. Up to now most discovered functional requirements focus on data quality (e. g. prevent transcription error by conducting automatic domain checks).
23823394	Including other system in E-Care telemonitoring platform.
Stud Health Technol Inform  2013
Nowadays, telemonitoring systems are increasingly used, due to the increasing of life expectancy and chronic diseases. Indeed, chronic diseases and disabilities due to advancing age are responsible for health care costs increasingly growing. Telemonitoring systems provide a low cost way to monitor patients and their needs in the comfort of their own homes. In first systems, the data were collected then sent directly to physicians to be interpreted. Nowadays, thanks to technological advancements, software and systems have been developed to process data, on a simple computer or even smartphone. In this paper, we present e-Care telemonitoring system that combines the semantic web and expert system. E-Care is based on generic ontologies and a decision support system. The decision support system uses ontologies as knowledge base and an inference engine to detect abnormal situations. E-Care platform has a generic open architecture, which cans include other knowledge coming from other systems. We'll show how to integrate data of auscultation sounds in this architecture.
23823413	INDIV-3D. A new model for INdividual Data Integration and Visualisation using spatial coordinates.
Stud Health Technol Inform  2013
This work describes a new three-dimensional model for the integration and visualization of comprehensive individual health-related information. The model could be used to represent longitudinal datasets corresponding to any kind of biomedical data (genome, phenome, exposome) relative to a single individual or to populations. Analogously with geographical information systems this model includes different layers at each time point, which are associated with different sets of biomedical data. Each data element represented in this model receives two angular and one linear (temporal) coordinates. This new model could be able to represent virtually any kind of biomedical data including those captured in real-time such as self-monitoring activities or other exposome-related information.
23823416	Annotation for information extraction from mammography reports.
Stud Health Technol Inform  2013
Inter and intra-observer variability in mammographic interpretation is a challenging problem, and decision support systems (DSS) may be helpful to reduce variation in practice. Since radiology reports are created as unstructured text reports, Natural language processing (NLP) techniques are needed to extract structured information from reports in order to provide the inputs to DSS. Before creating NLP systems, producing high quality annotated data set is essential. The goal of this project is to develop an annotation schema to guide the information extraction tasks needed from free-text mammography reports.
23823422	Standard reporting for medical apps.
Stud Health Technol Inform  2013
Apps running on mobile devices are continually gaining importance, for medical professionals as well as for patients. When used appropriately, they can support their users, have the potential to increase efficiency and to lower costs. However, the information available for "medical apps" that are currently being distributed in the official mobile app stores of different mobile platforms often rather raises than answers questions regarding important aspects such as functionality, limits, data integrity, security and privacy. In this paper, we analyze the current situation, including a basic overview over current reporting and regulatory mechanisms and propose the use of an app-synopsis as step in direction of transparency.
23823423	SOA governance in healthcare organisations.
Stud Health Technol Inform  2013
Service Oriented Architecture (SOA) is increasingly adopted by many sectors, including healthcare. Due to the nature of healthcare systems there is a need to increase SOA adoption success rates as the non integrated nature of healthcare systems is responsible for medical errors that cause the loss of tens of thousands patients per year. Following our previous research [1] we propose that SOA governance is a critical success factor for SOA success in healthcare. Literature reports multiple SOA governance models that have limitations and they are confusing. In addition to this, there is a lack of healthcare specific SOA governance models. This highlights a literature void and thus the purpose of this paper is to proposed a healthcare specific SOA governance framework.
24051765	UTOPIAN: user-driven topic modeling based on interactive nonnegative matrix factorization.
IEEE Trans Vis Comput Graph  2013Dec
Topic modeling has been widely used for analyzing text document collections. Recently, there have been significant advancements in various topic modeling techniques, particularly in the form of probabilistic graphical modeling. State-of-the-art techniques such as Latent Dirichlet Allocation (LDA) have been successfully applied in visual text analytics. However, most of the widely-used methods based on probabilistic modeling have drawbacks in terms of consistency from multiple runs and empirical convergence. Furthermore, due to the complicatedness in the formulation and the algorithm, LDA cannot easily incorporate various types of user feedback. To tackle this problem, we propose a reliable and flexible visual analytics system for topic modeling called UTOPIAN (User-driven Topic modeling based on Interactive Nonnegative Matrix Factorization). Centered around its semi-supervised formulation, UTOPIAN enables users to interact with the topic modeling method and steer the result in a user-driven manner. We demonstrate the capability of UTOPIAN via several usage scenarios with real-world document corpuses such as InfoVis/VAST paper data set and product review data sets.
24051766	HierarchicalTopics: visually exploring large text collections using topic hierarchies.
IEEE Trans Vis Comput Graph  2013Dec
Analyzing large textual collections has become increasingly challenging given the size of the data available and the rate that more data is being generated. Topic-based text summarization methods coupled with interactive visualizations have presented promising approaches to address the challenge of analyzing large text corpora. As the text corpora and vocabulary grow larger, more topics need to be generated in order to capture the meaningful latent themes and nuances in the corpora. However, it is difficult for most of current topic-based visualizations to represent large number of topics without being cluttered or illegible. To facilitate the representation and navigation of a large number of topics, we propose a visual analytics system--HierarchicalTopic (HT). HT integrates a computational algorithm, Topic Rose Tree, with an interactive visual interface. The Topic Rose Tree constructs a topic hierarchy based on a list of topics. The interactive visual interface is designed to present the topic content as well as temporal evolution of topics in a hierarchical fashion. User interactions are provided for users to make changes to the topic hierarchy based on their mental model of the topic space. To qualitatively evaluate HT, we present a case study that showcases how HierarchicalTopics aid expert users in making sense of a large number of topics and discovering interesting patterns of topic groups. We have also conducted a user study to quantitatively evaluate the effect of hierarchical topic structure. The study results reveal that the HT leads to faster identification of large number of relevant topics. We have also solicited user feedback during the experiments and incorporated some suggestions into the current version of HierarchicalTopics.
24051768	ScatterBlogs2: real-time monitoring of microblog messages through user-guided filtering.
IEEE Trans Vis Comput Graph  2013Dec
The number of microblog posts published daily has reached a level that hampers the effective retrieval of relevant messages, and the amount of information conveyed through services such as Twitter is still increasing. Analysts require new methods for monitoring their topic of interest, dealing with the data volume and its dynamic nature. It is of particular importance to provide situational awareness for decision making in time-critical tasks. Current tools for monitoring microblogs typically filter messages based on user-defined keyword queries and metadata restrictions. Used on their own, such methods can have drawbacks with respect to filter accuracy and adaptability to changes in trends and topic structure. We suggest ScatterBlogs2, a new approach to let analysts build task-tailored message filters in an interactive and visual manner based on recorded messages of well-understood previous events. These message filters include supervised classification and query creation backed by the statistical distribution of terms and their co-occurrences. The created filter methods can be orchestrated and adapted afterwards for interactive, visual real-time monitoring and analysis of microblog feeds. We demonstrate the feasibility of our approach for analyzing the Twitter stream in emergency management scenarios.
24051769	Visual analytics for multimodal social network analysis: a design study with social scientists.
IEEE Trans Vis Comput Graph  2013Dec
Social network analysis (SNA) is becoming increasingly concerned not only with actors and their relations, but also with distinguishing between different types of such entities. For example, social scientists may want to investigate asymmetric relations in organizations with strict chains of command, or incorporate non-actors such as conferences and projects when analyzing coauthorship patterns. Multimodal social networks are those where actors and relations belong to different types, or modes, and multimodal social network analysis (mSNA) is accordingly SNA for such networks. In this paper, we present a design study that we conducted with several social scientist collaborators on how to support mSNA using visual analytics tools. Based on an openended, formative design process, we devised a visual representation called parallel node-link bands (PNLBs) that splits modes into separate bands and renders connections between adjacent ones, similar to the list view in Jigsaw. We then used the tool in a qualitative evaluation involving five social scientists whose feedback informed a second design phase that incorporated additional network metrics. Finally, we conducted a second qualitative evaluation with our social scientist collaborators that provided further insights on the utility of the PNLBs representation and the potential of visual analytics for mSNA.
24051773	Visual analysis of higher-order conjunctive relationships in multidimensional data using a hypergraph query system.
IEEE Trans Vis Comput Graph  2013Dec
Visual exploration and analysis of multidimensional data becomes increasingly difficult with increasing dimensionality. We want to understand the relationships between dimensions of data, but lack flexible techniques for exploration beyond low-order relationships. Current visual techniques for multidimensional data analysis focus on binary conjunctive relationships between dimensions. Recent techniques, such as cross-filtering on an attribute relationship graph, facilitate the exploration of some higher-order conjunctive relationships, but require a great deal of care and precision to do so effectively. This paper provides a detailed analysis of the expressive power of existing visual querying systems and describes a more flexible approach in which users can explore n-ary conjunctive inter- and intra- dimensional relationships by interactively constructing queries as visual hypergraphs. In a hypergraph query, nodes represent subsets of values and hyperedges represent conjunctive relationships. Analysts can dynamically build and modify the query using sequences of simple interactions. The hypergraph serves not only as a query specification, but also as a compact visual representation of the interactive state. Using examples from several domains, focusing on the digital humanities, we describe the design considerations for developing the querying system and incorporating it into visual analysis tools. We analyze query expressiveness with regard to the kinds of questions it can and cannot pose, and describe how it simultaneously expands the expressiveness of and is complemented by cross-filtering.
24051777	Transformation of an uncertain video search pipeline to a sketch-based visual analytics loop.
IEEE Trans Vis Comput Graph  2013Dec
Traditional sketch-based image or video search systems rely on machine learning concepts as their core technology. However, in many applications, machine learning alone is impractical since videos may not be semantically annotated sufficiently, there may be a lack of suitable training data, and the search requirements of the user may frequently change for different tasks. In this work, we develop a visual analytics systems that overcomes the shortcomings of the traditional approach. We make use of a sketch-based interface to enable users to specify search requirement in a flexible manner without depending on semantic annotation. We employ active machine learning to train different analytical models for different types of search requirements. We use visualization to facilitate knowledge discovery at the different stages of visual analytics. This includes visualizing the parameter space of the trained model, visualizing the search space to support interactive browsing, visualizing candidature search results to support rapid interaction for active learning while minimizing watching videos, and visualizing aggregated information of the search results. We demonstrate the system for searching spatiotemporal attributes from sports video to identify key instances of the team and player performance.
24051779	Space-time visual analytics of eye-tracking data for dynamic stimuli.
IEEE Trans Vis Comput Graph  2013Dec
We introduce a visual analytics method to analyze eye movement data recorded for dynamic stimuli such as video or animated graphics. The focus lies on the analysis of data of several viewers to identify trends in the general viewing behavior, including time sequences of attentional synchrony and objects with strong attentional focus. By using a space-time cube visualization in combination with clustering, the dynamic stimuli and associated eye gazes can be analyzed in a static 3D representation. Shotbased, spatiotemporal clustering of the data generates potential areas of interest that can be filtered interactively. We also facilitate data drill-down: the gaze points are shown with density-based color mapping and individual scan paths as lines in the space-time cube. The analytical process is supported by multiple coordinated views that allow the user to focus on different aspects of spatial and temporal information in eye gaze data. Common eye-tracking visualization techniques are extended to incorporate the spatiotemporal characteristics of the data. For example, heat maps are extended to motion-compensated heat maps and trajectories of scan paths are included in the space-time visualization. Our visual analytics approach is assessed in a qualitative users study with expert users, which showed the usefulness of the approach and uncovered that the experts applied different analysis strategies supported by the system.
24051786	Identifying redundancy and exposing provenance in crowdsourced data analysis.
IEEE Trans Vis Comput Graph  2013Dec
We present a system that lets analysts use paid crowd workers to explore data sets and helps analysts interactively examine and build upon workers' insights. We take advantage of the fact that, for many types of data, independent crowd workers can readily perform basic analysis tasks like examining views and generating explanations for trends and patterns. However, workers operating in parallel can often generate redundant explanations. Moreover, because workers have different competencies and domain knowledge, some responses are likely to be more plausible than others. To efficiently utilize the crowd's work, analysts must be able to quickly identify and consolidate redundant responses and determine which explanations are the most plausible. In this paper, we demonstrate several crowd-assisted techniques to help analysts make better use of crowdsourced explanations: (1) We explore crowd-assisted strategies that utilize multiple workers to detect redundant explanations. We introduce color clustering with representative selection--a strategy in which multiple workers cluster explanations and we automatically select the most-representative result--and show that it generates clusterings that are as good as those produced by experts. (2) We capture explanation provenance by introducing highlighting tasks and capturing workers' browsing behavior via an embedded web browser, and refine that provenance information via source-review tasks. We expose this information in an explanation-management interface that allows analysts to interactively filter and sort responses, select the most plausible explanations, and decide which to explore further.
24051792	MotionExplorer: exploratory search in human motion capture data based on hierarchical aggregation.
IEEE Trans Vis Comput Graph  2013Dec
We present MotionExplorer, an exploratory search and analysis system for sequences of human motion in large motion capture data collections. This special type of multivariate time series data is relevant in many research fields including medicine, sports and animation. Key tasks in working with motion data include analysis of motion states and transitions, and synthesis of motion vectors by interpolation and combination. In the practice of research and application of human motion data, challenges exist in providing visual summaries and drill-down functionality for handling large motion data collections. We find that this domain can benefit from appropriate visual retrieval and analysis support to handle these tasks in presence of large motion data. To address this need, we developed MotionExplorer together with domain experts as an exploratory search system based on interactive aggregation and visualization of motion states as a basis for data navigation, exploration, and search. Based on an overview-first type visualization, users are able to search for interesting sub-sequences of motion based on a query-by-example metaphor, and explore search results by details on demand. We developed MotionExplorer in close collaboration with the targeted users who are researchers working on human motion synthesis and analysis, including a summative field study. Additionally, we conducted a laboratory design study to substantially improve MotionExplorer towards an intuitive, usable and robust design. MotionExplorer enables the search in human motion capture data with only a few mouse clicks. The researchers unanimously confirm that the system can efficiently support their work.
24051794	LineUp: visual analysis of multi-attribute rankings.
IEEE Trans Vis Comput Graph  2013Dec
Rankings are a popular and universal approach to structuring otherwise unorganized collections of items by computing a rank for each item based on the value of one or more of its attributes. This allows us, for example, to prioritize tasks or to evaluate the performance of products relative to each other. While the visualization of a ranking itself is straightforward, its interpretation is not, because the rank of an item represents only a summary of a potentially complicated relationship between its attributes and those of the other items. It is also common that alternative rankings exist which need to be compared and analyzed to gain insight into how multiple heterogeneous attributes affect the rankings. Advanced visual exploration tools are needed to make this process efficient. In this paper we present a comprehensive analysis of requirements for the visualization of multi-attribute rankings. Based on these considerations, we propose LineUp--a novel and scalable visualization technique that uses bar charts. This interactive technique supports the ranking of items based on multiple heterogeneous attributes with different scales and semantics. It enables users to interactively combine attributes and flexibly refine parameters to explore the effect of changes in the attribute combination. This process can be employed to derive actionable insights as to which attributes of an item need to be modified in order for its rank to change. Additionally, through integration of slope graphs, LineUp can also be used to compare multiple alternative rankings on the same set of items, for example, over time or across different attribute combinations. We evaluate the effectiveness of the proposed multi-attribute visualization technique in a qualitative study. The study shows that users are able to successfully solve complex ranking tasks in a short period of time.
24051795	A model for structure-based comparison of many categories in small-multiple displays.
IEEE Trans Vis Comput Graph  2013Dec
Many application domains deal with multi-variate data that consist of both categorical and numerical information. Smallmultiple displays are a powerful concept for comparing such data by juxtaposition. For comparison by overlay or by explicit encoding of computed differences, however, a specification of references is necessary. In this paper, we present a formal model for defining semantically meaningful comparisons between many categories in a small-multiple display. Based on pivotized data that are hierarchically partitioned by the categories assigned to the x and y axis of the display, we propose two alternatives for structure-based comparison within this hierarchy. With an absolute reference specification, categories are compared to a fixed reference category. With a relative reference specification, in contrast, a semantic ordering of the categories is considered when comparing them either to the previous or subsequent category each. Both reference specifications can be defined at multiple levels of the hierarchy (including aggregated summaries), enabling a multitude of useful comparisons. We demonstrate the general applicability of our model in several application examples using different visualizations that compare data by overlay or explicit encoding of differences.
24051801	Hybrid-image visualization for large viewing environments.
IEEE Trans Vis Comput Graph  2013Dec
We present a first investigation into hybrid-image visualization for data analysis in large-scale viewing environments. Hybrid-image visualizations blend two different visual representations into a single static view, such that each representation can be perceived at a different viewing distance. Our work is motivated by data analysis scenarios that incorporate one or more displays with sufficiently large size and resolution to be comfortably viewed by different people from various distances. Hybrid-image visualizations can be used, in particular, to enhance overview tasks from a distance and detail-in-context tasks when standing close to the display. By using a perception-based blending approach, hybrid-image visualizations make two full-screen visualizations accessible without tracking viewers in front of a display. We contribute a design space, discuss the perceptual rationale for our work, provide examples, and introduce a set of techniques and tools to aid the design of hybrid-image visualizations.
24051805	Information visualization and proxemics: design opportunities and empirical findings.
IEEE Trans Vis Comput Graph  2013Dec
People typically interact with information visualizations using a mouse. Their physical movement, orientation, and distance to visualizations are rarely used as input. We explore how to use such spatial relations among people and visualizations (i.e., proxemics) to drive interaction with visualizations, focusing here on the spatial relations between a single user and visualizations on a large display. We implement interaction techniques that zoom and pan, query and relate, and adapt visualizations based on tracking of users' position in relation to a large high-resolution display. Alternative prototypes are tested in three user studies and compared with baseline conditions that use a mouse. Our aim is to gain empirical data on the usefulness of a range of design possibilities and to generate more ideas. Among other things, the results show promise for changing zoom level or visual representation with the user's physical distance to a large display. We discuss possible benefits and potential issues to avoid when designing information visualizations that use proxemics.
24051810	StoryFlow: tracking the evolution of stories.
IEEE Trans Vis Comput Graph  2013Dec
Storyline visualizations, which are useful in many applications, aim to illustrate the dynamic relationships between entities in a story. However, the growing complexity and scalability of stories pose great challenges for existing approaches. In this paper, we propose an efficient optimization approach to generating an aesthetically appealing storyline visualization, which effectively handles the hierarchical relationships between entities over time. The approach formulates the storyline layout as a novel hybrid optimization approach that combines discrete and continuous optimization. The discrete method generates an initial layout through the ordering and alignment of entities, and the continuous method optimizes the initial layout to produce the optimal one. The efficient approach makes real-time interactions (e.g., bundling and straightening) possible, thus enabling users to better understand and track how the story evolves. Experiments and case studies are conducted to demonstrate the effectiveness and usefulness of the optimization approach.
24051811	Visual sedimentation.
IEEE Trans Vis Comput Graph  2013Dec
We introduce Visual Sedimentation, a novel design metaphor for visualizing data streams directly inspired by the physical process of sedimentation. Visualizing data streams (e. g., Tweets, RSS, Emails) is challenging as incoming data arrive at unpredictable rates and have to remain readable. For data streams, clearly expressing chronological order while avoiding clutter, and keeping aging data visible, are important. The metaphor is drawn from the real-world sedimentation processes: objects fall due to gravity, and aggregate into strata over time. Inspired by this metaphor, data is visually depicted as falling objects using a force model to land on a surface, aggregating into strata over time. In this paper, we discuss how this metaphor addresses the specific challenge of smoothing the transition between incoming and aging data. We describe the metaphor's design space, a toolkit developed to facilitate its implementation, and example applications to a range of case studies. We then explore the generative capabilities of the design space through our toolkit. We finally illustrate creative extensions of the metaphor when applied to real streams of data.
24051812	Nanocubes for real-time exploration of spatiotemporal datasets.
IEEE Trans Vis Comput Graph  2013Dec
Consider real-time exploration of large multidimensional spatiotemporal datasets with billions of entries, each defined by a location, a time, and other attributes. Are certain attributes correlated spatially or temporally? Are there trends or outliers in the data? Answering these questions requires aggregation over arbitrary regions of the domain and attributes of the data. Many relational databases implement the well-known data cube aggregation operation, which in a sense precomputes every possible aggregate query over the database. Data cubes are sometimes assumed to take a prohibitively large amount of space, and to consequently require disk storage. In contrast, we show how to construct a data cube that fits in a modern laptop's main memory, even for billions of entries; we call this data structure a nanocube. We present algorithms to compute and query a nanocube, and show how it can be used to generate well-known visual encodings such as heatmaps, histograms, and parallel coordinate plots. When compared to exact visualizations created by scanning an entire dataset, nanocube plots have bounded screen error across a variety of scales, thanks to a hierarchical structure in space and time. We demonstrate the effectiveness of our technique on a variety of real-world datasets, and present memory, timing, and network bandwidth measurements. We find that the timings for the queries in our examples are dominated by network and user-interaction latencies.
24051813	Visualizing request-flow comparison to aid performance diagnosis in distributed systems.
IEEE Trans Vis Comput Graph  2013Dec
Distributed systems are complex to develop and administer, and performance problem diagnosis is particularly challenging. When performance degrades, the problem might be in any of the system's many components or could be a result of poor interactions among them. Recent research efforts have created tools that automatically localize the problem to a small number of potential culprits, but research is needed to understand what visualization techniques work best for helping distributed systems developers understand and explore their results. This paper compares the relative merits of three well-known visualization approaches (side-by-side, diff, and animation) in the context of presenting the results of one proven automated localization technique called request-flow comparison. Via a 26-person user study, which included real distributed systems developers, we identify the unique benefits that each approach provides for different problem types and usage modes.
24051814	Evaluation of filesystem provenance visualization tools.
IEEE Trans Vis Comput Graph  2013Dec
Having effective visualizations of filesystem provenance data is valuable for understanding its complex hierarchical structure. The most common visual representation of provenance data is the node-link diagram. While effective for understanding local activity, the node-link diagram fails to offer a high-level summary of activity and inter-relationships within the data. We present a new tool, InProv, which displays filesystem provenance with an interactive radial-based tree layout. The tool also utilizes a new time-based hierarchical node grouping method for filesystem provenance data we developed to match the user's mental model and make data exploration more intuitive. We compared InProv to a conventional node-link based tool, Orbiter, in a quantitative evaluation with real users of filesystem provenance data including provenance data experts, IT professionals, and computational scientists. We also compared in the evaluation our new node grouping method to a conventional method. The results demonstrate that InProv results in higher accuracy in identifying system activity than Orbiter with large complex data sets. The results also show that our new time-based hierarchical node grouping method improves performance in both tools, and participants found both tools significantly easier to use with the new time-based node grouping method. Subjective measures show that participants found InProv to require less mental activity, less physical activity, less work, and is less stressful to use. Our study also reveals one of the first cases of gender differences in visualization; both genders had comparable performance with InProv, but women had a significantly lower average accuracy (56%) compared to men (70%) with Orbiter.
24051816	Radial sets: interactive visual analysis of large overlapping sets.
IEEE Trans Vis Comput Graph  2013Dec
In many applications, data tables contain multi-valued attributes that often store the memberships of the table entities to multiple sets such as which languages a person masters, which skills an applicant documents, or which features a product comes with. With a growing number of entities, the resulting element-set membership matrix becomes very rich of information about how these sets overlap. Many analysis tasks targeted at set-typed data are concerned with these overlaps as salient features of such data. This paper presents Radial Sets, a novel visual technique to analyze set memberships for a large number of elements. Our technique uses frequency-based representations to enable quickly finding and analyzing different kinds of overlaps between the sets, and relating these overlaps to other attributes of the table entities. Furthermore, it enables various interactions to select elements of interest, find out if they are over-represented in specific sets or overlaps, and if they exhibit a different distribution for a specific attribute compared to the rest of the elements. These interactions allow formulating highly-expressive visual queries on the elements in terms of their set memberships and attribute values. As we demonstrate via two usage scenarios, Radial Sets enable revealing and analyzing a multitude of overlapping patterns between large sets, beyond the limits of state-of-the-art techniques.
24051821	Variant view: visualizing sequence variants in their gene context.
IEEE Trans Vis Comput Graph  2013Dec
Scientists use DNA sequence differences between an individual's genome and a standard reference genome to study the genetic basis of disease. Such differences are called sequence variants, and determining their impact in the cell is difficult because it requires reasoning about both the type and location of the variant across several levels of biological context. In this design study, we worked with four analysts to design a visualization tool supporting variant impact assessment for three different tasks. We contribute data and task abstractions for the problem of variant impact assessment, and the carefully justified design and implementation of the Variant View tool. Variant View features an information-dense visual encoding that provides maximal information at the overview level, in contrast to the extensive navigation required by currently-prevalent genome browsers. We provide initial evidence that the tool simplified and accelerated workflows for these three tasks through three case studies. Finally, we reflect on the lessons learned in creating and refining data and task abstractions that allow for concise overviews of sprawling information spaces that can reduce or remove the need for the memory-intensive use of navigation.
24051822	DiffAni: visualizing dynamic graphs with a hybrid of difference maps and animation.
IEEE Trans Vis Comput Graph  2013Dec
Visualization of dynamically changing networks (graphs) is a significant challenge for researchers. Previous work has experimentally compared animation, small multiples, and other techniques, and found trade-offs between these. One potential way to avoid such trade-offs is to combine previous techniques in a hybrid visualization. We present two taxonomies of visualizations of dynamic graphs: one of non-hybrid techniques, and one of hybrid techniques. We also describe a prototype, called DiffAni, that allows a graph to be visualized as a sequence of three kinds of tiles: diff tiles that show difference maps over some time interval, animation tiles that show the evolution of the graph over some time interval, and small multiple tiles that show the graph state at an individual time slice. This sequence of tiles is ordered by time and covers all time slices in the data. An experimental evaluation of DiffAni shows that our hybrid approach has advantages over non-hybrid techniques in certain cases.
24051823	Visualizing change over time using dynamic hierarchies: TreeVersity2 and the StemView.
IEEE Trans Vis Comput Graph  2013Dec
To analyze data such as the US Federal Budget or characteristics of the student population of a University it is common to look for changes over time. This task can be made easier and more fruitful if the analysis is performed by grouping by attributes, such as by Agencies, Bureaus and Accounts for the Budget, or Ethnicity, Gender and Major in a University. We present TreeVersity2, a web based interactive data visualization tool that allows users to analyze change in datasets by creating dynamic hierarchies based on the data attributes. TreeVersity2 introduces a novel space filling visualization (StemView) to represent change in trees at multiple levels--not just at the leaf level. With this visualization users can explore absolute and relative changes, created and removed nodes, and each node's actual values, while maintaining the context of the tree. In addition, TreeVersity2 provides overviews of change over the entire time period, and a reporting tool that lists outliers in textual form, which helps users identify the major changes in the data without having to manually setup filters. We validated TreeVersity2 with 12 case studies with organizations as diverse as the National Cancer Institute, Federal Drug Administration, Department of Transportation, Office of the Bursar of the University of Maryland, or eBay. Our case studies demonstrated that TreeVersity2 is flexible enough to be used in different domains and provide useful insights for the data owners. A TreeVersity2 demo can be found at https://treeversity.cattlab.umd.edu.
24051824	Visual compression of workflow visualizations with automated detection of macro motifs.
IEEE Trans Vis Comput Graph  2013Dec
This paper is concerned with the creation of 'macros' in workflow visualization as a support tool to increase the efficiency of data curation tasks. We propose computation of candidate macros based on their usage in large collections of workflows in data repositories. We describe an efficient algorithm for extracting macro motifs from workflow graphs. We discovered that the state transition information, used to identify macro candidates, characterizes the structural pattern of the macro and can be harnessed as part of the visual design of the corresponding macro glyph. This facilitates partial automation and consistency in glyph design applicable to a large set of macro glyphs. We tested this approach against a repository of biological data holding some 9,670 workflows and found that the algorithmically generated candidate macros are in keeping with domain expert expectations.
24051825	Automatic layout of structured hierarchical reports.
IEEE Trans Vis Comput Graph  2013Dec
Domain-specific database applications tend to contain a sizable number of table-, form-, and report-style views that must each be designed and maintained by a software developer. A significant part of this job is the necessary tweaking of low-level presentation details such as label placements, text field dimensions, list or table styles, and so on. In this paper, we present a horizontally constrained layout management algorithm that automates the display of structured hierarchical data using the traditional visual idioms of hand-designed database UIs: tables, multi-column forms, and outline-style indented lists. We compare our system with pure outline and nested table layouts with respect to space efficiency and readability, the latter with an online user study on 27 subjects. Our layouts are 3.9 and 1.6 times more compact on average than outline layouts and horizontally unconstrained table layouts, respectively, and are as readable as table layouts even for large datasets.
24051827	GPLOM: the generalized plot matrix for visualizing multidimensional multivariate data.
IEEE Trans Vis Comput Graph  2013Dec
Scatterplot matrices (SPLOMs), parallel coordinates, and glyphs can all be used to visualize the multiple continuous variables (i.e., dependent variables or measures) in multidimensional multivariate data. However, these techniques are not well suited to visualizing many categorical variables (i.e., independent variables or dimensions). To visualize multiple categorical variables, 'hierarchical axes' that 'stack dimensions' have been used in systems like Polaris and Tableau. However, this approach does not scale well beyond a small number of categorical variables. Emerson et al. [8] extend the matrix paradigm of the SPLOM to simultaneously visualize several categorical and continuous variables, displaying many kinds of charts in the matrix depending on the kinds of variables involved. We propose a variant of their technique, called the Generalized Plot Matrix (GPLOM). The GPLOM restricts Emerson et al.'s technique to only three kinds of charts (scatterplots for pairs of continuous variables, heatmaps for pairs of categorical variables, and barcharts for pairings of categorical and continuous variable), in an effort to make it easier to understand. At the same time, the GPLOM extends Emerson et al.'s work by demonstrating interactive techniques suited to the matrix of charts. We discuss the visual design and interactive features of our GPLOM prototype, including a textual search feature allowing users to quickly locate values or variables by name. We also present a user study that compared performance with Tableau and our GPLOM prototype, that found that GPLOM is significantly faster in certain cases, and not significantly slower in other cases.
24051828	Orthographic star coordinates.
IEEE Trans Vis Comput Graph  2013Dec
Star coordinates is a popular projection technique from an nD data space to a 2D/3D visualization domain. It is defined by setting n coordinate axes in the visualization domain. Since it generally defines an affine projection, strong distortions can occur: an nD sphere can be mapped to an ellipse of arbitrary size and aspect ratio. We propose to restrict star coordinates to orthographic projections which map an nD sphere of radius r to a 2D circle of radius r. We achieve this by formulating conditions for the coordinate axes to define orthographic projections, and by running a repeated non-linear optimization in the background of every modification of the coordinate axes. This way, we define a number of orthographic interaction concepts as well as orthographic data tour sequences: a scatterplot tour, a principle component tour, and a grand tour. All concepts are illustrated and evaluated with synthetic and real data.
24051829	Dimension projection matrix/tree: interactive subspace visual exploration and analysis of high dimensional data.
IEEE Trans Vis Comput Graph  2013Dec
For high-dimensional data, this work proposes two novel visual exploration methods to gain insights into the data aspect and the dimension aspect of the data. The first is a Dimension Projection Matrix, as an extension of a scatterplot matrix. In the matrix, each row or column represents a group of dimensions, and each cell shows a dimension projection (such as MDS) of the data with the corresponding dimensions. The second is a Dimension Projection Tree, where every node is either a dimension projection plot or a Dimension Projection Matrix. Nodes are connected with links and each child node in the tree covers a subset of the parent node's dimensions or a subset of the parent node's data items. While the tree nodes visualize the subspaces of dimensions or subsets of the data items under exploration, the matrix nodes enable cross-comparison between different combinations of subspaces. Both Dimension Projection Matrix and Dimension Project Tree can be constructed algorithmically through automation, or manually through user interaction. Our implementation enables interactions such as drilling down to explore different levels of the data, merging or splitting the subspaces to adjust the matrix, and applying brushing to select data clusters. Our method enables simultaneously exploring data correlation and dimension correlation for data with high dimensions.
24051833	Detecting symmetry in scalar fields using augmented extremum graphs.
IEEE Trans Vis Comput Graph  2013Dec
Visualizing symmetric patterns in the data often helps the domain scientists make important observations and gain insights about the underlying experiment. Detecting symmetry in scalar fields is a nascent area of research and existing methods that detect symmetry are either not robust in the presence of noise or computationally costly. We propose a data structure called the augmented extremum graph and use it to design a novel symmetry detection method based on robust estimation of distances. The augmented extremum graph captures both topological and geometric information of the scalar field and enables robust and computationally efficient detection of symmetry. We apply the proposed method to detect symmetries in cryo-electron microscopy datasets and the experiments demonstrate that the algorithm is capable of detecting symmetry even in the presence of significant noise. We describe novel applications that use the detected symmetry to enhance visualization of scalar field data and facilitate their exploration.
24051835	An information-aware framework for exploring multivariate data sets.
IEEE Trans Vis Comput Graph  2013Dec
Information theory provides a theoretical framework for measuring information content for an observed variable, and has attracted much attention from visualization researchers for its ability to quantify saliency and similarity among variables. In this paper, we present a new approach towards building an exploration framework based on information theory to guide the users through the multivariate data exploration process. In our framework, we compute the total entropy of the multivariate data set and identify the contribution of individual variables to the total entropy. The variables are classified into groups based on a novel graph model where a node represents a variable and the links encode the mutual information shared between the variables. The variables inside the groups are analyzed for their representativeness and an information based importance is assigned. We exploit specific information metrics to analyze the relationship between the variables and use the metrics to choose isocontours of selected variables. For a chosen group of points, parallel coordinates plots (PCP) are used to show the states of the variables and provide an interface for the user to select values of interest. Experiments with different data sets reveal the effectiveness of our proposed framework in depicting the interesting regions of the data sets taking into account the interaction among the variables.
24051849	A systematic review on the practice of evaluating visualization.
IEEE Trans Vis Comput Graph  2013Dec
We present an assessment of the state and historic development of evaluation practices as reported in papers published at the IEEE Visualization conference. Our goal is to reflect on a meta-level about evaluation in our community through a systematic understanding of the characteristics and goals of presented evaluations. For this purpose we conducted a systematic review of ten years of evaluations in the published papers using and extending a coding scheme previously established by Lam et al. [2012]. The results of our review include an overview of the most common evaluation goals in the community, how they evolved over time, and how they contrast or align to those of the IEEE Information Visualization conference. In particular, we found that evaluations specific to assessing resulting images and algorithm performance are the most prevalent (with consistently 80-90% of all papers since 1997). However, especially over the last six years there is a steady increase in evaluation methods that include participants, either by evaluating their performances and subjective feedback or by evaluating their work practices and their improved analysis and reasoning capabilities using visual tools. Up to 2010, this trend in the IEEE Visualization conference was much more pronounced than in the IEEE Information Visualization conference which only showed an increasing percentage of evaluation through user performance and experience testing. Since 2011, however, also papers in IEEE Information Visualization show such an increase of evaluations of work practices and analysis as well as reasoning using visual tools. Further, we found that generally the studies reporting requirements analyses and domain-specific work practices are too informally reported which hinders cross-comparison and lowers external validity.
24008998	Some experiences and opportunities for big data in translational research.
Genet. Med. 20130905 2013Oct
Health care has become increasingly information intensive. The advent of genomic data, integrated into patient care, significantly accelerates the complexity and amount of clinical data. Translational research in the present day increasingly embraces new biomedical discovery in this data-intensive world, thus entering the domain of "big data." The Electronic Medical Records and Genomics consortium has taught us many lessons, while simultaneously advances in commodity computing methods enable the academic community to affordably manage and process big data. Although great promise can emerge from the adoption of big data methods and philosophy, the heterogeneity and complexity of clinical data, in particular, pose additional challenges for big data inferencing and clinical application. However, the ultimate comparability and consistency of heterogeneous clinical information sources can be enhanced by existing and emerging data standards, which promise to bring order to clinical data chaos. Meaningful Use data standards in particular have already simplified the task of identifying clinical phenotyping patterns in electronic health records.
23193319	Histology image retrieval in optimised multi-feature spaces.
IEEE J Biomed Health Inform 20121115 2013Jan
Content based histology image retrieval systems have shown great potential in supporting decision making in clinical activities, teaching, and biological research. In content based image retrieval, feature combination plays a key role. It aims at enhancing the descriptive power of visual features corresponding to semantically meaningful queries. It is particularly valuable in histology image analysis where intelligent mechanisms are needed for interpreting varying tissue composition and architecture into histological concepts. This paper presents an approach to automatically combine heterogeneous visual features for histology image retrieval. The aim is to obtain the most representative fusion model for a particular keyword that is associated to multiple query images. The core of this approach is a multi-objective learning method, which aims to understand an optimal visual-semantic matching function by jointly considering the different preferences of the group of query images. The task is posed as an optimisation problem, and a multi-objective optimisation strategy is employed in order to handle potential contradictions in the query images associated to the same keyword. Experiments were performed on two different collections of histology images. The results show that it is possible to improve a system for content based histology image retrieval by using an appropriately defined multi-feature fusion model, which takes careful consideration of the structure and distribution of visual features.
24730273	Recombinant protein-based nanoscale biomemory devices.
J Nanosci Nanotechnol  2014Jan
Biomolecular computing devices that are based on the properties of biomolecular activities offer a unique possibility for constructing new computing structures. A new concept of using various biomolecules has been proposed in order to develop a protein-based memory device that is capable of switching physical properties when electrical input signals are applied to perform memory switching. To clarify the proposed concept, redox protein is immobilized on Au nanoelectrodes to catalyze reversible reactions of redox-active molecules, which is controlled electrochemically and reversibly converted between its ON/OFF states. In this review, we summarize recent research towards developing nanoscale biomemory devices including design, synthesis, fabrication, and functionalization based on the proposed concept. At first we analyze the memory function properties of the proposed device at bulk material level and then explain the WORM (write-once-read-many times) nature of the device, later we extend the analysis to multi-bit and multi-level storage functions, and then we focus the developments in nanoscale biomemory devices based on the electron transport of redox molecules to the underlying Au patterned surface. The developed device operates at very low voltages and has good stability and excellent reversibility, proving to be a promising platform for future memory devices.
23769157	Assessing searches in NICE single technology appraisals: practice and checklist.
Int J Technol Assess Health Care 20130617 2013Jul
No guidelines exist in the approach that Evidence Review Groups (ERGs) should take to appraise search methodologies in the manufacturer's submission (MS) in Single Technology Appraisals (STA). As a result, ERGs are left to appraise searches using their own approach. This study investigates the limitations of manufacturers' search methodologies as critiqued by ergs in published sta reports and to provide a recommended checklist. Limitations from search critiques in 83 ERG reports published in the NIHR Web site between 2006 and May 2011 were extracted. The limitations were grouped into themes. Comparisons were made between limitations reported in the clinical effectiveness versus cost-effectiveness searches. Twelve themes were identified, six relating to the search strategy, source, limits, filters, translation, reporting, and missing studies. The search strategy theme contained the most limitations. Missing studies were frequently found by the ERG group in the clinical effectiveness searches. The omission of searches by manufacturers for unpublished and ongoing trials was frequently reported by the ERG. By contrast, failure of the manufacturer to report strategies was the most common limitation in the cost-effectiveness searches. Themes with the most frequent limitations in both types of searches are search strategy, reporting and source. It is recommended that a checklist that has reporting, source and search strategy elements be used in the appraisal of manufacturer's searches during the STA process.
23697624	Evaluating ecommerce websites cognitive efficiency: an integrative framework based on data envelopment analysis.
Appl Ergon 20130520 2013Nov
This paper presents an integrative framework to evaluate ecommerce website efficiency from the user viewpoint using Data Envelopment Analysis (DEA). This framework is inspired by concepts driven from theories of information processing and cognition and considers the website efficiency as a measure of its quality and performance. When the users interact with the website interfaces to perform a task, they are involved in a cognitive effort, sustaining a cognitive cost to search, interpret and process information, and experiencing either a sense of satisfaction or dissatisfaction for that. The amount of ambiguity and uncertainty, and the search (over-)time during navigation that they perceive determine the effort size - and, as a consequence, the cognitive cost amount - they have to bear to perform their task. On the contrary, task performing and result achievement provide the users with cognitive benefits, making interaction with the website potentially attractive, satisfying, and useful. In total, 9 variables are measured, classified in a set of 3 website macro-dimensions (user experience, site navigability and structure). The framework is implemented to compare 52 ecommerce websites that sell products in the information technology and media market. A stepwise regression is performed to assess the influence of cognitive costs and benefits that mostly affect website efficiency.
24581901	Systematic search and review procedures: results of the International Collaboration on Mild Traumatic Brain Injury Prognosis.
Arch Phys Med Rehabil  2014Mar
To update the last best-evidence synthesis conducted by the World Health Organization Collaborating Centre for Neurotrauma, Prevention, Management and Rehabilitation in 2002; and to describe the course, identify prognostic factors, determine long-term sequelae, identify effects of interventions for mild traumatic brain injury (MTBI), identify knowledge gaps in the literature, and make recommendations for future research. MEDLINE, Embase, PsycINFO, Cumulative Index to Nursing and Allied Health, and SPORTDiscus were searched between 2001 and 2012. Inclusion criteria included published peer-reviewed articles in English and 5 other languages. References were also identified from relevant reviews and meta-analyses and the bibliographies of eligible articles. Controlled trials and cohort and case-control studies were selected according to predefined inclusion/exclusion criteria. Studies had to have at least 30 MTBI cases and assess outcomes relevant to prognosis after MTBI. Eligible studies were critically appraised using modified Scottish Intercollegiate Guidelines Network (SIGN) criteria. Two reviewers independently reviewed each study and extracted data from accepted articles (ie, with a low risk of bias) into evidence tables. The evidence was synthesized qualitatively according to modified SIGN criteria and prioritized according to design as exploratory or confirmatory. The evidence was organized into separate articles according to population (eg, adults, children, and athletes) and outcomes (eg, risk of dementia after MTBI). After 77,914 records were screened, 299 articles were eligible and reviewed. Of these, 101 (34%) were accepted as scientifically admissible and form the basis of our findings, which are organized into 10 articles in this supplement. These reviews present the best available evidence on MTBI prognosis, but more research is needed.
24142185	Patients' acceptance towards a web-based personal health record system: an empirical study in Taiwan.
Int J Environ Res Public Health 20131017 2013Oct
The health care sector has become increasingly interested in developing personal health record (PHR) systems as an Internet-based telehealthcare implementation to improve the quality and decrease the cost of care. However, the factors that influence patients' intention to use PHR systems remain unclear. Based on physicians' therapeutic expertise, we implemented a web-based infertile PHR system and proposed an extended Technology Acceptance Model (TAM) that integrates the physician-patient relationship (PPR) construct into TAM's original perceived ease of use (PEOU) and perceived usefulness (PU) constructs to explore which factors will influence the behavioral intentions (BI) of infertile patients to use the PHR. From ninety participants from a medical center, 50 valid responses to a self-rating questionnaire were collected, yielding a response rate of 55.56%. The partial least squares (PLS) technique was used to assess the causal relationships that were hypothesized in the extended model. The results indicate that infertile patients expressed a moderately high intention to use the PHR system. The PPR and PU of patients had significant effects on their BI to use PHR, whereas the PEOU indirectly affected the patients' BI through the PU. This investigation confirms that PPR can have a critical role in shaping patients' perceptions of the use of healthcare information technologies. Hence, we suggest that hospitals should promote the potential usefulness of PHR and improve the quality of the physician-patient relationship to increase patients' intention of using PHR.
23563792	A novel similarity learning method via relative comparison for content-based medical image retrieval.
J Digit Imaging  2013Oct
Nowadays, the huge volume of medical images represents an enormous challenge towards health-care organizations, as it is often hard for clinicians and researchers to manage, access, and share the image database easily. Content-based medical image retrieval (CBMIR) techniques are employed to facilitate the above process. It is known that a few concrete factors, including visual attributes extracted from images, measures encoding the similarity between images, user interaction, etc. play important roles in determining the retrieval performance. This paper concentrates on the similarity learning problem of CBMIR. A novel similarity learning paradigm is proposed via relative comparison, and a large database composed of 5,000 images is utilized to evaluate the retrieval performance. Extensive experimental results and comprehensive statistical analysis demonstrate the superiority of adopting the newly introduced learning paradigm, compared with several conventional supervised and semi-supervised similarity learning methods, in the presented CBMIR application.
23817629	Cross-sectional relatedness between sentences in breast radiology reports: development of an SVM classifier and evaluation against annotations of five breast radiologists.
J Digit Imaging  2013Oct
Introduce the notion of cross-sectional relatedness as an informational dependence relation between sentences in the conclusion section of a breast radiology report and sentences in the findings section of the same report. Assess inter-rater agreement of breast radiologists. Develop and evaluate a support vector machine (SVM) classifier for automatically detecting cross-sectional relatedness. A standard reference is manually created from 444 breast radiology reports by the first author. A subset of 37 reports is annotated by five breast radiologists. Inter-rater agreement is computed among their annotations and standard reference. Thirteen numerical features are developed to characterize pairs of sentences; the optimal feature set is sought through forward selection. Inter-rater agreement is F-measure 0.623. SVM classifier has F-measure of 0.699 in the 12-fold cross-validation protocol against standard reference. Report length does not correlate with the classifier's performance (correlation coefficient = -0.073). SVM classifier has average F-measure of 0.505 against annotations by breast radiologists. Mediocre inter-rater agreement is possibly caused by: (1) definition is insufficiently actionable, (2) fine-grained nature of cross-sectional relatedness on sentence level, instead of, for instance, on paragraph level, and (3) higher-than-average complexity of 37-report sample. SVM classifier performs better against standard reference than against breast radiologists's annotations. This is supportive of (3). SVM's performance on standard reference is satisfactory. Since optimal feature set is not breast specific, results may transfer to non-breast anatomies. Applications include a smart report viewing environment and data mining.
23949573	Storing and interpreting genomic information in widely deployed electronic health record systems.
Genet. Med. 20130815 2013Oct
Electronic health record systems are in widespread use but currently have very limited genomic capabilities. Electronic health record systems are a logical place for patient genomic information to be stored and used for decision support and improvement of patient care. Genomic data and their automated interpretation are very challenging for electronic health record software development because they are substantially different than other kinds of electronic health record data and decision support. Those differences, the resulting challenges, and possible solutions are reviewed in this article.
24022476	Search strategies to identify diagnostic accuracy studies in MEDLINE and EMBASE.
Cochrane Database Syst Rev 20130911 2013
A systematic and extensive search for as many eligible studies as possible is essential in any systematic review. When searching for diagnostic test accuracy (DTA) studies in bibliographic databases, it is recommended that terms for disease (target condition) are combined with terms for the diagnostic test (index test). Researchers have developed methodological filters to try to increase the precision of these searches. These consist of text words and database indexing terms and would be added to the target condition and index test searches.Efficiently identifying reports of DTA studies presents challenges because the methods are often not well reported in their titles and abstracts, suitable indexing terms may not be available and relevant indexing terms do not seem to be consistently assigned. A consequence of using search filters to identify records for diagnostic reviews is that relevant studies might be missed, while the number of irrelevant studies that need to be assessed may not be reduced. The current guidance for Cochrane DTA reviews recommends against the addition of a methodological search filter to target condition and index test search, as the only search approach. To systematically review empirical studies that report the development or evaluation, or both, of methodological search filters designed to retrieve DTA studies in MEDLINE and EMBASE. We searched MEDLINE (1950 to week 1 November 2012); EMBASE (1980 to 2012 Week 48); the Cochrane Methodology Register (Issue 3, 2012); ISI Web of Science (11 January 2013); PsycINFO (13 March 2013); Library and Information Science Abstracts (LISA) (31 May 2010); and Library, Information Science &amp; Technology Abstracts (LISTA) (13 March 2013). We undertook citation searches on Web of Science, checked the reference lists of relevant studies, and searched the Search Filters Resource website of the InterTASC Information Specialists' Sub-Group (ISSG). Studies reporting the development or evaluation, or both, of a MEDLINE or EMBASE search filter aimed at retrieving DTA studies, which reported a measure of the filter's performance were eligible. The main outcome was a measure of filter performance, such as sensitivity or precision. We extracted data on the identification of the reference set (including the gold standard and, if used, the non-gold standard records), how the reference set was used and any limitations, the identification and combination of the search terms in the filters, internal and external validity testing, the number of filters evaluated, the date the study was conducted, the date the searches were completed, and the databases and search interfaces used. Where 2 x 2 data were available on filter performance, we used these to calculate sensitivity, specificity, precision and Number Needed to Read (NNR), and 95% confidence intervals (CIs). We compared the performance of a filter as reported by the original development study and any subsequent studies that evaluated the same filter. Ninteen studies were included, reporting on 57 MEDLINE filters and 13 EMBASE filters. Thirty MEDLINE and four EMBASE filters were tested in an evaluation study where the performance of one or more filters was tested against one or more gold standards. The reported outcome measures varied. Some studies reported specificity as well as sensitivity if a reference set containing non-gold standard records in addition to gold standard records was used. In some cases, the original development study did not report any performance data on the filters. Original performance from the development study was not available for 17 filters that were subsequently tested in evaluation studies. All 19 studies reported the sensitivity of the filters that they developed or evaluated, nine studies reported the specificities and 14 studies reported the precision.No filter which had original performance data from its development study, and was subsequently tested in an evaluation study, had what we defined a priori as acceptable sensitivity (&gt; 90%) and precision (&gt; 10%). In studies that developed MEDLINE filters that were evaluated in another study (n = 13), the sensitivity ranged from 55% to 100% (median 86%) and specificity from 73% to 98% (median 95%). Estimates of performance were lower in eight studies that evaluated the same 13 MEDLINE filters, with sensitivities ranging from 14% to 100% (median 73%) and specificities ranging from 15% to 96% (median 81%). Precision ranged from 1.1% to 40% (median 9.5%) in studies that developed MEDLINE filters and from 0.2% to 16.7% (median 4%) in studies that evaluated these filters. A similar range of specificities and precision were reported amongst the evaluation studies for MEDLINE filters without an original performance measure. Sensitivities ranged from 31% to 100% (median 71%), specificity ranged from 13% to 90% (median 55.5%) and precision from 1.0% to 11.0% (median 3.35%).For the EMBASE filters, the original sensitivities reported in two development studies ranged from 74% to 100% (median 90%) for three filters, and precision ranged from 1.2% to 17.6% (median 3.7%). Evaluation studies of these filters had sensitivities from 72% to 97% (median 86%) and precision from 1.2% to 9% (median 3.7%). The performance of EMBASE search filters in development and evaluation studies were more alike than the performance of MEDLINE filters in development and evaluation studies. None of the EMBASE filters in either type of study had a sensitivity above 90% and precision above 10%. None of the current methodological filters designed to identify reports of primary DTA studies in MEDLINE or EMBASE combine sufficiently high sensitivity, required for systematic reviews, with a reasonable degree of precision. This finding supports the current recommendation in the Cochrane Handbook for Systematic Reviews of Diagnostic Test Accuracy that the combination of methodological filter search terms with terms for the index test and target condition should not be used as the only approach when conducting formal searches to inform systematic reviews of DTA.
23990871	Morpheme matching based text tokenization for a scarce resourced language.
PLoS ONE 20130821 2013
Text tokenization is a fundamental pre-processing step for almost all the information processing applications. This task is nontrivial for the scarce resourced languages such as Urdu, as there is inconsistent use of space between words. In this paper a morpheme matching based approach has been proposed for Urdu text tokenization, along with some other algorithms to solve the additional issues of boundary detection of compound words, affixation, reduplication, names and abbreviations. This study resulted into 97.28% precision, 93.71% recall, and 95.46% F1-measure; while tokenizing a corpus of 57000 words by using a morpheme list with 6400 entries.
23913014	Secondary parenchymal and vascular changes after middle cerebral artery stroke in children.
Neuroradiology 20130803 2013Oct
Ischemic brain lesions might present with unexpected increased signal intensity at MR angiography within the ischemic lesion and secondary parenchymal changes in regions distal to the ischemia itself. We retrospectively investigated the rate and time course of vascular and parenchymal changes in children with isolated middle cerebral artery (MCA) stroke. Twelve children (mean age at stroke onset 4.8 years, range 0.8-15 years, six females, seven right MCA strokes) suffering from a first ever acute isolated MCA stroke had repeated MR scans (mean scan number, 3.5; range 2-6; mean follow-up, 11 months; range 0.5-24 months). Ipsilaterally to MCA stroke, we recorded increased vessel signal at MR angiography during first to fourth day in 4/7 children (all had MCA recanalization), corticospinal tract cytotoxic-like edema during second day to second month in 7/11 (three children with globus pallidum ischemia had concomitant substantia nigra changes during second to third week), corticospinal tract T2 abnormalities from fifth day onwards in 9/12, focal thalamic cytotoxic-like edema during fifth day to first month in 5/8, focal thalamic T2 hyperintensity during sixth day to third week in 2/4, and faint T2 hypointensity from second month in 7/10 children. Vascular and secondary parenchymal changes, likely due to luxury perfusion, Wallerian, retrograde, or trans-synaptic degeneration, are common in pediatric MCA stroke population. They might mimic new ischemic lesions or suggest conditions different from stroke leading to diagnostic pitfalls and inappropriate treatment.
23912507	Neuroinformatics Database (NiDB)--a modular, portable database for the storage, analysis, and sharing of neuroimaging data.
Neuroinformatics  2013Oct
We present a modular, high performance, open-source database system that incorporates popular neuroimaging database features with novel peer-to-peer sharing, and a simple installation. An increasing number of imaging centers have created a massive amount of neuroimaging data since fMRI became popular more than 20 years ago, with much of that data unshared. The Neuroinformatics Database (NiDB) provides a stable platform to store and manipulate neuroimaging data and addresses several of the impediments to data sharing presented by the INCF Task Force on Neuroimaging Datasharing, including 1) motivation to share data, 2) technical issues, and 3) standards development. NiDB solves these problems by 1) minimizing PHI use, providing a cost effective simple locally stored platform, 2) storing and associating all data (including genome) with a subject and creating a peer-to-peer sharing model, and 3) defining a sample, normalized definition of a data storage structure that is used in NiDB. NiDB not only simplifies the local storage and analysis of neuroimaging data, but also enables simple sharing of raw data and analysis methods, which may encourage further sharing.
22464414	Two methods provide similar signals for the need to update systematic reviews.
J Clin Epidemiol 20120329 2012Jun
Apply and compare two methods that identify signals for the need to update systematic reviews, using three Evidence-based Practice Center reports on omega-3 fatty acids as test cases. We applied the RAND method, which uses domain (subject matter) expert guidance, and a modified Ottawa method, which uses quantitative and qualitative signals. For both methods, we conducted focused electronic literature searches of recent studies using the key terms from the original reports. We assessed the agreement between the methods and qualitatively assessed the merits of each system. Agreement between the two methods was "substantial" or better (kappa&gt;0.62) in three of the four systematic reviews. Overall agreement between the methods was "substantial" (kappa=0.64, 95% confidence interval [CI] 0.45-0.83). The RAND and modified Ottawa methods appear to provide similar signals for the possible need to update systematic reviews in this pilot study. Future evaluation with a broader range of clinical topics and eventual comparisons between signals to update reports and the results of full evidence review updates will be needed. We propose a hybrid approach combining the best features of both methods, which should allow efficient review and assessment of the need to update.
23200683	Image domain propeller fast spin echo.
Magn Reson Imaging 20121130 2013Apr
A new pulse sequence for high-resolution T2-weighted (T2-w) imaging is proposed - image domain propeller fast spin echo (iProp-FSE). Similar to the T2-w PROPELLER sequence, iProp-FSE acquires data in a segmented fashion, as blades that are acquired in multiple TRs. However, the iProp-FSE blades are formed in the image domain instead of in the k-space domain. Each iProp-FSE blade resembles a single-shot fast spin echo (SSFSE) sequence with a very narrow phase-encoding field of view (FOV), after which N rotated blade replicas yield the final full circular FOV. Our method of combining the image domain blade data to a full FOV image is detailed, and optimal choices of phase-encoding FOVs and receiver bandwidths were evaluated on phantom and volunteers. The results suggest that a phase FOV of 15-20%, a receiver bandwidth of ±32-63 kHz and a subsequent readout time of about 300 ms provide a good tradeoff between signal-to-noise ratio (SNR) efficiency and T2 blurring. Comparisons between iProp-FSE, Cartesian FSE and PROPELLER were made on single-slice axial brain data, showing similar T2-w tissue contrast and SNR with great anatomical conspicuity at similar scan times - without colored noise or streaks from motion. A new slice interleaving order is also proposed to improve the multislice capabilities of iProp-FSE.
23553269	A system for extracting study design parameters from nutritional genomics abstracts.
J Integr Bioinform 20130404 2013
The extraction of study design parameters from biomedical journal articles is an important problem in natural language processing (NLP). Such parameters define the characteristics of a study, such as the duration, the number of subjects, and their profile. Here we present a system for extracting study design parameters from sentences in article abstracts. This system will be used as a component of a larger system for creating nutrigenomics networks from articles in the nutritional genomics domain. The algorithms presented consist of manually designed rules expressed either as regular expressions or in terms of sentence parse structure. A number of filters and NLP tools are also utilized within a pipelined algorithmic framework. Using this novel approach, our system performs extraction at a finer level of granularity than comparable systems, while generating results that surpass the current state of the art.
23715213	Aggregate health data in the United States: steps toward a public good.
Health Informatics J  2013Jun
The rise of electronic medical records promotes the collection and aggregation of medical data. These data have tremendous potential utility for health policy and public health; yet there are gaps in the scholarly literature. No articles in the medical or legal literature have mapped the "information flows" from patient to database, and commentary has focused more on privacy than on data's social value and incentives for production. Utilizing short case studies of data flows, I show that ample data exist, much of them are available online through government websites or hospital trade associations. However, available information comes from billing records rather than medical records. Turning to legal and policy recommendations for better provision, I note that weak intellectual property law has ironically led to stronger control over health data through private contracts and technological barriers, as these methods of protection lack any exceptions for noncommercial use. I conclude with a series of policy proposals to make data more available.
24078798	Query-biased preview over outsourced and encrypted data.
ScientificWorldJournal 20130902 2013
For both convenience and security, more and more users encrypt their sensitive data before outsourcing it to a third party such as cloud storage service. However, searching for the desired documents becomes problematic since it is costly to download and decrypt each possibly needed document to check if it contains the desired content. An informative query-biased preview feature, as applied in modern search engine, could help the users to learn about the content without downloading the entire document. However, when the data are encrypted, securely extracting a keyword-in-context snippet from the data as a preview becomes a challenge. Based on private information retrieval protocol and the core concept of searchable encryption, we propose a single-server and two-round solution to securely obtain a query-biased snippet over the encrypted data from the server. We achieve this novel result by making a document (plaintext) previewable under any cryptosystem and constructing a secure index to support dynamic computation for a best matched snippet when queried by some keywords. For each document, the scheme has O(d) storage complexity and O(log(d/s) + s + d/s) communication complexity, where d is the document size and s is the snippet length.
24660599	Expert knowledge maps for knowledge management: a case study in Traditional Chinese Medicine research.
J Tradit Chin Med  2013Oct
To design a model to capture information on the state and trends of knowledge creation, at both an individual and an organizational level, in order to enhance knowledge management. We designed a graph-theoretic knowledge model, the expert knowledge map (EKM), based on literature-based annotation. A case study in the domain of Traditional Chinese Medicine research was used to illustrate the usefulness of the model. The EKM successfully captured various aspects of knowledge and enhanced knowledge management within the case-study organization through the provision of knowledge graphs, expert graphs, and expert-knowledge biography. Our model could help to reveal the hot topics, trends, and products of the research done by an organization. It can potentially be used to facilitate knowledge learning, sharing and decision-making among researchers, academicians, students, and administrators of organizations.
23590742	Big bad data: law, public health, and biomedical databases.
J Law Med Ethics  2013Mar
The accelerating adoption of electronic health record (EHR) systems will have far-reaching implications for public health research and surveillance, which in turn could lead to changes in public policy, statutes, and regulations. The public health benefits of EHR use can be significant. However, researchers and analysts who rely on EHR data must proceed with caution and understand the potential limitations of EHRs. Because of clinicians' workloads, poor user-interface design, and other factors, EHR data can be erroneous, miscoded, fragmented, and incomplete. In addition, public health findings can be tainted by the problems of selection bias, confounding bias, and measurement bias. These flaws may become all the more troubling and important in an era of electronic "big data," in which a massive amount of information is processed automatically, without human checks. Thus, we conclude the paper by outlining several regulatory and other interventions to address data analysis difficulties that could result in invalid conclusions and unsound public health policies.
23739353	Virtual physiological human and its role for advanced pHealth service provision.
Stud Health Technol Inform  2013
This paper provides an introduction to some of the most important challenges that may occur when introducing the principle of Personal Portable Devices for providing information in terms of Big Data on the one hand, and the concept of the Virtual Physiological Human on the other. Both concepts can be applied to exploit their specific capability to collect and record personal health data of different levels of granularity into processes of personalized health service provision. The paper thus analyzes Big Data approaches and their capability to provide information for personalized service provision, and the same goes for the Virtual Physiological Human as such. But it is not only devices, concepts, models, and strategies that are involved in personalized health care as well as welfare and wellness service provision to human beings - it is the human being himself, too. This paper addresses technological and methodological aspects of using large amounts of data whereas another paper submitted to this conference will bring forward the aspects of applied sensor and device technology in relation to decision support and decision making for pHealth services.
23739354	Towards large-scale data analysis: challenges in the design of portable systems and use of Cloud computing.
Stud Health Technol Inform  2013
Portable systems and global communications open a broad spectrum for new health applications. In the framework of electrophysiological applications, several challenges are faced when developing portable systems embedded in Cloud computing services. In order to facilitate new developers in this area based on our experience, five areas of interest are presented in this paper where strategies can be applied for improving the performance of portable systems: transducer and conditioning, processing, wireless communications, battery and power management. Likewise, for Cloud services, scalability, portability, privacy and security guidelines have been highlighted.
23739367	Wearable data acquisition system of multimodal physiological signals for personal health care.
Stud Health Technol Inform  2013
The paper proposes a wearable multimodal data acquisition system for biological signals. The system enables logging of electrical bioimpedance signals from multiple electrodes, electrocardiographic signals (ECG), acceleration signals from multiple locations, and spirometric data from a moving object. Later it will be used to conduct field measurements for characterizing health of the object under investigation. Main goal is to acquire enough data for development, refinement, and simplification of signal processing algorithms. The system is center part of the new wearable compact data acquisition modules ZCardio. Those modules enable multichannel impedance spectroscopy by logging ECG signals and data from the spirometric sensor. Initial reference measurements were conducted. Alternatively, tests were performed using Plessey Semiconductors capacitive sensors. Acceleration signals are gathered.
23867104	Semantator: semantic annotator for converting biomedical text to linked data.
J Biomed Inform 20130715 2013Oct
More than 80% of biomedical data is embedded in plain text. The unstructured nature of these text-based documents makes it challenging to easily browse and query the data of interest in them. One approach to facilitate browsing and querying biomedical text is to convert the plain text to a linked web of data, i.e., converting data originally in free text to structured formats with defined meta-level semantics. In this paper, we introduce Semantator (Semantic Annotator), a semantic-web-based environment for annotating data of interest in biomedical documents, browsing and querying the annotated data, and interactively refining annotation results if needed. Through Semantator, information of interest can be either annotated manually or semi-automatically using plug-in information extraction tools. The annotated results will be stored in RDF and can be queried using the SPARQL query language. In addition, semantic reasoners can be directly applied to the annotated data for consistency checking and knowledge inference. Semantator has been released online and was used by the biomedical ontology community who provided positive feedbacks. Our evaluation results indicated that (1) Semantator can perform the annotation functionalities as designed; (2) Semantator can be adopted in real applications in clinical and transactional research; and (3) the annotated results using Semantator can be easily used in Semantic-web-based reasoning tools for further inference.
23807448	Weighted color and texture sample selection for image matting.
IEEE Trans Image Process 20130627 2013Nov
Color sampling based matting methods find the best known samples for foreground and background colors of unknown pixels. Such methods do not perform well if there is an overlap in the color distribution of foreground and background regions because color cannot distinguish between these regions and hence, the selected samples cannot reliably estimate the matte. Furthermore, current sampling based matting methods choose samples that are located around the boundaries of foreground and background regions. In this paper, we overcome these two problems. First, we propose texture as a feature that can complement color to improve matting by discriminating between known regions with similar colors. The contribution of texture and color is automatically estimated by analyzing the content of the image. Second, we combine local sampling with a global sampling scheme that prevents true foreground or background samples to be missed during the sample collection stage. An objective function containing color and texture components is optimized to choose the best foreground and background pair among a set of candidate pairs. Experiments are carried out on a benchmark data set and an independent evaluation of the results shows that the proposed method is ranked first among all other image matting methods.
23864203	Image classification via object-aware holistic superpixel selection.
IEEE Trans Image Process 20130709 2013Nov
In this paper, we propose an object-aware holistic superpixel selection (HPS) method to automatically select the discriminative superpixels of an image for image classification purpose. Through only considering the selected superpixels, the interference of cluttered background on the object can be alleviated effectively and thus the classification performance is significantly enhanced. In particular, for an image, HPS first selects the discriminative superpixels for the characteristics of certain class, which can together match the object template of this class well. In addition, these superpixels compose a class-specific matching region. Through performing such superpixel selection for several most probable classes, respectively, HPS generates multiple class-specific matching regions for a single image. Then, HPS merges these matching regions into an integral object region through exploiting their pixel-level intersection information. Finally, such object region instead of the original image is used for image classification. An appealing advantage of HPS is the ability to alleviate the interference of cluttered background yet not require the object to be segmented out accurately. We evaluate the proposed HPS on four challenging image classification benchmark datasets: Oxford-IIIT PET 37, Caltech-UCSD Birds 200, Caltech 101, and PASCAL VOC 2011. The experimental results consistently show that the proposed HPS can remarkably improve the classification performance.
24051721	A search-and-validate method for face identification from single line drawings.
IEEE Trans Pattern Anal Mach Intell  2013Nov
Several studies have been made in finding the faces of an object depicted in a line drawing, but the problem has not been completely solved. Although existing methods can find the correct faces in most cases, there is no mechanism to ascertain that they are indeed correct, leaving the human user to do so. This paper uses a two-stage approach--find potential faces, then validate their correctness--to ensure that only correct faces are delivered ultimately. The face finding itself uses a double breadth-first search algorithm, which yields the shortest path, to find the potential faces. The basic premise is that the smallest faces found are more likely the correct ones. They serve as the "seed" potential faces, from which the algorithm proceeds to search for more faces. If the potential faces found satisfy the validation rules, then they are accepted as correct. Otherwise, the wrong potential faces are identified and removed, and new ones found in their place. The validation process is then repeated. The algorithm is fast and reliable, can deal with planar-faced manifold and nonmanifold objects, and can deliver the different results when a drawing has multiple interpretations. Our extensive tests show that the method can deal with most cases efficiently, including those that previous methods cannot solve.
23893724	Multi-class multi-scale series contextual model for image segmentation.
IEEE Trans Image Process 20130723 2013Nov
Contextual information has been widely used as a rich source of information to segment multiple objects in an image. A contextual model uses the relationships between the objects in a scene to facilitate object detection and segmentation. Using contextual information from different objects in an effective way for object segmentation, however, remains a difficult problem. In this paper, we introduce a novel framework, called multiclass multiscale (MCMS) series contextual model, which uses contextual information from multiple objects and at different scales for learning discriminative models in a supervised setting. The MCMS model incorporates cross-object and inter-object information into one probabilistic framework and thus is able to capture geometrical relationships and dependencies among multiple objects in addition to local information from each single object present in an image. We demonstrate that our MCMS model improves object segmentation performance in electron microscopy images and provides a coherent segmentation of multiple objects. Through speeding up the segmentation process, the proposed method will allow neurobiologists to move beyond individual specimens and analyze populations paving the way for understanding neurodegenerative diseases at the microscopic level.
23955745	Efficient halftoning based on multiple look-up tables.
IEEE Trans Image Process 20130808 2013Nov
Look-up table (LUT) halftoning is an efficient way to construct halftone images and approximately simulate the dot distribution of the learned halftone image set. In this paper, a general mechanism named multiple look-up table (MLUT) halftoning is proposed to generate the halftones of direct binary search (DBS), whereas the high efficient characteristic of the LUT is still preserved. In the MLUT, the standard deviation is adopted as an important feature to classify various tables. In addition, the proposed quick standard deviation evaluation is employed to yield an extremely low computational complexity in calculating the standard deviation. In the parameter optimization, the autocorrelation is adopted because it can fully characterize the periodicity of dot distribution. Experimental results demonstrate that the dot distribution generated by the proposed method approximates to that of the DBS, which enables the proposed scheme as a very competitive candidate in the copying and printing industry.
23955748	A model based iterative reconstruction algorithm for high angle annular dark field-scanning transmission electron microscope (HAADF-STEM) tomography.
IEEE Trans Image Process 20130808 2013Nov
High angle annular dark field (HAADF)-scanning transmission electron microscope (STEM) data is increasingly being used in the physical sciences to research materials in 3D because it reduces the effects of Bragg diffraction seen in bright field TEM data. Typically, tomographic reconstructions are performed by directly applying either filtered back projection (FBP) or the simultaneous iterative reconstruction technique (SIRT) to the data. Since HAADF-STEM tomography is a limited angle tomography modality with low signal to noise ratio, these methods can result in significant artifacts in the reconstructed volume. In this paper, we develop a model based iterative reconstruction algorithm for HAADF-STEM tomography. We combine a model for image formation in HAADF-STEM tomography along with a prior model to formulate the tomographic reconstruction as a maximum a posteriori probability (MAP) estimation problem. Our formulation also accounts for certain missing measurements by treating them as nuisance parameters in the MAP estimation framework. We adapt the iterative coordinate descent algorithm to develop an efficient method to minimize the corresponding MAP cost function. Reconstructions of simulated as well as experimental data sets show results that are superior to FBP and SIRT reconstructions, significantly suppressing artifacts and enhancing contrast.
24057006	Automatic inpainting scheme for video text detection and removal.
IEEE Trans Image Process  2013Nov
We present a two stage framework for automatic video text removal to detect and remove embedded video texts and fill-in their remaining regions by appropriate data. In the video text detection stage, text locations in each frame are found via an unsupervised clustering performed on the connected components produced by the stroke width transform (SWT). Since SWT needs an accurate edge map, we develop a novel edge detector which benefits from the geometric features revealed by the bandlet transform. Next, the motion patterns of the text objects of each frame are analyzed to localize video texts. The detected video text regions are removed, then the video is restored by an inpainting scheme. The proposed video inpainting approach applies spatio-temporal geometric flows extracted by bandlets to reconstruct the missing data. A 3D volume regularization algorithm, which takes advantage of bandlet bases in exploiting the anisotropic regularities, is introduced to carry out the inpainting task. The method does not need extra processes to satisfy visual consistency. The experimental results demonstrate the effectiveness of both our proposed video text detection approach and the video completion technique, and consequently the entire automatic video text removal and restoration process.
21523460	A survey of metabolic databases emphasizing the MetaCyc family.
Arch. Toxicol. 20110427 2011Sep
Thanks to the confluence of genome sequencing and bioinformatics, the number of metabolic databases has expanded from a handful in the mid-1990s to several thousand today. These databases lie within distinct families that have common ancestry and common attributes. The main families are the MetaCyc, KEGG, Reactome, Model SEED, and BiGG families. We survey these database families, as well as important individual metabolic databases, including multiple human metabolic databases. The MetaCyc family is described in particular detail. It contains well over 1,000 databases, including highly curated databases for Escherichia coli, Saccharomyces cerevisiae, Mus musculus, and Arabidopsis thaliana. These databases are available through a number of web sites that offer a range of software tools for querying and visualizing metabolic networks. These web sites also provide multiple tools for analysis of gene expression and metabolomics data, including visualization of those datasets on metabolic network diagrams and over-representation analysis of gene sets and metabolite sets.
23823294	Using NLP to identify cancer cases in imaging reports drawn from radiology information systems.
Stud Health Technol Inform  2013
A Natural Language processing (NLP) classifier has been developed for the Victorian and NSW Cancer Registries with the purpose of automatically identifying cancer reports from imaging services, transmitting them to the Registries and then extracting pertinent cancer information. Large scale trials conducted on over 40,000 reports show the sensitivity for identifying reportable cancer reports is above 98% with a specificity above 96%. Detection of tumour stream, report purpose, and a variety of extracted content is generally above 90% specificity. The differences between report layout and authoring strategies across imaging services appear to require different classifiers to retain this high level of accuracy. Linkage of the imaging data with existing registry records (hospital and pathology reports) to derive stage and recurrence of cancer has commenced and shown very promising results.
23823295	Clinical Information Access Portal (CIAP) use by NSW Health staff over 15 years.
Stud Health Technol Inform  2013
Over the 15 years since the Clinical Information Access Portal (CIAP) was launched, usage has continued to increase. This demonstrates the value that NSW Health staff place on access to point of care evidence-based clinical information. Web log usage files for all resources available on CIAP have been collected since 1999. Analysis of resource usage was performed to identify patterns of use. Individual account demographics were analysed for the last 3 years to identify key clinical user groups in NSW Health. CIAP usage has increased significantly over the last 15 years when annual usage was only 2% of that seen in 2012. The most highly accessed resources include MIMS, eTheraputic Guidelines, ProQuest Nursing and Allied Health, Australian Medicines Handbook, and BMJ Best Practice. CIAP remains an exemplary example of integrating evidence-based practice at the point of care by providing the best available clinical information for all NSW Health.
20595311	Analyzing categorical information in two publicly available drug terminologies: RxNorm and NDF-RT.
J Am Med Inform Assoc  2010 Jul-Aug
The RxNorm and NDF-RT (National Drug File Reference Terminology) are a suite of terminology standards for clinical drugs designated for use in the US federal government systems for electronic exchange of clinical health information. Analyzing how different drug products described in these terminologies are categorized into drug classes will help in their better organization and classification of pharmaceutical information. Mappings between drug products in RxNorm and NDF-RT drug classes were extracted. Mappings were also extracted between drug products in RxNorm to five high-level NDF-RT categories: Chemical Structure; cellular or subcellular Mechanism of Action; organ-level or system-level Physiologic Effect; Therapeutic Intent; and Pharmacokinetics. Coverage for the mappings and the gaps were evaluated and analyzed algorithmically. Approximately 54% of RxNorm drug products (Semantic Clinical Drugs) were found not to have a correspondence in NDF-RT. Similarly, approximately 45% of drug products in NDF-RT are missing from RxNorm, most of which can be attributed to differences in dosage, strength, and route form. Approximately 81% of Chemical Structure classes, 42% of Mechanism of Action classes, 75% of Physiologic Effect classes, 76% of Therapeutic Intent classes, and 88% of Pharmacokinetics classes were also found not to have any RxNorm drug products classified under them. Finally, various issues regarding inconsistent mappings between drug concepts were identified in both terminologies. This investigation identified potential limitations of the existing classification systems and various issues in specification of correspondences between the concepts in RxNorm and NDF-RT. These proposals and methods provide the preliminary steps in addressing some of the requirements.
20962136	Comparing and evaluating terminology services application programming interfaces: RxNav, UMLSKS and LexBIG.
J Am Med Inform Assoc  2010 Nov-Dec
To facilitate the integration of terminologies into applications, various terminology services application programming interfaces (API) have been developed in the recent past. In this study, three publicly available terminology services API, RxNav, UMLSKS and LexBIG, are compared and functionally evaluated with respect to the retrieval of information from one biomedical terminology, RxNorm, to which all three services provide access. A list of queries is established covering a wide spectrum of terminology services functionalities such as finding RxNorm concepts by their name, or navigating different types of relationships. Test data were generated from the RxNorm dataset to evaluate the implementation of the functionalities in the three API. The results revealed issues with various aspects of the API implementation (eg, handling of obsolete terms by LexBIG) and documentation (eg, navigational paths used in RxNav) that were subsequently addressed by the development teams of the three API investigated. Knowledge about such discrepancies helps inform the choice of an API for a given use case.
21774133	English and Spanish oral cancer information on the internet: a pilot surface quality and content evaluation of oral cancer web sites.
J Public Health Dent  2011Spring
Oral and pharyngeal cancers are responsible for over 7600 deaths each year in the United States. Given the significance of the disease and the fact that many individuals increasingly rely on health information on the Internet, it is important that patients and others can access clear and accurate oral cancer information on the Web. The objective of this study was threefold: (a) develop an initial method to evaluate surface and content quality of selected English- and Spanish-language oral cancer Web sites; (b) conduct a pilot evaluation; and (c) discuss implications of our findings for dental public health. We developed a search strategy to find oral cancer sites frequented by the public using Medline Plus, Google, and Yahoo in English and Spanish. We adapted the Information Quality Tool (IQT) to perform a surface evaluation and developed a novel tool to evaluate site content for 24 sites each in English and Spanish. English-language sites had an average IQT score of 76.6 (out of 100) and an average content score of 52.1 (out of 100). Spanish-language sites had an average IQT score of 50.3 and an average content score of 25.6. The study produced a quality assessment of oral cancer Web sites useful for clinicians and patients. Sites provided more information on clinical presentation, and etiology, and risk factors, than other aspects of oral cancer. The surface and quality of Spanish-language sites was low, possibly putting Hispanic populations at a disadvantage regarding oral cancer information on the Web.
22195142	Extracting temporal constraints from clinical research eligibility criteria using conditional random fields.
AMIA Annu Symp Proc 20111022 2011
Temporal constraints are present in 38% of clinical research eligibility criteria and are crucial for screening patients. However, eligibility criteria are often written as free text, which is not amenable for computer processing. In this paper, we present an ontology-based approach to extracting temporal information from clinical research eligibility criteria. We generated temporal labels using a frame-based temporal ontology. We manually annotated 150 free-text eligibility criteria using the temporal labels and trained a parser using Conditional Random Fields (CRFs) to automatically extract temporal expressions from eligibility criteria. An evaluation of an additional 60 randomly selected eligibility criteria using manual review achieved an overall precision of 83%, a recall of 79%, and an F-score of 80%. We illustrate the application of temporal extraction with the use cases of question answering and free-text criteria querying.
22449720	Lexical patterns, features and knowledge resources for coreference resolution in clinical notes.
J Biomed Inform 20120317 2012Oct
Generation of entity coreference chains provides a means to extract linked narrative events from clinical notes, but despite being a well-researched topic in natural language processing, general-purpose coreference tools perform poorly on clinical texts. This paper presents a knowledge-centric and pattern-based approach to resolving coreference across a wide variety of clinical records from two corpora (Ontology Development and Information Extraction (ODIE) and i2b2/VA), and describes a method for generating coreference chains using progressively pruned linked lists that reduces the search space and facilitates evaluation by a number of metrics. Independent evaluation results give an F-measure for each corpus of 79.2% and 87.5%, respectively. A baseline of blind coreference of mentions of the same class gives F-measures of 65.3% and 51.9% respectively. For the ODIE corpus, recall is significantly improved over the baseline (p&lt;0.05) but overall there was no statistically significant improvement in F-measure (p&gt;0.05). For the i2b2/VA corpus, recall, precision, and F-measure are significantly improved over the baseline (p&lt;0.05). Overall, our approach offers performance at least as good as human annotators and greatly increased performance over general-purpose tools. The system uses a number of open-source components that are available to download.
22982356	Dissociation between process-based and data-based limitations for conscious perception in the human brain.
Neuroimage 20120914 2013Jan1
Successful performance of a cognitive task depends upon both the quality of the sensory information and the processing resources available to perform that task. Thus, task performance can either be data-limited or process-limited (D. A. Norman and D. G. Bobrow, 1975). Using fMRI, we show that these conceptual distinctions are neurally dissociable: A parieto-frontal network involved in conscious perception is modulated by target interference manipulations that strain attentional processing, but not by equally difficult manipulations that limit the quality of target information. These results suggest that limitations imposed by processing capacity have distinct neural effects from those arising from the quality of sensory input, and provide empirical support for an influential neurobiological theory of consciousness (S. Dehaene, J.-P. Changeux, L. Naccache, J. Sackur, and C. Sergent, 2006).
23529202	Bayesian approach to dynamically controlling data collection in P300 spellers.
IEEE Trans Neural Syst Rehabil Eng 20130321 2013May
P300 spellers provide a noninvasive method of communication for people who may not be able to use other communication aids due to severe neuromuscular disabilities. However, P300 spellers rely on event-related potentials (ERPs) which often have low signal-to-noise ratios (SNRs). In order to improve detection of the ERPs, P300 spellers typically collect multiple measurements of the electroencephalography (EEG) response for each character. The amount of collected data can affect both the accuracy and the communication rate of the speller system. The goal of the present study was to develop an algorithm that would automatically determine the necessary amount of data to collect during operation. Dynamic data collection was controlled by a threshold on the probabilities that each possible character was the target character, and these probabilities were continually updated with each additional measurement. This Bayesian technique differs from other dynamic data collection techniques by relying on a participant-independent, probability-based metric as the stopping criterion. The accuracy and communication rate for dynamic and static data collection in P300 spellers were compared for 26 users. Dynamic data collection resulted in a significant increase in accuracy and communication rate.
23774519	Identifying appropriate reference data models for comparative effectiveness research (CER) studies based on data from clinical information systems.
Med Care  2013Aug
The need for a common format for electronic exchange of clinical data prompted federal endorsement of applicable standards. However, despite obvious similarities, a consensus standard has not yet been selected in the comparative effectiveness research (CER) community. Using qualitative metrics for data retrieval and information loss across a variety of CER topic areas, we compare several existing models from a representative sample of organizations associated with clinical research: the Observational Medical Outcomes Partnership (OMOP), Biomedical Research Integrated Domain Group, the Clinical Data Interchange Standards Consortium, and the US Food and Drug Administration. While the models examined captured a majority of the data elements that are useful for CER studies, data elements related to insurance benefit design and plans were most detailed in OMOP's CDM version 4.0. Standardized vocabularies that facilitate semantic interoperability were included in the OMOP and US Food and Drug Administration Mini-Sentinel data models, but are left to the discretion of the end-user in Biomedical Research Integrated Domain Group and Analysis Data Model, limiting reuse opportunities. Among the challenges we encountered was the need to model data specific to a local setting. This was handled by extending the standard data models. We found that the Common Data Model from the OMOP met the broadest complement of CER objectives. Minimal information loss occurred in mapping data from institution-specific data warehouses onto the data models from the standards we assessed. However, to support certain scenarios, we found a need to enhance existing data dictionaries with local, institution-specific information.
23806801	Tools for identifying and prioritizing evidence-based obesity prevention strategies, Colorado.
Prev Chronic Dis 20130703 2013
Colorado's adult obesity rate has more than doubled since 1995, prompting its Department of Public Health and Environment to list obesity as its top prevention priority. To initiate comprehensive and effective action, the department used a well-known evidence-based public health framework developed by Brownson and others. This article describes the tools and process developed to conduct 2 of the 7 stages in this framework that challenge public health organizations: reviewing the literature and prioritizing effective strategies from that literature. Forty-five department staff participated in an intensive literature review training to identify physical activity and nutrition strategies that effectively address obesity and worked with external stakeholders to prioritize strategies for the state. Divided into 8 multidisciplinary teams organized by the setting where public health could exert leverage, they scanned the scientific literature to identify potential strategies to implement. These teams were trained to use standardized tools to critique findings, systematically abstract key information, and classify the evidence level for each of 58 identified strategies. Next, departmental subject matter experts and representatives from local public health and nonprofit health agencies selected and applied prioritization criteria to rank the 58 strategies. A team charter, group facilitation tools, and 2 web-based surveys were used in the prioritization stage. This process offered the staff a shared experience to gain hands-on practice completing literature reviews and selecting evidence-based strategies, thereby enhancing Colorado's obesity prevention efforts and improving public health capacity. Practitioners can use these tools and methodology to replicate this process for other health priorities.
22316870	User's demography and expectation regarding search, purchase and evaluation in mobile application store.
Work  2012
This article is result from a questionnaire about mobile app store usage. The objective of this work was to collect information about user needs and opinion regarding search, purchase and evaluation process in Android Market, Apple App Store, BlackBerry App World and Nokia Ovi Store. The data collected was analyzed to identify the positive and negative usability aspects, if the process to perform these task are any different in those stores and if the users are satisfy with their store or if they have any complains about it. Its covers the brazilian market only.
23938660	Information-theoretic secure key distribution based on common random-signal induced synchronization in unidirectionally-coupled cascades of semiconductor lasers.
Opt Express  2013Jul29
It has been proposed that a secure key distribution scheme using correlated random bit sequences can be implemented using common random-signal induced synchronization of semiconductor laser systems. In this scheme it is necessary to use laser systems consisting of multiple cascaded lasers to be secure against a powerful eavesdropper. In this paper, we report the results of an experimental study that demonstrate that the common random-signal induced synchronization is possible in cascaded semiconductor laser systems. We also show that the correlated random bit sequences generated in the synchronized cascaded laser systems can be used to create an information-theoretically secure key between two legitimate users.
23938672	Increasing the darkfield contrast-to-noise ratio using a deconvolution-based information retrieval algorithm in X-ray grating-based phase-contrast imaging.
Opt Express  2013Jul29
A novel information retrieval algorithm for X-ray grating-based phase-contrast imaging based on the deconvolution of the object and the reference phase stepping curve (PSC) as proposed by Modregger et al. was investigated in this paper. We applied the method for the first time on data obtained with a polychromatic spectrum and compared the results to those, received by applying the commonly used method, based on a Fourier analysis. We confirmed the expectation, that both methods deliver the same results for the absorption and the differential phase image. For the darkfield image, a mean contrast-to-noise ratio (CNR) increase by a factor of 1.17 using the new method was found. Furthermore, the dose saving potential was estimated for the deconvolution method experimentally. It is found, that for the conventional method a dose which is higher by a factor of 1.66 is needed to obtain a similar CNR value compared to the novel method. A further analysis of the data revealed, that the improvement in CNR and dose efficiency is due to the superior background noise properties of the deconvolution method, but at the cost of comparability between measurements at different applied dose values, as the mean value becomes dependent on the photon statistics used.
23938714	Super-resolution complex amplitude reconstruction of nanostructured binary data using an interference microscope with pattern matching.
Opt Express  2013Jul29
We propose a new method of optically reconstructing binary data formed by nanostructures with an elemental size several tens of nanometers smaller than the diffraction limit, implemented with an interference microscope and a complex-amplitude image pattern matching method. We examine the size dependency of the data reconstruction capacity using a light propagation simulation based on the finite-difference time-domain (FDTD) method and the Fourier spatial frequency filtering method. We demonstrated that the readable size of the binary nanostructure depends on the magnitude of noise.
22531763	Conjunctive patches subspace learning with side information for collaborative image retrieval.
IEEE Trans Image Process 20120417 2012Aug
Content-Based Image Retrieval (CBIR) has attracted substantial attention during the past few years for its potential practical applications to image management. A variety of Relevance Feedback (RF) schemes have been designed to bridge the semantic gap between the low-level visual features and the high-level semantic concepts for an image retrieval task. Various Collaborative Image Retrieval (CIR) schemes aim to utilize the user historical feedback log data with similar and dissimilar pairwise constraints to improve the performance of a CBIR system. However, existing subspace learning approaches with explicit label information cannot be applied for a CIR task, although the subspace learning techniques play a key role in various computer vision tasks, e.g., face recognition and image classification. In this paper, we propose a novel subspace learning framework, i.e., Conjunctive Patches Subspace Learning (CPSL) with side information, for learning an effective semantic subspace by exploiting the user historical feedback log data for a CIR task. The CPSL can effectively integrate the discriminative information of labeled log images, the geometrical information of labeled log images and the weakly similar information of unlabeled images together to learn a reliable subspace. We formally formulate this problem into a constrained optimization problem and then present a new subspace learning technique to exploit the user historical feedback log data. Extensive experiments on both synthetic data sets and a real-world image database demonstrate the effectiveness of the proposed scheme in improving the performance of a CBIR system by exploiting the user historical feedback log data.
22481822	Coupled kernel embedding for low resolution face image recognition.
IEEE Trans Image Process 20120403 2012Aug
Practical video scene and face recognition systems are sometimes confronted with low-resolution (LR) images. The faces may be very small even if the video is clear, thus it is difficult to directly measure the similarity between the faces and the high-resolution (HR) training samples. Traditional super-resolution (SR) methods based face recognition usually have limited performance because the target of SR may not be consistent with that of classification, and time-consuming SR algorithms are not suitable for real-time applications. In this paper, a new feature extraction method called Coupled Kernel Embedding (CKE) is proposed for LR face recognition without any SR preprocessing. In this method, the final kernel matrix is constructed by concatenating two individual kernel matrices in the diagonal direction, and the (semi-)positively definite properties are preserved for optimization. CKE addresses the problem of comparing multi-modal data that are difficult for conventional methods in practice due to the lack of an efficient similarity measure. Particularly, different kernel types (e.g., linear, Gaussian, polynomial) can be integrated into an uniformed optimization objective, which cannot be achieved by simple linear methods. CKE solves this problem by minimizing the dissimilarities captured by their kernel Gram matrices in the low- and high-resolution spaces. In the implementation, the nonlinear objective function is minimized by a generalized eigenvalue decomposition. Experiments on benchmark and real databases show that our CKE method indeed improves the recognition performance.
22481824	Shape error concealment based on a shape-preserving boundary approximation.
IEEE Trans Image Process 20120403 2012Aug
In objectbased video representation, video scenes are composed of several arbitrarily shaped video objects (VOs), defined by their texture, shape and motion. In errorprone communications, packet loss results in missing information at the decoder. The impact of transmission errors is minimised through error concealment. In this paper, we propose a spatial error concealment technique for recovering lost shape data. We consider a geometric shape representation consisting of the object boundary, which can be extracted from the -plane. Missing macroblocks result in a broken boundary. A Bspline curve is constructed to replace a missing boundary segment, based on a T spline representation of the received boundary. We use Tsplines because they produce shapepreserving approximations and do not change the characteristics of the original boundary. The representation ensures a good estimation of the first derivatives at the points touching the missing segment. Applying smoothing conditions, we manage to construct a new spline that joins smoothly with the received boundary, leading to successful concealment results. Experimental results on object shapes with different concealment difficulty demonstrate the performance of the proposed method. Comparisons with prior proposed methods are also presented.
24464060	[Development of a batched image delete system for multi-vender picture archiving and communication system environment].
Nihon Hoshasen Gijutsu Gakkai Zasshi  2014Jan
A picture archiving and communication system (PACS) for multi-vendor imaging servers is useful, since it can provide a variety of image-processing services. However, to delete an image file in the PACS, it is necessary to delete not only the image but all its associated images that are stored in multiple servers: this is a lengthy and painstaking process. To reduce this workload, we have developed a system consisting of a computer program with a graphical user interface that can delete the target image and all related images by means of batch processing. The developed system creates an extensible markup language (XML)-format file that describes the operation for deleting an image and forwards the XML file to the main server. Using a Windows file-sharing system (SMB/CIFS), each server shares the XML file and deletes the images in its own database in response to the instructions described in the XML file. We can also rigorously manage information concerning the deleted images using the information that is output from the main server to external storage. We also discuss the degree of load reduction in our system compared with that of ordinary systems.
23732685	Time-of-flight angiography: a viable alternative to contrast-enhanced MR angiography and fat-suppressed T1w images for the diagnosis of cervical artery dissection?
Eur Radiol 20130604 2013Oct
To compare the use of an unenhanced high-resolution time-of-flight MR angiography sequence (Hr-TOF MRA) with fat-suppressed axial/coronal T1-weighted images and contrast-enhanced angiography (standard MRI) for the diagnosis of cervical artery dissection (cDISS). Twenty consecutive patients (9 women, 11 men, aged 24-66 years) with proven cDISS on standard MRI underwent Hr-TOF MRA at 3.0 T using dedicated surface coils. Sensitivity (SE), specificity (SP), positive and negative predictive values (PPV, NPV), Cohen's kappa (к) and accuracy of Hr-TOF MRA were calculated using the standard protocol as the gold standard. Image quality and diagnostic confidence were assessed on a four-point scale. Image quality was rated better for standard MRI (P = 0.02), whereas diagnostic confidence did not differ significantly (P = 0.27). There was good agreement between Hr-TOF images and the standard protocol for the presence/absence of cDISS, with к = 0.95 for reader 1 and к = 0.89 for reader 2 (P &lt; 0.001). This resulted in SE, SP, PPV, NPV and accuracy of 97 %, 98 %, 97 %, 98 % and 97 % for reader 1 and 93 %, 96 %, 93 %, 96 % and 95 % for reader 2. Hr-TOF MRA can be used to diagnose cDISS with excellent agreement compared with the standard protocol. This might be useful in patients with renal insufficiency or if contrast-enhanced MR angiography is of insufficient image quality. • New magnetic resonance angiography sequences are increasingly used for vertebral artery assessment. • A high-resolution time-of-flight sequence allows the diagnosis of cervical artery dissection. • This technique allows the diagnosis without intravenous contrast medium. • It could help in renal insufficiency or when contrast-enhanced MRA fails.
24077319	Assessing routing strategies for cognitive radio sensor networks.
Sensors (Basel) 20130926 2013
Interest in the cognitive radio sensor network (CRSN) paradigm has gradually grown among researchers. This concept seeks to fuse the benefits of dynamic spectrum access into the sensor network, making it a potential player in the next generation (NextGen) network, which is characterized by ubiquity. Notwithstanding its massive potential, little research activity has been dedicated to the network layer. By contrast, we find recent research trends focusing on the physical layer, the link layer and the transport layers. The fact that the cross-layer approach is imperative, due to the resource-constrained nature of CRSNs, can make the design of unique solutions non-trivial in this respect. This paper seeks to explore possible design opportunities with wireless sensor networks (WSNs), cognitive radio ad-hoc networks (CRAHNs) and cross-layer considerations for implementing viable CRSN routing solutions. Additionally, a detailed performance evaluation of WSN routing strategies in a cognitive radio environment is performed to expose research gaps. With this work, we intend to lay a foundation for developing CRSN routing solutions and to establish a basis for future work in this area.
24082915	A semantic medical multimedia retrieval approach using ontology information hiding.
Comput Math Methods Med 20130909 2013
Searching useful information from unstructured medical multimedia data has been a difficult problem in information retrieval. This paper reports an effective semantic medical multimedia retrieval approach which can reflect the users' query intent. Firstly, semantic annotations will be given to the multimedia documents in the medical multimedia database. Secondly, the ontology that represented semantic information will be hidden in the head of the multimedia documents. The main innovations of this approach are cross-type retrieval support and semantic information preservation. Experimental results indicate a good precision and efficiency of our approach for medical multimedia retrieval in comparison with some traditional approaches.
24084116	Design and implementation of a MAC protocol for timely and reliable delivery of command and data in dynamic wireless sensor networks.
Sensors (Basel) 20130930 2013
This paper proposes and implements a new TDMA-based MAC protocol for providing timely and reliable delivery of data and command for monitoring and control networks. In this kind of network, sensor nodes are required to sense data from the monitoring environment periodically and then send the data to a sink. The sink determines whether the environment is safe or not by analyzing the acquired data. Sometimes, a command or control message is sent from the sink to a particular node or a group of nodes to execute the services or request further interested data. The proposed MAC protocol enables bidirectional communication, controls active and sleep modes of a sensor node to conserve energy, and addresses the problem of load unbalancing between the nodes near a sink and the other nodes. It can improve reliability of communication significantly while extending network lifetime. These claims are supported by the experimental results.
24135992	Dynamic task allocation in multi-hop multimedia wireless sensor networks with low mobility.
Sensors (Basel) 20131016 2013
This paper presents a task allocation-oriented framework to enable efficient in-network processing and cost-effective multi-hop resource sharing for dynamic multi-hop multimedia wireless sensor networks with low node mobility, e.g., pedestrian speeds. The proposed system incorporates a fast task reallocation algorithm to quickly recover from possible network service disruptions, such as node or link failures. An evolutional self-learning mechanism based on a genetic algorithm continuously adapts the system parameters in order to meet the desired application delay requirements, while also achieving a sufficiently long network lifetime. Since the algorithm runtime incurs considerable time delay while updating task assignments, we introduce an adaptive window size to limit the delay periods and ensure an up-to-date solution based on node mobility patterns and device processing capabilities. To the best of our knowledge, this is the first study that yields multi-objective task allocation in a mobile multi-hop wireless environment under dynamic conditions. Simulations are performed in various settings, and the results show considerable performance improvement in extending network lifetime compared to heuristic mechanisms. Furthermore, the proposed framework provides noticeable reduction in the frequency of missing application deadlines.
24152920	Adaptive multi-node multiple input and multiple output (MIMO) transmission for mobile wireless multimedia sensor networks.
Sensors (Basel) 20131002 2013
Mobile wireless multimedia sensor networks (WMSNs), which consist of mobile sink or sensor nodes and use rich sensing information, require much faster and more reliable wireless links than static wireless sensor networks (WSNs). This paper proposes an adaptive multi-node (MN) multiple input and multiple output (MIMO) transmission to improve the transmission reliability and capacity of mobile sink nodes when they experience spatial correlation. Unlike conventional single-node (SN) MIMO transmission, the proposed scheme considers the use of transmission antennas from more than two sensor nodes. To find an optimal antenna set and a MIMO transmission scheme, a MN MIMO channel model is introduced first, followed by derivation of closed-form ergodic capacity expressions with different MIMO transmission schemes, such as space-time transmit diversity coding and spatial multiplexing. The capacity varies according to the antenna correlation and the path gain from multiple sensor nodes. Based on these statistical results, we propose an adaptive MIMO mode and antenna set switching algorithm that maximizes the ergodic capacity of mobile sink nodes. The ergodic capacity of the proposed scheme is compared with conventional SN MIMO schemes, where the gain increases as the antenna correlation and path gain ratio increase.
24152921	GeoCENS: a geospatial cyberinfrastructure for the world-wide sensor web.
Sensors (Basel) 20131002 2013
The world-wide sensor web has become a very useful technique for monitoring the physical world at spatial and temporal scales that were previously impossible. Yet we believe that the full potential of sensor web has thus far not been revealed. In order to harvest the world-wide sensor web's full potential, a geospatial cyberinfrastructure is needed to store, process, and deliver large amount of sensor data collected worldwide. In this paper, we first define the issue of the sensor web long tail followed by our view of the world-wide sensor web architecture. Then, we introduce the Geospatial Cyberinfrastructure for Environmental Sensing (GeoCENS) architecture and explain each of its components. Finally, with demonstration of three real-world powered-by-GeoCENS sensor web applications, we believe that the GeoCENS architecture can successfully address the sensor web long tail issue and consequently realize the world-wide sensor web vision.
24152935	Remote driven and read MEMS sensors for harsh environments.
Sensors (Basel) 20131021 2013
The utilization of high accuracy sensors in harsh environments has been limited by the temperature constraints of the control electronics that must be co-located with the sensor. Several methods of remote interrogation for resonant sensors are presented in this paper which would allow these sensors to be extended to harsh environments. This work in particular demonstrates for the first time the ability to acoustically drive a silicon comb drive resonator into resonance and electromagnetically couple to the resonator to read its frequency. The performance of this system was studied as a function of standoff distance demonstrating the ability to excite and read the device from 22 cm when limited to drive powers of 30 mW. A feedback architecture was implemented that allowed the resonator to be driven into resonance from broadband noise and a standoff distance of 15 cm was demonstrated. It is emphasized that no junction-based electronic device was required to be co-located with the resonator, opening the door for the use of silicon-based, high accuracy MEMS devices in high temperature wireless applications.
24579194	Manifold learning of brain MRIs by deep learning.
Med Image Comput Comput Assist Interv  2013
Manifold learning of medical images plays a potentially important role for modeling anatomical variability within a population with pplications that include segmentation, registration, and prediction of clinical parameters. This paper describes a novel method for learning the manifold of 3D brain images that, unlike most existing manifold learning methods, does not require the manifold space to be locally linear, and does not require a predefined similarity measure or a prebuilt proximity graph. Our manifold learning method is based on deep learning, a machine learning approach that uses layered networks (called deep belief networks, or DBNs) and has received much attention recently in the computer vision field due to their success in object recognition tasks. DBNs have traditionally been too computationally expensive for application to 3D images due to the large number of trainable parameters. Our primary contributions are (1) a much more computationally efficient training method for DBNs that makes training on 3D medical images with a resolution of up to 128 x 128 x 128 practical, and (2) the demonstration that DBNs can learn a low-dimensional manifold of brain volumes that detects modes of variations that correlate to demographic and disease parameters.
24579195	Multiresolution hierarchical shape models in 3D subcortical brain structures.
Med Image Comput Comput Assist Interv  2013
Point distribution models (PDM) are one of the most extended methods to characterize the underlying population of set of samples, whose usefulness has been demonstrated in a wide variety of applications, including medical imaging. However, one important issue remains unsolved: the large number of training samples required. This problem becomes critical as the complexity of the problem increases, and the modeling of 3D multiobjects/organs represents one of the most challenging cases. Based on the 3D wavelet transform, this paper introduces a multiresolution hierarchical variant of PDM (MRH-PDM) able to efficiently characterize the different inter-object relationships, as well as the particular locality of each element separately. The significant advantage of this new method over two previous approaches in terms of accuracy has been successfully verified for the particular case of 3D subcortical brain structures.
23492829	Biggest challenges in bioinformatics.
EMBO Rep. 20130315 2013Apr
The third Heidelberg Unseminars in Bioinformatics (HUB) was held on 18th October 2012, at Heidelberg University, Germany. HUB brought together around 40 bioinformaticians from academia and industry to discuss the 'Biggest Challenges in Bioinformatics' in a 'World Café' style event.
23912716	Digitizing humanity.
Artif DNA PNA XNA  2013 Apr-Jun
The application of ex vivo synthetic DNA as a high capacity information storage medium is well documented. Herein, we consider the potential for synthetic DNA to be incorporated as part of the human genome; providing a definitive, accessible, in vivo database of patient history.
23201995	Towards a ubiquitous user model for profile sharing and reuse.
Sensors (Basel) 20120928 2012
People interact with systems and applications through several devices and are willing to share information about preferences, interests and characteristics. Social networking profiles, data from advanced sensors attached to personal gadgets, and semantic web technologies such as FOAF and microformats are valuable sources of personal information that could provide a fair understanding of the user, but profile information is scattered over different user models. Some researchers in the ubiquitous user modeling community envision the need to share user model's information from heterogeneous sources. In this paper, we address the syntactic and semantic heterogeneity of user models in order to enable user modeling interoperability. We present a dynamic user profile structure based in Simple Knowledge Organization for the Web (SKOS) to provide knowledge representation for ubiquitous user model. We propose a two-tier matching strategy for concept schemas alignment to enable user modeling interoperability. Our proposal is proved in the application scenario of sharing and reusing data in order to deal with overweight and obesity.
23874660	A DNA 'barcode blitz': rapid digitization and sequencing of a natural history collection.
PLoS ONE 20130710 2013
DNA barcoding protocols require the linkage of each sequence record to a voucher specimen that has, whenever possible, been authoritatively identified. Natural history collections would seem an ideal resource for barcode library construction, but they have never seen large-scale analysis because of concerns linked to DNA degradation. The present study examines the strength of this barrier, carrying out a comprehensive analysis of moth and butterfly (Lepidoptera) species in the Australian National Insect Collection. Protocols were developed that enabled tissue samples, specimen data, and images to be assembled rapidly. Using these methods, a five-person team processed 41,650 specimens representing 12,699 species in 14 weeks. Subsequent molecular analysis took about six months, reflecting the need for multiple rounds of PCR as sequence recovery was impacted by age, body size, and collection protocols. Despite these variables and the fact that specimens averaged 30.4 years old, barcode records were obtained from 86% of the species. In fact, one or more barcode compliant sequences (&gt;487 bp) were recovered from virtually all species represented by five or more individuals, even when the youngest was 50 years old. By assembling specimen images, distributional data, and DNA barcode sequences on a web-accessible informatics platform, this study has greatly advanced accessibility to information on thousands of species. Moreover, much of the specimen data became publically accessible within days of its acquisition, while most sequence results saw release within three months. As such, this study reveals the speed with which DNA barcode workflows can mobilize biodiversity data, often providing the first web-accessible information for a species. These results further suggest that existing collections can enable the rapid development of a comprehensive DNA barcode library for the most diverse compartment of terrestrial biodiversity - insects.
23625157	The EKZ/AMC childhood cancer survivor cohort: methodology, clinical characteristics, and data availability.
J Cancer Surviv 20130430 2013Sep
Childhood cancer survivors are at high risk of late adverse effects of cancer treatment, but there are still many gaps in evidence about these late effects. We described the methodology, clinical characteristics, data availability, and outcomes of our cohort study of childhood cancer survivors. The Emma Children's Hospital/Academic Medical Center (EKZ/AMC) childhood cancer survivor cohort is an ongoing single-center cohort study of ≥5-year childhood cancer survivors, which started in 1996 simultaneously with regular structured medical outcome assessments at our outpatient clinic. From 1966 to 2003, 3,183 eligible children received primary cancer treatment in the EKZ/AMC, of which 1,822 (57.2 %) survived ≥5 years since diagnosis. Follow-up time ranged from 5.0 to 42.5 years (median, 17.7). Baseline primary cancer treatment characteristics were complete for 1,781 (97.7 %) survivors, and 1,452 (79.7 %) survivors visited our outpatient clinic. Baseline characteristics of survivors who visited the clinic did not differ from those without follow-up. Within our cohort, 54 studies have been conducted studying a wide range of late treatment-related effects. The EKZ/AMC childhood cancer survivor cohort provides a strong structure for ongoing research on the late effects of childhood cancer treatment and will continuously contribute in reducing evidence gaps concerning risks and risk groups within this vulnerable population. Our large cohort study of childhood cancer survivors with complete baseline characteristics and unique, long-term medical follow-up decreases gaps in evidence about specific risks of late effects and high-risk groups, with the ultimate goal of improving the quality of care for childhood cancer survivors.
20017161	Parallel excitation in the human brain at 9.4 T counteracting k-space errors with RF pulse design.
Magn Reson Med  2010Feb
Multidimensional spatially selective radiofrequency (RF) pulses have been proposed as a method to mitigate transmit B1 inhomogeneity in MR experiments. These RF pulses, however, have been considered impractical for many years because they typically require very long RF pulse durations. The recent development of parallel excitation techniques makes it possible to design multidimensional RF pulses that are short enough for use in actual experiments. However, hardware and experimental imperfections can still severely alter the excitation patterns obtained with these accelerated pulses. In this note, we report at 9.4 T on a human eight-channel transmit system, substantial improvements in two-dimensional excitation pattern accuracy obtained when measuring k-space trajectories prior to parallel transmit RF pulse design (acceleration x4). Excitation patterns based on numerical simulations closely reproducing the experimental conditions were in good agreement with the experimental results.
22341232	High-throughput screening in primary neurons.
Meth. Enzymol.  2012
Despite years of incremental progress in our understanding of diseases such as Alzheimer's disease (AD), Parkinson's disease (PD), Huntington's disease (HD), and amyotrophic lateral sclerosis (ALS), there are still no disease-modifying therapeutics. The discrepancy between the number of lead compounds and approved drugs may partially be a result of the methods used to generate the leads and highlights the need for new technology to obtain more detailed and physiologically relevant information on cellular processes in normal and diseased states. Our high-throughput screening (HTS) system in a primary neuron model can help address this unmet need. HTS allows scientists to assay thousands of conditions in a short period of time which can reveal completely new aspects of biology and identify potential therapeutics in the span of a few months when conventional methods could take years or fail all together. HTS in primary neurons combines the advantages of HTS with the biological relevance of intact, fully differentiated neurons which can capture the critical cellular events or homeostatic states that make neurons uniquely susceptible to disease-associated proteins. We detail methodologies of our primary neuron HTS assay workflow from sample preparation to data reporting. We also discuss the adaptation of our HTS system into high-content screening (HCS), a type of HTS that uses multichannel fluorescence images to capture biological events in situ, and is uniquely suited to study dynamical processes in living cells.
22941985	A flexible, open, decentralized system for digital pathology networks.
Stud Health Technol Inform  2012
High-resolution digital imaging is enabling digital archiving and sharing of digitized microscopy slides and new methods for digital pathology. Collaborative research centers, outsourced medical services, and multi-site organizations stand to benefit from sharing pathology data in a digital pathology network. Yet significant technological challenges remain due to the large size and volume of digitized whole slide images. While information systems do exist for managing local pathology laboratories, they tend to be oriented toward narrow clinical use cases or offer closed ecosystems around proprietary formats. Few solutions exist for networking digital pathology operations. Here we present a system architecture and implementation of a digital pathology network and share results from a production system that federates major research centers.
23703917	Thousand-fold increase in optical storage density by polychromatic address multiplexing on self-assembled DNA nanostructures.
Adv. Mater. Weinheim 20130524 2013Jul12
A super-resolution optical storage technique enabled by DNA nanotechnology and the design of resonance energy transfer (RET) networks are demonstrated. The enhancement in storage density stems from non-linear interactions between excitons on the nanostructured RET circuits, which permit large-scale multiplexing with a small set of addressing wavelengths and a single output channel.
22929464	A service-oriented distributed semantic mediator: integrating multiscale biomedical information.
IEEE Trans Inf Technol Biomed 20120823 2012Nov
Biomedical research continuously generates large amounts of heterogeneous and multimodal data spread over multiple data sources. These data, if appropriately shared and exploited, could dramatically improve the research practice itself, and ultimately the quality of health care delivered. This paper presents DISMED (DIstributed Semantic MEDiator), an open source semantic mediator that provides a unified view of a federated environment of multiscale biomedical data sources. DISMED is a Web-based software application to query and retrieve information distributed over a set of registered data sources, using semantic technologies. It also offers a userfriendly interface specifically designed to simplify the usage of these technologies by non-expert users. Although the architecture of the software mediator is generic and domain independent, in the context of this paper, DISMED has been evaluated for managing biomedical environments and facilitating research with respect to the handling of scientific data distributed in multiple heterogeneous data sources. As part of this contribution, a quantitative evaluation framework has been developed. It consist of a benchmarking scenario and the definition of five realistic use-cases. This framework, created entirely with public datasets, has been used to compare the performance of DISMED against other available mediators. It is also available to the scientific community in order to evaluate progress in the domain of semantic mediation, in a systematic and comparable manner. The results show an average improvement in the execution time by DISMED of 55% compared to the second best alternative in four out of the five use-cases of the experimental evaluation.
23834263	Search strategies in systematic reviews in periodontology and implant dentistry.
J. Clin. Periodontol. 20130703 2013Sep
To perform an overview of literature search strategies in systematic reviews (SRs) published in periodontology and implant dentistry. Two electronic databases (PubMed and Cochrane Database of SRs) were searched, independently and in duplicate, for SRs with meta-analyses on interventions, with the last search performed on 11 November 2012. Manual searches of the reference lists of included SRs and 10 specialty dental journals were conducted. Methodological issues of the search strategies of included SRs were assessed with Cochrane collaboration guidelines and AMSTAR recommendations. The search strategies employed in Cochrane and paper-based SRs were compared. A total of 146 SRs with meta-analyses were included, including 19 Cochrane and 127 paper-based SRs. Some issues, such as "the use of keywords," were reported in most of the SRs (86%). Other issues, such as "search of grey literature" and "language restriction," were not fully reported (34% and 50% respectively). The quality of search strategy reporting in Cochrane SRs was better than that of paper-based SRs for seven of the eight criteria assessed. There is room for improving the quality of reporting of search strategies in SRs in periodontology and implant dentistry, particularly in SRs published in paper-based journals.
22958178	A healthcare management system for Turkey based on a service-oriented architecture.
Inform Health Soc Care 20120907 2013Sep
The current Turkish healthcare management system has a structure that is extremely inordinate, cumbersome and inflexible. Furthermore, this structure has no common point of view and thus has no interoperability and responds slowly to innovations. The purpose of this study is to show that using which methods can the Turkish healthcare management system provide a structure that could be more modern, more flexible and more quick to respond to innovations and changes taking advantage of the benefits given by a service-oriented architecture (SOA). In this paper, the Turkish healthcare management system is chosen to be examined since Turkey is considered as one of the Third World countries and the information architecture of the existing healthcare management system of Turkey has not yet been configured with SOA, which is a contemporary innovative approach and should provide the base architecture of the new solution. The innovation of this study is the symbiosis of two main integration approaches, SOA and Health Level 7 (HL7), for integrating divergent healthcare information systems. A model is developed which is based on SOA and enables obtaining a healthcare management system having the SSF standards (HSSP Service Specification Framework) developed by the framework of the HSSP (Healthcare Services Specification Project) under the leadership of HL7 and the Object Management Group.
22958198	Improving diagnostic accuracy using agent-based distributed data mining system.
Inform Health Soc Care 20120907 2013Sep
The use of data mining techniques to improve the diagnostic system accuracy is investigated in this paper. The data mining algorithms aim to discover patterns and extract useful knowledge from facts recorded in databases. Generally, the expert systems are constructed for automating diagnostic procedures. The learning component uses the data mining algorithms to extract the expert system rules from the database automatically. Learning algorithms can assist the clinicians in extracting knowledge automatically. As the number and variety of data sources is dramatically increasing, another way to acquire knowledge from databases is to apply various data mining algorithms that extract knowledge from data. As data sets are inherently distributed, the distributed system uses agents to transport the trained classifiers and uses meta learning to combine the knowledge. Commonsense reasoning is also used in association with distributed data mining to obtain better results. Combining human expert knowledge and data mining knowledge improves the performance of the diagnostic system. This work suggests a framework of combining the human knowledge and knowledge gained by better data mining algorithms on a renal and gallstone data set.
23857419	A portable database of adverse reactions and drug interactions with radiopharmaceuticals.
J Nucl Med Technol 20130715 2013Sep
Our objective was to develop a software application that allows us to easily manage a portable database of information on radiopharmaceutical interactions with drugs or other agents and on radiopharmaceutical adverse effects. The application was developed and compiled with a commercially available data management system and programming language. All data entered into the database came from the scientific literature and were accompanied by their bibliographic references. We developed the database, which we have called Datinrad. To date, it contains 275 drug interactions and 44 records of adverse reactions to radiopharmaceuticals. Datinrad contains all the information published to date on drug-radiopharmaceutical interactions and adverse effects of radiopharmaceuticals and allows users to introduce new data from future publications. The collection of these data and their easy availability to all nuclear medicine personnel will be useful in the recognition of a possible adverse reaction or drug interaction that may alter the radiopharmaceutical biodistribution and lead to a misdiagnosis. This open-access database application is available free of charge in both English and Spanish at www.radiopharmacy.net.
23392560	The Registry of Knowledge Translation Methods and Tools: a resource to support evidence-informed public health.
Int J Public Health 20130208 2013Aug
This paper examines the development of a globally accessible online Registry of Knowledge Translation Methods and Tools to support evidence-informed public health. A search strategy, screening and data extraction tools, and writing template were developed to find, assess, and summarize relevant methods and tools. An interactive website and searchable database were designed to house the registry. Formative evaluation was undertaken to inform refinements. Over 43,000 citations were screened; almost 700 were full-text reviewed, 140 of which were included. By November 2012, 133 summaries were available. Between January 1 and November 30, 2012 over 32,945 visitors from more than 190 countries accessed the registry. Results from 286 surveys and 19 interviews indicated the registry is valued and useful, but would benefit from a more intuitive indexing system and refinements to the summaries. User stories and promotional activities help expand the reach and uptake of knowledge translation methods and tools in public health contexts. The National Collaborating Centre for Methods and Tools' Registry of Methods and Tools is a unique and practical resource for public health decision makers worldwide.
23949142	Implementation of a departmental picture archiving and communication system: a productivity and cost analysis.
Neurosurgery  2013Sep
Digital radiology enhances productivity and results in long-term cost savings. However, the viewing, storage, and sharing of outside imaging studies on compact discs at ambulatory offices and hospitals pose a number of unique challenges to a surgeon's efficiency and clinical workflow. To improve the efficiency and clinical workflow of an academic neurosurgical practice when evaluating patients with outside radiological studies. Open-source software and commercial hardware were used to design and implement a departmental picture archiving and communications system (PACS). The implementation of a departmental PACS system significantly improved productivity and enhanced collaboration in a variety of clinical settings. Using published data on the rate of information technology problems associated with outside studies on compact discs, this system produced a cost savings ranging from $6250 to $33600 and from $43200 to $72000 for 2 cohorts, urgent transfer and spine clinic patients, respectively, therefore justifying the costs of the system in less than a year. The implementation of a departmental PACS system using open-source software is straightforward and cost-effective and results in significant gains in surgeon productivity when evaluating patients with outside imaging studies.
21047206	Recent progress in automatically extracting information from the pharmacogenomic literature.
Pharmacogenomics  2010Oct
The biomedical literature holds our understanding of pharmacogenomics, but it is dispersed across many journals. In order to integrate our knowledge, connect important facts across publications and generate new hypotheses we must organize and encode the contents of the literature. By creating databases of structured pharmocogenomic knowledge, we can make the value of the literature much greater than the sum of the individual reports. We can, for example, generate candidate gene lists or interpret surprising hits in genome-wide association studies. Text mining automatically adds structure to the unstructured knowledge embedded in millions of publications, and recent years have seen a surge in work on biomedical text mining, some specific to pharmacogenomics literature. These methods enable extraction of specific types of information and can also provide answers to general, systemic queries. In this article, we describe the main tasks of text mining in the context of pharmacogenomics, summarize recent applications and anticipate the next phase of text mining applications.
21552244	Minimum information about a marker gene sequence (MIMARKS) and minimum information about any (x) sequence (MIxS) specifications.
Nat. Biotechnol.  2011May
Here we present a standard developed by the Genomic Standards Consortium (GSC) for reporting marker gene sequences--the minimum information about a marker gene sequence (MIMARKS). We also introduce a system for describing the environment from which a biological sample originates. The 'environmental packages' apply to any genome sequence of known origin and can be used in combination with MIMARKS and other GSC checklists. Finally, to establish a unified standard for describing sequence data and to provide a single point of entry for the scientific community to access and learn about GSC checklists, we present the minimum information about any (x) sequence (MIxS). Adoption of MIxS will enhance our ability to analyze natural genetic diversity documented by massive DNA sequencing efforts from myriad ecosystems in our ever-changing biosphere.
21901085	Biomedical cloud computing with Amazon Web Services.
PLoS Comput. Biol. 20110825 2011Aug
In this overview to biomedical computing in the cloud, we discussed two primary ways to use the cloud (a single instance or cluster), provided a detailed example using NGS mapping, and highlighted the associated costs. While many users new to the cloud may assume that entry is as straightforward as uploading an application and selecting an instance type and storage options, we illustrated that there is substantial up-front effort required before an application can make full use of the cloud's vast resources. Our intention was to provide a set of best practices and to illustrate how those apply to a typical application pipeline for biomedical informatics, but also general enough for extrapolation to other types of computational problems. Our mapping example was intended to illustrate how to develop a scalable project and not to compare and contrast alignment algorithms for read mapping and genome assembly. Indeed, with a newer aligner such as Bowtie, it is possible to map the entire African genome using one m2.2xlarge instance in 48 hours for a total cost of approximately $48 in computation time. In our example, we were not concerned with data transfer rates, which are heavily influenced by the amount of available bandwidth, connection latency, and network availability. When transferring large amounts of data to the cloud, bandwidth limitations can be a major bottleneck, and in some cases it is more efficient to simply mail a storage device containing the data to AWS (http://aws.amazon.com/importexport/). More information about cloud computing, detailed cost analysis, and security can be found in references.
22514186	Adapting federated cyberinfrastructure for shared data collection facilities in structural biology.
J Synchrotron Radiat 20120406 2012May
Early stage experimental data in structural biology is generally unmaintained and inaccessible to the public. It is increasingly believed that this data, which forms the basis for each macromolecular structure discovered by this field, must be archived and, in due course, published. Furthermore, the widespread use of shared scientific facilities such as synchrotron beamlines complicates the issue of data storage, access and movement, as does the increase of remote users. This work describes a prototype system that adapts existing federated cyberinfrastructure technology and techniques to significantly improve the operational environment for users and administrators of synchrotron data collection facilities used in structural biology. This is achieved through software from the Virtual Data Toolkit and Globus, bringing together federated users and facilities from the Stanford Synchrotron Radiation Lightsource, the Advanced Photon Source, the Open Science Grid, the SBGrid Consortium and Harvard Medical School. The performance and experience with the prototype provide a model for data management at shared scientific facilities.
22942020	Assignment of protein sequences to existing domain and family classification systems: Pfam and the PDB.
Bioinformatics 20120831 2012Nov1
Automating the assignment of existing domain and protein family classifications to new sets of sequences is an important task. Current methods often miss assignments because remote relationships fail to achieve statistical significance. Some assignments are not as long as the actual domain definitions because local alignment methods often cut alignments short. Long insertions in query sequences often erroneously result in two copies of the domain assigned to the query. Divergent repeat sequences in proteins are often missed. We have developed a multilevel procedure to produce nearly complete assignments of protein families of an existing classification system to a large set of sequences. We apply this to the task of assigning Pfam domains to sequences and structures in the Protein Data Bank (PDB). We found that HHsearch alignments frequently scored more remotely related Pfams in Pfam clans higher than closely related Pfams, thus, leading to erroneous assignment at the Pfam family level. A greedy algorithm allowing for partial overlaps was, thus, applied first to sequence/HMM alignments, then HMM-HMM alignments and then structure alignments, taking care to join partial alignments split by large insertions into single-domain assignments. Additional assignment of repeat Pfams with weaker E-values was allowed after stronger assignments of the repeat HMM. Our database of assignments, presented in a database called PDBfam, contains Pfams for 99.4% of chains &gt;50 residues. The Pfam assignment data in PDBfam are available at http://dunbrack2.fccc.edu/ProtCid/PDBfam, which can be searched by PDB codes and Pfam identifiers. They will be updated regularly.
23304305	Automated extraction of reported statistical analyses: towards a logical representation of clinical trial literature.
AMIA Annu Symp Proc 20121103 2012
Randomized controlled trials are an important source of evidence for guiding clinical decisions when treating a patient. However, given the large number of studies and their variability in quality, determining how to summarize reported results and formalize them as part of practice guidelines continues to be a challenge. We have developed a set of information extraction and annotation tools to automate the identification of key information from papers related to the hypothesis, sample size, statistical test, confidence interval, significance level, and conclusions. We adapted the Automated Sequence Annotation Pipeline to map extracted phrases to relevant knowledge sources. We trained and tested our system on a corpus of 42 full-text articles related to chemotherapy of non-small cell lung cancer. On our test set of 7 papers, we obtained an overall precision of 86%, recall of 78%, and an F-score of 0.82 for classifying sentences. This work represents our efforts towards utilizing this information for quality assessment, meta-analysis, and modeling.
23311978	ISA-TAB-Nano: a specification for sharing nanomaterial research data in spreadsheet-based format.
BMC Biotechnol. 20130114 2013
The high-throughput genomics communities have been successfully using standardized spreadsheet-based formats to capture and share data within labs and among public repositories. The nanomedicine community has yet to adopt similar standards to share the diverse and multi-dimensional types of data (including metadata) pertaining to the description and characterization of nanomaterials. Owing to the lack of standardization in representing and sharing nanomaterial data, most of the data currently shared via publications and data resources are incomplete, poorly-integrated, and not suitable for meaningful interpretation and re-use of the data. Specifically, in its current state, data cannot be effectively utilized for the development of predictive models that will inform the rational design of nanomaterials. We have developed a specification called ISA-TAB-Nano, which comprises four spreadsheet-based file formats for representing and integrating various types of nanomaterial data. Three file formats (Investigation, Study, and Assay files) have been adapted from the established ISA-TAB specification; while the Material file format was developed de novo to more readily describe the complexity of nanomaterials and associated small molecules. In this paper, we have discussed the main features of each file format and how to use them for sharing nanomaterial descriptions and assay metadata. The ISA-TAB-Nano file formats provide a general and flexible framework to record and integrate nanomaterial descriptions, assay data (metadata and endpoint measurements) and protocol information. Like ISA-TAB, ISA-TAB-Nano supports the use of ontology terms to promote standardized descriptions and to facilitate search and integration of the data. The ISA-TAB-Nano specification has been submitted as an ASTM work item to obtain community feedback and to provide a nanotechnology data-sharing standard for public development and adoption.
22804825	HuPho: the human phosphatase portal.
FEBS J. 20120824 2013Jan
Phosphatases and kinases contribute to the regulation of protein phosphorylation homeostasis in the cell. Phosphorylation is a key post-translational modification underlying the regulation of many cellular processes. Thus, a comprehensive picture of phosphatase function and the identification of their target substrates would aid a systematic approach to a mechanistic description of cell signalling. Here we present a website designed to facilitate the retrieval of information about human protein phosphatases. To this end we developed a search engine to recover and integrate information annotated in several publicly available web resources. In addition we present a text-mining-assisted annotation effort aimed at extracting phosphatase related data reported in the scientific literature. The HuPho (human phosphatases) website can be accessed at http://hupho.uniroma2.it.
23981306	The Stanford/PAVA data collection form for coding J waves on routine screening 10second ECGs.
J Electrocardiol  2013 Sep-Oct
The study of J waves and slurs and their association with cardiovascular death is clouded by the lack of a standardized coding or classification methodology. Over the past three years of studying these ECG patterns, we have evolved a Data Entry Form that is designed to resolve some of the key issues. These issues include the effect of other ECG findings, whether the QRS-ST junction occurs before or after the J waves, if contiguous leads are required and rules to distinguish J waves from fragmented QRS complexes. This form is now being used to code the ECGs of 44,000 VA patients and the follow up is being extended to 15years to resolve these issues.
23969380	A coarse to fine minutiae-based latent palmprint matching.
IEEE Trans Pattern Anal Mach Intell  2013Oct
With the availability of live-scan palmprint technology, high resolution palmprint recognition has started to receive significant attention in forensics and law enforcement. In forensic applications, latent palmprints provide critical evidence as it is estimated that about 30 percent of the latents recovered at crime scenes are those of palms. Most of the available high-resolution palmprint matching algorithms essentially follow the minutiae-based fingerprint matching strategy. Considering the large number of minutiae (about 1,000 minutiae in a full palmprint compared to about 100 minutiae in a rolled fingerprint) and large area of foreground region in full palmprints, novel strategies need to be developed for efficient and robust latent palmprint matching. In this paper, a coarse to fine matching strategy based on minutiae clustering and minutiae match propagation is designed specifically for palmprint matching. To deal with the large number of minutiae, a local feature-based minutiae clustering algorithm is designed to cluster minutiae into several groups such that minutiae belonging to the same group have similar local characteristics. The coarse matching is then performed within each cluster to establish initial minutiae correspondences between two palmprints. Starting with each initial correspondence, a minutiae match propagation algorithm searches for mated minutiae in the full palmprint. The proposed palmprint matching algorithm has been evaluated on a latent-to-full palmprint database consisting of 446 latents and 12,489 background full prints. The matching results show a rank-1 identification accuracy of 79.4 percent, which is significantly higher than the 60.8 percent identification accuracy of a state-of-the-art latent palmprint matching algorithm on the same latent database. The average computation time of our algorithm for a single latent-to-full match is about 141 ms for genuine match and 50 ms for impostor match, on a Windows XP desktop system with 2.2-GHz CPU and 1.00-GB RAM. The computation time of our algorithm is an order of magnitude faster than a previously published state-of-the-art-algorithm.
23267200	A graph lattice approach to maintaining and learning dense collections of subgraphs as image features.
IEEE Trans Pattern Anal Mach Intell  2013Oct
Effective object and scene classification and indexing depend on extraction of informative image features. This paper shows how large families of complex image features in the form of subgraphs can be built out of simpler ones through construction of a graph lattice—a hierarchy of related subgraphs linked in a lattice. Robustness is achieved by matching many overlapping and redundant subgraphs, which allows the use of inexpensive exact graph matching, instead of relying on expensive error-tolerant graph matching to a minimal set of ideal model graphs. Efficiency in exact matching is gained by exploitation of the graph lattice data structure. Additionally, the graph lattice enables methods for adaptively growing a feature space of subgraphs tailored to observed data. We develop the approach in the domain of rectilinear line art, specifically for the practical problem of document forms recognition. We are especially interested in methods that require only one or very few labeled training examples per category. We demonstrate two approaches to using the subgraph features for this purpose. Using a bag-of-words feature vector we achieve essentially single-instance learning on a benchmark forms database, following an unsupervised clustering stage. Further performance gains are achieved on a more difficult dataset using a feature voting method and feature selection procedure.
20423399	Evaluation of five search strategies in retrieving qualitative patient-reported electronic data on the impact of pressure ulcers on quality of life.
J Adv Nurs  2010Mar
This paper is a report of a study conducted to compare the effectiveness of qualitative methodology search strategies with subject-specific (health-related quality of life) search strategies in the retrieval of qualitative patient-reported data of the impact of pressure ulcers on health-related quality of life. Methods to locate qualitative patient-reported health-related quality of life research data electronically have undergone little replication and validation. A major problem in searching for this type of data is that it is reported in accounts of both primary qualitative research as well as mixed methods research. We combined five search strategies with terms for pressure ulcer and searched seven electronic databases from inception to October 2007. The sensitivity, specificity, precision and accuracy for each search strategy were assessed. A subject-specific (health-related quality of life) search strategy, developed by us, had a high yield (100% sensitivity), but low specificity (&lt;50%). The research methodology-based strategies had lower yields (sensitivity 72-83%) but high specificity (79-83%). Importantly, subject-specific search strategies identified all studies reporting qualitative patient-reported health-related quality of life data, whereas, research methodology-based strategies did not identify qualitative data reported in mixed method studies, making subject-based strategies more effective in retrieving qualitative patient-reported health-related quality of life research. An important consideration in the health-related quality of life field is that qualitative data are reported in both qualitative and mixed methodology research and searching for this type data involves trade-offs between yield, sensitivity and specificity. Accurate indexing of subject-specific outcomes and methodology used in electronic databases and publications is also needed.
23798227	PIMiner: a web tool for extraction of protein interactions from biomedical literature.
Int J Data Min Bioinform  2013
Information on Protein Interactions (Pls) is valuable for biomedical research, but often lies buried in the scientific literature and cannot be readily retrieved. While much progress has been made over the years in extracting Pls from the literature using computational methods, there is a lack of free, public, user-friendly tools for the discovery of Pls. We developed an online tool for the extraction of PI relationships from PubMed-abstracts, which we name PIMiner. Protein pairs and the words that describe their interactions are reported by PIMiner so that new interactions can be easily detected within text. The interaction likelihood levels are reported too. The option to extract only specific types of interactions is also provided. The PIMiner server can be accessed through a web browser or remotely through a client's command line. PIMiner can process 50,000 PubMed abstracts in approximately 7 min and thus appears suitable for large-scale processing of biological/biomedical literature.
23883409	Pseudonymization of patient identifiers for translational research.
BMC Med Inform Decis Mak 20130724 2013
The usage of patient data for research poses risks concerning the patients' privacy and informational self-determination. Next-generation-sequencing technologies and various other methods gain data from biospecimen, both for translational research and personalized medicine. If these biospecimen are anonymized, individual research results from genomic research, which should be offered to patients in a clinically relevant timeframe, cannot be associated back to the individual. This raises an ethical concern and challenges the legitimacy of anonymized patient samples. In this paper we present a new approach which supports both data privacy and the possibility to give feedback to patients about their individual research results. We examined previously published privacy concepts regarding a streamlined de-pseudonymization process and a patient-based pseudonym as applicable to research with genomic data and warehousing approaches. All concepts identified in the literature review were compared to each other and analyzed for their applicability to translational research projects. We evaluated how these concepts cope with challenges implicated by personalized medicine. Therefore, both person-centricity issues and a separation of pseudonymization and de-pseudonymization stood out as a central theme in our examination. This motivated us to enhance an existing pseudonymization method regarding a separation of duties. The existing concepts rely on external trusted third parties, making de-pseudonymization a multistage process involving additional interpersonal communication, which might cause critical delays in patient care. Therefore we propose an enhanced method with an asymmetric encryption scheme separating the duties of pseudonymization and de-pseudonymization. The pseudonymization service provider is unable to conclude the patient identifier from the pseudonym, but assigns this ability to an authorized third party (ombudsman) instead. To solve person-centricity issues, a collision-resistant function is incorporated into the method. These two facts combined enable us to address essential challenges in translational research. A productive software prototype was implemented to prove the functionality of the suggested translational, data privacy-preserving method. Eventually, we performed a threat analysis to evaluate potential hazards connected with this pseudonymization method. The proposed method offers sustainable organizational simplification regarding an ethically indicated, but secure and controlled process of de-pseudonymizing patients. A pseudonym is patient-centered to allow correlating separate datasets from one patient. Therefore, this method bridges the gap between bench and bedside in translational research while preserving patient privacy. Assigned ombudsmen are able to de-pseudonymize a patient, if an individual research result is clinically relevant.
23947398	Differentiating innovation priorities among stakeholder in hospital care.
BMC Med Inform Decis Mak 20130816 2013
Decisions to adopt a particular innovation may vary between stakeholders because individual stakeholders may disagree on the costs and benefits involved. This may translate to disagreement between stakeholders on priorities in the implementation process, possibly explaining the slow diffusion of innovations in health care. In this study, we explore the differences in stakeholder preferences for innovations, and quantify the difference in stakeholder priorities regarding costs and benefits. The decision support technique called the analytic hierarchy process was used to quantify the preferences of stakeholders for nine information technology (IT) innovations in hospital care. The selection of the innovations was based on a literature review and expert judgments. Decision criteria related to the costs and benefits of the innovations were defined. These criteria were improvement in efficiency, health gains, satisfaction with care process, and investments required. Stakeholders judged the importance of the decision criteria and subsequently prioritized the selected IT innovations according to their expectations of how well the innovations would perform for these decision criteria. The stakeholder groups (patients, nurses, physicians, managers, health care insurers, and policy makers) had different preference structures for the innovations selected. For instance, self-tests were one of the innovations most preferred by health care insurers and managers, owing to their expected positive impacts on efficiency and health gains. However, physicians, nurses and patients strongly doubted the health gains of self-tests, and accordingly ranked self-tests as the least-preferred innovation. The various stakeholder groups had different expectations of the value of the nine IT innovations. The differences are likely due to perceived stakeholder benefits of each innovation, and less to the costs to individual stakeholder groups. This study provides a first exploratory quantitative insight into stakeholder positions concerning innovation in health care, and presents a novel way to study differences in stakeholder preferences. The results may be taken into account by decision makers involved in the implementation of innovations.
22411711	Visualizing next-generation sequencing data with JBrowse.
Brief. Bioinformatics 20120312 2013Mar
JBrowse is a web-based genome browser, allowing many sources of data to be visualized, interpreted and navigated in a coherent visual framework. JBrowse uses efficient data structures, pre-generation of image tiles and client-side rendering to provide a fast, interactive browsing experience. Many of JBrowse's design features make it well suited for visualizing high-volume data, such as aligned next-generation sequencing reads.
23019242	MEDLINE clinical queries are robust when searching in recent publishing years.
J Am Med Inform Assoc 20120927 2013 Mar-Apr
To determine if the PubMed and Ovid MEDLINE clinical queries (which were developed in the publishing year 2000, for the purpose categories therapy, diagnosis, prognosis, etiology, and clinical prediction guides) perform as well when searching in current publishing years. A gold standard database of recently published research literature was created using the McMaster health knowledge refinery (http://hiru.mcmaster.ca/hiru/HIRU_McMaster_HKR.aspx) and its continuously updated database, McMaster PLUS (http://hiru.mcmaster.ca/hiru/HIRU_McMaster_PLUS_projects.aspx). This database contains articles from over 120 clinical journals that are tagged for meeting or not meeting criteria for scientific merit and clinical relevance. The clinical queries sensitive ('broad') and specific ('narrow') search filters were tested in this gold standard database, and sensitivity and specificity were calculated and compared with those originally reported for the clinical queries. In all cases, the sensitivity of the highly sensitive search filters and the specificity of the highly specific search filters did not differ substantively when comparing results derived in 2000 with those derived in a more current database. In addition, in all cases, the specificities for the highly sensitive search filters and the sensitivities for the highly specific search filters remained above 50% when testing them in the current database. These results are reassuring for modern-day searchers. The clinical queries that were derived in the year 2000 perform equally well a decade later. The PubMed and Ovid MEDLINE clinical queries have been revalidated and remain a useful public resource for searching the world's medical literature for research that is most relevant to clinical care.
23077131	Terminology challenges implementing the HL7 context-aware knowledge retrieval ('Infobutton') standard.
J Am Med Inform Assoc 20121016 2013 Mar-Apr
Point-of-care information needs are common and frequently unmet. One solution to this problem is the use of Infobuttons, which are context-sensitive links from electronic health records (EHR) to knowledge resources, sometimes involving an intermediate broker known as an Infobutton Manager. Health Level Seven (HL7) has developed the Context-Aware Knowledge Retrieval (Infobutton) standard to standardize the integration between EHR systems and knowledge resources. While the standard specifies a set of context attributes and standard terminologies, it leaves to knowledge resources the flexibility to decide how to use these attributes and terminologies to retrieve the most relevant content. This paper describes some of the challenges faced by knowledge resources in trying to locate the most relevant content based on the attribute values for a given Infobutton request. Various approaches to content retrieval are discussed, including the role of indexing with standardized codes, the role of text-based search engines together with their ranking algorithms, and the role of hybrid approaches. Knowledge resource developers must carefully consider business rules, heuristics, and precision/recall tradeoffs when implementing the HL7 Infobutton standard.
23819261	Enhancing biomedical concept extraction using semantic relationship weights.
Int J Data Min Bioinform  2013
Scientific publications are often associated with a set of keywords to describe their content. Automating the process of keyword extraction and assignment could be useful in indexing electronic documents and building digital libraries. In this paper we propose a new approach to biomedical Concept Extraction (CE) using semantic features of concept graphs. We represent full-text documents by graphs and map biomedical terms to predefined ontology concepts. We adopt concept relation weights to improve the ranking process of potential key concepts. We perform both objective and human-based subjective evaluations. The results show that using relation weights significantly improves the performance of CE. The results also highlight the subjectivity of the CE procedure as well as of its evaluation.
23865162	MAIL: mining sequential patterns with wildcards.
Int J Data Min Bioinform  2013
Sequential pattern mining is an important research task in many domains, such as biological science. In this paper, we study the problem of mining frequent patterns from sequences with wildcards. The user can specify the gap constraints with flexibility. Given a subject sequence, a minimal support threshold and a gap constraint, we aim to find frequent patterns whose supports in the sequence are no less than the given support threshold. We design an efficient mining algorithm MAIL. Two pattern growth strategies are proposed to improve the completeness and the time efficiency. One is based on the candidate occurrence pruning, and the other uses an occurrence graph. A random data generator is designed to test the completeness on artificial data. Experiments on DNA sequences show that MAIL mines four times more patterns than one of its peers and the time performance is six times faster on average than its another peer. We also give a concrete example in which our algorithm is applied on DNA sequences to find interesting patterns.
24010269	Sample-space-based feature extraction and class preserving projection for gene expression data.
Int J Data Min Bioinform  2013
In order to overcome the problems of high computational complexity and serious matrix singularity for feature extraction using Principal Component Analysis (PCA) and Fisher's Linear Discrinimant Analysis (LDA) in high-dimensional data, sample-space-based feature extraction is presented, which transforms the computation procedure of feature extraction from gene space to sample space by representing the optimal transformation vector with the weighted sum of samples. The technique is used in the implementation of PCA, LDA, Class Preserving Projection (CPP) which is a new method for discriminant feature extraction proposed, and the experimental results on gene expression data demonstrate the effectiveness of the method.
23475332	A new approach for combining knowledge from multiple coexpression networks of microRNAs.
IEEE Trans Biomed Eng 20130307 2013Aug
MicroRNAs (miRNAs) are a class of small noncoding RNAs that are known to have critical functions across various biological processes. Simultaneous activities of multiple miRNAs can be monitored from their expression profiles under various conditions. We often build up coexpression networks from such profiles. Unfortunately, due to the change of experimental setups (or conditions), the expression profiles do change, and consequently, the patterns of the coexpression networks vary. To obtain a robust functional relationship between miRNAs, by integrating different coexpression networks in a systems biology approach, we have to combine them properly. Here, we evaluate the state-of-the-art techniques and propose a novel integrative measure, and a corresponding methodology, that might be useful for identifying the dependence between coexpression and functional similarity. We establish the results by evaluating the expression profiles of miRNAs taken from bone marrow samples of patients with leukemia. The findings highlight the potential of the integrative algorithm in analyzing the expression profiles of miRNAs for further study.
23443959	Developing a smartphone 'app' for public health research: the example of measuring observed smoking in vehicles.
J Epidemiol Community Health 20130226 2013May
We have developed manual methods to gather data on the point prevalence of observed smoking in road vehicles. To enable the widespread international collection of such data, we aimed to develop a smartphone application (app) for this work. We developed specifications for an app that described the: (1) variables that could be collected; (2) transfer of data to an online repository; (3) user interface (including visual schematics) and (4) processes to ensure the data authenticity from distant observers. The app functionality was trialled in roadside situations and the app was made publicly available. The smartphone app and its accompanying website were developed, tested and released over a period of 6 months. Users (n=18) who have registered themselves (and who met authentication criteria), have reported no significant problems with this application to date (observing 20 535 vehicles as of 5 July 2012). The framework, methodology and source code for this project are now freely available online and can be easily adapted for other research purposes. The prevalence of smoking in vehicles was observed in: Poland 2.7% (95% CI 2.3% to 3.1%); Australia 1.0% (95% CI 0.7% to 1.3%); New Zealand 2.9% (95% CI 2.6% to 3.2%)-similar to results using preapp methods in 2011 (3.2%, 95% CI 3.1% to 3.3%). This project indicates that it can be practical and feasible for health researchers to work together with information science researchers and software developers to create smartphone apps for field research in public health. Such apps may be used to collect observational data more widely, effectively and easily than through traditional (non-electronic) methods.
23328262	Using existing data to address important clinical questions in critical care.
Crit. Care Med.  2013Mar
With important technological advances in healthcare delivery and the Internet, clinicians and scientists now have access to overwhelming number of available databases capturing patients with critical illness. Yet, investigators seeking to answer important clinical or research questions with existing data have few resources that adequately describe the available sources and the strengths and limitations of each. This article reviews an approach to selecting a database to address health services and outcomes research questions in critical care, examines several databases that are commonly used for this purpose, and briefly describes some strengths and limitations of each. Narrative review of the medical literature. The available databases that collect information on critically ill patients are numerous and vary in the types of questions they can optimally answer. Selection of a data source must consider not only accessibility but also the quality of the data contained within the database, and the extent to which it captures the necessary variables for the research question. Questions seeking causal associations (e.g., effect of treatment on mortality) usually either require secondary data that contain detailed information about demographics, laboratories, and physiology to best address nonrandom selection or sophisticated study design. Purely descriptive questions (e.g., incidence of respiratory failure) can often be addressed using secondary data with less detail such as administrative claims. Although each database has its own inherent limitations, all secondary analyses will be subject to the same challenges of appropriate study design and good observational research. The literature demonstrates that secondary analyses can have significant impact on critical care practice. While selection of the optimal database for a particular question is a necessary part of high-quality analyses, it is not sufficient to guarantee an unbiased study. Thoughtful and well-constructed study design and analysis approaches remain equally important pillars of robust science. Only through responsible use of existing data will investigators ensure that their study has the greatest impact on critical care practice and outcomes.
23948488	Retrieving clinical evidence: a comparison of PubMed and Google Scholar for quick clinical searches.
J. Med. Internet Res. 20130815 2013
Physicians frequently search PubMed for information to guide patient care. More recently, Google Scholar has gained popularity as another freely accessible bibliographic database. To compare the performance of searches in PubMed and Google Scholar. We surveyed nephrologists (kidney specialists) and provided each with a unique clinical question derived from 100 renal therapy systematic reviews. Each physician provided the search terms they would type into a bibliographic database to locate evidence to answer the clinical question. We executed each of these searches in PubMed and Google Scholar and compared results for the first 40 records retrieved (equivalent to 2 default search pages in PubMed). We evaluated the recall (proportion of relevant articles found) and precision (ratio of relevant to nonrelevant articles) of the searches performed in PubMed and Google Scholar. Primary studies included in the systematic reviews served as the reference standard for relevant articles. We further documented whether relevant articles were available as free full-texts. Compared with PubMed, the average search in Google Scholar retrieved twice as many relevant articles (PubMed: 11%; Google Scholar: 22%; P&lt;.001). Precision was similar in both databases (PubMed: 6%; Google Scholar: 8%; P=.07). Google Scholar provided significantly greater access to free full-text publications (PubMed: 5%; Google Scholar: 14%; P&lt;.001). For quick clinical searches, Google Scholar returns twice as many relevant articles as PubMed and provides greater access to free full-text articles.
23968998	The emergent discipline of health web science.
J. Med. Internet Res. 20130822 2013
The transformative power of the Internet on all aspects of daily life, including health care, has been widely recognized both in the scientific literature and in public discourse. Viewed through the various lenses of diverse academic disciplines, these transformations reveal opportunities realized, the promise of future advances, and even potential problems created by the penetration of the World Wide Web for both individuals and for society at large. Discussions about the clinical and health research implications of the widespread adoption of information technologies, including the Internet, have been subsumed under the disciplinary label of Medicine 2.0. More recently, however, multi-disciplinary research has emerged that is focused on the achievement and promise of the Web itself, as it relates to healthcare issues. In this paper, we explore and interrogate the contributions of the burgeoning field of Web Science in relation to health maintenance, health care, and health policy. From this, we introduce Health Web Science as a subdiscipline of Web Science, distinct from but overlapping with Medicine 2.0. This paper builds on the presentations and subsequent interdisciplinary dialogue that developed among Web-oriented investigators present at the 2012 Medicine 2.0 Conference in Boston, Massachusetts.
23567011	Practical use of self-monitoring of blood glucose data.
J Diabetes Sci Technol 20130301 2013
Self-monitoring of blood glucose provides information about blood glucose control. The data become useful information and knowledge through careful analysis for patterns that are appropriate or can be corrected. Some analyses can be performed on newer blood glucose meters, but most often, this needs to be done on a computer, tablet, or smartphone. There are a few established methods of presenting the data that make analysis easier. In this article, we discuss four types of data presentations and the methods for utilizing them.
24018519	Organising and presenting information.
Stud Health Technol Inform  2013
Information management can be a daunting process for clinicians, health care providers and policy makers within the health care industry. This chapter discusses the importance of information classification and information architecture in the information economy and specific challenges faced within the health care industry. The healthcare sector has industry specific requirements for information management, standards and specifications for information presentation. Classification of information based on information criticality and the value in the health care industry is discussed in this paper. Presentation of information with reference to eHealth standards and specifications for healthcare information systems and their key requirements are also discussed, as are information architecture for eHealth implementation in Australia. This chapter also touches on information management and clinical governance since the importance of information governance is discussed by various researchers and how this is becoming of value to healthcare information management.
23940544	Rotation-invariant features for multi-oriented text detection in natural images.
PLoS ONE 20130805 2013
Texts in natural scenes carry rich semantic information, which can be used to assist a wide range of applications, such as object recognition, image/video retrieval, mapping/navigation, and human computer interaction. However, most existing systems are designed to detect and recognize horizontal (or near-horizontal) texts. Due to the increasing popularity of mobile-computing devices and applications, detecting texts of varying orientations from natural images under less controlled conditions has become an important but challenging task. In this paper, we propose a new algorithm to detect texts of varying orientations. Our algorithm is based on a two-level classification scheme and two sets of features specially designed for capturing the intrinsic characteristics of texts. To better evaluate the proposed method and compare it with the competing algorithms, we generate a comprehensive dataset with various types of texts in diverse real-world scenes. We also propose a new evaluation protocol, which is more suitable for benchmarking algorithms for detecting texts in varying orientations. Experiments on benchmark datasets demonstrate that our system compares favorably with the state-of-the-art algorithms when handling horizontal texts and achieves significantly enhanced performance on variant texts in complex natural scenes.
23047880	Dynamic detection-rate-based bit allocation with genuine interval concealment for binary biometric representation.
IEEE Trans Cybern 20121002 2013Jun
Biometric discretization is a key component in biometric cryptographic key generation. It converts an extracted biometric feature vector into a binary string via typical steps such as segmentation of each feature element into a number of labeled intervals, mapping of each interval-captured feature element onto a binary space, and concatenation of the resulted binary output of all feature elements into a binary string. Currently, the detection rate optimized bit allocation (DROBA) scheme is one of the most effective biometric discretization schemes in terms of its capability to assign binary bits dynamically to user-specific features with respect to their discriminability. However, we learn that DROBA suffers from potential discriminative feature misdetection and underdiscretization in its bit allocation process. This paper highlights such drawbacks and improves upon DROBA based on a novel two-stage algorithm: 1) a dynamic search method to efficiently recapture such misdetected features and to optimize the bit allocation of underdiscretized features and 2) a genuine interval concealment technique to alleviate crucial information leakage resulted from the dynamic search. Improvements in classification accuracy on two popular face data sets vindicate the feasibility of our approach compared with DROBA.
23144039	Efficient shortest-path-tree computation in network routing based on pulse-coupled neural networks.
IEEE Trans Cybern 20121023 2013Jun
Shortest path tree (SPT) computation is a critical issue for routers using link-state routing protocols, such as the most commonly used open shortest path first and intermediate system to intermediate system. Each router needs to recompute a new SPT rooted from itself whenever a change happens in the link state. Most commercial routers do this computation by deleting the current SPT and building a new one using static algorithms such as the Dijkstra algorithm at the beginning. Such recomputation of an entire SPT is inefficient, which may consume a considerable amount of CPU time and result in a time delay in the network. Some dynamic updating methods using the information in the updated SPT have been proposed in recent years. However, there are still many limitations in those dynamic algorithms. In this paper, a new modified model of pulse-coupled neural networks (M-PCNNs) is proposed for the SPT computation. It is rigorously proved that the proposed model is capable of solving some optimization problems, such as the SPT. A static algorithm is proposed based on the M-PCNNs to compute the SPT efficiently for large-scale problems. In addition, a dynamic algorithm that makes use of the structure of the previously computed SPT is proposed, which significantly improves the efficiency of the algorithm. Simulation results demonstrate the effective and efficient performance of the proposed approach.
23193244	GFM-based methods for speaker identification.
IEEE Trans Cybern 20121026 2013Jun
This paper presents three novel methods for speaker identification of which two methods utilize both the continuous density hidden Markov model (HMM) and the generalized fuzzy model (GFM), which has the advantages of both Mamdani and Takagi-Sugeno models. In the first method, the HMM is utilized for the extraction of shape-based batch feature vector that is fitted with the GFM to identify the speaker. On the other hand, the second method makes use of the Gaussian mixture model (GMM) and the GFM for the identification of speakers. Finally, the third method has been inspired by the way humans cash in on the mutual acquaintances while identifying a speaker. To see the validity of the proposed models [HMM-GFM, GMM-GFM, and HMM-GFM (fusion)] in a real-life scenario, they are tested on VoxForge speech corpus and on the subset of the 2003 National Institute of Standards and Technology evaluation data set. These models are also evaluated on the corrupted VoxForge speech corpus by mixing with different types of noisy signals at different values of signal-to-noise ratios, and their performance is found superior to that of the well-known models.
23047881	Most probable longest common subsequence for recognition of gesture character input.
IEEE Trans Cybern 20121003 2013Jun
This paper presents a technique for trajectory classification with applications to dynamic free-air hand gesture recognition. Such gestures are unencumbered and drawn in free air. Our approach is an extension to the longest common subsequence (LCS) classification algorithm. A learning preprocessing stage is performed to create a probabilistic 2-D template for each gesture, which allows taking into account different trajectory distortions with different probabilities. The modified LCS, termed the most probable LCS (MPLCS), is developed to measure the similarity between the probabilistic template and the hand gesture sample. The final decision is based on the length and probability of the extracted subsequence. Validation tests using a cohort of gesture digits from video-based capture show that the approach is promising with a recognition rate of more than 98 % for video stream preisolated digits. The MPLCS algorithm can be integrated into a gesture recognition interface to facilitate gesture character input. This can greatly enhance the usability of such interfaces.
24083315	Error analysis of stochastic gradient descent ranking.
IEEE Trans Cybern  2013Jun
Ranking is always an important task in machine learning and information retrieval, e.g., collaborative filtering, recommender systems, drug discovery, etc. A kernel-based stochastic gradient descent algorithm with the least squares loss is proposed for ranking in this paper. The implementation of this algorithm is simple, and an expression of the solution is derived via a sampling operator and an integral operator. An explicit convergence rate for leaning a ranking function is given in terms of the suitable choices of the step size and the regularization parameter. The analysis technique used here is capacity independent and is novel in error analysis of ranking learning. Experimental results on real-world data have shown the effectiveness of the proposed algorithm in ranking tasks, which verifies the theoretical analysis in ranking error.
23731824	A study of the influence of task familiarity on user behaviors and performance with a MeSH term suggestion interface for PubMed bibliographic search.
Int J Med Inform 20130531 2013Sep
Previous research has shown that information seekers in biomedical domain need more support in formulating their queries. A user study was conducted to evaluate the effectiveness of a metadata based query suggestion interface for PubMed bibliographic search. The study also investigated the impact of search task familiarity on search behaviors and the effectiveness of the interface. A real user, user search request and real system approach was used for the study. Unlike tradition IR evaluation, where assigned tasks were used, the participants were asked to search requests of their own. Forty-four researchers in Health Sciences participated in the evaluation - each conducted two research requests of their own, alternately with the proposed interface and the PubMed baseline. Several performance criteria were measured to assess the potential benefits of the experimental interface, including users' assessment of their original and eventual queries, the perceived usefulness of the interfaces, satisfaction with the search results, and the average relevance score of the saved records. The results show that, when searching for an unfamiliar topic, users were more likely to change their queries, indicating the effect of familiarity on search behaviors. The results also show that the interface scored higher on several of the performance criteria, such as the "goodness" of the queries, perceived usefulness, and user satisfaction. Furthermore, in line with our hypothesis, the proposed interface was relatively more effective when less familiar search requests were attempted. Results indicate that there is a selective compatibility between search familiarity and search interface. One implication of the research for system evaluation is the importance of taking into consideration task familiarity when assessing the effectiveness of interactive IR systems.
23770027	Unintended adverse consequences of introducing electronic health records in residential aged care homes.
Int J Med Inform 20130614 2013Sep
The aim of this study was to investigate the unintended adverse consequences of introducing electronic health records (EHR) in residential aged care homes (RACHs) and to examine the causes of these unintended adverse consequences. A qualitative interview study was conducted in nine RACHs belonging to three organisations in the Australian Capital Territory (ACT), New South Wales (NSW) and Queensland, Australia. A longitudinal investigation after the implementation of the aged care EHR systems was conducted at two data points: January 2009 to December 2009 and December 2010 to February 2011. Semi-structured interviews were conducted with 110 care staff members identified through convenience sampling, representing all levels of care staff who worked in these facilities. Data analysis was guided by DeLone and McLean Information Systems Success Model, in reference with the previous studies of unintended consequences for the introduction of computerised provider order entry systems in hospitals. Eight categories of unintended adverse consequences emerged from 266 data items mentioned by the interviewees. In descending order of the number and percentage of staff mentioning them, they are: inability/difficulty in data entry and information retrieval, end user resistance to using the system, increased complexity of information management, end user concerns about access, increased documentation burden, the reduction of communication, lack of space to place enough computers in the work place and increasing difficulties in delivering care services. The unintended consequences were caused by the initial conditions, the nature of the EHR system and the way the system was implemented and used by nursing staff members. Although the benefits of the EHR systems were obvious, as found by our previous study, introducing EHR systems in RACH can also cause adverse consequences of EHR avoidance, difficulty in access, increased complexity in information management, increased documentation burden, reduction of communication and the risks of lacking care follow-up, which may cause negative effects on aged care services. Further research can focus on investigating how the unintended adverse consequences can be mitigated or eliminated by understanding more about nursing staff's work as well as the information flow in RACH. This will help to improve the design, introduction and management of EHR systems in this setting.
23786709	Description and comparison of documentation of nursing assessment between paper-based and electronic systems in Australian aged care homes.
Int J Med Inform 20130617 2013Sep
To describe nursing assessment documentation practices in aged care organizations and to evaluate the quality of electronic versus paper-based documentation of nursing assessment. This was a retrospective nursing documentation audit study. Study samples were 2299 paper-based and 6997 electronic resident assessment forms contained in 159 paper-based and 249 electronic resident nursing records, respectively, from three aged care organizations. The practice of nursing assessment documentation in participating aged care homes was described. Three attributes of quality of nursing assessment documentation were evaluated: format and structure, process, and content by seven measures: quantity, completeness, timeliness comprehensiveness, frequencies of documentation specific to care domains and data items, and whether assessment forms were signed and dated. Varying practice in documentation of nursing assessment was found among different aged care organizations and homes. Electronic resident records contained higher numbers and more comprehensive resident assessment forms than paper-based records. The frequency of documentation was higher in electronic than in paper-based records in relation to most care domains. There was no difference between the two types of documentation systems on other aspects of nursing assessment documentation (overall completeness and timeliness, variation of frequencies among different care domains, and item completion in personal hygiene assessment forms). Electronic nursing documentation systems could improve the quality of documentation structure and format, process and content in the aspects of quantity, comprehensiveness and signing and dating of assessment forms. Further studies are needed to understand the factors leading to the variations of practice and the limitations of nursing assessment documentation and to evaluate documentation quality from a clinical perspective.
24289158	wKinMut: an integrated tool for the analysis and interpretation of mutations in human protein kinases.
BMC Bioinformatics 20131129 2013
Protein kinases are involved in relevant physiological functions and a broad number of mutations in this superfamily have been reported in the literature to affect protein function and stability. Unfortunately, the exploration of the consequences on the phenotypes of each individual mutation remains a considerable challenge. The wKinMut web-server offers direct prediction of the potential pathogenicity of the mutations from a number of methods, including our recently developed prediction method based on the combination of information from a range of diverse sources, including physicochemical properties and functional annotations from FireDB and Swissprot and kinase-specific characteristics such as the membership to specific kinase groups, the annotation with disease-associated GO terms or the occurrence of the mutation in PFAM domains, and the relevance of the residues in determining kinase subfamily specificity from S3Det. This predictor yields interesting results that compare favourably with other methods in the field when applied to protein kinases.Together with the predictions, wKinMut offers a number of integrated services for the analysis of mutations. These include: the classification of the kinase, information about associations of the kinase with other proteins extracted from iHop, the mapping of the mutations onto PDB structures, pathogenicity records from a number of databases and the classification of mutations in large-scale cancer studies. Importantly, wKinMut is connected with the SNP2L system that extracts mentions of mutations directly from the literature, and therefore increases the possibilities of finding interesting functional information associated to the studied mutations. wKinMut facilitates the exploration of the information available about individual mutations by integrating prediction approaches with the automatic extraction of information from the literature (text mining) and several state-of-the-art databases.wKinMut has been used during the last year for the analysis of the consequences of mutations in the context of a number of cancer genome projects, including the recent analysis of Chronic Lymphocytic Leukemia cases and is publicly available at http://wkinmut.bioinfo.cnio.es.
23274962	Three tools for the real-time simulation of embodied spiking neural networks using GPUs.
Neuroinformatics  2013Jul
This paper presents a toolbox of solutions that enable the user to construct biologically-inspired spiking neural networks with tens of thousands of neurons and millions of connections that can be simulated in real time, visualized in 3D and connected to robots and other devices. NeMo is a high performance simulator that works with a variety of neural and oscillator models and performs parallel simulations on either GPUs or multi-core processors. SpikeStream is a visualization and analysis environment that works with NeMo and can construct networks, store them in a database and visualize their activity in 3D. The iSpike library provides biologically-inspired conversion between real data and spike representations to support work with robots, such as the iCub. Each of the tools described in this paper can be used independently with other software, and they also work well together.
23979480	GPS/MEMS INS data fusion and map matching in urban areas.
Sensors (Basel) 20130823 2013
This paper presents an evaluation of the map-matching scheme of an integrated GPS/INS system in urban areas. Data fusion using a Kalman filter and map matching are effective approaches to improve the performance of navigation system applications based on GPS/MEMS IMUs. The study considers the curve-to-curve matching algorithm after Kalman filtering to correct mismatch and eliminate redundancy. By applying data fusion and map matching, the study easily accomplished mapping of a GPS/INS trajectory onto the road network. The results demonstrate the effectiveness of the algorithms in controlling the INS drift error and indicate the potential of low-cost MEMS IMUs in navigation applications.
24002231	A security analysis of the 802.11s wireless mesh network routing protocol and its secure routing protocols.
Sensors (Basel) 20130902 2013
Wireless mesh networks (WMNs) can act as a scalable backbone by connecting separate sensor networks and even by connecting WMNs to a wired network. The Hybrid Wireless Mesh Protocol (HWMP) is the default routing protocol for the 802.11s WMN. The routing protocol is one of the most important parts of the network, and it requires protection, especially in the wireless environment. The existing security protocols, such as the Broadcast Integrity Protocol (BIP), Counter with cipher block chaining message authentication code protocol (CCMP), Secure Hybrid Wireless Mesh Protocol (SHWMP), Identity Based Cryptography HWMP (IBC-HWMP), Elliptic Curve Digital Signature Algorithm HWMP (ECDSA-HWMP), and Watchdog-HWMP aim to protect the HWMP frames. In this paper, we have analyzed the vulnerabilities of the HWMP and developed security requirements to protect these identified vulnerabilities. We applied the security requirements to analyze the existing secure schemes for HWMP. The results of our analysis indicate that none of these protocols is able to satisfy all of the security requirements. We also present a quantitative complexity comparison among the protocols and an example of a security scheme for HWMP to demonstrate how the result of our research can be utilized. Our research results thus provide a tool for designing secure schemes for the HWMP.
24008284	Distributed pedestrian detection alerts based on data fusion with accurate localization.
Sensors (Basel) 20130904 2013
Among Advanced Driver Assistance Systems (ADAS) pedestrian detection is a common issue due to the vulnerability of pedestrians in the event of accidents. In the present work, a novel approach for pedestrian detection based on data fusion is presented. Data fusion helps to overcome the limitations inherent to each detection system (computer vision and laser scanner) and provides accurate and trustable tracking of any pedestrian movement. The application is complemented by an efficient communication protocol, able to alert vehicles in the surroundings by a fast and reliable communication. The combination of a powerful location, based on a GPS with inertial measurement, and accurate obstacle localization based on data fusion has allowed locating the detected pedestrians with high accuracy. Tests proved the viability of the detection system and the efficiency of the communication, even at long distances. By the use of the alert communication, dangerous situations such as occlusions or misdetections can be avoided.
24036583	Analytic performance prediction of track-to-track association with biased data in multi-sensor multi-target tracking scenarios.
Sensors (Basel) 20130912 2013
An analytic method for predicting the performance of track-to-track association (TTTA) with biased data in multi-sensor multi-target tracking scenarios is proposed in this paper. The proposed method extends the existing results of the bias-free situation by accounting for the impact of sensor biases. Since little insight of the intrinsic relationship between scenario parameters and the performance of TTTA can be obtained by numerical simulations, the proposed analytic approach is a potential substitute for the costly Monte Carlo simulation method. Analytic expressions are developed for the global nearest neighbor (GNN) association algorithm in terms of correct association probability. The translational biases of sensors are incorporated in the expressions, which provide good insight into how the TTTA performance is affected by sensor biases, as well as other scenario parameters, including the target spatial density, the extraneous track density and the average association uncertainty error. To show the validity of the analytic predictions, we compare them with the simulation results, and the analytic predictions agree reasonably well with the simulations in a large range of normally anticipated scenario parameters.
24036585	Optimal sensor arrangements in Angle of Arrival (AoA) and range based localization with linear sensor arrays.
Sensors (Basel) 20130912 2013
This paper investigates the linear separation requirements for Angle-of-Arrival (AoA) and range sensors, in order to achieve the optimal performance in estimating the position of a target from multiple and typically noisy sensor measurements. We analyse the sensor-target geometry in terms of the Cramer-Rao inequality and the corresponding Fisher information matrix, in order to characterize localization performance with respect to the linear spatial distribution of sensors. Here in this paper, we consider both fixed and adjustable linear sensor arrays.
24048340	Data processing and quality evaluation of a boat-based mobile laser scanning system.
Sensors (Basel) 20130917 2013
Mobile mapping systems (MMSs) are used for mapping topographic and urban features which are difficult and time consuming to measure with other instruments. The benefits of MMSs include efficient data collection and versatile usability. This paper investigates the data processing steps and quality of a boat-based mobile mapping system (BoMMS) data for generating terrain and vegetation points in a river environment. Our aim in data processing was to filter noise points, detect shorelines as well as points below water surface and conduct ground point classification. Previous studies of BoMMS have investigated elevation accuracies and usability in detection of fluvial erosion and deposition areas. The new findings concerning BoMMS data are that the improved data processing approach allows for identification of multipath reflections and shoreline delineation. We demonstrate the possibility to measure bathymetry data in shallow (0-1 m) and clear water. Furthermore, we evaluate for the first time the accuracy of the BoMMS ground points classification compared to manually classified data. We also demonstrate the spatial variations of the ground point density and assess elevation and vertical accuracies of the BoMMS data.
24051523	Ontology alignment architecture for semantic sensor Web integration.
Sensors (Basel) 20130918 2013
Sensor networks are a concept that has become very popular in data acquisition and processing for multiple applications in different fields such as industrial, medicine, home automation, environmental detection, etc. Today, with the proliferation of small communication devices with sensors that collect environmental data, semantic Web technologies are becoming closely related with sensor networks. The linking of elements from Semantic Web technologies with sensor networks has been called Semantic Sensor Web and has among its main features the use of ontologies. One of the key challenges of using ontologies in sensor networks is to provide mechanisms to integrate and exchange knowledge from heterogeneous sources (that is, dealing with semantic heterogeneity). Ontology alignment is the process of bringing ontologies into mutual agreement by the automatic discovery of mappings between related concepts. This paper presents a system for ontology alignment in the Semantic Sensor Web which uses fuzzy logic techniques to combine similarity measures between entities of different ontologies. The proposed approach focuses on two key elements: the terminological similarity, which takes into account the linguistic and semantic information of the context of the entity's names, and the structural similarity, based on both the internal and relational structure of the concepts. This work has been validated using sensor network ontologies and the Ontology Alignment Evaluation Initiative (OAEI) tests. The results show that the proposed techniques outperform previous approaches in terms of precision and recall.
24494442	The use and misuse of biomedical data: is bigger really better?
Am J Law Med  2013
Very large biomedical research databases, containing electronic health records (EHR) and genomic data from millions of patients, have been heralded recently for their potential to accelerate scientific discovery and produce dramatic improvements in medical treatments. Research enabled by these databases may also lead to profound changes in law, regulation, social policy, and even litigation strategies. Yet, is "big data" necessarily better data? This paper makes an original contribution to the legal literature by focusing on what can go wrong in the process of biomedical database research and what precautions are necessary to avoid critical mistakes. We address three main reasons for approaching such research with care and being cautious in relying on its outcomes for purposes of public policy or litigation. First, the data contained in biomedical databases is surprisingly likely to be incorrect or incomplete. Second, systematic biases, arising from both the nature of the data and the preconceptions of investigators, are serious threats to the validity of research results, especially in answering causal questions. Third, data mining of biomedical databases makes it easier for individuals with political, social, or economic agendas to generate ostensibly scientific but misleading research findings for the purpose of manipulating public opinion and swaying policymakers. In short, this paper sheds much-needed light on the problems of credulous and uninformed acceptance of research results derived from biomedical databases. An understanding of the pitfalls of big data analysis is of critical importance to anyone who will rely on or dispute its outcomes, including lawyers, policymakers, and the public at large. The Article also recommends technical, methodological, and educational interventions to combat the dangers of database errors and abuses.
24505652	Estimating constrained multi-fiber diffusion MR volumes by orientation clustering.
Med Image Comput Comput Assist Interv  2013
Diffusion MRI is a valuable tool for mapping tissue microstructure; however, multi-fiber models present challenges to image analysis operations. In this paper, we present a method for estimating models for such operations by clustering fiber orientations. Our approach is applied to ball-and-stick diffusion models, which include an isotropic tensor and multiple sticks encoding fiber volume and orientation. We consider operations which can be generalized to a weighted combination of fibers and present a method for representing such combinations with a mixture-of-Watsons model, learning its parameters by Expectation Maximization. We evaluate this approach with two experiments. First, we show it is effective for filtering in the presence of synthetic noise. Second, we demonstrate interpolation and averaging by construction of a tractography atlas, showing improved reconstruction of white matter pathways. These experiments indicate that our method is useful in estimating multi-fiber ball-and-stick diffusion volumes resulting from a range of image analysis operations.
24505669	Constructing an un-biased whole body atlas from clinical imaging data by fragment bundling.
Med Image Comput Comput Assist Interv  2013
Atlases have a tremendous impact on the study of anatomy and function, such as in neuroimaging, or cardiac analysis. They provide a means to compare corresponding measurements across populations, or model the variability in a population. Current approaches to construct atlases rely on examples that show the same anatomical structure (e.g., the brain). If we study large heterogeneous clinical populations to capture subtle characteristics of diseases, we cannot assume consistent image acquisition any more. Instead we have to build atlases from imaging data that show only parts of the overall anatomical structure. In this paper we propose a method for the automatic contruction of an un-biased whole body atlas from so-called fragments. Experimental results indicate that the fragment based atlas improves the representation accuracy of the atlas over an initial whole body template initialization.
24505701	Segmentation of the left ventricle using distance regularized two-layer level set approach.
Med Image Comput Comput Assist Interv  2013
We propose a novel two-layer level set approach for segmentation of the left ventricle (LV) from cardiac magnetic resonance (CMR) short-axis images. In our method, endocardium and epicardium are represented by two specified level contours of a level set function. Segmentation of the LV is formulated as a problem of optimizing the level set function such that these two level contours best fit the epicardium and endocardium. More importantly, a distance regularization (DR) constraint on the level contours is introduced to preserve smoothly varying distance between them. This DR constraint leads to a desirable interaction between the level contours that contributes to maintain the anatomical geometry of the endocardium and epicardium. The negative influence of intensity inhomogeneities on image segmentation are overcome by using a data term derived from a local intensity clustering property. Our method is quantitatively validated by experiments on the datasets for the MICCAI grand challenge on left ventricular segmentation, which demonstrates the advantages of our method in terms of segmentation accuracy and consistency with anatomical geometry.
24505710	Globally optimal curvature-regularized fast marching for vessel segmentation.
Med Image Comput Comput Assist Interv  2013
We introduce a novel fast marching approach with curvature regularization for vessel segmentation. Since most vessels have a smooth path, curvature can be used to distinguish desired vessels from short cuts, which usually contain parts with high curvature. However, in previous fast marching approaches, curvature information is not available, so it cannot be used for regularization directly. Instead, usually length regularization is used under the assumption that shorter paths should also have a lower curvature. However, for vessel segmentation, this assumption often does not hold and leads to short cuts. We propose an approach, which integrates curvature regularization directly into the fast marching framework, independent of length regularization. Our approach is globally optimal, and numerical experiments on synthetic and real retina images show that our approach yields more accurate results than two previous approaches.
24505713	Normalisation of neonatal brain network measures using stochastic approaches.
Med Image Comput Comput Assist Interv  2013
Diffusion tensor imaging, tractography and the subsequent derivation of network measures are becoming an established approach in the exploration of brain connectivity. However, no gold standard exists in respect to how the brain should be parcellated and therefore a variety of atlas- and random-based parcellation methods are used. The resulting challenge of comparing graphs with differing numbers of nodes and uncertain node correspondences necessitates the use of normalisation schemes to enable meaningful intra- and inter-subject comparisons. This work proposes methods for normalising brain network measures using random graphs. We show that the normalised measures are locally stable over distinct random parcellations of the same subject and, applying it to a neonatal serial diffusion MRI data set, we demonstrate their potential in characterising changes in brain connectivity during early development.
24505733	Multiple sclerosis lesion segmentation using dictionary learning and sparse coding.
Med Image Comput Comput Assist Interv  2013
The segmentation of lesions in the brain during the development of Multiple Sclerosis is part of the diagnostic assessment for this disease and gives information on its current severity. This laborious process is still carried out in a manual or semiautomatic fashion by clinicians because published automatic approaches have not been universal enough to be widely employed in clinical practice. Thus Multiple Sclerosis lesion segmentation remains an open problem. In this paper we present a new unsupervised approach addressing this problem with dictionary learning and sparse coding methods. We show its general applicability to the problem of lesion segmentation by evaluating our approach on synthetic and clinical image data and comparing it to state-of-the-art methods. Furthermore the potential of using dictionary learning and sparse coding for such segmentation tasks is investigated and various possibilities for further experiments are discussed.
24505734	Deformable atlas for multi-structure segmentation.
Med Image Comput Comput Assist Interv  2013
We develop a novel deformable atlas method for multistructure segmentation that seamlessly combines the advantages of image-based and atlas-based methods. The method formulates a probabilistic framework that combines prior anatomical knowledge with image-based cues that are specific to the subject's anatomy, and solves it using expectation-maximization method. It improves the segmentation over conventional label fusion methods especially around the structure boundaries, and is robust to large anatomical variation. The proposed method was applied to segment multiple structures in both normal and diseased brains and was shown to significantly improve results especially in diseased brains.
24505735	Hierarchical probabilistic Gabor and MRF segmentation of brain tumours in MRI volumes.
Med Image Comput Comput Assist Interv  2013
In this paper, we present a fully automated hierarchical probabilistic framework for segmenting brain tumours from multispectral human brain magnetic resonance images (MRIs) using multiwindow Gabor filters and an adapted Markov Random Field (MRF) framework. In the first stage, a customised Gabor decomposition is developed, based on the combined-space characteristics of the two classes (tumour and non-tumour) in multispectral brain MRIs in order to optimally separate tumour (including edema) from healthy brain tissues. A Bayesian framework then provides a coarse probabilistic texture-based segmentation of tumours (including edema) whose boundaries are then refined at the voxel level through a modified MRF framework that carefully separates the edema from the main tumour. This customised MRF is not only built on the voxel intensities and class labels as in traditional MRFs, but also models the intensity differences between neighbouring voxels in the likelihood model, along with employing a prior based on local tissue class transition probabilities. The second inference stage is shown to resolve local inhomogeneities and impose a smoothing constraint, while also maintaining the appropriate boundaries as supported by the local intensity difference observations. The method was trained and tested on the publicly available MICCAI 2012 Brain Tumour Segmentation Challenge (BRATS) Database [1] on both synthetic and clinical volumes (low grade and high grade tumours). Our method performs well compared to state-of-the-art techniques, outperforming the results of the top methods in cases of clinical high grade and low grade tumour core segmentation by 40% and 45% respectively.
24505745	Atlas encoding by randomized forests for efficient label propagation.
Med Image Comput Comput Assist Interv  2013
We propose a method for multi-atlas label propagation based on encoding the individual atlases by randomized classification forests. Most current approaches perform a non-linear registration between all atlases and the target image, followed by a sophisticated fusion scheme. While these approaches can achieve high accuracy, in general they do so at high computational cost. This negatively affects the scalability to large databases and experimentation. To tackle this issue, we propose to use a small and deep classification forest to encode each atlas individually in reference to an aligned probabilistic atlas, resulting in an Atlas Forest (AF). At test time, each AF yields a probabilistic label estimate, and fusion is done by averaging. Our scheme performs only one registration per target image, achieves good results with a simple fusion scheme, and allows for efficient experimentation. In contrast to standard forest schemes, incorporation of new scans is possible without retraining, and target-specific selection of atlases remains possible. The evaluation on three different databases shows accuracy at the level of the state of the art, at a significantly lower runtime.
21785142	Ensembl BioMarts: a hub for data retrieval across taxonomic space.
Database (Oxford) 20110723 2011
For a number of years the BioMart data warehousing system has proven to be a valuable resource for scientists seeking a fast and versatile means of accessing the growing volume of genomic data provided by the Ensembl project. The launch of the Ensembl Genomes project in 2009 complemented the Ensembl project by utilizing the same visualization, interactive and programming tools to provide users with a means for accessing genome data from a further five domains: protists, bacteria, metazoa, plants and fungi. The Ensembl and Ensembl Genomes BioMarts provide a point of access to the high-quality gene annotation, variation data, functional and regulatory annotation and evolutionary relationships from genomes spanning the taxonomic space. This article aims to give a comprehensive overview of the Ensembl and Ensembl Genomes BioMarts as well as some useful examples and a description of current data content and future objectives. Database URLs: http://www.ensembl.org/biomart/martview/; http://metazoa.ensembl.org/biomart/martview/; http://plants.ensembl.org/biomart/martview/; http://protists.ensembl.org/biomart/martview/; http://fungi.ensembl.org/biomart/martview/; http://bacteria.ensembl.org/biomart/martview/.
21936816	Collation and data-mining of literature bioactivity data for drug discovery.
Biochem. Soc. Trans.  2011Oct
The challenge of translating the huge amount of genomic and biochemical data into new drugs is a costly and challenging task. Historically, there has been comparatively little focus on linking the biochemical and chemical worlds. To address this need, we have developed ChEMBL, an online resource of small-molecule SAR (structure-activity relationship) data, which can be used to support chemical biology, lead discovery and target selection in drug discovery. The database contains the abstracted structures, properties and biological activities for over 700000 distinct compounds and in excess of more than 3 million bioactivity records abstracted from over 40000 publications. Additional public domain resources can be readily integrated into the same data model (e.g. PubChem BioAssay data). The compounds in ChEMBL are largely extracted from the primary medicinal chemistry literature, and are therefore usually 'drug-like' or 'lead-like' small molecules with full experimental context. The data cover a significant fraction of the discovery of modern drugs, and are useful in a wide range of drug design and discovery tasks. In addition to the compound data, ChEMBL also contains information for over 8000 protein, cell line and whole-organism 'targets', with over 4000 of those being proteins linked to their underlying genes. The database is searchable both chemically, using an interactive compound sketch tool, protein sequences, family hierarchies, SMILES strings, compound research codes and key words, and biologically, using a variety of gene identifiers, protein sequence similarity and protein families. The information retrieved can then be readily filtered and downloaded into various formats. ChEMBL can be accessed online at https://www.ebi.ac.uk/chembldb.
22581690	Toward a mtDNA locus-specific mutation database using the LOVD platform.
Hum. Mutat. 20120702 2012Sep
The Human Variome Project (HVP) is a global effort to collect and curate all human genetic variation affecting health. Mutations of mitochondrial DNA (mtDNA) are an important cause of neurogenetic disease in humans; however, identification of the pathogenic mutations responsible can be problematic. In this article, we provide explanations as to why and suggest how such difficulties might be overcome. We put forward a case in support of a new Locus Specific Mutation Database (LSDB) implemented using the Leiden Open-source Variation Database (LOVD) system that will not only list primary mutations, but also present the evidence supporting their role in disease. Critically, we feel that this new database should have the capacity to store information on the observed phenotypes alongside the genetic variation, thereby facilitating our understanding of the complex and variable presentation of mtDNA disease. LOVD supports fast queries of both seen and hidden data and allows storage of sequence variants from high-throughput sequence analysis. The LOVD platform will allow construction of a secure mtDNA database; one that can fully utilize currently available data, as well as that being generated by high-throughput sequencing, to link genotype with phenotype enhancing our understanding of mitochondrial disease, with a view to providing better prognostic information.
23715317	[Medical image compression: a review].
Biomedica  2013 Jan-Mar
Modern medicine is an increasingly complex activity , based on the evidence ; it consists of information from multiple sources : medical record text , sound recordings , images and videos generated by a large number of devices . Medical imaging is one of the most important sources of information since they offer comprehensive support of medical procedures for diagnosis and follow-up . However , the amount of information generated by image capturing gadgets quickly exceeds storage availability in radiology services , generating additional costs in devices with greater storage capacity . Besides , the current trend of developing applications in cloud computing has limitations, even though virtual storage is available from anywhere, connections are made through internet . In these scenarios the optimal use of information necessarily requires powerful compression algorithms adapted to medical activity needs . In this paper we present a review of compression techniques used for image storage , and a critical analysis of them from the point of view of their use in clinical settings.
23887962	An innovative technique for recording picture-in-picture ultrasound videos.
J Ultrasound Med  2013Aug
Many ultrasound educational products and ultrasound researchers present diagnostic and interventional ultrasound information using picture-in-picture videos, which simultaneously show the ultrasound image and transducer and patient positions. Traditional techniques for creating picture-in-picture videos are expensive, nonportable, or time-consuming. This article describes an inexpensive, simple, and portable way of creating picture-in-picture ultrasound videos. This technique uses a laptop computer with a video capture device to acquire the ultrasound feed. Simultaneously, a webcam captures a live video feed of the transducer and patient position and live audio. Both sources are streamed onto the computer screen and recorded by screen capture software. This technique makes the process of recording picture-in-picture ultrasound videos more accessible for ultrasound educators and researchers for use in their presentations or publications.
23550210	Integrative analysis of complex cancer genomics and clinical profiles using the cBioPortal.
Sci Signal 20130402 2013Apr2
The cBioPortal for Cancer Genomics (http://cbioportal.org) provides a Web resource for exploring, visualizing, and analyzing multidimensional cancer genomics data. The portal reduces molecular profiling data from cancer tissues and cell lines into readily understandable genetic, epigenetic, gene expression, and proteomic events. The query interface combined with customized data storage enables researchers to interactively explore genetic alterations across samples, genes, and pathways and, when available in the underlying data, to link these to clinical outcomes. The portal provides graphical summaries of gene-level data from multiple platforms, network visualization and analysis, survival analysis, patient-centric queries, and software programmatic access. The intuitive Web interface of the portal makes complex cancer genomics profiles accessible to researchers and clinicians without requiring bioinformatics expertise, thus facilitating biological discoveries. Here, we provide a practical guide to the analysis and visualization features of the cBioPortal for Cancer Genomics.
20879272	A framework for using diffusion weighted imaging to improve cortical parcellation.
Med Image Comput Comput Assist Interv  2010
Cortical parcellation refers to anatomical labelling of every point in the cortex. An accurate parcellation is useful in many analysis techniques including the study of regional changes in cortical thickness or volume in ageing and neurodegeneration. Parcellation is also key to anatomic apportioning of functional imaging changes. We present preliminary work on a novel algorithm that takes an entire cortical parcellation and iteratively updates it to better match connectivity information derived from diffusion weighted imaging. We demonstrate the algorithm on a cohort of 17 healthy controls. Initial results show the algorithm recovering artificially induced mis-registrations of the parcellation and also converging to a group-wise average. This work introduces a framework to investigate the relationship between structure and function, with no a-priori knowledge of specific regions of interest.
20630989	DataSHIELD: resolving a conflict in contemporary bioscience--performing a pooled analysis of individual-level data without sharing the data.
Int J Epidemiol 20100714 2010Oct
Contemporary bioscience sometimes demands vast sample sizes and there is often then no choice but to synthesize data across several studies and to undertake an appropriate pooled analysis. This same need is also faced in health-services and socio-economic research. When a pooled analysis is required, analytic efficiency and flexibility are often best served by combining the individual-level data from all sources and analysing them as a single large data set. But ethico-legal constraints, including the wording of consent forms and privacy legislation, often prohibit or discourage the sharing of individual-level data, particularly across national or other jurisdictional boundaries. This leads to a fundamental conflict in competing public goods: individual-level analysis is desirable from a scientific perspective, but is prevented by ethico-legal considerations that are entirely valid. Data aggregation through anonymous summary-statistics from harmonized individual-level databases (DataSHIELD), provides a simple approach to analysing pooled data that circumvents this conflict. This is achieved via parallelized analysis and modern distributed computing and, in one key setting, takes advantage of the properties of the updating algorithm for generalized linear models (GLMs). The conceptual use of DataSHIELD is illustrated in two different settings. As the study of the aetiological architecture of chronic diseases advances to encompass more complex causal pathways-e.g. to include the joint effects of genes, lifestyle and environment-sample size requirements will increase further and the analysis of pooled individual-level data will become ever more important. An aim of this conceptual article is to encourage others to address the challenges and opportunities that DataSHIELD presents, and to explore potential extensions, for example to its use when different data sources hold different data on the same individuals.
20813861	Quality, quantity and harmony: the DataSHaPER approach to integrating data across bioclinical studies.
Int J Epidemiol 20100902 2010Oct
Vast sample sizes are often essential in the quest to disentangle the complex interplay of the genetic, lifestyle, environmental and social factors that determine the aetiology and progression of chronic diseases. The pooling of information between studies is therefore of central importance to contemporary bioscience. However, there are many technical, ethico-legal and scientific challenges to be overcome if an effective, valid, pooled analysis is to be achieved. Perhaps most critically, any data that are to be analysed in this way must be adequately 'harmonized'. This implies that the collection and recording of information and data must be done in a manner that is sufficiently similar in the different studies to allow valid synthesis to take place. This conceptual article describes the origins, purpose and scientific foundations of the DataSHaPER (DataSchema and Harmonization Platform for Epidemiological Research; http://www.datashaper.org), which has been created by a multidisciplinary consortium of experts that was pulled together and coordinated by three international organizations: P³G (Public Population Project in Genomics), PHOEBE (Promoting Harmonization of Epidemiological Biobanks in Europe) and CPT (Canadian Partnership for Tomorrow Project). The DataSHaPER provides a flexible, structured approach to the harmonization and pooling of information between studies. Its two primary components, the 'DataSchema' and 'Harmonization Platforms', together support the preparation of effective data-collection protocols and provide a central reference to facilitate harmonization. The DataSHaPER supports both 'prospective' and 'retrospective' harmonization. It is hoped that this article will encourage readers to investigate the project further: the more the research groups and studies are actively involved, the more effective the DataSHaPER programme will ultimately be.
21075596	Inclusion of methodological filters in searches for diagnostic test accuracy studies misses relevant studies.
J Clin Epidemiol 20101113 2011Jun
To compare the performance of MEDLINE searches using index test(s) and target condition (subject searches) with the same searches combined with methodological filters for test accuracy studies. We derived a reference set of 506 test accuracy studies indexed on MEDLINE from seven systematic reviews that conducted extensive searches. We compared the performance of "subject" with "filtered" searches (same searches combined with each of 22 filters). Outcome measures were number of reference set records missed, sensitivity, number needed to read (NNR), and precision (Number of reference set studies identified for every 100 records screened). Subject searches missed 47 of the 506 reference studies; filtered searches missed an additional 21 to 241 studies. Sensitivity was 91% for subject searches and ranged from 43% to 87% for filtered searches. The NNR was 56 (precision 2%) for subject searches and ranged from 7 to 51 (precision 2-15%) for filtered searches. Filtered searches miss additional studies compared with searches based on index test and target condition. None of the existing filters provided reductions in the NNR for acceptable sensitivity; currently available methodological filters should not be used to identify studies for inclusion in test accuracy reviews.
22281772	Toward interoperable bioscience data.
Nat. Genet. 20120127 2012Feb
To make full use of research data, the bioscience community needs to adopt technologies and reward mechanisms that support interoperability and promote the growth of an open 'data commoning' culture. Here we describe the prerequisites for data commoning and present an established and growing ecosystem of solutions using the shared 'Investigation-Study-Assay' framework to support that vision.
22720999	Validating self-reported strokes in a longitudinal UK cohort study (Whitehall II): Extracting information from hospital medical records versus the Hospital Episode Statistics database.
BMC Med Res Methodol 20120621 2012
Valuable information on the determinants of non-fatal stroke can be obtained from longitudinal observational cohort studies. Such studies often rely on self-reported stroke events, which are best validated with external medical evidence. The aim of this paper is to compare the information on incident non-fatal stroke events arising from different sources. We carried out a validation of self-reported stoke events among participants in the Whitehall II Study, a large UK based cohort study (baseline sample size 10,308 men and women). 106 stroke events were self-reported in three self-administered questionnaires between 2002 and 2009. Eight (7.5%) of these events were discarded as false positives after medical review, 66 were validated by information from the NHS Hospital Episode Statistics (HES) database in England, 16 by manual searches of hospital records alone, and 12 by letters from general practitioners alone. HES provided information on an additional (i.e. not self-reported) 47 events coded as stroke during the period 2002 to 2009 in hospitals in England among the original baseline participants. Of these, 43 participants were no longer active in the study and 4 had completed questionnaires but not reported a stroke event. Validating self-reported strokes in cohort studies with information from the NHS HES database was efficient and provided information on probable non-fatal stroke events among cohort members no longer in active follow-up. Manual extraction from hospital notes can provide supplementary information beyond that available in the HES discharge summary and was used to sub-type some strokes. However, the process was labour intensive. Multiple sources are needed to capture maximum information on stroke events but increasingly with hospitalisation in the acute phase of stroke, HES has an important role. Further development of HES is required to assure validity and coverage.
22875554	DICOM relay over the cloud.
Int J Comput Assist Radiol Surg 20120809 2013May
Healthcare institutions worldwide have adopted picture archiving and communication system (PACS) for enterprise access to images, relying on Digital Imaging Communication in Medicine (DICOM) standards for data exchange. However, communication over a wider domain of independent medical institutions is not well standardized. A DICOM-compliant bridge was developed for extending and sharing DICOM services across healthcare institutions without requiring complex network setups or dedicated communication channels. A set of DICOM routers interconnected through a public cloud infrastructure was implemented to support medical image exchange among institutions. Despite the advantages of cloud computing, new challenges were encountered regarding data privacy, particularly when medical data are transmitted over different domains. To address this issue, a solution was introduced by creating a ciphered data channel between the entities sharing DICOM services. Two main DICOM services were implemented in the bridge: Storage and Query/Retrieve. The performance measures demonstrated it is quite simple to exchange information and processes between several institutions. The solution can be integrated with any currently installed PACS-DICOM infrastructure. This method works transparently with well-known cloud service providers. Cloud computing was introduced to augment enterprise PACS by providing standard medical imaging services across different institutions, offering communication privacy and enabling creation of wider PACS scenarios with suitable technical solutions.
23439867	Tachycardia detection performance of implantable loop recorders: results from a large 'real-life' patient cohort and patients with induced ventricular arrhythmias.
Europace 20130224 2013Aug
Implantable loop recorders (ILRs) are valuable for diagnosing arrhythmias. We evaluated tachycardia detection performance of the Medtronic Reveal(®) ILR with FullView™ Software. The rate of occurrence of tachycardia detection [supraventricular tachycardia, ventricular tachycardia (VT), and ventricular fibrillation (VF)] and the percentage of appropriately detected tachycardias were determined from all 2190 ILR patients that transmitted to CareLink over a 4-month period (total follow-up = 135.6 patient-years). All 1909 tachycardia episodes were reviewed. Episodes with actual heart rate above the programmed tachycardia detection rate were classified as appropriate. Sensitivity to detect true ventricular arrhythmias was assessed in another group of 215 patients undergoing implantable cardioverter defibrillator (ICD) implant testing. Skin electrodes represented ILR electrodes. Induced VF (404 episodes) and VT (93 episodes) were processed by an emulation of FullView Software. Generalized estimation equation analysis adjusted for multiple episodes per patient. In the CareLink cohort, 68.7% (63.9% adjusted) of detected episodes had tachycardia above the detection rate. Of 1642 episodes detected in the VT zone (12.1 episodes/patient-year), 78.8% (79.0% adjusted) had tachycardia above the detection rate. Of 267 episodes detected in the fast VT zone (1.9 episodes/patient-year), 6.7% (9.4% adjusted) had tachycardia above the detection rate. Twelve true VT/VF episodes were observed in 10 patients. In the ICD patient cohort, 95.9% (96.5% adjusted) of induced VT/VF segments were correctly detected at nominal rate cutoffs. When VT detection was set to 130 b.p.m. (to include the slowest VT), 99.0% (99.3% adjusted) were correctly detected. The majority (63.9%) of detected tachycardias contained true tachycardia. Sensitivity to detect induced VT/VF was 99.3%.
20627863	Finding and sharing: new approaches to registries of databases and services for the biomedical sciences.
Database (Oxford) 20100706 2010
The recent explosion of biological data and the concomitant proliferation of distributed databases make it challenging for biologists and bioinformaticians to discover the best data resources for their needs, and the most efficient way to access and use them. Despite a rapid acceleration in uptake of syntactic and semantic standards for interoperability, it is still difficult for users to find which databases support the standards and interfaces that they need. To solve these problems, several groups are developing registries of databases that capture key metadata describing the biological scope, utility, accessibility, ease-of-use and existence of web services allowing interoperability between resources. Here, we describe some of these initiatives including a novel formalism, the Database Description Framework, for describing database operations and functionality and encouraging good database practise. We expect such approaches will result in improved discovery, uptake and utilization of data resources. Database URL: http://www.casimir.org.uk/casimir_ddf.
20544714	Simultaneous myocardial strain and dark-blood perfusion imaging using a displacement-encoded MRI pulse sequence.
Magn Reson Med  2010Sep
The purpose of this study is to develop and evaluate a displacement-encoded pulse sequence for simultaneous perfusion and strain imaging. Displacement-encoded images in two to three myocardial slices were repeatedly acquired using a single-shot pulse sequence for 3 to 4 min, which covers a bolus infusion of Gadolinium contrast. The magnitudes of the images were T(1) weighted and provided quantitative measures of perfusion, while the phase maps yielded strain measurements. In an acute coronary occlusion swine protocol (n = 9), segmental perfusion measurements were validated against microsphere reference standard with a linear regression (slope 0.986, R(2) = 0.765, Bland-Altman standard deviation = 0.15 mL/min/g). In a group of ST-elevation myocardial infarction patients (n = 11), the scan success rate was 76%. Short-term contrast washout rate and perfusion are highly correlated (R(2) = 0.72), and the pixelwise relationship between circumferential strain and perfusion was better described with a sigmoidal Hill curve than linear functions. This study demonstrates the feasibility of measuring strain and perfusion from a single set of images.
22941987	Legal constraints on genetic data processing in European grids.
Stud Health Technol Inform  2012
European laws on privacy and data security are not explicit about the storage and processing of genetic data. Especially whole-genome data is identifying and contains a lot of personal information. Is processing of such data allowed in computing grids? To find out, we looked at legal precedents in related fields, current literature, and interviews with legal experts. We found that processing of genetic data is only allowed on distributed systems with specific security measures, both technical and organizational. Informed consent, although important, offers no substitute for such requirements.
23388250	Challenges in data quality assurance for electronic health records.
Stud Health Technol Inform  2013
Data quality is an integral part of EHR systems. Quality assurance for these systems not only identifies the current defects in the data but also aims for minimizing the risk of their future occurrence. Previous studies for secondary use of data in research projects presented several dimensions for such defects and proposed few methods for identifying them. Although those methods were successful in small scale research studies, their application to large scale day-to-day flow of information in EHR systems involves many challenges. In this paper, we highlighted those challenges for each method and each dimension and proposed a framework for using existing technologies to address those challenges.
23388282	Processing medical reports to automatically populate ontologies.
Stud Health Technol Inform  2013
Medical reports are, quite often, written and stored in computer systems in a non-structured free text form. As a consequence, the information contained in these reports is not easily available and it is not possible to take it into account by medical decision support systems. We propose a methodology to automatically process and analyze medical reports, identifying concepts and their instances, and populating a new ontology. This methodology is based in natural language processing techniques using linguistic and statistical information. The proposed system was applied successfully to a set of medical reports from the Veterinary Hospital of the University of Évora.
23542959	Knowledge representation and management enabling intelligent interoperability - principles and standards.
Stud Health Technol Inform  2013
Based on the paradigm changes for health, health services and underlying technologies as well as the need for at best comprehensive and increasingly automated interoperability, the paper addresses the challenge of knowledge representation and management for medical decision support. After introducing related definitions, a system-theoretical, architecture-centric approach to decision support systems (DSSs) and appropriate ways for representing them using systems of ontologies is given. Finally, existing and emerging knowledge representation and management standards are presented. The paper focuses on the knowledge representation and management part of DSSs, excluding the reasoning part from consideration.
23900459	Nonvolatile multilevel data storage memory device from controlled ambipolar charge trapping mechanism.
Sci Rep  2013
The capability of storing multi-bit information is one of the most important challenges in memory technologies. An ambipolar polymer which intrinsically has the ability to transport electrons and holes as a semiconducting layer provides an opportunity for the charge trapping layer to trap both electrons and holes efficiently. Here, we achieved large memory window and distinct multilevel data storage by utilizing the phenomena of ambipolar charge trapping mechanism. As fabricated flexible memory devices display five well-defined data levels with good endurance and retention properties showing potential application in printed electronics.
23965597	Design of electronic medical record user interfaces: a matrix-based method for improving usability.
J Healthc Eng  2013
This study examines a new approach of using the Design Structure Matrix (DSM) modeling technique to improve the design of Electronic Medical Record (EMR) user interfaces. The usability of an EMR medication dosage calculator used for placing orders in an academic hospital setting was investigated. The proposed method captures and analyzes the interactions between user interface elements of the EMR system and groups elements based on information exchange, spatial adjacency, and similarity to improve screen density and time-on-task. Medication dose adjustment task time was recorded for the existing and new designs using a cognitive simulation model that predicts user performance. We estimate that the design improvement could reduce time-on-task by saving an average of 21 hours of hospital physicians' time over the course of a month. The study suggests that the application of DSM can improve the usability of an EMR user interface.
23762313	Information exploration system for sickle cell disease and repurposing of hydroxyfasudil.
PLoS ONE 20130610 2013
Sickle cell disease (SCD) is a fatal monogenic disorder with no effective cure and thus high rates of morbidity and sequelae. Efforts toward discovery of disease modifying drugs and curative strategies can be augmented by leveraging the plethora of information contained in available biomedical literature. To facilitate research in this direction we have developed a resource, Dragon Exploration System for Sickle Cell Disease (DESSCD) (http://cbrc.kaust.edu.sa/desscd/) that aims to promote the easy exploration of SCD-related data. The Dragon Exploration System (DES), developed based on text mining and complemented by data mining, processed 419,612 MEDLINE abstracts retrieved from a PubMed query using SCD-related keywords. The processed SCD-related data has been made available via the DESSCD web query interface that enables: a/information retrieval using specified concepts, keywords and phrases, and b/the generation of inferred association networks and hypotheses. The usefulness of the system is demonstrated by: a/reproducing a known scientific fact, the "Sickle_Cell_Anemia-Hydroxyurea" association, and b/generating novel and plausible "Sickle_Cell_Anemia-Hydroxyfasudil" hypothesis. A PCT patent (PCT/US12/55042) has been filed for the latter drug repurposing for SCD treatment. We developed the DESSCD resource dedicated to exploration of text-mined and data-mined information about SCD. No similar SCD-related resource exists. Thus, we anticipate that DESSCD will serve as a valuable tool for physicians and researchers interested in SCD.
23796182	Enhancing genomics information retrieval through dimensional analysis.
J Bioinform Comput Biol 20130415 2013Jun
We propose a novel dimensional analysis approach to employing meta information in order to find the relationships within the unstructured or semi-structured document/passages for improving genomics information retrieval performance. First, we make use of the auxiliary information as three basic dimensions, namely "temporal", "journal", and "author". The reference section is treated as a commensurable quantity of the three basic dimensions. Then, the sample space and subspaces are built up and a set of events are defined to meet the basic requirement of dimensional homogeneity to be commensurable quantities. After that, the classic graph analysis algorithm in the Web environments is applied on each dimension respectively to calculate the importance of each dimension. Finally, we integrate all the dimension networks and re-rank the outputs for evaluation. Our experimental results show the proposed approach is superior and promising.
23666794	From health search to healthcare: explorations of intention and utilization via query logs and user surveys.
J Am Med Inform Assoc 20130511 2014 Jan-Feb
To better understand the relationship between online health-seeking behaviors and in-world healthcare utilization (HU) by studies of online search and access activities before and after queries that pursue medical professionals and facilities. We analyzed data collected from logs of online searches gathered from consenting users of a browser toolbar from Microsoft (N=9740). We employed a complementary survey (N=489) to seek a deeper understanding of information-gathering, reflection, and action on the pursuit of professional healthcare. We provide insights about HU through the survey, breaking out its findings by different respondent marginalizations as appropriate. Observations made from search logs may be explained by trends observed in our survey responses, even though the user populations differ. The results provide insights about how users decide if and when to utilize healthcare resources, and how online health information seeking transitions to in-world HU. The findings from both the survey and the logs reveal behavioral patterns and suggest a strong relationship between search behavior and HU. Although the diversity of our survey respondents is limited and we cannot be certain that users visited medical facilities, we demonstrate that it may be possible to infer HU from long-term search behavior by the apparent influence that health concerns and professional advice have on search activity. Our findings highlight different phases of online activities around queries pursuing professional healthcare facilities and services. We also show that it may be possible to infer HU from logs without tracking people's physical location, based on the effect of HU on pre- and post-HU search behavior. This allows search providers and others to develop more robust models of interests and preferences by modeling utilization rather than simply the intention to utilize that is expressed in search queries.
23676247	A corpus-based approach for automated LOINC mapping.
J Am Med Inform Assoc 20130515 2014 Jan-Feb
To determine whether the knowledge contained in a rich corpus of local terms mapped to LOINC (Logical Observation Identifiers Names and Codes) could be leveraged to help map local terms from other institutions. We developed two models to test our hypothesis. The first based on supervised machine learning was created using Apache's OpenNLP Maxent and the second based on information retrieval was created using Apache's Lucene. The models were validated by a random subsampling method that was repeated 20 times and that used 80/20 splits for training and testing, respectively. We also evaluated the performance of these models on all laboratory terms from three test institutions. For the 20 iterations used for validation of our 80/20 splits Maxent and Lucene ranked the correct LOINC code first for between 70.5% and 71.4% and between 63.7% and 65.0% of local terms, respectively. For all laboratory terms from the three test institutions Maxent ranked the correct LOINC code first for between 73.5% and 84.6% (mean 78.9%) of local terms, whereas Lucene's performance was between 66.5% and 76.6% (mean 71.9%). Using a cut-off score of 0.46 Maxent always ranked the correct LOINC code first for over 57% of local terms. This study showed that a rich corpus of local terms mapped to LOINC contains collective knowledge that can help map terms from other institutions. Using freely available software tools, we developed a data-driven automated approach that operates on term descriptions from existing mappings in the corpus. Accurate and efficient automated mapping methods can help to accelerate adoption of vocabulary standards and promote widespread health information exchange.
23135215	Towards a more cloud-friendly medical imaging applications architecture: a modest proposal.
J Digit Imaging  2013Feb
Recent information technology literature, in general, and radiology trade journals, in particular, are rife with allusions to the "cloud" suggesting that moving one's compute and storage assets into someone else's data center magically solves cost, performance, and elasticity problems. More likely, one is only trading one set of problems for another, including greater latency (aka slower turnaround times) since the image data must now leave the local area network and travel longer paths via encrypted tunnels. To offset this, an imaging system design is needed that reduces the number of high-latency image transmissions, yet can still leverage cloud strengths. This work explores the requirements for such a design.
22846169	A human-computer collaborative approach to identifying common data elements in clinical trial eligibility criteria.
J Biomed Inform 20120727 2013Feb
To identify Common Data Elements (CDEs) in eligibility criteria of multiple clinical trials studying the same disease using a human-computer collaborative approach. A set of free-text eligibility criteria from clinical trials on two representative diseases, breast cancer and cardiovascular diseases, was sampled to identify disease-specific eligibility criteria CDEs. In this proposed approach, a semantic annotator is used to recognize Unified Medical Language Systems (UMLSs) terms within the eligibility criteria text. The Apriori algorithm is applied to mine frequent disease-specific UMLS terms, which are then filtered by a list of preferred UMLS semantic types, grouped by similarity based on the Dice coefficient, and, finally, manually reviewed. Standard precision, recall, and F-score of the CDEs recommended by the proposed approach were measured with respect to manually identified CDEs. Average precision and recall of the recommended CDEs for the two diseases were 0.823 and 0.797, respectively, leading to an average F-score of 0.810. In addition, the machine-powered CDEs covered 80% of the cardiovascular CDEs published by The American Heart Association and assigned by human experts. It is feasible and effort saving to use a human-computer collaborative approach to augment domain experts for identifying disease-specific CDEs from free-text clinical trial eligibility criteria.
23413930	Data mining nursing care plans of end-of-life patients: a study to improve healthcare decision making.
Int J Nurs Knowl 20120817 2013Feb
To reveal hidden patterns and knowledge present in nursing care information documented with standardized nursing terminologies on end-of-life (EOL) hospitalized patients. 596 episodes of care that included pain as a problem on a patient's care plan were examined using statistical and data mining tools. The data were extracted from the Hands-On Automated Nursing Data System database of nursing care plan episodes (n = 40,747) coded with NANDA-I, Nursing Outcomes Classification, and Nursing Intervention Classification (NNN) terminologies. System episode data (episode = care plans updated at every hand-off on a patient while staying on a hospital unit) had been previously gathered in eight units located in four different healthcare facilities (total episodes = 40,747; EOL episodes = 1,425) over 2 years and anonymized prior to this analyses. Results show multiple discoveries, including EOL patients with hospital stays (&lt;72 hr) are less likely (p &lt; .005) to meet the pain relief goals compared with EOL patients with longer hospital stays.   The study demonstrates some major benefits of systematically integrating NNN into electronic health records.
23347886	Acquisition and evaluation of verb subcategorization resources for biomedicine.
J Biomed Inform 20130122 2013Apr
Biomedical natural language processing (NLP) applications that have access to detailed resources about the linguistic characteristics of biomedical language demonstrate improved performance on tasks such as relation extraction and syntactic or semantic parsing. Such applications are important for transforming the growing unstructured information buried in the biomedical literature into structured, actionable information. In this paper, we address the creation of linguistic resources that capture how individual biomedical verbs behave. We specifically consider verb subcategorization, or the tendency of verbs to "select" co-occurrence with particular phrase types, which influences the interpretation of verbs and identification of verbal arguments in context. There are currently a limited number of biomedical resources containing information about subcategorization frames (SCFs), and these are the result of either labor-intensive manual collation, or automatic methods that use tools adapted to a single biomedical subdomain. Either method may result in resources that lack coverage. Moreover, the quality of existing verb SCF resources for biomedicine is unknown, due to a lack of available gold standards for evaluation. This paper presents three new resources related to verb subcategorization frames in biomedicine, and four experiments making use of the new resources. We present the first biomedical SCF gold standards, capturing two different but widely-used definitions of subcategorization, and a new SCF lexicon, BioCat, covering a large number of biomedical sub-domains. We evaluate the SCF acquisition methodologies for BioCat with respect to the gold standards, and compare the results with the accuracy of the only previously existing automatically-acquired SCF lexicon for biomedicine, the BioLexicon. Our results show that the BioLexicon has greater precision while BioCat has better coverage of SCFs. Finally, we explore the definition of subcategorization using these resources and its implications for biomedical NLP. All resources are made publicly available. The SCF resources we have evaluated still show considerably lower accuracy than that reported with general English lexicons, demonstrating the need for domain- and subdomain-specific SCF acquisition tools for biomedicine. Our new gold standards reveal major differences when annotators use the different definitions. Moreover, evaluation of BioCat yields major differences in accuracy depending on the gold standard, demonstrating that the definition of subcategorization adopted will have a direct impact on perceived system accuracy for specific tasks.
23674197	[New options for digital photo documentation during routine examination for ophthalmologists].
Klin Monbl Augenheilkd 20130514 2013Jun
Many clinical investigations cannot be carried out at the examination unit or a slit lamp. Here we present three new options to obtain digital pictures in a routine clinic at a slit lamp and evaluate how user friendly they are. A) First, a digital photo documentation is examined at a conventional slit lamp by a modified binocular ray splitter. One ray of the binocular ray splitter is connected to a digital camera, while the second ray in this patented prototype is connected to the light sources of a synchronised flash light. B) A Smartphone generated fundus images via the monocular of a microscope. Macroscopic details up to 60× at the external eye were obtained by a magnifying gadget of the iPhone. C) With a USB microscope, high resolutions pictures were generated without large technical expense directly at the job and digitally were archived over an USB connection. A trained ophthalmologist demonstrated an excellent documentation at a slit lamp using all 3 digital camera systems. The new ray splitter allows enhanced the image quality at the anterior and posterior segments of the eye. Also the Smartphone obtained by its autofocus and automatic exposure control stunningly high resolution images at the fundus. An attached magnifying aperture glass enables documentation with a 20-fold magnification and is already used in dermatology as the "Handyskope". The USB microscope may be used to record macroscopic details with a 200-fold magnification and resolution of 2 million pixels. It is connected to a PC desktop at the workstation and has only a limited depth resolution, requiring a precise focus. The increasing distribution of the Smartphone and significant improvement of its digital camera make its use in medicine meaningful. Low-priced attempts and mobile applications open new implications in the evaluation of ophthalmological patients.
23707591	Human Connectome Project informatics: quality control, database services, and data visualization.
Neuroimage 20130524 2013Oct15
The Human Connectome Project (HCP) has developed protocols, standard operating and quality control procedures, and a suite of informatics tools to enable high throughput data collection, data sharing, automated data processing and analysis, and data mining and visualization. Quality control procedures include methods to maintain data collection consistency over time, to measure head motion, and to establish quantitative modality-specific overall quality assessments. Database services developed as customizations of the XNAT imaging informatics platform support both internal daily operations and open access data sharing. The Connectome Workbench visualization environment enables user interaction with HCP data and is increasingly integrated with the HCP's database services. Here we describe the current state of these procedures and tools and their application in the ongoing HCP study.
23344737	The semantic priming project.
Behav Res Methods  2013Dec
Speeded naming and lexical decision data for 1,661 target words following related and unrelated primes were collected from 768 subjects across four different universities. These behavioral measures have been integrated with demographic information for each subject and descriptive characteristics for every item. Subjects also completed portions of the Woodcock-Johnson reading battery, three attentional control tasks, and a circadian rhythm measure. These data are available at a user-friendly Internet-based repository ( http://spp.montana.edu ). This Web site includes a search engine designed to generate lists of prime-target pairs with specific characteristics (e.g., length, frequency, associative strength, latent semantic similarity, priming effect in standardized and raw reaction times). We illustrate the types of questions that can be addressed via the Semantic Priming Project. These data represent the largest behavioral database on semantic priming and are available to researchers to aid in selecting stimuli, testing theories, and reducing potential confounds in their studies.
22890939	SNR-optimized phase-sensitive dual-acquisition turbo spin echo imaging: a fast alternative to FLAIR.
Magn Reson Med 20120813 2013Jul
Phase-sensitive dual-acquisition single-slab three-dimensional turbo spin echo imaging was recently introduced, producing high-resolution isotropic cerebrospinal fluid attenuated brain images without long inversion recovery preparation. Despite the advantages, the weighted-averaging-based technique suffers from noise amplification resulting from different levels of cerebrospinal fluid signal modulations over the two acquisitions. The purpose of this work is to develop a signal-to-noise ratio-optimized version of the phase-sensitive dual-acquisition single-slab three-dimensional turbo spin echo. Variable refocusing flip angles in the first acquisition are calculated using a three-step prescribed signal evolution while those in the second acquisition are calculated using a two-step pseudo-steady state signal transition with a high flip-angle pseudo-steady state at a later portion of the echo train, balancing the levels of cerebrospinal fluid signals in both the acquisitions. Low spatial frequency signals are sampled during the high flip-angle pseudo-steady state to further suppress noise. Numerical simulations of the Bloch equations were performed to evaluate signal evolutions of brain tissues along the echo train and optimize imaging parameters. In vivo studies demonstrate that compared with conventional phase-sensitive dual-acquisition single-slab three-dimensional turbo spin echo, the proposed optimization yields 74% increase in apparent signal-to-noise ratio for gray matter and 32% decrease in imaging time. The proposed method can be a potential alternative to conventional fluid-attenuated imaging.
22926830	Multidimensionally encoded magnetic resonance imaging.
Magn Reson Med 20120824 2013Jul
Magnetic resonance imaging (MRI) typically achieves spatial encoding by measuring the projection of a q-dimensional object over q-dimensional spatial bases created by linear spatial encoding magnetic fields (SEMs). Recently, imaging strategies using nonlinear SEMs have demonstrated potential advantages for reconstructing images with higher spatiotemporal resolution and reducing peripheral nerve stimulation. In practice, nonlinear SEMs and linear SEMs can be used jointly to further improve the image reconstruction performance. Here, we propose the multidimensionally encoded (MDE) MRI to map a q-dimensional object onto a p-dimensional encoding space where p &gt; q. MDE MRI is a theoretical framework linking imaging strategies using linear and nonlinear SEMs. Using a system of eight surface SEM coils with an eight-channel radiofrequency coil array, we demonstrate the five-dimensional MDE MRI for a two-dimensional object as a further generalization of PatLoc imaging and O-space imaging. We also present a method of optimizing spatial bases in MDE MRI. Results show that MDE MRI with a higher dimensional encoding space can reconstruct images more efficiently and with a smaller reconstruction error when the k-space sampling distribution and the number of samples are controlled.
23703591	Application of information retrieval approaches to case classification in the vaccine adverse event reporting system.
Drug Saf  2013Jul
Automating the classification of adverse event reports is an important step to improve the efficiency of vaccine safety surveillance. Previously we showed it was possible to classify reports using features extracted from the text of the reports. The aim of this study was to use the information encoded in the Medical Dictionary for Regulatory Activities (MedDRA(®)) in the US Vaccine Adverse Event Reporting System (VAERS) to support and evaluate two classification approaches: a multiple information retrieval strategy and a rule-based approach. To evaluate the performance of these approaches, we selected the conditions of anaphylaxis and Guillain-Barré syndrome (GBS). We used MedDRA(®) Preferred Terms stored in the VAERS, and two standardized medical terminologies: the Brighton Collaboration (BC) case definitions and Standardized MedDRA(®) Queries (SMQ) to classify two sets of reports for GBS and anaphylaxis. Two approaches were used: (i) the rule-based instruments that are available by the two terminologies (the Automatic Brighton Classification [ABC] tool and the SMQ algorithms); and (ii) the vector space model. We found that the rule-based instruments, particularly the SMQ algorithms, achieved a high degree of specificity; however, there was a cost in terms of sensitivity in all but the narrow GBS SMQ algorithm that outperformed the remaining approaches (sensitivity in the testing set was equal to 99.06 % for this algorithm vs. 93.40 % for the vector space model). In the case of anaphylaxis, the vector space model achieved higher sensitivity compared with the best values of both the ABC tool and the SMQ algorithms in the testing set (86.44 % vs. 64.11 % and 52.54 %, respectively). Our results showed the superiority of the vector space model over the existing rule-based approaches irrespective of the standardized medical knowledge represented by either the SMQ or the BC case definition. The vector space model might make automation of case definitions for spontaneous report review more efficient than current rule-based approaches, allowing more time for critical assessment and decision making by pharmacovigilance experts.
22492893	Direct-to-consumer Internet promotion of robotic prostatectomy exhibits varying quality of information.
Health Aff (Millwood)  2012Apr
Robotic surgery to remove a cancerous prostate has become a popular treatment. Internet marketing of this surgery provides an intriguing case study of direct-to-consumer promotions of medical devices, which are more loosely regulated than pharmaceutical promotions. We investigated whether the claims made in online promotions of robotic prostatectomy were consistent with evidence from comparative effectiveness studies. After performing a search and cross-sectional analysis of websites that mentioned the procedure, we found that many sites claimed benefits that were unsupported by evidence and that 42 percent of the sites failed to mention risks. Most sites were published by hospitals and physicians, which the public may regard as more objective than pages published by manufacturers. Unbalanced information may inappropriately raise patients' expectations. Increasing enforcement and regulation of online promotions may be beyond the capabilities of federal authorities. Thus, the most feasible solution may be for the government and medical societies to promote the production of balanced educational material.
23607465	PharmGuide: your guide to free online drug information.
Med Ref Serv Q  2013
PharmGuide &lt; http://goo.gl/f14Me &gt;is an annotated directory of high quality, freely available, online drug information resources intended for use by librarians, pharmacists, and the public. Given the plethora of drug information websites with varying levels of authoritativeness and accuracy, PharmGuide is intended to facilitate the search for resources by providing links to only those sources that have been critically appraised and that meet specific quality criteria. Methods used in developing the site, evidence of its utility based on usage statistics, and examples of its application in practice are presented within the context of the drug information landscape.
23607468	NIHSeniorHealth: a free tool for online health information for older adults.
Med Ref Serv Q  2013
NIHSeniorHealth is a free, consumer health website that covers health topics affecting older adults. The website was created and is maintained by the National Library of Medicine (NLM) and features more than 55 health topics and nearly 150 videos. The easy-to-use navigational and visual tools create a user-friendly experience for older adults, their families, and caregivers who seek senior-specific information on the web. This column will include an overview of the website, a simple search, and a review of the features of NIHSeniorHealth.
23607470	Shared ownership: what's the future?
Med Ref Serv Q  2013
The status of library consortia has evolved over time in terms of their composition and alternative negotiating models. New purchasing models may allow improved library involvement in the acquisitions process and improved methods for meeting users' future needs. Ever-increasing costs of library resources and the need to reduce expenses make it necessary to continue the exploration of library consortia for group purchases.
24199312	Haiti's St. Boniface looks to cloud IT to help earthquake victims.
Health Manag Technol  2013Oct
St. Boniface Rehabilitation and Community Reintegration Program, St Boniface.
24279835	Achieving integration in mixed methods designs-principles and practices.
Health Serv Res 20131023 2013Dec
Mixed methods research offers powerful tools for investigating complex processes and systems in health and health care. This article describes integration principles and practices at three levels in mixed methods research and provides illustrative examples. Integration at the study design level occurs through three basic mixed method designs-exploratory sequential, explanatory sequential, and convergent-and through four advanced frameworks-multistage, intervention, case study, and participatory. Integration at the methods level occurs through four approaches. In connecting, one database links to the other through sampling. With building, one database informs the data collection approach of the other. When merging, the two databases are brought together for analysis. With embedding, data collection and analysis link at multiple points. Integration at the interpretation and reporting level occurs through narrative, data transformation, and joint display. The fit of integration describes the extent the qualitative and quantitative findings cohere. Understanding these principles and practices of integration can help health services researchers leverage the strengths of mixed methods.
22460401	An optimized video system for augmented reality in endodontics: a feasibility study.
Clin Oral Investig 20120331 2013Mar
We propose an augmented reality system for the reliable detection of root canals in video sequences based on a k-nearest neighbor color classification and introduce a simple geometric criterion for teeth. The new software was implemented using C++, Qt, and the image processing library OpenCV. Teeth are detected in video images to restrict the segmentation of the root canal orifices by using a k-nearest neighbor algorithm. The location of the root canal orifices were determined using Euclidean distance-based image segmentation. A set of 126 human teeth with known and verified locations of the root canal orifices was used for evaluation. The software detects root canals orifices for automatic classification of the teeth in video images and stores location and size of the found structures. Overall 287 of 305 root canals were correctly detected. The overall sensitivity was about 94 %. Classification accuracy for molars ranged from 65.0 to 81.2 % and from 85.7 to 96.7 % for premolars. The realized software shows that observations made in anatomical studies can be exploited to automate real-time detection of root canal orifices and tooth classification with a software system. Automatic storage of location, size, and orientation of the found structures with this software can be used for future anatomical studies. Thus, statistical tables with canal locations will be derived, which can improve anatomical knowledge of the teeth to alleviate root canal detection in the future. For this purpose the software is freely available at: http://www.dental-imaging.zahnmedizin.uni-mainz.de/.
24261387	Extended outlook: description, utilization, and daily applications of cloud technology in radiology.
AJR Am J Roentgenol  2013Dec
The purpose of this article is to discuss the concept of cloud technology, its role in medical applications and radiology, the role of the radiologist in using and accessing these vast resources of information, and privacy concerns and HIPAA compliance strategies. Cloud computing is the delivery of shared resources, software, and information to computers and other devices as a metered service. This technology has a promising role in the sharing of patient medical information and appears to be particularly suited for application in radiology, given the field's inherent need for storage and access to large amounts of data. The radiology cloud has significant strengths, such as providing centralized storage and access, reducing unnecessary repeat radiologic studies, and potentially allowing radiologic second opinions more easily. There are significant cost advantages to cloud computing because of a decreased need for infrastructure and equipment by the institution. Private clouds may be used to ensure secure storage of data and compliance with HIPAA. In choosing a cloud service, there are important aspects, such as disaster recovery plans, uptime, and security audits, that must be considered. Given that the field of radiology has become almost exclusively digital in recent years, the future of secure storage and easy access to imaging studies lies within cloud computing technology.
24064442	An adaptable architecture for patient cohort identification from diverse data sources.
J Am Med Inform Assoc 20130924 2013Dec
We define and validate an architecture for systems that identify patient cohorts for clinical trials from multiple heterogeneous data sources. This architecture has an explicit query model capable of supporting temporal reasoning and expressing eligibility criteria independently of the representation of the data used to evaluate them. The architecture has the key feature that queries defined according to the query model are both pre and post-processed and this is used to address both structural and semantic heterogeneity. The process of extracting the relevant clinical facts is separated from the process of reasoning about them. A specific instance of the query model is then defined and implemented. We show that the specific instance of the query model has wide applicability. We then describe how it is used to access three diverse data warehouses to determine patient counts. Although the proposed architecture requires greater effort to implement the query model than would be the case for using just SQL and accessing a data-based management system directly, this effort is justified because it supports both temporal reasoning and heterogeneous data sources. The query model only needs to be implemented once no matter how many data sources are accessed. Each additional source requires only the implementation of a lightweight adaptor. The architecture has been used to implement a specific query model that can express complex eligibility criteria and access three diverse data warehouses thus demonstrating the feasibility of this approach in dealing with temporal reasoning and data heterogeneity.
24144838	AmphoraNet: the webserver implementation of the AMPHORA2 metagenomic workflow suite.
Gene 20131019 2014Jan10
Metagenomics went through an astonishing development in the past few years. Today not only gene sequencing experts, but numerous laboratories of other specializations need to analyze DNA sequences gained from clinical or environmental samples. Phylogenetic analysis of the metagenomic data presents significant challenges for the biologist and the bioinformatician. The program suite AMPHORA and its workflow version are examples of publicly available software that yields reliable phylogenetic results for metagenomic data. Here we present AmphoraNet, an easy-to-use webserver that is capable of assigning a probability-weighted taxonomic group for each phylogenetic marker gene found in the input metagenomic sample; the webserver is based on the AMPHORA2 workflow. Since a large proportion of molecular biologists uses the BLAST program and its clones on public webservers instead of the locally installed versions, we believe that the occasional user may find it comfortable that, in this version, no time-consuming installation of every component of the AMPHORA2 suite or expertise in Linux environment is required. The webserver is freely available at http://amphoranet.pitgroup.org; no registration is required.
21613015	[Useful web sites for information about the recommendations of good practices in laboratory medicine].
Ann. Biol. Clin. (Paris)  2010Dec
In this paper are presented some useful web sites to find updated reference tables concerning the recommendations of professional practices in laboratory medicine. The knowledge of these reference tables can allow the biologist to develop its role of advice to the clinicians. It can also help him to assure a relevant interpretation of the laboratory results and to value the interest for the patient.
21613016	[Examination procedures].
Ann. Biol. Clin. (Paris)  2010Dec
Examination procedures have to be written for each examination according to the standard requirements. Using CE marked devices, technical inserts can be used, but because of their lack of homogeneity, it could be easier to document their use as a standard procedure. Document control policy applies for those procedures, the content of which could be as provided in this document. Electronic manuals can be used as well.
23636887	PhenoTips: patient phenotyping software for clinical and research use.
Hum. Mutat. 20130524 2013Aug
We have developed PhenoTips: open source software for collecting and analyzing phenotypic information for patients with genetic disorders. Our software combines an easy-to-use interface, compatible with any device that runs a Web browser, with a standardized database back end. The PhenoTips' user interface closely mirrors clinician workflows so as to facilitate the recording of observations made during the patient encounter. Collected data include demographics, medical history, family history, physical and laboratory measurements, physical findings, and additional notes. Phenotypic information is represented using the Human Phenotype Ontology; however, the complexity of the ontology is hidden behind a user interface, which combines simple selection of common phenotypes with error-tolerant, predictive search of the entire ontology. PhenoTips supports accurate diagnosis by analyzing the entered data, then suggesting additional clinical investigations and providing Online Mendelian Inheritance in Man (OMIM) links to likely disorders. By collecting, classifying, and analyzing phenotypic information during the patient encounter, PhenoTips allows for streamlining of clinic workflow, efficient data entry, improved diagnosis, standardization of collected patient phenotypes, and sharing of anonymized patient phenotype data for the study of rare disorders. Our source code and a demo version of PhenoTips are available at http://phenotips.org.
23807401	[Surveillance of antibiotic consumption : clarification of the "definition of data on the nature and extent of antibiotic consumption in hospitals according to § 23 paragraph 4 sentence 2 of the IfSG"].
Bundesgesundheitsblatt Gesundheitsforschung Gesundheitsschutz  2013Jul
According to § 23 paragraph 4 of the German Infection Prevention Act (IfSG; July 2011), hospitals and clinics for ambulatory surgery are obliged to establish a continuous monitoring system of antibiotic consumption. This is aimed at contributing to an optimization of antibiotic prescription practices in order to confine the development and spread of resistant pathogens. The general requirements (restricted to hospitals) on the method and extent of data collection are provided by the national public health institution after discussion with representatives of various professional societies (Robert Koch-Institut, Bundesgesundheitsblatt Gesundheitsforschung Gesundheitsschutz 59, 2013). The article aims to clarify these specifications and to provide background details. In agreement with national and European surveillance systems, the Anatomical Therapeutic Chemical (ATC)/Defined Daily Dose (DDD) classification system recommended by the WHO should be used as reference standard. Antibiotic consumption should be expressed as the number of DDDs per 100 patient days and per 100 admissions. The categories of antimicrobials and hospital organizational units to be monitored and the time intervals in which analyses should be conducted are determined. Furthermore, various approaches of data assessment are described.
23887085	An advanced temporal credential-based security scheme with mutual authentication and key agreement for wireless sensor networks.
Sensors (Basel) 20130724 2013
Wireless sensor networks (WSNs) can be quickly and randomly deployed in any harsh and unattended environment and only authorized users are allowed to access reliable sensor nodes in WSNs with the aid of gateways (GWNs). Secure authentication models among the users, the sensor nodes and GWN are important research issues for ensuring communication security and data privacy in WSNs. In 2013, Xue et al. proposed a temporal-credential-based mutual authentication and key agreement scheme for WSNs. However, in this paper, we point out that Xue et al.'s scheme cannot resist stolen-verifier, insider, off-line password guessing, smart card lost problem and many logged-in users' attacks and these security weaknesses make the scheme inapplicable to practical WSN applications. To tackle these problems, we suggest a simple countermeasure to prevent proposed attacks while the other merits of Xue et al.'s authentication scheme are left unchanged.
23899932	An infrastructure to enable lightweight context-awareness for mobile users.
Sensors (Basel) 20130729 2013
Mobile phones enable us to carry out a wider range of tasks every day, and as a result they have become more ubiquitous than ever. However, they are still more limited in terms of processing power and interaction capabilities than traditional computers, and the often distracting and time-constricted scenarios in which we use them do not help in alleviating these limitations. Context-awareness is a valuable technique to address these issues, as it enables to adapt application behaviour to each situation. In this paper we present a context management infrastructure for mobile environments, aimed at controlling context information life-cycle in this kind of scenarios, with the main goal of enabling application and services to adapt their behaviour to better meet end-user needs. This infrastructure relies on semantic technologies and open standards to improve interoperability, and is based on a central element, the context manager. This element acts as a central context repository and takes most of the computational burden derived from dealing with this kind of information, thus relieving from these tasks to more resource-scarce devices in the system.
23899936	Extending the IEEE 802.15.4 security suite with a compact implementation of the NIST P-192/B-163 elliptic curves.
Sensors (Basel) 20130729 2013
Typically, commercial sensor nodes are equipped with MCUsclocked at a low-frequency (i.e., within the 4-12 MHz range). Consequently, executing cryptographic algorithms in those MCUs generally requires a huge amount of time. In this respect, the required energy consumption can be higher than using a separate accelerator based on a Field-programmable Gate Array (FPGA) that is switched on when needed. In this manuscript, we present the design of a cryptographic accelerator suitable for an FPGA-based sensor node and compliant with the IEEE802.15.4 standard. All the embedded resources of the target platform (Xilinx Artix-7) have been maximized in order to provide a cost-effective solution. Moreover, we have added key negotiation capabilities to the IEEE 802.15.4 security suite based on Elliptic Curve Cryptography (ECC). Our results suggest that tailored accelerators based on FPGA can behave better in terms of energy than contemporary software solutions for motes, such as the TinyECC and NanoECC libraries. In this regard, a point multiplication (PM) can be performed between 8.58- and 15.4-times faster, 3.40- to 23.59-times faster (Elliptic Curve Diffie-Hellman, ECDH) and between 5.45- and 34.26-times faster (Elliptic Curve Integrated Encryption Scheme, ECIES). Moreover, the energy consumption was also improved with a factor of 8.96 (PM).
23912426	Symbolic and graphical representation scheme for sensors deployed in large-scale structures.
Sensors (Basel) 20130731 2013
As wireless sensor network (WSN)-based structural health monitoring (SHM) systems are increasingly being employed in civil infrastructures and building structures, the management of large numbers of sensing devices and the large amount of data acquired from WSNs will become increasingly difficult unless systematic expressions of the sensor network are provided. This study introduces a practical WSN for SHM that consists of sensors, wireless sensor nodes, repeater nodes, master nodes, and monitoring servers. This study also proposes a symbolic and graphical representation scheme (SGRS) for this system, in which the communication relationships and respective location information of the distributed sensing components are expressed in a concise manner. The SGRS was applied to the proposed WSN, which is employed in an actual large-scale irregular structure in which three types of sensors (75 vibrating wire strain gauges, 10 inclinometers, and three laser displacement sensors) and customized wireless sensor nodes are installed. The application results demonstrate that prompt identification of sensing units and effective management of the distributed sensor network can be realized from the SGRS. The results also demonstrate the superiority of the SGRS over conventional expression methods in which a box diagram or tree diagram representing the ID of sensors and data loggers is used.
23955435	ESB-based Sensor Web integration for the prediction of electric power supply system vulnerability.
Sensors (Basel) 20130815 2013
Electric power supply companies increasingly rely on enterprise IT systems to provide them with a comprehensive view of the state of the distribution network. Within a utility-wide network, enterprise IT systems collect data from various metering devices. Such data can be effectively used for the prediction of power supply network vulnerability. The purpose of this paper is to present the Enterprise Service Bus (ESB)-based Sensor Web integration solution that we have developed with the purpose of enabling prediction of power supply network vulnerability, in terms of a prediction of defect probability for a particular network element. We will give an example of its usage and demonstrate our vulnerability prediction model on data collected from two different power supply companies. The proposed solution is an extension of the GinisSense Sensor Web-based architecture for collecting, processing, analyzing, decision making and alerting based on the data received from heterogeneous data sources. In this case, GinisSense has been upgraded to be capable of operating in an ESB environment and combine Sensor Web and GIS technologies to enable prediction of electric power supply system vulnerability. Aside from electrical values, the proposed solution gathers ambient values from additional sensors installed in the existing power supply network infrastructure. GinisSense aggregates gathered data according to an adapted Omnibus data fusion model and applies decision-making logic on the aggregated data. Detected vulnerabilities are visualized to end-users through means of a specialized Web GIS application.
23966191	On the support of scientific workflows over Pub/Sub brokers.
Sensors (Basel) 20130820 2013
The execution of scientific workflows is gaining importance as more computing resources are available in the form of grid environments. The Publish/Subscribe paradigm offers well-proven solutions for sustaining distributed scenarios while maintaining the high level of task decoupling required by scientific workflows. In this paper, we propose a new model for supporting scientific workflows that improves the dissemination of control events. The proposed solution is based on the mapping of workflow tasks to the underlying Pub/Sub event layer, and the definition of interfaces and procedures for execution on brokers. In this paper we also analyze the strengths and weaknesses of current solutions that are based on existing message exchange models for scientific workflows. Finally, we explain how our model improves the information dissemination, event filtering, task decoupling and the monitoring of scientific workflows.
23966200	Enhancing the simulation speed of sensor network applications by asynchronization of interrupt service routines.
Sensors (Basel) 20130821 2013
Sensor network simulations require high fidelity and timing accuracy to be used as an implementation and evaluation tool. The cycle-accurate and instruction-level simulator is the known solution for these purposes. However, this type of simulation incurs a high computation cost since it has to model not only the instruction level behavior but also the synchronization between multiple sensors for their causality. This paper presents a novel technique that exploits asynchronous simulations of interrupt service routines (ISR). We can avoid the synchronization overheads when the interrupt service routines are simulated without preemption. If the causality errors occur, we devise a rollback procedure to restore the original synchronized simulation. This concept can be extended to any instruction-level sensor network simulator. Evaluation results show our method can enhance the simulation speed up to 52% in the case of our experiments. For applications with longer interrupt service routines and smaller number of preemptions, the speedup becomes greater. In addition, our simulator is 2 to 11 times faster than the well-known sensor network simulator.
23100128	Automatically extracting sentences from Medline citations to support clinicians' information needs.
J Am Med Inform Assoc 20121025 2013 Sep-Oct
Online health knowledge resources contain answers to most of the information needs raised by clinicians in the course of care. However, significant barriers limit the use of these resources for decision-making, especially clinicians' lack of time. In this study we assessed the feasibility of automatically generating knowledge summaries for a particular clinical topic composed of relevant sentences extracted from Medline citations. The proposed approach combines information retrieval and semantic information extraction techniques to identify relevant sentences from Medline abstracts. We assessed this approach in two case studies on the treatment alternatives for depression and Alzheimer's disease. A total of 515 of 564 (91.3%) sentences retrieved in the two case studies were relevant to the topic of interest. About one-third of the relevant sentences described factual knowledge or a study conclusion that can be used for supporting information needs at the point of care. The high rate of relevant sentences is desirable, given that clinicians' lack of time is one of the main barriers to using knowledge resources at the point of care. Sentence rank was not significantly associated with relevancy, possibly due to most sentences being highly relevant. Sentences located closer to the end of the abstract and sentences with treatment and comparative predications were likely to be conclusive sentences. Our proposed technical approach to helping clinicians meet their information needs is promising. The approach can be extended for other knowledge resources and information need types.
23792805	Ontology-guided organ detection to retrieve web images of disease manifestation: towards the construction of a consumer-based health image library.
J Am Med Inform Assoc 20130621 2013 Nov-Dec
Visual information is a crucial aspect of medical knowledge. Building a comprehensive medical image base, in the spirit of the Unified Medical Language System (UMLS), would greatly benefit patient education and self-care. However, collection and annotation of such a large-scale image base is challenging. To combine visual object detection techniques with medical ontology to automatically mine web photos and retrieve a large number of disease manifestation images with minimal manual labeling effort. As a proof of concept, we first learnt five organ detectors on three detection scales for eyes, ears, lips, hands, and feet. Given a disease, we used information from the UMLS to select affected body parts, ran the pretrained organ detectors on web images, and combined the detection outputs to retrieve disease images. Compared with a supervised image retrieval approach that requires training images for every disease, our ontology-guided approach exploits shared visual information of body parts across diseases. In retrieving 2220 web images of 32 diseases, we reduced manual labeling effort to 15.6% while improving the average precision by 3.9% from 77.7% to 81.6%. For 40.6% of the diseases, we improved the precision by 10%. The results confirm the concept that the web is a feasible source for automatic disease image retrieval for health image database construction. Our approach requires a small amount of manual effort to collect complex disease images, and to annotate them by standard medical ontology terms.
22874153	A ClaML-based interface for the import of monohierarchical classifications.
Stud Health Technol Inform  2012
The relevance of controlled vocabularies in promoting the standardized representation and exchange of clinical data is no longer to be proved. As part of a national project we evaluated the integration of classifications and terminologies in metadata registries based on the standard ISO/IEC 11179 Information technology - Metadata Registries (MDR). To overcome integration and maintenance tasks of monohierarchical classifications in the MDR, and to provide an exchange format between classifications publishers and the MDR, we implemented an import interface based on the Classification Markup Language (ClaML). The implementation transforms classifications from ClaML into a MDR conform structure using the Extensible Stylesheet Language Transformation (XSLT). Several XSLT-scripts were linked and successively executed. The national obliged classifications ICD-10-GM and OPS could be hence imported into the MDR. Problems arose with different interpretations and extensions of ClaML by WHO and the national publisher. We therefore advocate a unique interpretation of ClaML as prerequisite for a general use of the ClaML import interface.
23595662	A self-updating road map of The Cancer Genome Atlas.
Bioinformatics 20130417 2013May15
Since 2011, The Cancer Genome Atlas' (TCGA) files have been accessible through HTTP from a public site, creating entirely new possibilities for cancer informatics by enhancing data discovery and retrieval. Significantly, these enhancements enable the reporting of analysis results that can be fully traced to and reproduced using their source data. However, to realize this possibility, a continually updated road map of files in the TCGA is required. Creation of such a road map represents a significant data modeling challenge, due to the size and fluidity of this resource: each of the 33 cancer types is instantiated in only partially overlapping sets of analytical platforms, while the number of data files available doubles approximately every 7 months. We developed an engine to index and annotate the TCGA files, relying exclusively on third-generation web technologies (Web 3.0). Specifically, this engine uses JavaScript in conjunction with the World Wide Web Consortium's (W3C) Resource Description Framework (RDF), and SPARQL, the query language for RDF, to capture metadata of files in the TCGA open-access HTTP directory. The resulting index may be queried using SPARQL, and enables file-level provenance annotations as well as discovery of arbitrary subsets of files, based on their metadata, using web standard languages. In turn, these abilities enhance the reproducibility and distribution of novel results delivered as elements of a web-based computational ecosystem. The development of the TCGA Roadmap engine was found to provide specific clues about how biomedical big data initiatives should be exposed as public resources for exploratory analysis, data mining and reproducible research. These specific design elements align with the concept of knowledge reengineering and represent a sharp departure from top-down approaches in grid initiatives such as CaBIG. They also present a much more interoperable and reproducible alternative to the still pervasive use of data portals. A prepared dashboard, including links to source code and a SPARQL endpoint, is available at http://bit.ly/TCGARoadmap. A video tutorial is available at http://bit.ly/TCGARoadmapTutorial. robbinsd@uab.edu.
23603162	Neuropsychiatric lupus: classification criteria in neuroimaging studies.
Can J Neurol Sci  2013May
This systematic review described the criteria and main evaluations methods procedures used to classify neuropsychiatric systemic lupus erythematosus (NPSLE) patients. Also, within the evaluations methods, this review aimed to identify the main contributions of neuropsychological measurements in neuroimaging studies. A search was conducted in PubMed, EMBASE and SCOPUS databases with the terms related to neuropsychiatric syndromes, systemic lupus erythematosus, and neuroimaging techniques. Sixty-six abstracts were found; only 20 were completely analyzed and included. Results indicated that the 1999 American College of Rheumatology (ACR) criteria is the most used to classify NPSLE samples together with laboratorial, cognitive, neurological and psychiatric assessment procedures. However, the recommended ACR assessment procedures to classify NPSLE patients are being used incompletely, especially the neuropsychological batteries. Neuropsychological instruments and neuroimaging techniques have been used mostly to characterize NPSLE samples, instead of contributing to their classifications. The most described syndromes in neuroimaging studies have been seizure/cerebrovascular disease followed by cognitive dysfunctions as well as headache disorder.
23772554	PRIMOS: an integrated database of reassessed protein-protein interactions providing web-based access to in silico validation of experimentally derived data.
Assay Drug Dev Technol  2013Jun
Steady improvements in proteomics present a bioinformatic challenge to retrieve, store, and process the accumulating and often redundant amount of information. In particular, a large-scale comparison and analysis of protein-protein interaction (PPI) data requires tools for data interpretation as well as validation. At this juncture, the Protein Interaction and Molecule Search (PRIMOS) platform represents a novel web portal that unifies six primary PPI databases (BIND, Biomolecular Interaction Network Database; DIP, Database of Interacting Proteins; HPRD, Human Protein Reference Database; IntAct; MINT, Molecular Interaction Database; and MIPS, Munich Information Center for Protein Sequences) into a single consistent repository, which currently includes more than 196,700 redundancy-removed PPIs. PRIMOS supports three advanced search strategies centering on disease-relevant PPIs, on inter- and intra-organismal crosstalk relations (e.g., pathogen-host interactions), and on highly connected protein nodes analysis ("hub" identification). The main novelties distinguishing PRIMOS from other secondary PPI databases are the reassessment of known PPIs, and the capacity to validate personal experimental data by our peer-reviewed, homology-based validation. This article focuses on definite PRIMOS use cases (presentation of embedded biological concepts, example applications) to demonstrate its broad functionality and practical value. PRIMOS is publicly available at http://primos.fh-hagenberg.at.
24196281	[Multimodal document management in radiotherapy].
Strahlenther Onkol 20131108 2013Dec
After incorporating treatment planning and the organisational model of treatment planning in the operating schedule system (BAS, "Betriebsablaufsystem"), complete document qualities were embedded in the digital environment. The aim of this project was to integrate all documents independent of their source (paper-bound or digital) and to make content from the BAS available in a structured manner. As many workflow steps as possible should be automated, e.g. assigning a document to a patient in the BAS. Additionally it must be guaranteed that at all times it could be traced who, when, how and from which source documents were imported into the departmental system. Furthermore work procedures should be changed that the documentation conducted either directly in the departmental system or from external systems can be incorporated digitally and paper document can be completely avoided (e.g. documents such as treatment certificate, treatment plans or documentation). It was a further aim, if possible, to automate the removal of paper documents from the departmental work flow, or even to make such paper documents superfluous. In this way patient letters for follow-up appointments should automatically generated from the BAS. Similarly patient record extracts in the form of PDF files should be enabled, e.g. for controlling purposes. The available document qualities were analysed in detail by a multidisciplinary working group (BAS-AG) and after this examination and assessment of the possibility of modelling in our departmental workflow (BAS) they were transcribed into a flow diagram. The gathered specifications were implemented in a test environment by the clinical and administrative IT group of the department of radiation oncology and subsequent to a detailed analysis introduced into clinical routine. The department has succeeded under the conditions of the aforementioned criteria to embed all relevant documents in the departmental workflow via continuous processes. Since the completion of the concepts and the implementation in our test environment 15,000 documents were introduced into the departmental workflow following routine approval. Furthermore approximately 5000 appointment letters for patient aftercare per year were automatically generated by the BAS. In addition patient record extracts in the form of PDF files for the medical services of the healthcare insurer can be generated.
23592298	iMole, a web based image retrieval system from biomedical literature.
Electrophoresis 20130617 2013Jul
iMole is a platform that automatically extracts images and captions from biomedical literature. Images are tagged with terms contained in figure captions by means of a sophisticate text-mining tool. Moreover, iMole allows the user to upload directly their own images within the database and manually tag images by curated dictionary. Using iMole the researchers can develop a proper biomedical image database, storing the images extracted from paper of interest, image found on the web repositories, and their own experimental images. In order to show the functioning of the platform, we used iMole to build a 2DE database. Briefly, tagged 2DE gel images were collected and stored in a searchable 2DE gel database, available to users through an interactive web interface. Images were obtained by automatically parsing 16,608 proteomic publications, which yielded more than 16,500 images. The database can be further expanded by users with images of interest trough a manual uploading process. iMole is available with a preloaded set of 2DE gel data at http://imole.biodigitalvalley.com.
22865672	Two open access, high-quality datasets from anesthetic records.
J Am Med Inform Assoc 20120804 2013Jan1
To provide a set of high-quality time-series physiologic and event data from anesthetic cases formatted in an easy-to-use structure. With ethics committee approval, data from surgical operations under general anesthesia were collected, including physiologic data, drug administrations, events, and clinicians' comments. These data were de-identified, formatted in a combined CSV/XML structure and made publicly available. Two separate datasets were collected containing physiologic time-series data and time-stamped events for 34 patients. For 20 patients, the data included 400 physiologic signals collected over 20 h, 274 events, and 597 drug administrations. For 14 patients, the data included 23 physiologic signals collected over 69 h, with 286 time stamped comments. Data reuse potentially saves significant time and financial costs. However, there are few high-quality repositories for accessible physiologic data and clinical interventions from surgical cases. De-identifying records assists with overcoming problems of privacy and storing the data in a format which is easily manipulated with computing resources facilitates access by the wider research community. It is hoped that additional high-quality data will be added. Future work includes developing tools to explore and visualize the data more efficiently, and establishing quality control measures. An approach to collecting and storing high-quality datasets from surgical operations under anesthesia such that they can be easily accessed by others for use in research has been demonstrated.
22886546	Patient-controlled sharing of medical imaging data across unaffiliated healthcare organizations.
J Am Med Inform Assoc 20120811 2013Jan1
Current image sharing is carried out by manual transportation of CDs by patients or organization-coordinated sharing networks. The former places a significant burden on patients and providers. The latter faces challenges to patient privacy. To allow healthcare providers efficient access to medical imaging data acquired at other unaffiliated healthcare facilities while ensuring strong protection of patient privacy and minimizing burden on patients, providers, and the information technology infrastructure. An image sharing framework is described that involves patients as an integral part of, and with full control of, the image sharing process. Central to this framework is the Patient Controlled Access-key REgistry (PCARE) which manages the access keys issued by image source facilities. When digitally signed by patients, the access keys are used by any requesting facility to retrieve the associated imaging data from the source facility. A centralized patient portal, called a PCARE patient control portal, allows patients to manage all the access keys in PCARE. A prototype of the PCARE framework has been developed by extending open-source technology. The results for feasibility, performance, and user assessments are encouraging and demonstrate the benefits of patient-controlled image sharing. The PCARE framework is effective in many important clinical cases of image sharing and can be used to integrate organization-coordinated sharing networks. The same framework can also be used to realize a longitudinal virtual electronic health record. The PCARE framework allows prior imaging data to be shared among unaffiliated healthcare facilities while protecting patient privacy with minimal burden on patients, providers, and infrastructure. A prototype has been implemented to demonstrate the feasibility and benefits of this approach.
22955495	Harmonization process for the identification of medical events in eight European healthcare databases: the experience from the EU-ADR project.
J Am Med Inform Assoc 20120906 2013Jan1
Data from electronic healthcare records (EHR) can be used to monitor drug safety, but in order to compare and pool data from different EHR databases, the extraction of potential adverse events must be harmonized. In this paper, we describe the procedure used for harmonizing the extraction from eight European EHR databases of five events of interest deemed to be important in pharmacovigilance: acute myocardial infarction (AMI); acute renal failure (ARF); anaphylactic shock (AS); bullous eruption (BE); and rhabdomyolysis (RHABD). The participating databases comprise general practitioners' medical records and claims for hospitalization and other healthcare services. Clinical information is collected using four different disease terminologies and free text in two different languages. The Unified Medical Language System was used to identify concepts and corresponding codes in each terminology. A common database model was used to share and pool data and verify the semantic basis of the event extraction queries. Feedback from the database holders was obtained at various stages to refine the extraction queries. Standardized and age specific incidence rates (IRs) were calculated to facilitate benchmarking and harmonization of event data extraction across the databases. This was an iterative process. The study population comprised overall 19 647 445 individuals with a follow-up of 59 929 690 person-years (PYs). Age adjusted IRs for the five events of interest across the databases were as follows: (1) AMI: 60-148/100 000 PYs; (2) ARF: 3-49/100 000 PYs; (3) AS: 2-12/100 000 PYs; (4) BE: 2-17/100 000 PYs; and (5) RHABD: 0.1-8/100 000 PYs. The iterative harmonization process enabled a more homogeneous identification of events across differently structured databases using different coding based algorithms. This workflow can facilitate transparent and reproducible event extractions and understanding of differences between databases.
22962195	The future state of clinical data capture and documentation: a report from AMIA's 2011 Policy Meeting.
J Am Med Inform Assoc 20120908 2013Jan1
Much of what is currently documented in the electronic health record is in response toincreasingly complex and prescriptive medicolegal, reimbursement, and regulatory requirements. These requirements often result in redundant data capture and cumbersome documentation processes. AMIA's 2011 Health Policy Meeting examined key issues in this arena and envisioned changes to help move toward an ideal future state of clinical data capture and documentation. The consensus of the meeting was that, in the move to a technology-enabled healthcare environment, the main purpose of documentation should be to support patient care and improved outcomes for individuals and populations and that documentation for other purposes should be generated as a byproduct of care delivery. This paper summarizes meeting deliberations, and highlights policy recommendations and research priorities. The authors recommend development of a national strategy to review and amend public policies to better support technology-enabled data capture and documentation practices.
22591030	Literature search strategies for conducting knowledge-building and theory-generating qualitative systematic reviews.
J Adv Nurs 20120517 2013Jan
To report literature search strategies for the purpose of conducting knowledge-building and theory-generating qualitative systematic reviews. Qualitative systematic reviews lie on a continuum from knowledge-building and theory-generating to aggregating and summarizing. Different types of literature searches are needed to optimally support these dissimilar reviews. Articles published between 1989-Autumn 2011. These documents were identified using a hermeneutic approach and multiple literature search strategies. Redundancy is not the sole measure of validity when conducting knowledge-building and theory-generating systematic reviews. When conducting these types of reviews, literature searches should be consistent with the goal of fully explicating concepts and the interrelationships among them. To accomplish this objective, a 'berry picking' approach is recommended along with strategies for overcoming barriers to finding qualitative research reports. To enhance integrity of knowledge-building and theory-generating systematic reviews, reviewers are urged to make literature search processes as transparent as possible, despite their complexity. This includes fully explaining and rationalizing what databases were used and how they were searched. It also means describing how literature tracking was conducted and grey literature was searched. In the end, the decision to cease searching also needs to be fully explained and rationalized. Predetermined linear search strategies are unlikely to generate search results that are adequate for purposes of conducting knowledge-building and theory-generating qualitative systematic reviews. Instead, it is recommended that iterative search strategies take shape as reviews evolve.
22782491	Optimum coupling and multimode excitation of traveling-waves in a whole-body 9.4T scanner.
Magn Reson Med 20120710 2013Jun
Given the absence of a body coil, the radio frequency screen of a whole-body 9.4T magnetic resonance imaging scanner can be used as a circular waveguide. In the unloaded case, the screen allows propagation of the dominant TE11- as well as the TM01-mode. In the first part of this study, the optimum coupling of a circular polarized TE11-mode was determined empirically for excitation and reception with a rectangular patch antenna. Employing full-wave simulations, two simulation models and two phantoms, different patch positions were tested to find the optimum position with respect to coupled power and homogenous excitation field. The best simulation results were validated with measurements. The second part of this study describes the design and measurements of a multimode excitation device. Using the parallel transmit system of the MR scanner, all propagable traveling wave modes could be excited and detected independently. The performance of the multimode device related to field of view, B1+-efficiency and radio frequency shimming was assessed by phantom measurements. Initial results show that three modes are sufficient to homogeneously excite regions of interest at 9.4 T.
23749752	Using the iPlant collaborative discovery environment.
Curr Protoc Bioinformatics  2013Jun
The iPlant Collaborative is an academic consortium whose mission is to develop an informatics and social infrastructure to address the "grand challenges" in plant biology. Its cyberinfrastructure supports the computational needs of the research community and facilitates solving major challenges in plant science. The Discovery Environment provides a powerful and rich graphical interface to the iPlant Collaborative cyberinfrastructure by creating an accessible virtual workbench that enables all levels of expertise, ranging from students to traditional biology researchers and computational experts, to explore, analyze, and share their data. By providing access to iPlant's robust data-management system and high-performance computing resources, the Discovery Environment also creates a unified space in which researchers can access scalable tools. Researchers can use available Applications (Apps) to execute analyses on their data, as well as customize or integrate their own tools to better meet the specific needs of their research. These Apps can also be used in workflows that automate more complicated analyses. This module describes how to use the main features of the Discovery Environment, using bioinformatics workflows for high-throughput sequence data as examples.
23749755	Creating databases for biological information: an introduction.
Curr Protoc Bioinformatics  2013Jun
The essence of bioinformatics is dealing with large quantities of information. Whether it be sequencing data, microarray data files, mass spectrometric data (e.g., fingerprints), the catalog of strains arising from an insertional mutagenesis project, or even large numbers of PDF files, there inevitably comes a time when the information can simply no longer be managed with files and directories. This is where databases come into play. This unit briefly reviews the characteristics of several database management systems, including flat file, indexed file, relational databases, and NoSQL databases. It compares their strengths and weaknesses and offers some general guidelines for selecting an appropriate database management system.
23773974	Incidence of online health information search: a useful proxy for public health risk perception.
J. Med. Internet Res. 20130617 2013
Internet users use search engines to look for information online, including health information. Researchers in medical informatics have found a high correlation of the occurrence of certain search queries and the incidence of certain diseases. Consumers' search for information about diseases is related to current health status with regard to a disease and to the social environments that shape the public's attitudes and behaviors. This study aimed to investigate the extent to which public health risk perception as demonstrated by online information searches related to a health risk can be explained by the incidence of the health risk and social components of a specific population's environment. Using an ecological perspective, we suggest that a population's general concern for a health risk is formed by the incidence of the risk and social (eg, media attention) factors related with the risk. We constructed a dataset that included state-level data from 32 states on the incidence of the flu; a number of social factors, such as media attention to the flu; private resources, such as education and health insurance coverage; public resources, such as hospital beds and primary physicians; and utilization of these resources, including inpatient days and outpatient visits. We then explored whether online information searches about the flu (seasonal and pandemic flu) can be predicted using these variables. We used factor analysis to construct indexes for sets of social factors (private resources, public resources). We then applied panel data multiple regression analysis to exploit both time-series and cross-sectional variation in the data over a 7-year period. Overall, the results provide evidence that the main effects of independent variables-the incidence of the flu (P&lt;.001); social factors, including media attention (P&lt;.001); private resources, including life quality (P&lt;.001) and health lifestyles (P=.009); and public resources, such as hospital care utilization (P=.008) and public health funds (P=.02)-have significant effects on Web searches for queries related to the flu. After controlling for the number of reported disease cases and Internet access rate by state, we estimate the contribution of social factors to the public health risk perception levels by state (R(2)=23.37%). The interaction effects between flu incidence and social factors for our search terms did not add to the explanatory power of our regression models (R(2)&lt;1%). Our study suggests a practical way to measure the public's health risk perception for certain diseases using online information search volume by state. The social environment influences public risk perception regardless of disease incidence. Thus, monitoring the social variables can be very helpful in being ready to respond to the public's behavior in dealing with public health threats.
23613023	[Cardiovascular ultrahigh field magnetic resonance imaging : challenges, technical solutions and opportunities].
Radiologe  2013May
This involves high spatial resolution cardiac imaging with ultrahigh magnetic fields (7 T) and clinically acceptable image quality. Cardiovascular magnetic resonance imaging (MRI) at a field strength of 1.5 T using a spatial resolution of (2 × 2 × 6-8) mm(3). Cardiac MRI at ultrahigh field strength makes use of multitransmit/receive radiofrequency (RF) technology and development of novel technology that utilizes the traits of ultrahigh field MRI. Enhanced spatial resolution which is superior by a factor of 6-10 to what can be achieved by current clinical cardiac MRI. The relative spatial resolution (pixels per anatomical structure) comes close to what can be accomplished by current cardiac MRI in small rodents. Feasibility studies demonstrate the gain in spatial resolution at 7.0 T due to the sensitivity advantage inherent to ultrahigh magnetic fields. Please stay tuned and please put further weight behind the solution of the remaining technical problems of cardiac MRI at 7.0 T.
23204286	Fast computation of rotation-invariant image features by an approximate radial gradient transform.
IEEE Trans Image Process 20121129 2013Aug
We present the radial gradient transform (RGT) and a fast approximation, the approximate RGT (ARGT). We analyze the effects of the approximation on gradient quantization and histogramming. The ARGT is incorporated into the rotation-invariant fast feature (RIFF) algorithm. We demonstrate that, using the ARGT, RIFF extracts features 16× faster than SURF while achieving a similar performance for image matching and retrieval.
23528753	Secure SCADA communication by using a modified key management scheme.
ISA Trans 20130322 2013Jul
This paper presents and evaluates a new cryptographic key management scheme which increases the efficiency and security of the Supervisory Control And Data Acquisition (SCADA) communication. In the proposed key management scheme, two key update phases are used: session key update and master key update. In the session key update phase, session keys are generated in the master station. In the master key update phase, the Elliptic Curve Diffie-Hellman (ECDH) protocol is used. The Poisson process is also used to model the Security Index (SI) and Quality of Service (QoS). Our analysis shows that the proposed key management not only supports the required speed in the MODBUS implementation but also has several advantages compared to other key management schemes for secure communication in SCADA networks.
23555848	Analysis of free online physician advice services.
PLoS ONE 20130326 2013
Online Consumer Health websites are a major source of information for patients worldwide. We focus on another modality, online physician advice. We aim to evaluate and compare the freely available online expert physicians' advice in different countries, its scope and the type of content provided. Using automated methods for information retrieval and analysis, we compared consumer health portals from the US, Canada, the UK and Israel (WebMD,NetDoctor,AskTheDoctor and BeOK). The evaluated content was generated between 2002 and 2011. We analyzed the different sites, looking at the distribution of questions in the various health topics, answer lengths and content type. Answers could be categorized into longer broad-educational answers versus shorter patient-specific ones, with different physicians having personal preferences as to answer type. The Israeli website BeOK, providing 10 times the number of answers than in the other three health portals, supplied answers that are shorter on average than in the other websites. Response times in these sites may be rapid with 32% of the WebMD answers and 64% of the BeOK answers provided in less than 24 hours. The voluntary contribution model used by BeOK and WebMD enables generation of large numbers of physician expert answers at low cost, providing 50,000 and 3,500 answers per year, respectively. Unlike health information in online databases or advice and support in patient-forums, online physician advice provides qualified specialists' responses directly relevant to the questions asked. Our analysis showed that high numbers of expert answers could be generated in a timely fashion using a voluntary model. The length of answers varied significantly between the internet sites. Longer answers were associated with educational content while short answers were associated with patient-specific content. Standard site-specific guidelines for expert answers will allow for more desirable content (educational content) or better throughput (patient-specific content).
23835312	Searching CINAHL did not add value to clinical questions posed in NICE guidelines.
J Clin Epidemiol 20130705 2013Sep
This study aims to quantify the unique useful yield from the Cumulative Index to Nursing and Allied Health Literature (CINAHL) database to National Institute for Health and Clinical Excellence (NICE) clinical guidelines. A secondary objective is to investigate the relationship between this yield and different clinical question types. It is hypothesized that the unique useful yield from CINAHL is low, and this database can therefore be relegated to selective rather than routine searching. A retrospective sample of 15 NICE guidelines published between 2005 and 2009 was taken. Information on clinical review question type, number of references, and reference source was extracted. Only 0.33% (95% confidence interval: 0.01-0.64%) of references per guideline were unique to CINAHL. Nursing- or allied health (AH)-related questions were nearly three times as likely to have references unique to CINAHL as non-nursing- or AH-related questions (14.89% vs. 5.11%), and this relationship was found to be significant (P&lt;0.05). No significant relationship was found between question type and unique CINAHL yield for drug-related questions. The very low proportion of references unique to CINAHL strongly suggests that this database can be safely relegated to selective rather than routine searching. Nursing- and AH-related questions would benefit from selective searching of CINAHL.
23669956	Linearity in the response of photopolymers as optical recording media.
Opt Express  2013May6
Photopolymer are appealing materials for diffractive elements recording. Two of their properties when they are illuminated are useful for this goal: the relief surface changes and the refractive index modifications. To this goal the linearity in the material response is crucial to design the optimum irradiance for each element. In this paper we measured directly some parameters to know how linear is the material response, in terms of the refractive index modulation versus exposure, then we can predict the refractive index distributions during recording. We have analyzed at different recording intensities the evolution of monomer diffusion during recording for photopolymers based on PVA/Acrylamide. This model has been successfully applied to PVA/Acrylamide photopolymers to predict the transmitted diffracted orders and the agreement with experimental values has been increased.
23670014	Fast generation of video holograms of three-dimensional moving objects using a motion compensation-based novel look-up table.
Opt Express  2013May6
A novel approach for fast generation of video holograms of three-dimensional (3-D) moving objects using a motion compensation-based novel-look-up-table (MC-N-LUT) method is proposed. Motion compensation has been widely employed in compression of conventional 2-D video data because of its ability to exploit high temporal correlation between successive video frames. Here, this concept of motion-compensation is firstly applied to the N-LUT based on its inherent property of shift-invariance. That is, motion vectors of 3-D moving objects are extracted between the two consecutive video frames, and with them motions of the 3-D objects at each frame are compensated. Then, through this process, 3-D object data to be calculated for its video holograms are massively reduced, which results in a dramatic increase of the computational speed of the proposed method. Experimental results with three kinds of 3-D video scenarios reveal that the average number of calculated object points and the average calculation time for one object point of the proposed method, have found to be reduced down to 86.95%, 86.53% and 34.99%, 32.30%, respectively compared to those of the conventional N-LUT and temporal redundancy-based N-LUT (TR-N-LUT) methods.
23736447	An energy-efficient and elastic optical multiple access system based on coherent interleaved frequency division multiple access.
Opt Express  2013May20
This paper proposes a novel bandwidth-elastic and energy-efficient passive optical network (PON) based on the coherent interleaved frequency division multiple access (IFDMA) scheme. We experimentally demonstrate the coherent IFDMA-PON uplink transmission up-to 30 Gbps over a 30 km standard single-mode fiber with 2 × optical network units (ONUs). A low-complexity digital carrier synchronization technique enables multiple access of the ONUs on the basis of 78.1 MHz narrow band orthogonal subcarriers without any guard-bands.
24064509	Extension of the crosstalk cancellation method in ultrasonic transducer arrays from the harmonic regime to the transient one.
Ultrasonics 20130914 2014Feb
This paper describes a procedure to extend the crosstalk correction method presented in a previous paper [A. Bybi, S. Grondel, J. Assaad, A.-C. Hladky-Hennion, M. Rguiti, Reducing crosstalk in array structures by controlling the excitation voltage of individual elements: a feasibility study, Ultrasonics, 53 (6) (2013) 1135-1140] from the harmonic regime to the transient one. For this purpose a part of an ultrasonic transducer array radiating in water is modeled around the frequency 0.5 MHz using the finite element method. The study is carried out at low frequency in order to respect the same operating conditions than the previous paper. This choice facilitated the fabrication of the transducer arrays and the comparison of the numerical results with the experimental ones. The modeled array is composed of seventeen elements with the central element excited, while the others are grounded. The matching layers and the backing are not taken into account which limits the crosstalk only to the piezoelectric elements and fluid. This consideration reduces the structure density mesh and results in faster computation time (about 25 min for each configuration using a computer with a processor Intel Core i5-3210M, frequency 2.5 GHz and having 4 Go memory (RAM)). The novelty of this research work is to prove the efficiency of the crosstalk correction method in large frequency band as it is the case in medical imaging. The numerical results show the validity of the approach and demonstrate that crosstalk can be reduced by at least 13 dB in terms of displacement. Consequently, the directivity pattern of the individual element can be improved.
22679739	[F1000: a new medical literature evaluation and retrieval system].
Zhongguo Zhong Xi Yi Jie He Za Zhi  2012May
This article introduced a new clinical research evaluating system, Faculty of 1000 Medicine (F1000M), in which articles are selected, rated, and evaluated by its faculty of over 10 000 expert scientists and clinical researchers engaging in more than 3 000 peer-reviewed journals. It is a kind of post-publication peer review. A few evaluated articles about Chinese medicine were selected and introduced. It is hoped that F1000 could inspire both traditional Chinese medicine researchers and Western medicine researchers to conduct good quality research and write good clinical research reports.
22978119	[Exploring a new mode of integrative medicine information service].
Zhongguo Zhong Xi Yi Jie He Za Zhi  2012Jun
With the significant and continuous growth of the research and application of complementary and alternative medicine (CAM) all over the world, the demand for medical information services has been increasing correspondingly. However, the barriers of accessing and utilizing non-English literature, and the barrier of language have blocked English speaking clinicians and researchers of CAM from obtaining high quality and authoritative medical evidence from the non-English medical resources. This article, with introducing the UCLA Information Center for East-West Integrative Medicine, will demonstrate a new collaborative mode of integrative medicine information service between China and the US, and discuss the perceived challenges.
23025338	Time-varying latent effect model for longitudinal data with informative observation times.
Biometrics 20121001 2012Dec
In analysis of longitudinal data, it is not uncommon that observation times of repeated measurements are subject-specific and correlated with underlying longitudinal outcomes. Taking account of the dependence between observation times and longitudinal outcomes is critical under these situations to assure the validity of statistical inference. In this article, we propose a flexible joint model for longitudinal data analysis in the presence of informative observation times. In particular, the new procedure considers the shared random-effect model and assumes a time-varying coefficient for the latent variable, allowing a flexible way of modeling longitudinal outcomes while adjusting their association with observation times. Estimating equations are developed for parameter estimation. We show that the resulting estimators are consistent and asymptotically normal, with variance-covariance matrix that has a closed form and can be consistently estimated by the usual plug-in method. One additional advantage of the procedure is that it provides a unified framework to test whether the effect of the latent variable is zero, constant, or time-varying. Simulation studies show that the proposed approach is appropriate for practical use. An application to a bladder cancer data is also given to illustrate the methodology.
23346965	Bayesian network meta-analysis of root coverage procedures: ranking efficacy and identification of best treatment.
J. Clin. Periodontol. 20130124 2013Apr
The aim of this work was to conduct a Bayesian network meta-analysis (NM) of randomized controlled trials (RCTs) to establish a ranking in efficacy and the best technique for coronally advanced flap (CAF)-based root coverage procedures. A literature search on PubMed, Cochrane libraries, EMBASE, and hand-searched journals until June 2012 was conducted to identify RCTs on treatments of Miller Class I and II gingival recessions with at least 6 months of follow-up. The treatment outcomes were recession reduction (RecRed), clinical attachment gain (CALgain), keratinized tissue gain (KTgain), and complete root coverage (CRC). Twenty-nine studies met the inclusion criteria, 20 of which were classified as at high risk of bias. The CAF+connective tissue graft (CTG) combination ranked highest in effectiveness for RecRed (Probability of being the best = 40%) and CALgain (Pr = 33%); CAF+enamel matrix derivative (EMD) was slightly better for CRC; CAF+Collagen Matrix (CM) appeared effective for KTgain (Pr = 69%). Network inconsistency was low for all outcomes excluding CALgain. CAF+CTG might be considered the gold standard in root coverage procedures. The low amount of inconsistency gives support to the reliability of the present findings.
23235451	Prioritized degree distribution in wireless sensor networks with a network coded data collection method.
Sensors (Basel) 20121212 2012
The reliability of wireless sensor networks (WSNs) can be greatly affected by failures of sensor nodes due to energy exhaustion or the influence of brutal external environment conditions. Such failures seriously affect the data persistence and collection efficiency. Strategies based on network coding technology for WSNs such as LTCDS can improve the data persistence without mass redundancy. However, due to the bad intermediate performance of LTCDS, a serious 'cliff effect' may appear during the decoding period, and source data are hard to recover from sink nodes before sufficient encoded packets are collected. In this paper, the influence of coding degree distribution strategy on the 'cliff effect' is observed and the prioritized data storage and dissemination algorithm PLTD-ALPHA is presented to achieve better data persistence and recovering performance. With PLTD-ALPHA, the data in sensor network nodes present a trend that their degree distribution increases along with the degree level predefined, and the persistent data packets can be submitted to the sink node according to its degree in order. Finally, the performance of PLTD-ALPHA is evaluated and experiment results show that PLTD-ALPHA can greatly improve the data collection performance and decoding efficiency, while data persistence is not notably affected.
23447005	Software for storage and management of microclimatic data for preventive conservation of cultural heritage.
Sensors (Basel) 20130227 2013
Cultural Heritage preventive conservation requires the monitoring of the parameters involved in the process of deterioration of artworks. Thus, both long-term monitoring of the environmental parameters as well as further analysis of the recorded data are necessary. The long-term monitoring at frequencies higher than 1 data point/day generates large volumes of data that are difficult to store, manage and analyze. This paper presents software which uses a free open source database engine that allows managing and interacting with huge amounts of data from environmental monitoring of cultural heritage sites. It is of simple operation and offers multiple capabilities, such as detection of anomalous data, inquiries, graph plotting and mean trajectories. It is also possible to export the data to a spreadsheet for analyses with more advanced statistical methods (principal component analysis, ANOVA, linear regression, etc.). This paper also deals with a practical application developed for the Renaissance frescoes of the Cathedral of Valencia. The results suggest infiltration of rainwater in the vault and weekly relative humidity changes related with the religious service schedules.
23467056	Fingerprint identification using SIFT-based minutia descriptors and improved all descriptor-pair matching.
Sensors (Basel) 20130306 2013
The performance of conventional minutiae-based fingerprint authentication algorithms degrades significantly when dealing with low quality fingerprints with lots of cuts or scratches. A similar degradation of the minutiae-based algorithms is observed when small overlapping areas appear because of the quite narrow width of the sensors. Based on the detection of minutiae, Scale Invariant Feature Transformation (SIFT) descriptors are employed to fulfill verification tasks in the above difficult scenarios. However, the original SIFT algorithm is not suitable for fingerprint because of: (1) the similar patterns of parallel ridges; and (2) high computational resource consumption. To enhance the efficiency and effectiveness of the algorithm for fingerprint verification, we propose a SIFT-based Minutia Descriptor (SMD) to improve the SIFT algorithm through image processing, descriptor extraction and matcher. A two-step fast matcher, named improved All Descriptor-Pair Matching (iADM), is also proposed to implement the 1:N verifications in real-time. Fingerprint Identification using SMD and iADM (FISiA) achieved a significant improvement with respect to accuracy in representative databases compared with the conventional minutiae-based method. The speed of FISiA also can meet real-time requirements.
23967896	Strategies to assess the validity of recommendations: a study protocol.
Implement Sci 20130822 2013
Clinical practice guidelines (CPGs) become quickly outdated and require a periodic reassessment of evidence research to maintain their validity. However, there is little research about this topic. Our project will provide evidence for some of the most pressing questions in this field: 1) what is the average time for recommendations to become out of date?; 2) what is the comparative performance of two restricted search strategies to evaluate the need to update recommendations?; and 3) what is the feasibility of a more regular monitoring and updating strategy compared to usual practice?. In this protocol we will focus on questions one and two. The CPG Development Programme of the Spanish Ministry of Health developed 14 CPGs between 2008 and 2009. We will stratify guidelines by topic and by publication year, and include one CPG by strata.We will develop a strategy to assess the validity of CPG recommendations, which includes a baseline survey of clinical experts, an update of the original exhaustive literature searches, the identification of key references (reference that trigger a potential recommendation update), and the assessment of the potential changes in each recommendation.We will run two alternative search strategies to efficiently identify important new evidence: 1) PLUS search based in McMaster Premium LiteratUre Service (PLUS) database; and 2) a Restrictive Search (ReSe) based on the least number of MeSH terms and free text words needed to locate all the references of each original recommendation.We will perform a survival analysis of recommendations using the Kaplan-Meier method and we will use the log-rank test to analyse differences between survival curves according to the topic, the purpose, the strength of recommendations and the turnover. We will retrieve key references from the exhaustive search and evaluate their presence in the PLUS and ReSe search results. Our project, using a highly structured and transparent methodology, will provide guidance of when recommendations are likely to be at risk of being out of date. We will also assess two novel restrictive search strategies which could reduce the workload without compromising rigour when CPGs developers check for the need of updating.
23284199	Systematic archiving and access to health research data: rationale, current status and way forward.
Bull. World Health Organ. 20121010 2012Dec1
Systematically archiving data from health research and large-scale surveys and ensuring access to databases offer economic benefits and can improve the accountability, efficiency and quality of scientific research. Recently, interest in data archiving and sharing has grown and, in developed countries, research funders and institutions are increasingly adopting data-sharing policies. In developing countries, however, there is a lack of awareness of the benefits of data archiving and little discussion of policy. Many databases, even those of large-scale surveys, are not preserved systematically and access for secondary use is limited, which reduces the return on research investment. Several obstacles exist: organizational responsibility is unclear; infrastructure and personnel with appropriate data management and analysis skills are scarce; and researchers may be reluctant to share.This article considers recent progress in data sharing and the strategies and models used to encourage and facilitate it, with a focus on the World Health Organization Western Pacific Region. A case study from the Philippines demonstrates the benefits of data sharing by comparing the number and type of publications associated with two large-scale surveys with different approaches to sharing.Advocacy and leadership are needed at both national and regional levels to increase awareness. A step-by-step approach may be the most effective: initially large national databases could be made available to develop the methods and skills needed and to foster a data-sharing culture. Duplication of costs and effort could be avoided by collaboration between countries. In developing countries, interventions are required to build capacity in data management and analysis.
23269527	Regulatory administrative databases in FDA's Center for Biologics Evaluation and Research: convergence toward a unified database.
AAPS J 20121227 2013Apr
Regulatory administrative database systems within the Food and Drug Administration's (FDA) Center for Biologics Evaluation and Research (CBER) are essential to supporting its core mission, as a regulatory agency. Such systems are used within FDA to manage information and processes surrounding the processing, review, and tracking of investigational and marketed product submissions. This is an area of increasing interest in the pharmaceutical industry and has been a topic at trade association conferences (Buckley 2012). Such databases in CBER are complex, not for the type or relevance of the data to any particular scientific discipline but because of the variety of regulatory submission types and processes the systems support using the data. Commonalities among different data domains of CBER's regulatory administrative databases are discussed. These commonalities have evolved enough to constitute real database convergence and provide a valuable asset for business process intelligence. Balancing review workload across staff, exploring areas of risk in review capacity, process improvement, and presenting a clear and comprehensive landscape of review obligations are just some of the opportunities of such intelligence. This convergence has been occurring in the presence of usual forces that tend to drive information technology (IT) systems development toward separate stovepipes and data silos. CBER has achieved a significant level of convergence through a gradual process, using a clear goal, agreed upon development practices, and transparency of database objects, rather than through a single, discrete project or IT vendor solution. This approach offers a path forward for FDA systems toward a unified database.
23756888	Managing large SNP datasets with SNPpy.
Methods Mol. Biol.  2013
Using relational databases to manage SNP datasets is a very useful technique that has significant advantages over alternative methods, including the ability to leverage the power of relational databases to perform data validation, and the use of the powerful SQL query language to export data. SNPpy is a Python program which uses the PostgreSQL database and the SQLAlchemy Python library to automate SNP data management. This chapter shows how to use SNPpy to store and manage large datasets.
23756889	Quality control for genome-wide association studies.
Methods Mol. Biol.  2013
This chapter overviews the quality control (QC) issues for SNP-based genotyping methods used in genome-wide association studies. The main metrics for evaluating the quality of the genotypes are discussed followed by a worked out example of QC pipeline starting with raw data and finishing with a fully filtered dataset ready for downstream analysis. The emphasis is on automation of data storage, filtering, and manipulation to ensure data integrity throughput the process and on how to extract a global summary from these high dimensional datasets to allow better-informed downstream analytical decisions. All examples will be run using the R statistical programming language followed by a practical example using a fully automated QC pipeline for the Illumina platform.
23669178	RDFBuilder: a tool to automatically build RDF-based interfaces for MAGE-OM microarray data sources.
Comput Methods Programs Biomed 20130511 2013Jul
This paper presents RDFBuilder, a tool that enables RDF-based access to MAGE-ML-compliant microarray databases. We have developed a system that automatically transforms the MAGE-OM model and microarray data stored in the ArrayExpress database into RDF format. Additionally, the system automatically enables a SPARQL endpoint. This allows users to execute SPARQL queries for retrieving microarray data, either from specific experiments or from more than one experiment at a time. Our system optimizes response times by caching and reusing information from previous queries. In this paper, we describe our methods for achieving this transformation. We show that our approach is complementary to other existing initiatives, such as Bio2RDF, for accessing and retrieving data from the ArrayExpress database.
23562400	Spiking neural network model for memorizing sequences with forward and backward recall.
BioSystems 20130402 2013Jun
We present an oscillatory network of conductance based spiking neurons of Hodgkin-Huxley type as a model of memory storage and retrieval of sequences of events (or objects). The model is inspired by psychological and neurobiological evidence on sequential memories. The building block of the model is an oscillatory module which contains excitatory and inhibitory neurons with all-to-all connections. The connection architecture comprises two layers. A lower layer represents consecutive events during their storage and recall. This layer is composed of oscillatory modules. Plastic excitatory connections between the modules are implemented using an STDP type learning rule for sequential storage. Excitatory neurons in the upper layer project star-like modifiable connections toward the excitatory lower layer neurons. These neurons in the upper layer are used to tag sequences of events represented in the lower layer. Computer simulations demonstrate good performance of the model including difficult cases when different sequences contain overlapping events. We show that the model with STDP type or anti-STDP type learning rules can be applied for the simulation of forward and backward replay of neural spikes respectively.
23533138	From Peptidome to PRIDE: public proteomics data migration at a large scale.
Proteomics 20130420 2013May
The PRIDE database, developed and maintained at the European Bioinformatics Institute (EBI), is one of the most prominent data repositories dedicated to high throughput MS-based proteomics data. Peptidome, developed by the National Center for Biotechnology Information (NCBI) as a sibling resource to PRIDE, was discontinued due to funding constraints in April 2011. A joint effort between the two teams was started soon after the Peptidome closure to ensure that data were not "lost" to the wider proteomics community by exporting it to PRIDE. As a result, data in the low terabyte range have been migrated from Peptidome to PRIDE and made publicly available under experiment accessions 17 900-18 271, representing 54 projects, ~53 million mass spectra, ~10 million peptide identifications, ~650,000 protein identifications, ~1.1 million biologically relevant protein modifications, and 28 species, from more than 30 different labs.
23188548	Costs of cloud computing for a biometry department. A case study.
Methods Inf Med 20121127 2013
"Cloud" computing providers, such as the Amazon Web Services (AWS), offer stable and scalable computational resources based on hardware virtualization, with short, usually hourly, billing periods. The idea of pay-as-you-use seems appealing for biometry research units which have only limited access to university or corporate data center resources or grids. This case study compares the costs of an existing heterogeneous on-site hardware pool in a Medical Biometry and Statistics department to a comparable AWS offer. The "total cost of ownership", including all direct costs, is determined for the on-site hardware, and hourly prices are derived, based on actual system utilization during the year 2011. Indirect costs, which are difficult to quantify are not included in this comparison, but nevertheless some rough guidance from our experience is given. To indicate the scale of costs for a methodological research project, a simulation study of a permutation-based statistical approach is performed using AWS and on-site hardware. In the presented case, with a system utilization of 25-30 percent and 3-5-year amortization, on-site hardware can result in smaller costs, compared to hourly rental in the cloud dependent on the instance chosen. Renting cloud instances with sufficient main memory is a deciding factor in this comparison. Costs for on-site hardware may vary, depending on the specific infrastructure at a research unit, but have only moderate impact on the overall comparison and subsequent decision for obtaining affordable scientific computing resources. Overall utilization has a much stronger impact as it determines the actual computing hours needed per year. Taking this into ac count, cloud computing might still be a viable option for projects with limited maturity, or as a supplement for short peaks in demand.
23223611	Exploiting parallel R in the cloud with SPRINT.
Methods Inf Med 20121207 2013
Advances in DNA Microarray devices and next-generation massively parallel DNA sequencing platforms have led to an exponential growth in data availability but the arising opportunities require adequate computing resources. High Performance Computing (HPC) in the Cloud offers an affordable way of meeting this need. Bioconductor, a popular tool for high-throughput genomic data analysis, is distributed as add-on modules for the R statistical programming language but R has no native capabilities for exploiting multi-processor architectures. SPRINT is an R package that enables easy access to HPC for genomics researchers. This paper investigates: setting up and running SPRINT-enabled genomic analyses on Amazon's Elastic Compute Cloud (EC2), the advantages of submitting applications to EC2 from different parts of the world and, if resource underutilization can improve application performance. The SPRINT parallel implementations of correlation, permutation testing, partitioning around medoids and the multi-purpose papply have been benchmarked on data sets of various size on Amazon EC2. Jobs have been submitted from both the UK and Thailand to investigate monetary differences. It is possible to obtain good, scalable performance but the level of improvement is dependent upon the nature of the algorithm. Resource underutilization can further improve the time to result. End-user's location impacts on costs due to factors such as local taxation. Although not designed to satisfy HPC requirements, Amazon EC2 and cloud computing in general provides an interesting alternative and provides new possibilities for smaller organisations with limited funds.
23223786	An easily implemented method for abbreviation expansion for the medical domain in Japanese text. A preliminary study.
Methods Inf Med 20121207 2013
One of the barriers for the effective use of computerized health-care related text is the ambiguity of abbreviations. To date, the task of disambiguating abbreviations has been treated as a classification task based on surrounding words. Application of this framework for languages that have no word boundaries requires pre-processing to segment a sentence into separate word sequences. While the segmentation processing is often a source of problem, it is unknown whether word information is really requisite for abbreviation expansion. The present study examined and compared abbreviation expansion methods with and without the incorporation of word information as a preliminary study. We implemented two abbreviation expansion methods: 1) a morpheme-based method that relied on word information and therefore required pre-processing, and 2) a character-based method that relied on simple character information. We compared the expansion accuracies for these two methods using eight medical abbreviations. Experimental data were automatically built as a pseudo-annotated corpus using the Internet. As a result of the experiment, accuracies for the character-based method were from 0.890 to 0.942 while accuracies for the morpheme-based method were from 0.796 to 0.932. The character-based method significantly outperformed the morpheme-based method for three of the eight abbreviations (p &lt; 0.05). For the remaining five abbreviations, no significant differences were found between the two methods. Character information may be a good alternative in terms of simplicity to morphological information for abbreviation expansion in English medical abbreviations appeared in Japanese texts on the Internet.
22733975	An i2b2-based, generalizable, open source, self-scaling chronic disease registry.
J Am Med Inform Assoc 20120625 2013Jan1
Registries are a well-established mechanism for obtaining high quality, disease-specific data, but are often highly project-specific in their design, implementation, and policies for data use. In contrast to the conventional model of centralized data contribution, warehousing, and control, we design a self-scaling registry technology for collaborative data sharing, based upon the widely adopted Integrating Biology &amp; the Bedside (i2b2) data warehousing framework and the Shared Health Research Information Network (SHRINE) peer-to-peer networking software. Focusing our design around creation of a scalable solution for collaboration within multi-site disease registries, we leverage the i2b2 and SHRINE open source software to create a modular, ontology-based, federated infrastructure that provides research investigators full ownership and access to their contributed data while supporting permissioned yet robust data sharing. We accomplish these objectives via web services supporting peer-group overlays, group-aware data aggregation, and administrative functions. The 56-site Childhood Arthritis &amp; Rheumatology Research Alliance (CARRA) Registry and 3-site Harvard Inflammatory Bowel Diseases Longitudinal Data Repository now utilize i2b2 self-scaling registry technology (i2b2-SSR). This platform, extensible to federation of multiple projects within and between research networks, encompasses &gt;6000 subjects at sites throughout the USA. We utilize the i2b2-SSR platform to minimize technical barriers to collaboration while enabling fine-grained control over data sharing. The implementation of i2b2-SSR for the multi-site, multi-stakeholder CARRA Registry has established a digital infrastructure for community-driven research data sharing in pediatric rheumatology in the USA. We envision i2b2-SSR as a scalable, reusable solution facilitating interdisciplinary research across diseases.
22735615	Building public trust in uses of Health Insurance Portability and Accountability Act de-identified data.
J Am Med Inform Assoc 20120626 2013Jan1
The aim of this paper is to summarize concerns with the de-identification standard and methodologies established under the Health Insurance Portability and Accountability Act (HIPAA) regulations, and report some potential policies to address those concerns that were discussed at a recent workshop attended by industry, consumer, academic and research stakeholders. The target audience includes researchers, industry stakeholders, policy makers and consumer advocates concerned about preserving the ability to use HIPAA de-identified data for a range of important secondary uses. HIPAA sets forth methodologies for de-identifying health data; once such data are de-identified, they are no longer subject to HIPAA regulations and can be used for any purpose. Concerns have been raised about the sufficiency of HIPAA de-identification methodologies, the lack of legal accountability for unauthorized re-identification of de-identified data, and insufficient public transparency about de-identified data uses. Although there is little published evidence of the re-identification of properly de-identified datasets, such concerns appear to be increasing. This article discusses policy proposals intended to address de-identification concerns while maintaining de-identification as an effective tool for protecting privacy and preserving the ability to leverage health data for secondary purposes.
23043124	Using rule-based natural language processing to improve disease normalization in biomedical text.
J Am Med Inform Assoc 20121006 2013 Sep-Oct
In order for computers to extract useful information from unstructured text, a concept normalization system is needed to link relevant concepts in a text to sources that contain further information about the concept. Popular concept normalization tools in the biomedical field are dictionary-based. In this study we investigate the usefulness of natural language processing (NLP) as an adjunct to dictionary-based concept normalization. We compared the performance of two biomedical concept normalization systems, MetaMap and Peregrine, on the Arizona Disease Corpus, with and without the use of a rule-based NLP module. Performance was assessed for exact and inexact boundary matching of the system annotations with those of the gold standard and for concept identifier matching. Without the NLP module, MetaMap and Peregrine attained F-scores of 61.0% and 63.9%, respectively, for exact boundary matching, and 55.1% and 56.9% for concept identifier matching. With the aid of the NLP module, the F-scores of MetaMap and Peregrine improved to 73.3% and 78.0% for boundary matching, and to 66.2% and 69.8% for concept identifier matching. For inexact boundary matching, performances further increased to 85.5% and 85.4%, and to 73.6% and 73.3% for concept identifier matching. We have shown the added value of NLP for the recognition and normalization of diseases with MetaMap and Peregrine. The NLP module is general and can be applied in combination with any concept normalization system. Whether its use for concept types other than disease is equally advantageous remains to be investigated.
23645553	Identifying medical terms in patient-authored text: a crowdsourcing-based approach.
J Am Med Inform Assoc 20130505 2013 Nov-Dec
As people increasingly engage in online health-seeking behavior and contribute to health-oriented websites, the volume of medical text authored by patients and other medical novices grows rapidly. However, we lack an effective method for automatically identifying medical terms in patient-authored text (PAT). We demonstrate that crowdsourcing PAT medical term identification tasks to non-experts is a viable method for creating large, accurately-labeled PAT datasets; moreover, such datasets can be used to train classifiers that outperform existing medical term identification tools. To evaluate the viability of using non-expert crowds to label PAT, we compare expert (registered nurses) and non-expert (Amazon Mechanical Turk workers; Turkers) responses to a PAT medical term identification task. Next, we build a crowd-labeled dataset comprising 10 000 sentences from MedHelp. We train two models on this dataset and evaluate their performance, as well as that of MetaMap, Open Biomedical Annotator (OBA), and NaCTeM's TerMINE, against two gold standard datasets: one from MedHelp and the other from CureTogether. When aggregated according to a corroborative voting policy, Turker responses predict expert responses with an F1 score of 84%. A conditional random field (CRF) trained on 10 000 crowd-labeled MedHelp sentences achieves an F1 score of 78% against the CureTogether gold standard, widely outperforming OBA (47%), TerMINE (43%), and MetaMap (39%). A failure analysis of the CRF suggests that misclassified terms are likely to be either generic or rare. Our results show that combining statistical models sensitive to sentence-level context with crowd-labeled data is a scalable and effective technique for automatically identifying medical terms in PAT.
23394418	Web-scale discovery in an academic health sciences library: development and implementation of the EBSCO Discovery Service.
Med Ref Serv Q  2013
Funds made available at the close of the 2010-11 fiscal year allowed purchase of the EBSCO Discovery Service (EDS) for a year-long trial. The appeal of this web-scale discovery product that offers a Google-like interface to library resources was counter-balanced by concerns about quality of search results in an academic health science setting and the challenge of configuring an interface that serves the needs of a diverse group of library users. After initial configuration, usability testing with library users revealed the need for further work before general release. Of greatest concern were continuing issues with the relevance of items retrieved, appropriateness of system-supplied facet terms, and user difficulties with navigating the interface. EBSCO has worked with the library to better understand and identify problems and solutions. External roll-out to users occurred in June 2012.
22082153	Optimal imaging of in vitro clot sonothrombolysis by MR-guided focused ultrasound.
J Neuroimaging 20111114 2013Apr
As magnetic resonance-guided focused ultrasound (MRgFUS) sonothrombolysis relies on mechanical rather than thermal mechanisms to achieve clot lysis, thermometry is not useful for the intraoperative monitoring of clot breakdown by MRgFUS. Therefore, the purpose of this study was to evaluate the optimum imaging sequence for sonothrombolysis. In vitro blood drawn from 6 healthy volunteers was imaged using T1, T2 spin-echo, and T2 gradient-echo (GRE) sequences both before and after sonication using an Insightec ExAblate 4000 FUS transducer. Signal intensities of the three MR imaging sequences were measured and normalized to background signal for each time point. Representative samples of the pre- and postsonication clot were also sent to pathology for hematologic analysis. After sonication, the clot in the treatment tube was fully lysed as evidenced by physical and hematologic evaluation. The difference between pre- and postsonicated normalized signal intensity ratios demonstrated statistical significance only on T2 and GRE sequences (P &lt; .001). However, significant blooming artifact limited interpretation on all GRE images. T2 is the most appropriate sequence for the evaluation of mechanical MRgFUS sonothrombolysis of an in vitro clot. These findings are consistent across the oxidative states of clot up to 48 hours.
23671578	Dicoogle, a PACS featuring profiled content based image retrieval.
PLoS ONE 20130506 2013
Content-based image retrieval (CBIR) has been heralded as a mechanism to cope with the increasingly larger volumes of information present in medical imaging repositories. However, generic, extensible CBIR frameworks that work natively with Picture Archive and Communication Systems (PACS) are scarce. In this article we propose a methodology for parametric CBIR based on similarity profiles. The architecture and implementation of a profiled CBIR system, based on query by example, atop Dicoogle, an open-source, full-fletched PACS is also presented and discussed. In this solution, CBIR profiles allow the specification of both a distance function to be applied and the feature set that must be present for that function to operate. The presented framework provides the basis for a CBIR expansion mechanism and the solution developed integrates with DICOM based PACS networks where it provides CBIR functionality in a seamless manner.
23558168	Comprehensive temporal information detection from clinical text: medical events, time, and TLINK identification.
J Am Med Inform Assoc 20130404 2013 Sep-Oct
Temporal information detection systems have been developed by the Mayo Clinic for the 2012 i2b2 Natural Language Processing Challenge. To construct automated systems for EVENT/TIMEX3 extraction and temporal link (TLINK) identification from clinical text. The i2b2 organizers provided 190 annotated discharge summaries as the training set and 120 discharge summaries as the test set. Our Event system used a conditional random field classifier with a variety of features including lexical information, natural language elements, and medical ontology. The TIMEX3 system employed a rule-based method using regular expression pattern match and systematic reasoning to determine normalized values. The TLINK system employed both rule-based reasoning and machine learning. All three systems were built in an Apache Unstructured Information Management Architecture framework. Our TIMEX3 system performed the best (F-measure of 0.900, value accuracy 0.731) among the challenge teams. The Event system produced an F-measure of 0.870, and the TLINK system an F-measure of 0.537. Our TIMEX3 system demonstrated good capability of regular expression rules to extract and normalize time information. Event and TLINK machine learning systems required well-defined feature sets to perform well. We could also leverage expert knowledge as part of the machine learning features to further improve TLINK identification performance.
23571851	Eventual situations for timeline extraction from clinical reports.
J Am Med Inform Assoc 20130409 2013 Sep-Oct
To identify the temporal relations between clinical events and temporal expressions in clinical reports, as defined in the i2b2/VA 2012 challenge. To detect clinical events, we used rules and Conditional Random Fields. We built Random Forest models to identify event modality and polarity. To identify temporal expressions we built on the HeidelTime system. To detect temporal relations, we systematically studied their breakdown into distinct situations; we designed an oracle method to determine the most prominent situations and the most suitable associated classifiers, and combined their results. We achieved F-measures of 0.8307 for event identification, based on rules, and 0.8385 for temporal expression identification. In the temporal relation task, we identified nine main situations in three groups, experimentally confirming shared intuitions: within-sentence relations, section-related time, and across-sentence relations. Logistic regression and Naïve Bayes performed best on the first and third groups, and decision trees on the second. We reached a 0.6231 global F-measure, improving by 7.5 points our official submission. Carefully hand-crafted rules obtained good results for the detection of events and temporal expressions, while a combination of classifiers improved temporal link prediction. The characterization of the oracle recall of situations allowed us to point at directions where further work would be most useful for temporal relation detection: within-sentence relations and linking History of Present Illness events to the admission date. We suggest that the systematic situation breakdown proposed in this paper could also help improve other systems addressing this task.
23605114	Combining rules and machine learning for extraction of temporal expressions and events from clinical narratives.
J Am Med Inform Assoc 20130420 2013 Sep-Oct
Identification of clinical events (eg, problems, tests, treatments) and associated temporal expressions (eg, dates and times) are key tasks in extracting and managing data from electronic health records. As part of the i2b2 2012 Natural Language Processing for Clinical Data challenge, we developed and evaluated a system to automatically extract temporal expressions and events from clinical narratives. The extracted temporal expressions were additionally normalized by assigning type, value, and modifier. The system combines rule-based and machine learning approaches that rely on morphological, lexical, syntactic, semantic, and domain-specific features. Rule-based components were designed to handle the recognition and normalization of temporal expressions, while conditional random fields models were trained for event and temporal recognition. The system achieved micro F scores of 90% for the extraction of temporal expressions and 87% for clinical event extraction. The normalization component for temporal expressions achieved accuracies of 84.73% (expression's type), 70.44% (value), and 82.75% (modifier). Compared to the initial agreement between human annotators (87-89%), the system provided comparable performance for both event and temporal expression mining. While (lenient) identification of such mentions is achievable, finding the exact boundaries proved challenging. The system provides a state-of-the-art method that can be used to support automated identification of mentions of clinical events and temporal expressions in narratives either to support the manual review process or as a part of a large-scale processing of electronic health databases.
23676245	Temporal reasoning over clinical text: the state of the art.
J Am Med Inform Assoc 20130515 2013 Sep-Oct
To provide an overview of the problem of temporal reasoning over clinical text and to summarize the state of the art in clinical natural language processing for this task. This overview targets medical informatics researchers who are unfamiliar with the problems and applications of temporal reasoning over clinical text. We review the major applications of text-based temporal reasoning, describe the challenges for software systems handling temporal information in clinical text, and give an overview of the state of the art. Finally, we present some perspectives on future research directions that emerged during the recent community-wide challenge on text-based temporal reasoning in the clinical domain.
23686936	A flexible framework for recognizing events, temporal expressions, and temporal relations in clinical text.
J Am Med Inform Assoc 20130518 2013 Sep-Oct
To provide a natural language processing method for the automatic recognition of events, temporal expressions, and temporal relations in clinical records. A combination of supervised, unsupervised, and rule-based methods were used. Supervised methods include conditional random fields and support vector machines. A flexible automated feature selection technique was used to select the best subset of features for each supervised task. Unsupervised methods include Brown clustering on several corpora, which result in our method being considered semisupervised. On the 2012 Informatics for Integrating Biology and the Bedside (i2b2) shared task data, we achieved an overall event F1-measure of 0.8045, an overall temporal expression F1-measure of 0.6154, an overall temporal link detection F1-measure of 0.5594, and an end-to-end temporal link detection F1-measure of 0.5258. The most competitive system was our event recognition method, which ranked third out of the 14 participants in the event task. Analysis reveals the event recognition method has difficulty determining which modifiers to include/exclude in the event span. The temporal expression recognition method requires significantly more normalization rules, although many of these rules apply only to a small number of cases. Finally, the temporal relation recognition method requires more advanced medical knowledge and could be improved by separating the single discourse relation classifier into multiple, more targeted component classifiers. Recognizing events and temporal expressions can be achieved accurately by combining supervised and unsupervised methods, even when only minimal medical knowledge is available. Temporal normalization and temporal relation recognition, however, are far more dependent on the modeling of medical knowledge.
22952301	Improving image retrieval effectiveness via query expansion using MeSH hierarchical structure.
J Am Med Inform Assoc 20120905 2013 Nov-Dec
We explored two strategies for query expansion utilizing medical subject headings (MeSH) ontology to improve the effectiveness of medical image retrieval systems. In order to achieve greater effectiveness in the expansion, the search text was analyzed to identify which terms were most amenable to being expanded. To perform the expansions we utilized the hierarchical structure by which the MeSH descriptors are organized. Two strategies for selecting the terms to be expanded in each query were studied. The first consisted of identifying the medical concepts using the unified medical language system metathesaurus. In the second strategy the text of the query was divided into n-grams, resulting in sequences corresponding to MeSH descriptors. For the evaluation of the system, we used the collection made available by the ImageCLEF organization in its 2011 medical image retrieval task. The main measure of efficiency employed for evaluating the techniques developed was the mean average precision (MAP). Both strategies exceeded the average MAP score in the ImageCLEF 2011 competition (0.1644). The n-gram expansion strategy achieved a MAP of 0.2004, which represents an improvement of 21.89% over the average MAP score in the competition. On the other hand, the medical concepts expansion strategy scored 0.2172 in the MAP, representing a 32.11% improvement. This run won the text-based medical image retrieval task in 2011. Query expansion exploiting the hierarchical structure of the MeSH descriptors achieved a significant improvement in image retrieval systems.
23568395	[The subject repositories of strategy of the Open Access initiative].
Nutr Hosp  2012Nov
The subject repositories are defined as a set of digital objects resulting from the research related to a specific disciplinary field and occupy a still restricted space in the discussion agenda of the Free Access Movement when compared to amplitude reached in the discussion of Institutional Repositories. Although the Subject Repository comes to prominence in the field, especially for the success of initiatives such as the arXiv, PubMed and E-prints, the literature on the subject is recognized as very limited. Despite its roots in the Library and Information Science, and focus on the management of disciplinary collections (subject area literature), there is little information available about the development and management of subject repositories. The following text seeks to make a brief summary on the topic as a way to present the potential to develop subject repositories in order to strengthen the initiative of open access.
23568399	[Design of a semantic framework for contextualized retrieval of scientific documents in the health domain].
Nutr Hosp  2012Nov
Personalized healthcare requires recombining heterogeneous publicly available data with a patient's or group of patient's profile. A well-known problem in state-of-the-art information management is the overwhelming amount of information available. Besides, state-of-the-art solutions do not take advantage of modern semantic processing to adequately transform data into knowledge. This issue is especially relevant in the health domain, as key processes depend dramatically on the access to high quality, complete, up-to-date, and relevant content (e.g. diagnostics, risk assessment, public health interventions, etc.). This proposal aims to provide novel information management and retrieval solutions in the domain of health sciences to address the situation discussed above. More specifically, we introduce semantic reasoning to retrieve the most relevant knowledge available according to the health profile of a given person. For this, we developed a semantic model to represent health profiles of people and to characterize existing sources of relevant information in order to crawl them to populate a semantic repository with content references and properties. We outline the tools needed to query the knowledge base using the semantic profiles of individuals to get the most relevant content. The proposed solution, discussed here as a proof-of-concept, aims to contribute to the realm of personal health and evidence-based medicine technologies. The tools developed could also be used to take advantage of existing knowledge to facilitate a systematic review of reports, studies and analysis that may be relevant to the health conditions of single patients or patient profiles.
22958988	Derivation and validation of automated electronic search strategies to extract Charlson comorbidities from electronic medical records.
Mayo Clin. Proc.  2012Sep
To develop and validate automated electronic note search strategies (automated digital algorithm) to identify Charlson comorbidities. The automated digital algorithm was built by a series of programmatic queries applied to an institutional electronic medical record database. The automated digital algorithm was derived from secondary analysis of an observational cohort study of 1447 patients admitted to the intensive care unit from January 1 through December 31, 2006, and validated in an independent cohort of 240 patients. The sensitivity, specificity, and positive and negative predictive values of the automated digital algorithm and International Classification of Diseases, Ninth Revision (ICD-9) codes were compared with comprehensive medical record review (reference standard) for the Charlson comorbidities. In the derivation cohort, the automated digital algorithm achieved a median sensitivity of 100% (range, 99%-100%) and a median specificity of 99.7% (range, 99%-100%). In the validation cohort, the sensitivity of the automated digital algorithm ranged from 91% to 100%, and the specificity ranged from 98% to 100%. The sensitivity of the ICD-9 codes ranged from 8% for dementia to 100% for leukemia, whereas specificity ranged from 86% for congestive heart failure to 100% for leukemia, dementia, and AIDS. Our results suggest that search strategies that use automated electronic search strategies to extract Charlson comorbidities from the clinical notes contained within the electronic medical record are feasible and reliable. Automated digital algorithm outperformed ICD-9 codes in all the Charlson variables except leukemia, with greater sensitivity, specificity, and positive and negative predictive values.
23646024	Comparative effectiveness research designs: an analysis of terms and coverage in Medical Subject Headings (MeSH) and Emtree.
J Med Libr Assoc  2013Apr
We analyzed the extent to which comparative effectiveness research (CER) organizations share terms for designs, analyzed coverage of CER designs in Medical Subject Headings (MeSH) and Emtree, and explored whether scientists use CER design terms. We developed local terminologies (LTs) and a CER design terminology by extracting terms in documents from five organizations. We defined coverage as the distribution over match type in MeSH and Emtree. We created a crosswalk by recording terms to which design terms mapped in both controlled vocabularies. We analyzed the hits for queries restricted to titles and abstracts to explore scientists' language. Pairwise LT overlap ranged from 22.64% (12/53) to 75.61% (31/41). The CER design terminology (n = 78 terms) consisted of terms for primary study designs and a few terms useful for evaluating evidence, such as opinion paper and systematic review. Patterns of coverage were similar in MeSH and Emtree (gamma = 0.581, P = 0.002). Stakeholder terminologies vary, and terms are inconsistently covered in MeSH and Emtree. The CER design terminology and crosswalk may be useful for expert searchers. For partially mapped terms, queries could consist of free text for modifiers such as nonrandomized or interrupted added to broad or related controlled terms.
23715987	Supporting SBML as a model exchange format in software applications.
Methods Mol. Biol.  2013
This chapter describes the Systems Biology Markup Language (SBML) from its origins. It describes the rationale behind and importance of having a common language when it comes to representing models. This chapter mentions the development of SBML and outlines the structure of an SBML model. It provides a section on libSBML, a useful application programming interface (API) library for reading, writing, manipulating and validating content expressed in the SBML format. Finally the chapter also provides a description of the SBML Toolbox which provides a means of facilitating the import and export of SBML from both MATLAB and Octave ( http://www.gnu.org/software/octave/) environments.
23715992	Bioinformatics workflows and web services in systems biology made easy for experimentalists.
Methods Mol. Biol.  2013
Workflows are useful to perform data analysis and integration in systems biology. Workflow management systems can help users create workflows without any previous knowledge in programming and web services. However the computational skills required to build such workflows are usually above the level most biological experimentalists are comfortable with. In this chapter we introduce workflow management systems that reuse existing workflows instead of creating them, making it easier for experimentalists to perform computational tasks.
23402961	SNOMED CT module-driven clinical archetype management.
J Biomed Inform 20130209 2013Jun
To explore semantic search to improve management and user navigation in clinical archetype repositories. In order to support semantic searches across archetypes, an automated method based on SNOMED CT modularization is implemented to transform clinical archetypes into SNOMED CT extracts. Concurrently, query terms are converted into SNOMED CT concepts using the search engine Lucene. Retrieval is then carried out by matching query concepts with the corresponding SNOMED CT segments. A test collection of the 16 clinical archetypes, including over 250 terms, and a subset of 55 clinical terms from two medical dictionaries, MediLexicon and MedlinePlus, were used to test our method. The keyword-based service supported by the OpenEHR repository offered us a benchmark to evaluate the enhancement of performance. In total, our approach reached 97.4% precision and 69.1% recall, providing a substantial improvement of recall (more than 70%) compared to the benchmark. Exploiting medical domain knowledge from ontologies such as SNOMED CT may overcome some limitations of the keyword-based systems and thus improve the search experience of repository users. An automated approach based on ontology segmentation is an efficient and feasible way for supporting modeling, management and user navigation in clinical archetype repositories.
24180066	Location selection criteria for a second data center or off-site storage of materials.
J Emerg Manag  2013 May-Jun
As organizations develop secondary data centers, it is critical that they be placed in locations that serve the organization yet do not have a shared risk with the primary data center. The organization needs to consider factors or guidelines which mitigate potential issues that could affect both the primary and secondary data center. It is impossible to eliminate all risk to a single data center but an organization needs to ensure that at least one data center remains operable. The article will propose that data centers be located 50 km or approximately 30 miles apart. The proposal is supported by evaluating earthquake intensity maps that will show that earthquakes damage drops to relatively safe levels after the 30 miles from the epicenter. The article will show that other environmental factors such as power, floods, fire, transportation, fire, and soil are also mitigated by a 30-mile separation guideline.
23928109	Cancer markers: integratively annotated classification.
Gene 20130806 2013Nov10
Translational cancer genomics research aims to ensure that experimental knowledge is subject to computational analysis, and integrated with a variety of records from omics and clinical sources. The data retrieval from such sources is not trivial, due to their redundancy and heterogeneity, and the presence of false evidence. In silico marker identification, therefore, remains a complex task that is mainly motivated by the impact that target identification from the elucidation of gene co-expression dynamics and regulation mechanisms, combined with the discovery of genotype-phenotype associations, may have for clinical validation. Based on the reuse of publicly available gene expression data, our aim is to propose cancer marker classification by integrating the prediction power of multiple annotation sources. In particular, with reference to the functional annotation for colorectal markers, we indicate a classification of markers into diagnostic and prognostic classes combined with susceptibility and risk factors.
23761195	Biomimetic synthesis of materials for technology.
Chemistry 20130612 2013Jul1
In a world with ever decreasing natural reserves, researchers are striving to find sustainable methods of producing components for technology. Bioinspired, biokleptic and biomimetic materials can be used to form a wide range of technologically relevant materials under environmentally friendly conditions. Here we investigate a range of biotemplated and bioinspired materials that can be used to develop components for devices, such as optics, photonics, photovoltaics, circuits and data storage.
23372073	Monitoring of cigarette smoking using wearable sensors and support vector machines.
IEEE Trans Biomed Eng 20130130 2013Jul
Cigarette smoking is a serious risk factor for cancer, cardiovascular, and pulmonary diseases. Current methods of monitoring of cigarette smoking habits rely on various forms of self-report that are prone to errors and under reporting. This paper presents a first step in the development of a methodology for accurate and objective assessment of smoking using noninvasive wearable sensors (Personal Automatic Cigarette Tracker-PACT) by demonstrating feasibility of automatic recognition of smoke inhalations from signals arising from continuous monitoring of breathing and hand-to-mouth gestures by support vector machine classifiers. The performance of subject-dependent (individually calibrated) models was compared to performance of subject-independent (group) classification models. The models were trained and validated on a dataset collected from 20 subjects performing 12 different activities representative of everyday living (total duration 19.5 h or 21,411 breath cycles). Precision and recall were used as the accuracy metrics. Group models obtained 87% and 80% of average precision and recall, respectively. Individual models resulted in 90% of average precision and recall, indicating a significant presence of individual traits in signal patterns. These results suggest the feasibility of monitoring cigarette smoking by means of a wearable and noninvasive sensor system in free living conditions.
23446028	Hierarchical information fusion for global displacement estimation in microsensor motion capture.
IEEE Trans Biomed Eng 20130222 2013Jul
This paper presents a novel hierarchical information fusion algorithm to obtain human global displacement for different gait patterns, including walking, running, and hopping based on seven body-worn inertial and magnetic measurement units. In the first-level sensor fusion, the orientation for each segment is achieved by a complementary Kalman filter (CKF) which compensates for the orientation error of the inertial navigation system solution through its error state vector. For each foot segment, the displacement is also estimated by the CKF, and zero velocity update is included for the drift reduction in foot displacement estimation. Based on the segment orientations and left/right foot locations, two global displacement estimates can be acquired from left/right lower limb separately using a linked biomechanical model. In the second-level geometric fusion, another Kalman filter is deployed to compensate for the difference between the two estimates from the sensor fusion and get more accurate overall global displacement estimation. The updated global displacement will be transmitted to left/right foot based on the human lower biomechanical model to restrict the drifts in both feet displacements. The experimental results have shown that our proposed method can accurately estimate human locomotion for the three different gait patterns with regard to the optical motion tracker.
23807687	D-MSR: a distributed network management scheme for real-time monitoring and process control applications in wireless industrial automation.
Sensors (Basel) 20130627 2013
Current wireless technologies for industrial applications, such as WirelessHART and ISA100.11a, use a centralized management approach where a central network manager handles the requirements of the static network. However, such a centralized approach has several drawbacks. For example, it cannot cope with dynamicity/disturbance in large-scale networks in a real-time manner and it incurs a high communication overhead and latency for exchanging management traffic. In this paper, we therefore propose a distributed network management scheme, D-MSR. It enables the network devices to join the network, schedule their communications, establish end-to-end connections by reserving the communication resources for addressing real-time requirements, and cope with network dynamicity (e.g., node/edge failures) in a distributed manner. According to our knowledge, this is the first distributed management scheme based on IEEE 802.15.4e standard, which guides the nodes in different phases from joining until publishing their sensor data in the network. We demonstrate via simulation that D-MSR can address real-time and reliable communication as well as the high throughput requirements of industrial automation wireless networks, while also achieving higher efficiency in network management than WirelessHART, in terms of delay and overhead.
23807688	ADVICE: a new approach for near-real-time monitoring of surface displacements in landslide hazard scenarios.
Sensors (Basel) 20130627 2013
We present a new method for near-real-time monitoring of surface displacements due to landslide phenomena, namely ADVanced dIsplaCement monitoring system for Early warning (ADVICE). The procedure includes: (i) data acquisition and transfer protocols; (ii) data collection, filtering, and validation; (iii) data analysis and restitution through a set of dedicated software; (iv) recognition of displacement/velocity threshold, early warning messages via SMS and/or emails; (v) automatic publication of the results on a dedicated webpage. We show how the system evolved and the results obtained by applying ADVICE over three years into a real early warning scenario relevant to a large earthflow located in southern Italy. ADVICE has speed-up and facilitated the understanding of the landslide phenomenon, the communication of the monitoring results to the partners, and consequently the decision-making process in a critical scenario. Our work might have potential applications not only for landslide monitoring but also in other contexts, as monitoring of other geohazards and of complex infrastructures, as open-pit mines, buildings, dams, etc.
23650539	GTXOP: a game theoretic approach for QoS provisioning using transmission opportunity tuning.
PLoS ONE 20130501 2013
In unsupervised contention-based networks such as EDCA mode of IEEE 802.11(e)(s), upon winning the channel, each node gets a transmission opportunity (TXOP) in which the node can transmit multiple frames consequently without releasing the channel. Adjusting TXOP can lead to better bandwidth utilization and QoS provisioning. To improve WLAN throughput performance, EDCA packet bursting can be used in 802.11e, meaning that once a station has gained an EDCA-TXOP, it can be allowed to transmit more than one frame without re-contending for the channel. Following the access to the channel, the station can send multiple frames as long as the total access time does not exceed the TXOP Limit. This mechanism can reduce the network overhead and increase the channel utilization instead. However, packet bursting may cause unfairness in addition to increasing jitter, delay and loss. To the best of the authors' knowledge, although TXOP tuning has been investigated through different methods, it has not been considered within a game theory framework. In this study, based on the analytical models of EDCA, a game theoretic approach called GTXOP is proposed to determine TXOP dynamically (i.e. according to the dynamisms of WLAN networks and the number of nodes in the network). Using GTXOP, each node can choose its TXOP autonomously, such that in addition to QoS improvement, the overall network performance is also improved.
23249674	Computer-aided beam arrangement based on similar cases in radiation treatment-planning databases for stereotactic lung radiation therapy.
J. Radiat. Res. 20121218 2013May
The purpose of this study was to develop a computer-aided method for determination of beam arrangements based on similar cases in a radiotherapy treatment-planning database for stereotactic lung radiation therapy. Similar-case-based beam arrangements were automatically determined based on the following two steps. First, the five most similar cases were searched, based on geometrical features related to the location, size and shape of the planning target volume, lung and spinal cord. Second, five beam arrangements of an objective case were automatically determined by registering five similar cases with the objective case, with respect to lung regions, by means of a linear registration technique. For evaluation of the beam arrangements five treatment plans were manually created by applying the beam arrangements determined in the second step to the objective case. The most usable beam arrangement was selected by sorting the five treatment plans based on eight plan evaluation indices, including the D95, mean lung dose and spinal cord maximum dose. We applied the proposed method to 10 test cases, by using an RTP database of 81 cases with lung cancer, and compared the eight plan evaluation indices between the original treatment plan and the corresponding most usable similar-case-based treatment plan. As a result, the proposed method may provide usable beam arrangements, which have no statistically significant differences from the original beam arrangements (P &gt; 0.05) in terms of the eight plan evaluation indices. Therefore, the proposed method could be employed as an educational tool for less experienced treatment planners.
23669845	High-speed free-space quantum key distribution system for urban daylight applications.
Appl Opt  2013May10
We report a free-space quantum key distribution system designed for high-speed key transmission in urban areas. Clocking the system at gigahertz frequencies and efficiently filtering background enables higher secure key rates than those previously achieved by similar systems. The transmitter and receiver are located in two separate buildings 300 m apart in downtown Madrid and they exchange secure keys at rates up to 1 Mbps. The system operates in full bright daylight conditions with an average secure key rate of 0.5 Mbps and 24 h stability without human intervention.
23514938	Synthetic DNA: the next generation of big data storage.
Bioengineered 20130320 2013 May-Jun
With world wide data predicted to exceed 40 trillion gigabytes by 2020, big data storage is a very real and escalating problem. Herein, we discuss the utility of synthetic DNA as a robust and eco-friendly archival data storage solution of the future.
23649280	Questionnaire study of electronic wear-time tracking as experienced by patients and parents during treatment with removable orthodontic appliances.
J Orofac Orthop 20130508 2013May
To survey how patients and parents rate microelectronic wear-time tracking (TheraMon(®)) during treatment with removable orthodontic appliances. A total of 125 patients with a mean age of 11.99 years whose treatment involved removable appliances with a built-in microsensor for wear-time documentation were enrolled in a questionnaire study addressing electronic wear-time tracking. Respondents included the patients and their parents. A total of 86% of the patients reported that the orthodontic appliance's comfort was unaffected by the installed sensor. A majority of respondents had a favorable impression of wear-time tracking. Printed wear-time documents from the clinician's computer were considered a "nice certificate of compliance" by 46% of patients, and 38% of them stated that they intended to improve their compliance when faced with a poor record. Indeed, 48% of parents believe that wear-time tracking can improve the therapeutic success, while 32% believe that it can reduce the duration of treatment. Around 10% of respondents felt that the sensors were unnecessary and not recommendable. These favorable ratings by patients and their parents may help future patients and users to decide for or against microelectronic wear-time tracking. Randomized studies are needed to demonstrate whether the sheer presence of a wear-time sensor stimulates compliance on its own.
23587045	Treetrimmer: a method for phylogenetic dataset size reduction.
BMC Res Notes 20130412 2013
With rapid advances in genome sequencing and bioinformatics, it is now possible to generate phylogenetic trees containing thousands of operational taxonomic units (OTUs) from a wide range of organisms. However, use of rigorous tree-building methods on such large datasets is prohibitive and manual 'pruning' of sequence alignments is time consuming and raises concerns over reproducibility. There is a need for bioinformatic tools with which to objectively carry out such pruning procedures. Here we present 'TreeTrimmer', a bioinformatics procedure that removes unnecessary redundancy in large phylogenetic datasets, alleviating the size effect on more rigorous downstream analyses. The method identifies and removes user-defined 'redundant' sequences, e.g., orthologous sequences from closely related organisms and 'recently' evolved lineage-specific paralogs. Representative OTUs are retained for more rigorous re-analysis. TreeTrimmer reduces the OTU density of phylogenetic trees without sacrificing taxonomic diversity while retaining the original tree topology, thereby speeding up downstream computer-intensive analyses, e.g., Bayesian and maximum likelihood tree reconstructions, in a reproducible fashion.
22275480	Bypassing primary sensory cortices--a direct thalamocortical pathway for transmitting salient sensory information.
Cereb. Cortex 20120123 2013Jan
Detection and appropriate reaction to sudden and intense events happening in the sensory environment is crucial for survival. By combining Bayesian model selection with dynamic causal modeling of functional magnetic resonance imaging data, a novel analysis approach that allows inferring the causality between neural activities in different brain areas, we demonstrate that salient sensory information reaches the multimodal cortical areas responsible for its detection directly from the thalamus, without being first processed in primary and secondary sensory-specific areas. This direct thalamocortical transmission of multimodal salient information is parallel to the processing of finer stimulus attributes, which are transmitted in a modality-specific fashion from the thalamus to the relevant primary sensory areas. Such direct thalamocortical connections bypassing primary sensory cortices provide a fast and efficient way for transmitting information from subcortical structures to multimodal cortical areas, to allow the early detection of salient events and, thereby, trigger immediate and appropriate behavior.
23270313	Content-based management service for medical videos.
Telemed J E Health 20121227 2013Jan
Development of health information technology has had a dramatic impact to improve the efficiency and quality of medical care. Developing interoperable health information systems for healthcare providers has the potential to improve the quality and equitability of patient-centered healthcare. In this article, we describe an automated content-based medical video analysis and management service that provides convenience and ease in accessing the relevant medical video content without sequential scanning. The system facilitates effective temporal video segmentation and content-based visual information retrieval that enable a more reliable understanding of medical video content. The system is implemented as a Web- and mobile-based service and has the potential to offer a knowledge-sharing platform for the purpose of efficient medical video content access.
23367280	A data mining approach to reduce the false alarm rate of patient monitors.
Conf Proc IEEE Eng Med Biol Soc  2012
Patient monitors in intensive care units trigger alarms if the state of the patient deteriorates or if there is a technical problem, e.g. loose sensors. Monitoring systems have a high sensitivity in order to detect relevant changes in the patient state. However, multiple studies revealed a high rate of either false or clinically not relevant alarms. It was found that the high rate of false alarms has a negative impact on both patients and staff. In this study we apply data mining methods to reduce the false alarm rate of monitoring systems. We follow a multi-parameter approach where multiple signals of a monitoring system are used to classify given alarm situations. In particular we focus on five alarm types and let our system decide whether the triggered alarm is clinically relevant or can be considered as a false alarm. Several classification algorithms (Naive Bayes, Decision Trees, SVM, kNN and Multi-Layer Perceptron) were evaluated. For training and test sets a subset of the freely available MIMIC II database was used. Alarm-specific classification accuracy was between 78.56% and 98.84%. Suppression rates for false alarms were between 75.24% and 99.23%. Classification results strongly depend on available training data, which is still limited in the intensive care domain. However, this study shows that data mining methods are useful and applicable for alarm classification.
23424149	Computational biology in the cloud: methods and new insights from computing at scale.
Pac Symp Biocomput  2013
The past few years have seen both explosions in the size of biological data sets and the proliferation of new, highly flexible on-demand computing capabilities. The sheer amount of information available from genomic and metagenomic sequencing, high-throughput proteomics, experimental and simulation datasets on molecular structure and dynamics affords an opportunity for greatly expanded insight, but it creates new challenges of scale for computation, storage, and interpretation of petascale data. Cloud computing resources have the potential to help solve these problems by offering a utility model of computing and storage: near-unlimited capacity, the ability to burst usage, and cheap and flexible payment models. Effective use of cloud computing on large biological datasets requires dealing with non-trivial problems of scale and robustness, since performance-limiting factors can change substantially when a dataset grows by a factor of 10,000 or more. New computing paradigms are thus often needed. The use of cloud platforms also creates new opportunities to share data, reduce duplication, and to provide easy reproducibility by making the datasets and computational methods easily available.
22692260	Data model considerations for clinical effectiveness researchers.
Med Care  2012Jul
Growing adoption of electronic health records and increased emphasis on the reuse and integration of clinical care and administration data require a robust informatics infrastructure to inform health care effectiveness in real-world settings. The Scalable Architecture for Federated Translational Inquiries Network (SAFTINet) was one of 3 projects receiving Agency for Healthcare Quality and Research funds to create a scalable, distributed network to support Comparative Effectiveness Research. SAFTINet's method of extracting and compiling data from disparate entities requires the use of a shared common data model. DATA MODELS: Focusing on the needs of CER investigators, in addition to other project considerations, we examined the suitability of several data models. Data modeling is the process of determining which data elements will be stored and how they will be stored, including their relationships and constraints. Addressing compromises between complexity and usability is critical to modeling decisions. The SAFTINet project provides the case study for describing data model evaluation. A sample use case defines a cohort of asthma subjects that illustrates the need to identify patients by age, diagnoses, and medication use while excluding those with diagnoses that may often be misdiagnosed as asthma. The SAFTINet team explored several data models against a set of technical and investigator requirements to select a data model that best fit its needs and was conducive to expansion with new research requirements. Although SAFTINet ultimately chose the Observation Medical Outcomes Partnership common data model, other valid options exist and prioritization of requirements is dependent upon many factors.
23079357	[Learning to search on the Web of Science: a reply to the Aluja, Becoña, Botella, Colom, Echeburúa, Forns, Pérez and Vila (2011) study].
Psicothema  2012Nov
The aims of this paper are, on the one hand, to present the differences in the results that may be incurred when working with some automated database functions to obtain scientific production and, on the other hand, to reveal the mistakes made by Aluja et al. (2011) in their considerations and suggestions to the study of Olivas-Avila and Musi-Lechuga (2010a). In this paper, we show that the procedure used to replicate the study is incorrect and, therefore, the authors did not only confuse production with diffusion indicators, but also suggest many indicators that have no discriminative power with the sample studied,- and the most striking-one hundred percent out of the ten cases analyzed were incorrectly analyzed and, therefore, the results do not correspond to the real record count in the Web of Science (WoS). In some cases, the errors involved items omitted from the WoS. The authors also propose rates like the percentage of articles in journals of Spanish spheres, or the percentage of items as first author. These indexes have no theoretical or empirical justification or previous studies that support them.
23092414	Rethinking Responsible Literature Searching using LibGuides.
Med Ref Serv Q  2012
Drawing from the Responsible Literature Searching project developed by the Health Sciences Library System-University of Pittsburgh, the Mayo Clinic Libraries utilized Springshare's LibGuides software to create an Effective Database Searching Guide for its diverse set of users. Library databases are organized under broad subject categories with overview information, links to help materials, and news on system updates. Additionally, the guide features a visual site map, searching best practices, a database comparison chart, responsible literature searching guidelines, classic evidence-based practice articles, and recommendations on when to contact a librarian for assistance. The self-guided tool is both easy to use and maintain.
23092415	Literature searching in medical education: online tutorial development from idea to creation.
Med Ref Serv Q  2012
The medical education literature is growing, and the result is not only greater knowledge, but an increasing complexity in locating quality evidence-based information. In 2008, eight librarians partnered with the Association of American Medical Colleges to research, conceptualize, and build an online module to develop medical educators' search skills. Developing an online instructional module is a time-consuming, multi-stage process requiring the expertise of content, technical, and design specialists working in concert. Many lessons were learned, including the power of collaborative tools; the benefits of including specialists, such as graphic designers; the benefit of thoroughly surveying existing resources; and the importance of choosing technology wisely.
22535192	Integration of the Image-Guided Surgery Toolkit (IGSTK) into the Medical Imaging Interaction Toolkit (MITK).
J Digit Imaging  2012Dec
The development cycle of an image-guided surgery navigation system is too long to meet current clinical needs. This paper presents an integrated system developed by the integration of two open-source software (IGSTK and MITK) to shorten the development cycle of the image-guided surgery navigation system and save human resources simultaneously. An image-guided surgery navigation system was established by connecting the two aforementioned open-source software libraries. It used the Medical Imaging Interaction Toolkit (MITK) as a framework providing image processing tools for the image-guided surgery navigation system of medical imaging software with a high degree of interaction and used the Image-Guided Surgery Toolkit (IGSTK) as a library that provided the basic components of the system for location, tracking, and registration. The electromagnetic tracking device was used to measure the real-time position of surgical tools and fiducials attached to the patient's anatomy. IGSTK was integrated into MITK; at the same time, the compatibility and the stability of this system were emphasized. Experiments showed that an integrated system of the image-guided surgery navigation system could be developed in 2 months. The integration of IGSTK into MITK is feasible. Several techniques for 3D reconstruction, geometric analysis, mesh generation, and surface data analysis for medical image analysis of MITK can connect with the techniques for location, tracking, and registration of IGSTK. This integration of advanced modalities can decrease software development time and emphasize the precision, safety, and robustness of the image-guided surgery navigation system.
22766799	Workflow continuity--moving beyond business continuity in a multisite 24-7 healthcare organization.
J Digit Imaging  2012Dec
As hospitals move towards providing in-house 24 × 7 services, there is an increasing need for information systems to be available around the clock. This study investigates one organization's need for a workflow continuity solution that provides around the clock availability for information systems that do not provide highly available services. The organization investigated is a large multifacility healthcare organization that consists of 20 hospitals and more than 30 imaging centers. A case analysis approach was used to investigate the organization's efforts. The results show an overall reduction in downtimes where radiologists could not continue their normal workflow on the integrated Picture Archiving and Communications System (PACS) solution by 94 % from 2008 to 2011. The impact of unplanned downtimes was reduced by 72 % while the impact of planned downtimes was reduced by 99.66 % over the same period. Additionally more than 98 h of radiologist impact due to a PACS upgrade in 2008 was entirely eliminated in 2011 utilizing the system created by the workflow continuity approach. Workflow continuity differs from high availability and business continuity in its design process and available services. Workflow continuity only ensures that critical workflows are available when the production system is unavailable due to scheduled or unscheduled downtimes. Workflow continuity works in conjunction with business continuity and highly available system designs. The results of this investigation revealed that this approach can add significant value to organizations because impact on users is minimized if not eliminated entirely.
22752153	Health literacy programs for older adults: a systematic literature review.
Health Educ Res 20120629 2012Dec
Older adults make up the fastest growing age group in North America. This has demanded increased attention in supporting the health and well-being of this population and, in particular, the role of health information in promoting the health and well-being of older adults. Increased availability and accessibility of information as well as a greater emphasis on self-management and care have raised concern about an individual's health literacy skills. The purpose of this study is to conduct a systematic literature review using explicit systematic literature review methodology. This includes a detailed online search process of recent publications on programs that focus on health literacy in the older adult population using the Rychetnik et al. guiding questions and the Population Intervention Comparison Outcome framework. The search yielded nine articles describing functional (n = 4) and interactive (n = 5) health literacy programs. Overall, the selected articles demonstrated positive outcomes in supporting the health literacy skills of older adults. However, there are limitations in study designs and evaluation measures and outcomes of the programs remain unknown in demonstrating long-term impact in supporting health literacy skills. Further high quality studies with clear and strong research methodology are needed to develop and evaluate evidence-based interactive health literacy programs targeted specifically to older adults.
21654312	The utilization of oncology web-based resources in Spanish-speaking Internet users.
Am. J. Clin. Oncol.  2012Dec
There currently are few web-based resources written in Spanish providing oncology-specific information. This study examines utilization of Spanish-language oncology web-based resources and evaluates oncology-related Internet browsing practices of Spanish-speaking patients. OncoLink (http://www.oncolink.org) is the oldest and among the largest Internet-based cancer information resources. In September 2005, OncoLink pioneered OncoLink en español (OEE) (http://es.oncolink.org), a Spanish translation of OncoLink. Internet utilization data on these sites for 2006 to 2007 were compared. Visits to OncoLink rose from 4,440,843 in 2006 to 5,125,952 in 2007. OEE had 204,578 unique visitors and 240,442 visits in 2006, and 351,228 visitors and 412,153 visits in 2007. Although there was no time predilection for viewing OncoLink, less relative browsing on OEE was conducted during weekends and early morning hours. Although OncoLink readers searched for information on the most common cancers in the United States, OEE readers most often search for gastric, vaginal, osteosarcoma, leukemia, penile, cervical, and testicular malignancies. Average visit duration on OEE was shorter, and fewer readers surveyed OEE more than 15 minutes (4.5% vs. 14.9%, P &lt; 0.001). Spanish-speaking users of web-based oncology resources are increasingly using the Internet to supplement their cancer knowledge. Limited available resources written in Spanish contribute to disparities in information access and disease outcomes. Spanish-speaking oncology readers differ from English-speaking readers in day and time of Internet browsing, visit duration, Internet search patterns, and types of cancers searched. By acknowledging these differences, content of web-based oncology resources can be developed to best target the needs of Spanish-speaking viewers.
22683889	Identifying well-formed biomedical phrases in MEDLINE® text.
J Biomed Inform 20120608 2012Dec
In the modern world people frequently interact with retrieval systems to satisfy their information needs. Humanly understandable well-formed phrases represent a crucial interface between humans and the web, and the ability to index and search with such phrases is beneficial for human-web interactions. In this paper we consider the problem of identifying humanly understandable, well formed, and high quality biomedical phrases in MEDLINE documents. The main approaches used previously for detecting such phrases are syntactic, statistical, and a hybrid approach combining these two. In this paper we propose a supervised learning approach for identifying high quality phrases. First we obtain a set of known well-formed useful phrases from an existing source and label these phrases as positive. We then extract from MEDLINE a large set of multiword strings that do not contain stop words or punctuation. We believe this unlabeled set contains many well-formed phrases. Our goal is to identify these additional high quality phrases. We examine various feature combinations and several machine learning strategies designed to solve this problem. A proper choice of machine learning methods and features identifies in the large collection strings that are likely to be high quality phrases. We evaluate our approach by making human judgments on multiword strings extracted from MEDLINE using our methods. We find that over 85% of such extracted phrase candidates are humanly judged to be of high quality.
21784346	Building an integrated neurodegenerative disease database at an academic health center.
Alzheimers Dement  2011Jul
It is becoming increasingly important to study common and distinct etiologies, clinical and pathological features, and mechanisms related to neurodegenerative diseases such as Alzheimer's disease, Parkinson's disease, amyotrophic lateral sclerosis, and frontotemporal lobar degeneration. These comparative studies rely on powerful database tools to quickly generate data sets that match diverse and complementary criteria set by them. In this article, we present a novel integrated neurodegenerative disease (INDD) database, which was developed at the University of Pennsylvania (Penn) with the help of a consortium of Penn investigators. Because the work of these investigators are based on Alzheimer's disease, Parkinson's disease, amyotrophic lateral sclerosis, and frontotemporal lobar degeneration, it allowed us to achieve the goal of developing an INDD database for these major neurodegenerative disorders. We used the Microsoft SQL server as a platform, with built-in "backwards" functionality to provide Access as a frontend client to interface with the database. We used PHP Hypertext Preprocessor to create the "frontend" web interface and then used a master lookup table to integrate individual neurodegenerative disease databases. We also present methods of data entry, database security, database backups, and database audit trails for this INDD database. Using the INDD database, we compared the results of a biomarker study with those using an alternative approach by querying individual databases separately. We have demonstrated that the Penn INDD database has the ability to query multiple database tables from a single console with high accuracy and reliability. The INDD database provides a powerful tool for generating data sets in comparative studies on several neurodegenerative diseases.
24002849	MedEdPORTAL: a report on oral health resources for health professions educators.
J Dent Educ  2013Sep
MedEdPORTAL is a unique web-based peer-reviewed publication venue for clinical health educators sponsored by the Association of American Medical Colleges (AAMC). The open exchange of educational resources promotes professional collaboration across health professions. In 2008, the American Dental Education Association (ADEA) collaborated with AAMC to allow dental educators to use the platform to publish dental curriculum resources. Oral health is integral to general health; hence, collaboration among health care professionals brings enormous value to patient-centered care. The aim of this study was to conduct a current survey of metrics and submission statistics of MedEdPORTAL resources. The data were collected using the MedEdPORTAL search engine and ADEA and AAMC staff. The data collected were categorized and reported in tables and charts. Results showed that at the time of this study there were over 2,000 medical and dental resources available to anyone worldwide. Oral health resources constituted approximately 30 percent of the total resources, which included cross-indexing with information relevant to both medical and dental audiences. There were several types of dental resources available; the most common were the ones focusing on critical thinking. The usage of MedEdPORTAL has been growing, with participation from over 190 countries and 10,000 educational institutions around the world. The findings of this report suggest that MedEdPORTAL is succeeding in its aim to foster global collaborative education, professional education, and educational scholarship. As such, MedEdPORTAL is providing a new forum for collaboration and opens venues for promising future work in professional education.
24151795	Oral health-related quality of life in non-syndromic cleft lip and/or palate patients: a systematic review.
Community Dent Health  2013Sep
To evaluate oral health-related quality of life (OHRQoL) in non-syndromic patients with cleft lip and/or palate (CLP), in comparison to a general non-cleft population. Systematic review. A literature search was conducted to identify papers reporting on OHRQoL in cleft samples. Only studies with suitable control groups were included. From each included paper were extracted the study and sample characteristics and results. OHRQoL score. Three papers were chosen according to the preset inclusion and exclusion criteria. All used an OHRQoL generic patient-reported questionnaire with evidence of a development and validation process, with responses recorded on a five-point scale. The results could not be combined for the purposes of meta-analysis due to lack of standardisation. In 2 of the 3 studies, the OHRQoL was found to be significantly lower in the cleft than in the non-cleft samples (in patients 8-18 or 18-65 years of age). The third study, based on a relatively small sample size, could not detect significant differences between cleft and non-cleft individuals. Based on the results of the few studies included in the present systematic review, non-syndromic patients with CLP tend to have a lower OHRQoL than a general non-cleft population. This seems to hold true both for children and adults.
23767791	Boolean network-based model of the Bcl-2 family mediated MOMP regulation.
Theor Biol Med Model 20130614 2013
Mitochondrial outer membrane permeabilization (MOMP) is one of the most important points in the majority of apoptotic signaling cascades and it is controlled by a network of interactions between the members of the Bcl-2 family. To understand the role of individual members of this family within the MOMP regulation, we have constructed a Boolean network-based model of interactions between the Bcl-2 proteins. Computational simulations have revealed the existence of trapping states which, independently from the incoming stimuli, block the occurrence of MOMP. Our results emphasize the role of the antiapoptotic protein Mcl-1 in the majority of these configurations. We demonstrate here the importance of the Bid and Bim for activation of effectors Bax and Bak, and the irreversibility of this activation. The model further points to the antiapoptotic protein Bcl-w as a key factor preventing Bax activation. In spite of relative simplicity, the Boolean network-based model provides useful insight into main functioning logic of the Bcl-2 switch, consistent with experimental findings.
23894185	Rapid storage and retrieval of genomic intervals from a relational database system using nested containment lists.
Database (Oxford) 20130726 2013
Efficient storage and retrieval of genomic annotations based on range intervals is necessary, given the amount of data produced by next-generation sequencing studies. The indexing strategies of relational database systems (such as MySQL) greatly inhibit their use in genomic annotation tasks. This has led to the development of stand-alone applications that are dependent on flat-file libraries. In this work, we introduce MyNCList, an implementation of the NCList data structure within a MySQL database. MyNCList enables the storage, update and rapid retrieval of genomic annotations from the convenience of a relational database system. Range-based annotations of 1 million variants are retrieved in under a minute, making this approach feasible for whole-genome annotation tasks. Database URL: https://github.com/bushlab/mynclist.
23731545	Using a cloud-based electronic health record during disaster response: a case study in Fukushima, March 2011.
Prehosp Disaster Med 20130426 2013Aug
Following the Great East Japan Earthquake on March 11, 2011, the Japan Medical Association deployed medical disaster teams to Shinchi-town (population: approximately 8,000), which is located 50 km north of the Fukushima Daiichi nuclear power plant. The mission of the medical disaster teams sent from Fukuoka, 1,400 km south of Fukushima, was to provide medical services and staff a temporary clinic for six weeks. Fear of radiation exposure restricted the use of large medical teams and local infrastructure. Therefore, small volunteer groups and a cloud-hosted, web-based electronic health record were implemented. The mission was successfully completed by the end of May 2011. Cloud-based electronic health records deployed using a "software as a service" model worked well during the response to the large-scale disaster.
23902523	PoPLAR: Portal for Petascale Lifescience Applications and Research.
BMC Bioinformatics 20130628 2013
We are focusing specifically on fast data analysis and retrieval in bioinformatics that will have a direct impact on the quality of human health and the environment. The exponential growth of data generated in biology research, from small atoms to big ecosystems, necessitates an increasingly large computational component to perform analyses. Novel DNA sequencing technologies and complementary high-throughput approaches--such as proteomics, genomics, metabolomics, and meta-genomics--drive data-intensive bioinformatics. While individual research centers or universities could once provide for these applications, this is no longer the case. Today, only specialized national centers can deliver the level of computing resources required to meet the challenges posed by rapid data growth and the resulting computational demand. Consequently, we are developing massively parallel applications to analyze the growing flood of biological data and contribute to the rapid discovery of novel knowledge. The efforts of previous National Science Foundation (NSF) projects provided for the generation of parallel modules for widely used bioinformatics applications on the Kraken supercomputer. We have profiled and optimized the code of some of the scientific community's most widely used desktop and small-cluster-based applications, including BLAST from the National Center for Biotechnology Information (NCBI), HMMER, and MUSCLE; scaled them to tens of thousands of cores on high-performance computing (HPC) architectures; made them robust and portable to next-generation architectures; and incorporated these parallel applications in science gateways with a web-based portal. This paper will discuss the various developmental stages, challenges, and solutions involved in taking bioinformatics applications from the desktop to petascale with a front-end portal for very-large-scale data analysis in the life sciences. This research will help to bridge the gap between the rate of data generation and the speed at which scientists can study this data. The ability to rapidly analyze data at such a large scale is having a significant, direct impact on science achieved by collaborators who are currently using these tools on supercomputers.
23923239	Understanding nursing units with data and theory.
Nurs Econ  2013 May-Jun
Nursing units are social systems whose function depends on many variables. Available nursing data, combined with a theory of organizational diagnosis, can be used to understand nursing unit performance. One troubled unit served as a case study in organizational diagnosis and treatment using modern methods of data mining and performance improvement. Systems theory did not prescribe how to fix an underbounded system. The theory did suggest, however, that addressing the characteristics of overbounded and underbounded systems can provide some order and structure and identify helpful resources. In this instance, the data analysis served to help define the unit's problems in conjunction with information gained from talking with the nurses and touring the unit, but it was the theory that gave hints for direction for change.
23895370	Confero: an integrated contrast data and gene set platform for computational analysis and biological interpretation of omics data.
BMC Genomics 20130729 2013
High-throughput omics technologies such as microarrays and next-generation sequencing (NGS) have become indispensable tools in biological research. Computational analysis and biological interpretation of omics data can pose significant challenges due to a number of factors, in particular the systems integration required to fully exploit and compare data from different studies and/or technology platforms. In transcriptomics, the identification of differentially expressed genes when studying effect(s) or contrast(s) of interest constitutes the starting point for further downstream computational analysis (e.g. gene over-representation/enrichment analysis, reverse engineering) leading to mechanistic insights. Therefore, it is important to systematically store the full list of genes with their associated statistical analysis results (differential expression, t-statistics, p-value) corresponding to one or more effect(s) or contrast(s) of interest (shortly termed as " contrast data") in a comparable manner and extract gene sets in order to efficiently support downstream analyses and further leverage data on a long-term basis. Filling this gap would open new research perspectives for biologists to discover disease-related biomarkers and to support the understanding of molecular mechanisms underlying specific biological perturbation effects (e.g. disease, genetic, environmental, etc.). To address these challenges, we developed Confero, a contrast data and gene set platform for downstream analysis and biological interpretation of omics data. The Confero software platform provides storage of contrast data in a simple and standard format, data transformation to enable cross-study and platform data comparison, and automatic extraction and storage of gene sets to build new a priori knowledge which is leveraged by integrated and extensible downstream computational analysis tools. Gene Set Enrichment Analysis (GSEA) and Over-Representation Analysis (ORA) are currently integrated as an analysis module as well as additional tools to support biological interpretation. Confero is a standalone system that also integrates with Galaxy, an open-source workflow management and data integration system. To illustrate Confero platform functionality we walk through major aspects of the Confero workflow and results using the Bioconductor estrogen package dataset. Confero provides a unique and flexible platform to support downstream computational analysis facilitating biological interpretation. The system has been designed in order to provide the researcher with a simple, innovative, and extensible solution to store and exploit analyzed data in a sustainable and reproducible manner thereby accelerating knowledge-driven research. Confero source code is freely available from http://sourceforge.net/projects/confero/.
23962139	Licensing the future: report on BioMed Central's public consultation on open data in peer-reviewed journals.
BMC Res Notes 20130821 2013
We report the outcomes of BioMed Central's public consultation on implementing open data-compliant licensing in peer-reviewed open access journals. Respondents (42) to the 2012 consultation were six to one in favor (29 in support; 5 against; 8 abstentions) of changing our authors' default open access copyright license agreement, to introduce the Creative Commons CC0 public domain waiver for data published in BioMed Central's journals. We summarize the different questions we received in response to the consultation and our responses to them - matters such as citation, plagiarism, patient privacy, and commercial use were raised. In light of the support for open data in our journals we outline our plans to implement, in September 2013, a combined Creative Commons Attribution license for published articles (papers) and Creative Commons CC0 waiver for published data.
23818831	A fragile zero watermarking scheme to detect and characterize malicious modifications in database relations.
ScientificWorldJournal 20130602 2013
We put forward a fragile zero watermarking scheme to detect and characterize malicious modifications made to a database relation. Most of the existing watermarking schemes for relational databases introduce intentional errors or permanent distortions as marks into the database original content. These distortions inevitably degrade the data quality and data usability as the integrity of a relational database is violated. Moreover, these fragile schemes can detect malicious data modifications but do not characterize the tempering attack, that is, the nature of tempering. The proposed fragile scheme is based on zero watermarking approach to detect malicious modifications made to a database relation. In zero watermarking, the watermark is generated (constructed) from the contents of the original data rather than introduction of permanent distortions as marks into the data. As a result, the proposed scheme is distortion-free; thus, it also resolves the inherent conflict between security and imperceptibility. The proposed scheme also characterizes the malicious data modifications to quantify the nature of tempering attacks. Experimental results show that even minor malicious modifications made to a database relation can be detected and characterized successfully.
23554455	Public accessibility of biomedical articles from PubMed Central reduces journal readership--retrospective cohort analysis.
FASEB J. 20130403 2013Jul
Does PubMed Central--a government-run digital archive of biomedical articles--compete with scientific society journals? A longitudinal, retrospective cohort analysis of 13,223 articles (5999 treatment, 7224 control) published in 14 society-run biomedical research journals in nutrition, experimental biology, physiology, and radiology between February 2008 and January 2011 reveals a 21.4% reduction in full-text hypertext markup language (HTML) article downloads and a 13.8% reduction in portable document format (PDF) article downloads from the journals' websites when U.S. National Institutes of Health-sponsored articles (treatment) become freely available from the PubMed Central repository. In addition, the effect of PubMed Central on reducing PDF article downloads is increasing over time, growing at a rate of 1.6% per year. There was no longitudinal effect for full-text HTML downloads. While PubMed Central may be providing complementary access to readers traditionally underserved by scientific journals, the loss of article readership from the journal website may weaken the ability of the journal to build communities of interest around research papers, impede the communication of news and events to scientific society members and journal readers, and reduce the perceived value of the journal to institutional subscribers.
23819658	OvidSP Medline-to-PubMed search filter translation: a methodology for extending search filter range to include PubMed's unique content.
BMC Med Res Methodol 20130702 2013
PubMed translations of OvidSP Medline search filters offer searchers improved ease of access. They may also facilitate access to PubMed's unique content, including citations for the most recently published biomedical evidence. Retrieving this content requires a search strategy comprising natural language terms ('textwords'), rather than Medical Subject Headings (MeSH). We describe a reproducible methodology that uses a validated PubMed search filter translation to create a textword-only strategy to extend retrieval to PubMed's unique heart failure literature. We translated an OvidSP Medline heart failure search filter for PubMed and established version equivalence in terms of indexed literature retrieval. The PubMed version was then run within PubMed to identify citations retrieved by the filter's MeSH terms (Heart failure, Left ventricular dysfunction, and Cardiomyopathy). It was then rerun with the same MeSH terms restricted to searching on title and abstract fields (i.e. as 'textwords'). Citations retrieved by the MeSH search but not the textword search were isolated. Frequency analysis of their titles/abstracts identified natural language alternatives for those MeSH terms that performed less effectively as textwords. These terms were tested in combination to determine the best performing search string for reclaiming this 'lost set'. This string, restricted to searching on PubMed's unique content, was then combined with the validated PubMed translation to extend the filter's performance in this database. The PubMed heart failure filter retrieved 6829 citations. Of these, 834 (12%) failed to be retrieved when MeSH terms were converted to textwords. Frequency analysis of the 834 citations identified five high frequency natural language alternatives that could improve retrieval of this set (cardiac failure, cardiac resynchronization, left ventricular systolic dysfunction, left ventricular diastolic dysfunction, and LV dysfunction). Together these terms reclaimed 157/834 (18.8%) of lost citations. MeSH terms facilitate precise searching in PubMed's indexed subset. They may, however, work less effectively as search terms prior to subject indexing. A validated PubMed search filter can be used to develop a supplementary textword-only search strategy to extend retrieval to PubMed's unique content. A PubMed heart failure search filter is available on the CareSearch website (http://www.caresearch.com.au) providing access to both indexed and non-indexed heart failure evidence.
23844390	Efficient and accurate optimal linear phase FIR filter design using opposition-based harmony search algorithm.
ScientificWorldJournal 20130610 2013
In this paper, opposition-based harmony search has been applied for the optimal design of linear phase FIR filters. RGA, PSO, and DE have also been adopted for the sake of comparison. The original harmony search algorithm is chosen as the parent one, and opposition-based approach is applied. During the initialization, randomly generated population of solutions is chosen, opposite solutions are also considered, and the fitter one is selected as a priori guess. In harmony memory, each such solution passes through memory consideration rule, pitch adjustment rule, and then opposition-based reinitialization generation jumping, which gives the optimum result corresponding to the least error fitness in multidimensional search space of FIR filter design. Incorporation of different control parameters in the basic HS algorithm results in the balancing of exploration and exploitation of search space. Low pass, high pass, band pass, and band stop FIR filters are designed with the proposed OHS and other aforementioned algorithms individually for comparative optimization performance. A comparison of simulation results reveals the optimization efficacy of the OHS over the other optimization techniques for the solution of the multimodal, nondifferentiable, nonlinear, and constrained FIR filter design problems.
23844397	An anonymous user authentication with key agreement scheme without pairings for multiserver architecture using SCPKs.
ScientificWorldJournal 20130609 2013
With advancement of computer community and widespread dissemination of network applications, users generally need multiple servers to provide different services. Accordingly, the multiserver architecture has been prevalent, and designing a secure and efficient remote user authentication under multiserver architecture becomes a nontrivial challenge. In last decade, various remote user authentication protocols have been put forward to correspond to the multi-server scenario requirements. However, these schemes suffered from certain security problems or their cost consumption exceeded users' own constrained ability. In this paper, we present an anonymous remote user authentication with key agreement scheme for multi-server architecture employing self-certified public keys without pairings. The proposed scheme can not only retain previous schemes' advantages but also achieve user privacy concern. Moreover, our proposal can gain higher efficiency by removing the pairings operation compared with the related schemes. Through analysis and comparison with the related schemes, we can say that our proposal is in accordance with the scenario requirements and feasible to the multi-server architecture.
23847913	[Information retrieval and reading routines of young doctors].
Duodecim  2013
We performed a survey on information management and reading routines in a random sample of Finnish doctors graduated during the last 2-10 years. The mean time spent on reading medical data sources and literature was three hours per week. The most appreciated sources of information were Current Care and other guidelines written in Finnish, especially among female doctors. The most important problem the doctors encountered was lack of time. Even though a physician who works as an expert needs continuous following of scientific literature the present medical education does not give sufficient expertise to use electronic data sources and international medical literature to solve problems faced with the patients.
23858846	A four-bit-per-cell program method with substrate-bias assisted hot electron injection for charge trap flash memory devices.
J Nanosci Nanotechnol  2013May
We propose a four-bit-per-cell program method using a two-step sequence with substrate-bias assisted hot electron (SAHE) injection into the charge trap flash memory devices in order to overcome the limitations of conventional four-bit program methods, which use channel hot electron (CHE) injection. With this proposed method, a localized charge injection near the junction edge with an acceptable read margin was clearly observed, along with a threshold voltage difference of 1 V between the forward and the reverse read. In addition, a multi-level storage was easily obtained using a drain voltage step of 1 V at each level of the three programmed states, along with a fast program time of 1 micros. Finally, by using charge pumping methods, we directly observed the detailed information on the spatial distribution of the local threshold voltage in each level of the four states, for each physical bit, as a function of the program voltage.
23055170	Seriousness checks are useful to improve data validity in online research.
Behav Res Methods  2013Jun
Nonserious answering behavior increases noise and reduces experimental power; it is therefore one of the most important threats to the validity of online research. A simple way to address the problem is to ask respondents about the seriousness of their participation and to exclude self-declared nonserious participants from analysis. To validate this approach, a survey was conducted in the week prior to the German 2009 federal election to the Bundestag. Serious participants answered a number of attitudinal and behavioral questions in a more consistent and predictively valid manner than did nonserious participants. We therefore recommend routinely employing seriousness checks in online surveys to improve data validity.
23707966	Cataloging the biomedical world of pain through semi-automated curation of molecular interactions.
Database (Oxford) 20130523 2013
The vast collection of biomedical literature and its continued expansion has presented a number of challenges to researchers who require structured findings to stay abreast of and analyze molecular mechanisms relevant to their domain of interest. By structuring literature content into topic-specific machine-readable databases, the aggregate data from multiple articles can be used to infer trends that can be compared and contrasted with similar findings from topic-independent resources. Our study presents a generalized procedure for semi-automatically creating a custom topic-specific molecular interaction database through the use of text mining to assist manual curation. We apply the procedure to capture molecular events that underlie 'pain', a complex phenomenon with a large societal burden and unmet medical need. We describe how existing text mining solutions are used to build a pain-specific corpus, extract molecular events from it, add context to the extracted events and assess their relevance. The pain-specific corpus contains 765 692 documents from Medline and PubMed Central, from which we extracted 356 499 unique normalized molecular events, with 261 438 single protein events and 93 271 molecular interactions supplied by BioContext. Event chains are annotated with negation, speculation, anatomy, Gene Ontology terms, mutations, pain and disease relevance, which collectively provide detailed insight into how that event chain is associated with pain. The extracted relations are visualized in a wiki platform (wiki-pain.org) that enables efficient manual curation and exploration of the molecular mechanisms that underlie pain. Curation of 1500 grouped event chains ranked by pain relevance revealed 613 accurately extracted unique molecular interactions that in the future can be used to study the underlying mechanisms involved in pain. Our approach demonstrates that combining existing text mining tools with domain-specific terms and wiki-based visualization can facilitate rapid curation of molecular interactions to create a custom database. Database URL: •••
23710147	An efficient and secure certificateless authentication protocol for healthcare system on wireless medical sensor networks.
ScientificWorldJournal 20130421 2013
Sensor networks have opened up new opportunities in healthcare systems, which can transmit patient's condition to health professional's hand-held devices in time. The patient's physiological signals are very sensitive and the networks are extremely vulnerable to many attacks. It must be ensured that patient's privacy is not exposed to unauthorized entities. Therefore, the control of access to healthcare systems has become a crucial challenge. An efficient and secure authentication protocol will thus be needed in wireless medical sensor networks. In this paper, we propose a certificateless authentication scheme without bilinear pairing while providing patient anonymity. Compared with other related protocols, the proposed scheme needs less computation and communication cost and preserves stronger security. Our performance evaluations show that this protocol is more practical for healthcare system in wireless medical sensor networks.
22732681	PoseShop: human image database construction and personalized content synthesis.
IEEE Trans Vis Comput Graph  2013May
We present PoseShop--a pipeline to construct segmented human image database with minimal manual intervention. By downloading, analyzing, and filtering massive amounts of human images from the Internet, we achieve a database which contains 400 thousands human figures that are segmented out of their background. The human figures are organized based on action semantic, clothes attributes, and indexed by the shape of their poses. They can be queried using either silhouette sketch or a skeleton to find a given pose. We demonstrate applications for this database for multiframe personalized content synthesis in the form of comic-strips, where the main character is the user or his/her friends. We address the two challenges of such synthesis, namely personalization and consistency over a set of frames, by introducing head swapping and clothes swapping techniques. We also demonstrate an action correlation analysis application to show the usefulness of the database for vision application.
23731937	How to perform a systematic search.
Best Pract Res Clin Rheumatol  2013Apr
All medical practice and research must be evidence-based, as far as this is possible. With medical knowledge constantly growing, it has become necessary to possess a high level of information literacy to stay competent and professional. Furthermore, as patients can now search information on the Internet, clinicians must be able to respond to this type of information in a professional way, when needed. Here, the development of viable systematic search strategies for journal articles, books, book chapters and other sources, selection of appropriate databases, search tools and selection methods are described and illustrated with examples from rheumatology. The up-keep of skills over time, and the acquisition of localised information sources, are discussed.
22641700	Automatic caption generation for news images.
IEEE Trans Pattern Anal Mach Intell  2013Apr
This paper is concerned with the task of automatically generating captions for images, which is important for many image-related applications. Examples include video and image retrieval as well as the development of tools that aid visually impaired individuals to access pictorial information. Our approach leverages the vast resource of pictures available on the web and the fact that many of them are captioned and colocated with thematically related documents. Our model learns to create captions from a database of news articles, the pictures embedded in them, and their captions, and consists of two stages. Content selection identifies what the image and accompanying article are about, whereas surface realization determines how to verbalize the chosen content. We approximate content selection with a probabilistic image annotation model that suggests keywords for an image. The model postulates that images and their textual descriptions are generated by a shared set of latent variables (topics) and is trained on a weakly labeled dataset (which treats the captions and associated news articles as image labels). Inspired by recent work in summarization, we propose extractive and abstractive surface realization models. Experimental results show that it is viable to generate captions that are pertinent to the specific content of an image and its associated article, while permitting creativity in the description. Indeed, the output of our abstractive model compares favorably to handwritten captions and is often superior to extractive methods.
23516309	International validation study for interim PET in ABVD-treated, advanced-stage hodgkin lymphoma: interpretation criteria and concordance rate among reviewers.
J. Nucl. Med. 20130320 2013May
At present, there are no standard criteria that have been validated for interim PET reporting in lymphoma. In 2009, an international workshop attended by hematologists and nuclear medicine experts in Deauville, France, proposed to develop simple and reproducible rules for interim PET reporting in lymphoma. Accordingly, an international validation study was undertaken with the primary aim of validating the prognostic role of interim PET using the Deauville 5-point score to evaluate images and with the secondary aim of measuring concordance rates among reviewers using the same 5-point score. This paper focuses on the criteria for interpretation of interim PET and on concordance rates. A cohort of advanced-stage Hodgkin lymphoma patients treated with doxorubicin, bleomycin, vinblastine, and dacarbazine (ABVD) were enrolled retrospectively from centers worldwide. Baseline and interim scans were reviewed by an international panel of 6 nuclear medicine experts using the 5-point score. Complete scan datasets of acceptable diagnostic quality were available for 260 of 440 (59%) enrolled patients. Independent agreement among reviewers was reached on 252 of 260 patients (97%), for whom at least 4 reviewers agreed the findings were negative (score of 1-3) or positive (score of 4-5). After discussion, consensus was reached in all cases. There were 45 of 260 patients (17%) with positive interim PET findings and 215 of 260 patients (83%) with negative interim PET findings. Thirty-three interim PET-positive scans were true-positive, and 12 were false-positive. Two hundred three interim PET-negative scans were true-negative, and 12 were false-negative. Sensitivity, specificity, and accuracy were 0.73, 0.94, and 0.91, respectively. Negative predictive value and positive predictive value were 0.94 and 0.73, respectively. The 3-y failure-free survival was 83%, 28%, and 95% for the entire population and for interim PET-positive and -negative patients, respectively (P &lt; 0.0001). The agreement between pairs of reviewers was good or very good, ranging from 0.69 to 0.84 as measured with the Cohen kappa. Overall agreement was good at 0.76 as measured with the Krippendorf α. The 5-point score proposed at Deauville for reviewing interim PET scans in advanced Hodgkin lymphoma is accurate and reproducible enough to be accepted as a standard reporting criterion in clinical practice and for clinical trials.
23766690	A Cyber-ITS framework for massive traffic data analysis using cyber infrastructure.
ScientificWorldJournal 20130516 2013
Traffic data is commonly collected from widely deployed sensors in urban areas. This brings up a new research topic, data-driven intelligent transportation systems (ITSs), which means to integrate heterogeneous traffic data from different kinds of sensors and apply it for ITS applications. This research, taking into consideration the significant increase in the amount of traffic data and the complexity of data analysis, focuses mainly on the challenge of solving data-intensive and computation-intensive problems. As a solution to the problems, this paper proposes a Cyber-ITS framework to perform data analysis on Cyber Infrastructure (CI), by nature parallel-computing hardware and software systems, in the context of ITS. The techniques of the framework include data representation, domain decomposition, resource allocation, and parallel processing. All these techniques are based on data-driven and application-oriented models and are organized as a component-and-workflow-based model in order to achieve technical interoperability and data reusability. A case study of the Cyber-ITS framework is presented later based on a traffic state estimation application that uses the fusion of massive Sydney Coordinated Adaptive Traffic System (SCATS) data and GPS data. The results prove that the Cyber-ITS-based implementation can achieve a high accuracy rate of traffic state estimation and provide a significant computational speedup for the data fusion by parallel computing.
23766725	A multihop key agreement scheme for wireless ad hoc networks based on channel characteristics.
ScientificWorldJournal 20130520 2013
A number of key agreement schemes based on wireless channel characteristics have been proposed recently. However, previous key agreement schemes require that two nodes which need to agree on a key are within the communication range of each other. Hence, they are not suitable for multihop wireless networks, in which nodes do not always have direct connections with each other. In this paper, we first propose a basic multihop key agreement scheme for wireless ad hoc networks. The proposed basic scheme is resistant to external eavesdroppers. Nevertheless, this basic scheme is not secure when there exist internal eavesdroppers or Man-in-the-Middle (MITM) adversaries. In order to cope with these adversaries, we propose an improved multihop key agreement scheme. We show that the improved scheme is secure against internal eavesdroppers and MITM adversaries in a single path. Both performance analysis and simulation results demonstrate that the improved scheme is efficient. Consequently, the improved key agreement scheme is suitable for multihop wireless ad hoc networks.
23419611	Simple search techniques in PubMed are potentially suitable for evaluating the completeness of systematic reviews.
J Clin Epidemiol 20130215 2013Jun
The Institute for Quality and Efficiency in Health Care (IQWiG) assesses the added benefit of new drugs by means of company dossiers. The pharmaceutical company performs the information retrieval, which is then assessed by IQWiG. Our aim was to determine whether PubMed's Related Citations (RelCits) and/or a simple-structured Boolean search (SSBS) are efficient and reliable search techniques to assess the completeness of an evidence base consisting of published randomized controlled trials (RCTs). Retrospective analysis of citations included as relevant in systematic reviews (SRs) of drugs. The proportion of relevant citations identified by the above-mentioned search techniques was determined. Relative sensitivity, precision, and the number needed to read (NNR) were then calculated. A total of 19 SRs included 330 relevant PubMed citations. The single techniques yielded either insufficient completeness, reliability, or efficiency. The first 20 RelCits plus SSBS achieved high completeness and reliability (sensitivity: 98.1%, range: 80-100%) and sufficient efficiency (precision: 5.0%, NNR: 25). The first 50 RelCit plus SSBS achieved slightly better completeness and reliability, but slightly worse efficiency. Combining the first 20 RelCits and an SSBS in PubMed is a suitable method to assess the completeness of an evidence base of published RCTs.
23462202	Methods for presenting and visualising electrocardiographic data: From temporal signals to spatial imaging.
J Electrocardiol 20130222 2013 May-Jun
The electrocardiogram (ECG) is a recording of the electrical activity of the heart. It is commonly used to non-invasively assess the cardiac activity of a patient. Since 1938, ECG data has been visualised as 12 scalar traces (known as the standard 12-lead ECG). Although this is known as the standard approach, there has been a myriad of alternative methods proposed to visualise ECG data. The purpose of this paper is to provide an overview of these methods and to introduce the field of ECG visualisation to early stage researchers. A scientific purpose is to consider the future of ECG visualisation within routine clinical practice. This paper structures the different ECG visualisation methods using four categories, i.e. temporal, vectorial, spatial and interactive. Temporal methods present the data with respect to time, vectorial methods present data with respect to direction and magnitude, spatial methods present data in 2D or 3D space and interactive methods utilise interactive computing to facilitate efficient interrogation of ECG data at different levels of detail. Spatial visualisation has been around since its introduction by Waller and vector based visualisation has been around since the 1920s. Given these approaches have already been given the 'test of time', they are unlikely to be replaced as the standard in the near future. Instead of being replaced, the standard is more likely to be 'supplemented'. However, the design and presentation of these ECG visualisation supplements need to be universally standardised. Subsequent to the development of 'standardised supplements', as a requirement, they could then be integrated into all ECG machines. We recognise that without intuitive software and interactivity on mobile devices (e.g. tablet PCs), it is impractical to integrate the more advanced ECG visualisation methods into routine practice (i.e. epicardial mapping using an inverse solution).
23630576	CMS: a web-based system for visualization and analysis of genome-wide methylation data of human cancers.
PLoS ONE 20130422 2013
DNA methylation of promoter CpG islands is associated with gene suppression, and its unique genome-wide profiles have been linked to tumor progression. Coupled with high-throughput sequencing technologies, it can now efficiently determine genome-wide methylation profiles in cancer cells. Also, experimental and computational technologies make it possible to find the functional relationship between cancer-specific methylation patterns and their clinicopathological parameters. Cancer methylome system (CMS) is a web-based database application designed for the visualization, comparison and statistical analysis of human cancer-specific DNA methylation. Methylation intensities were obtained from MBDCap-sequencing, pre-processed and stored in the database. 191 patient samples (169 tumor and 22 normal specimen) and 41 breast cancer cell-lines are deposited in the database, comprising about 6.6 billion uniquely mapped sequence reads. This provides comprehensive and genome-wide epigenetic portraits of human breast cancer and endometrial cancer to date. Two views are proposed for users to better understand methylation structure at the genomic level or systemic methylation alteration at the gene level. In addition, a variety of annotation tracks are provided to cover genomic information. CMS includes important analytic functions for interpretation of methylation data, such as the detection of differentially methylated regions, statistical calculation of global methylation intensities, multiple gene sets of biologically significant categories, interactivity with UCSC via custom-track data. We also present examples of discoveries utilizing the framework. CMS provides visualization and analytic functions for cancer methylome datasets. A comprehensive collection of datasets, a variety of embedded analytic functions and extensive applications with biological and translational significance make this system powerful and unique in cancer methylation research. CMS is freely accessible at: http://cbbiweb.uthscsa.edu/KMethylomes/.
23547764	MGcV: the microbial genomic context viewer for comparative genome analysis.
BMC Genomics 20130401 2013
Conserved gene context is used in many types of comparative genome analyses. It is used to provide leads on gene function, to guide the discovery of regulatory sequences, but also to aid in the reconstruction of metabolic networks. We present the Microbial Genomic context Viewer (MGcV), an interactive, web-based application tailored to strengthen the practice of manual comparative genome context analysis for bacteria. MGcV is a versatile, easy-to-use tool that renders a visualization of the genomic context of any set of selected genes, genes within a phylogenetic tree, genomic segments, or regulatory elements. It is tailored to facilitate laborious tasks such as the interactive annotation of gene function, the discovery of regulatory elements, or the sequence-based reconstruction of gene regulatory networks. We illustrate that MGcV can be used in gene function annotation by visually integrating information on prokaryotic genes, like their annotation as available from NCBI with other annotation data such as Pfam domains, sub-cellular location predictions and gene-sequence characteristics such as GC content. We also illustrate the usefulness of the interactive features that allow the graphical selection of genes to facilitate data gathering (e.g. upstream regions, ID's or annotation), in the analysis and reconstruction of transcription regulation. Moreover, putative regulatory elements and their corresponding scores or data from RNA-seq and microarray experiments can be uploaded, visualized and interpreted in (ranked-) comparative context maps. The ranked maps allow the interpretation of predicted regulatory elements and experimental data in light of each other. MGcV advances the manual comparative analysis of genes and regulatory elements by providing fast and flexible integration of gene related data combined with straightforward data retrieval. MGcV is available at http://mgcv.cmbi.ru.nl.
23601912	Development and initial evaluation of a treatment decision dashboard.
BMC Med Inform Decis Mak 20130421 2013
For many healthcare decisions, multiple alternatives are available with different combinations of advantages and disadvantages across several important dimensions. The complexity of current healthcare decisions thus presents a significant barrier to informed decision making, a key element of patient-centered care.Interactive decision dashboards were developed to facilitate decision making in Management, a field marked by similarly complicated choices. These dashboards utilize data visualization techniques to reduce the cognitive effort needed to evaluate decision alternatives and a non-linear flow of information that enables users to review information in a self-directed fashion. Theoretically, both of these features should facilitate informed decision making by increasing user engagement with and understanding of the decision at hand. We sought to determine if the interactive decision dashboard format can be successfully adapted to create a clinically realistic prototype patient decision aid suitable for further evaluation and refinement. We created a computerized, interactive clinical decision dashboard and performed a pilot test of its clinical feasibility and acceptability using a multi-method analysis. The dashboard summarized information about the effectiveness, risks of side effects and drug-drug interactions, out-of-pocket costs, and ease of use of nine analgesic treatment options for knee osteoarthritis. Outcome evaluations included observations of how study participants utilized the dashboard, questionnaires to assess usability, acceptability, and decisional conflict, and an open-ended qualitative analysis. The study sample consisted of 25 volunteers - 7 men and 18 women - with an average age of 51 years. The mean time spent interacting with the dashboard was 4.6 minutes. Mean evaluation scores on scales ranging from 1 (low) to 7 (high) were: mechanical ease of use 6.1, cognitive ease of use 6.2, emotional difficulty 2.7, decision-aiding effectiveness 5.9, clarification of values 6.5, reduction in decisional uncertainty 6.1, and provision of decision-related information 6.0. Qualitative findings were similarly positive. Interactive decision dashboards can be adapted for clinical use and have the potential to foster informed decision making. Additional research is warranted to more rigorously test the effectiveness and efficiency of patient decision dashboards for supporting informed decision making and other aspects of patient-centered care, including shared decision making.
23247259	Towards virtual knowledge broker services for semantic integration of life science literature and data sources.
Drug Discov. Today 20121212 2013May
Research in the life sciences requires ready access to primary data, derived information and relevant knowledge from a multitude of sources. Integration and interoperability of such resources are crucial for sharing content across research domains relevant to the life sciences. In this article we present a perspective review of data integration with emphasis on a semantics driven approach to data integration that pushes content into a shared infrastructure, reduces data redundancy and clarifies any inconsistencies. This enables much improved access to life science data from numerous primary sources. The Semantic Enrichment of the Scientific Literature (SESL) pilot project demonstrates feasibility for using already available open semantic web standards and technologies to integrate public and proprietary data resources, which span structured and unstructured content. This has been accomplished through a precompetitive consortium, which provides a cost effective approach for numerous stakeholders to work together to solve common problems.
23643018	Oral and dental imaging equipment and techniques for small animals.
Vet. Clin. North Am. Small Anim. Pract. 20130329 2013May
In the diagnosis and treatment of oral and dental diseases in dogs and cats, digital intraoral radiography offers many advantages over the use of standard dental radiographic film, including rapid image generation, easier exposure correction, enhancement, and paperless storage. Digital image receptors can be divided into 2 main types, direct digital systems using charged coupled devices and complementary metal oxide semiconductor sensors, and indirect digital systems using phosphor plates with a computerized scanner. Each system is paired with a computer software system to allow handling, visualization, enhancement, sharing, and archiving of the images.
23670402	High domain wall velocities via spin transfer torque using vertical current injection.
Sci Rep  2013
Domain walls, nanoscale transition regions separating oppositely oriented ferromagnetic domains, have significant promise for use in spintronic devices for data storage and memristive applications. The state of these devices is related to the wall position and thus rapid operation will require a controllable onset of domain wall motion and high speed wall displacement. These processes are traditionally driven by spin transfer torque due to lateral injection of spin polarized current through a ferromagnetic nanostrip. However, this geometry is often hampered by low maximum wall velocities and/or a need for prohibitively high current densities. Here, using time-resolved magnetotransport measurements, we show that vertical injection of spin currents through a magnetic tunnel junction can drive domain walls over hundreds of nanometers at ~500 m/s using current densities on the order of 6 MA/cm(2). Moreover, these measurements provide information about the stochastic and deterministic aspects of current driven domain wall mediated switching.
23561034	Revising the ECRIN standard requirements for information technology and data management in clinical trials.
Trials 20130405 2013
The pilot phase of the ECRIN (European Clinical Research Infrastructure Network) certification programme for European data centres, in late 2011, led to a substantial revision of the original ECRIN standards, completed by June 2012. The pilot phase, the conclusions drawn from it and the revised set of standards are described. Issues concerning the further development of standards and related material are discussed, as are the methods available to best support that development. A strategy is outlined based on short-lived specific task groups, established as necessary by a steering group drawn from ECRIN-ERIC. A final section discusses possible future developments.
23678688	What CFOs should know before venturing into the cloud.
Healthc Financ Manage  2013May
There are three major trends in the use of cloud-based services for healthcare IT: Cloud computing involves the hosting of health IT applications in a service provider cloud. Cloud storage is a data storage service that can involve, for example, long-term storage and archival of information such as clinical data, medical images, and scanned documents. Data center colocation involves rental of secure space in the cloud from a vendor, an approach that allows a hospital to share power capacity and proven security protocols, reducing costs.
23691543	Efficient protein structure search using indexing  methods.
BMC Med Inform Decis Mak 20130405 2013
Understanding functions of proteins is one of the most important challenges in many studies of biological processes. The function of a protein can be predicted by analyzing the functions of structurally similar proteins, thus finding structurally similar proteins accurately and efficiently from a large set of proteins is crucial. A protein structure can be represented as a vector by 3D-Zernike Descriptor (3DZD) which compactly represents the surface shape of the protein tertiary structure. This simplified representation accelerates the searching process. However, computing the similarity of two protein structures is still computationally expensive, thus it is hard to efficiently process many simultaneous requests of structurally similar protein search. This paper proposes indexing techniques which substantially reduce the search time to find structurally similar proteins. In particular, we first exploit two indexing techniques, i.e., iDistance and iKernel, on the 3DZDs. After that, we extend the techniques to further improve the search speed for protein structures. The extended indexing techniques build and utilize an reduced index constructed from the first few attributes of 3DZDs of protein structures. To retrieve top-k similar structures, top-10 × k similar structures are first found using the reduced index, and top-k structures are selected among them. We also modify the indexing techniques to support θ-based nearest neighbor search, which returns data points less than θ to the query point. The results show that both iDistance and iKernel significantly enhance the searching speed. In top-k nearest neighbor search, the searching time is reduced 69.6%, 77%, 77.4% and 87.9%, respectively using iDistance, iKernel, the extended iDistance, and the extended iKernel. In θ-based nearest neighbor serach, the searching time is reduced 80%, 81%, 95.6% and 95.6% using iDistance, iKernel, the extended iDistance, and the extended iKernel, respectively.
23635868	Telepathology in cytopathology: challenges and opportunities.
Acta Cytol. 20130425 2013
Telepathology in cytopathology is becoming more commonly utilized, and newer technologic infrastructures afford the laboratory a variety of options. The options and design of a telepathology system are driven by the clinical needs. This is primarily focused on providing rapid on-site evaluation service for fine needle aspiration. The clinical requirements and needs of a system are described. Available tools to design and implement a telepathology system are covered, including methods of image capture, network connectivity and remote viewing options. The primary telepathology method currently used and described involves the delivery via a network connection of a live video image to a remote site which is passively viewed by an internet web-based browser. By utilizing live video information and a voice connection to the on-site location, the remote viewer can collect clinical information and direct their view of the slides. Telepathology systems for use in cytopathology can be designed and implemented with commercially available infrastructure. It is necessary for the laboratory to validate the designed system and adhere to the required regulatory requirements. Telepathology for cytopathology can be reliably utilized by adapting existing technology, and newer advances hold great promise for further applications in the cytopathology laboratory.
23642009	What Google Maps can do for biomedical data dissemination: examples and a design study.
BMC Res Notes 20130504 2013
Biologists often need to assess whether unfamiliar datasets warrant the time investment required for more detailed exploration. Basing such assessments on brief descriptions provided by data publishers is unwieldy for large datasets that contain insights dependent on specific scientific questions. Alternatively, using complex software systems for a preliminary analysis may be deemed as too time consuming in itself, especially for unfamiliar data types and formats. This may lead to wasted analysis time and discarding of potentially useful data. We present an exploration of design opportunities that the Google Maps interface offers to biomedical data visualization. In particular, we focus on synergies between visualization techniques and Google Maps that facilitate the development of biological visualizations which have both low-overhead and sufficient expressivity to support the exploration of data at multiple scales. The methods we explore rely on displaying pre-rendered visualizations of biological data in browsers, with sparse yet powerful interactions, by using the Google Maps API. We structure our discussion around five visualizations: a gene co-regulation visualization, a heatmap viewer, a genome browser, a protein interaction network, and a planar visualization of white matter in the brain. Feedback from collaborative work with domain experts suggests that our Google Maps visualizations offer multiple, scale-dependent perspectives and can be particularly helpful for unfamiliar datasets due to their accessibility. We also find that users, particularly those less experienced with computer use, are attracted by the familiarity of the Google Maps API. Our five implementations introduce design elements that can benefit visualization developers. We describe a low-overhead approach that lets biologists access readily analyzed views of unfamiliar scientific datasets. We rely on pre-computed visualizations prepared by data experts, accompanied by sparse and intuitive interactions, and distributed via the familiar Google Maps framework. Our contributions are an evaluation demonstrating the validity and opportunities of this approach, a set of design guidelines benefiting those wanting to create such visualizations, and five concrete example visualizations.
23437862	Transformer-4 version 2.0.1, a free multi-platform software to quickly reformat genotype matrices of any marker type, and archive them in the Demiurge information system.
Mol Ecol Resour 20130226 2013May
Transformer-4 version 2.0.1 (T4) is a multi-platform freeware programmed in java that can transform a genotype matrix in Excel or XML format into the input formats of one or several of the most commonly used population genetic software, for any possible combination of the populations that the matrix contains. T4 also allows the users to (i) draw allozyme gel interpretations for any number of diploid individuals, and then generate a genotype matrix ready to be used by T4; and (ii) produce basic reports about the data in the matrices. Furthermore, T4 is the only way to optionally submit 'genetic diversity digests' for publication in the Demiurge online information system (http://www.demiurge-project.org). Each such digest undergoes peer-review, and it consists of a geo-referenced data matrix in the tfm4 format plus any ancillary document or hyperlink that the digest authors see fit to include. The complementarity between T4 and Demiurge facilitates a free, safe, permanent, and standardized data archival and analysis system for researchers, and may also be a convenient resource for scientific journals, public administrations, or higher educators. T4 and its converters are freely available (at, respectively, http://www.demiurge-project.org/download_t4 and http://www.demiurge-project.org/converterstore) upon registration in the Demiurge information system (http://demiurge-project.org/register). Users have to click on the link provided on an account validation email, and accept Demiurge's terms of use (see http://www.demiurge-project.org/termsofuse). A thorough user's guide is available within T4. A 3-min promotional video about T4 and Demiurge can be seen at http://vimeo.com/29828406.
23586054	Translational biomedical informatics in the cloud: present and future.
Biomed Res Int 20130317 2013
Next generation sequencing and other high-throughput experimental techniques of recent decades have driven the exponential growth in publicly available molecular and clinical data. This information explosion has prepared the ground for the development of translational bioinformatics. The scale and dimensionality of data, however, pose obvious challenges in data mining, storage, and integration. In this paper we demonstrated the utility and promise of cloud computing for tackling the big data problems. We also outline our vision that cloud computing could be an enabling tool to facilitate translational bioinformatics research.
23585031	HORDE: comprehensive resource for olfactory receptor genomics.
Methods Mol. Biol.  2013
Olfactory receptors (ORs) constitute the largest gene family in the mammalian genome. The existence of these proteins underlies the nature of, and variability in, odorant perception. The Human Olfactory Receptor Data Explorer (HORDE, http://genome.weizmann.ac.il/horde/ ) is a free online resource, which presents a complete compendium of all OR genes and pseudogenes in the genome of human and four other vertebrates. HORDE includes three parts: (1) an automated pipeline, which mines OR gene and pseudogene sequences out of complete genomes, and generates gene symbols based on sequence similarity; (2) a card generator that obtains and displays annotative information on individual ORs retrieved from external databases and relevant studies; and (3) a search engine that allows user retrieval of OR information. For human ORs, HORDE specifically addresses the universe of interindividual variation, as obtained from several sources, including whole genome sequences made possible by next-generation sequencing. This encompasses single nucleotide polymorphisms (SNP) and copy number variation (CNV), including deleterious mutational events. HORDE also hosts a number of tools designed specifically to assist in the study of OR evolution and function. In this chapter, we describe the status of HORDE (build #43). We also discuss plans for future enhancements and a road map for HORDE to become a better community-based bioinformatics tool. We highlight HORDE's role as a major research tool in the study of an expanding cohort of OR repertoires.
23589706	Many-to-many multicast routing schemes under a fixed topology.
ScientificWorldJournal 20130325 2013
Many-to-many multicast routing can be extensively applied in computer or communication networks supporting various continuous multimedia applications. The paper focuses on the case where all users share a common communication channel while each user is both a sender and a receiver of messages in multicasting as well as an end user. In this case, the multicast tree appears as a terminal Steiner tree (TeST). The problem of finding a TeST with a quality-of-service (QoS) optimization is frequently NP-hard. However, we discover that it is a good idea to find a many-to-many multicast tree with QoS optimization under a fixed topology. In this paper, we are concerned with three kinds of QoS optimization objectives of multicast tree, that is, the minimum cost, minimum diameter, and maximum reliability. All of three optimization problems are distributed into two types, the centralized and decentralized version. This paper uses the dynamic programming method to devise an exact algorithm, respectively, for the centralized and decentralized versions of each optimization problem.
23597693	Systematic reviews informing occupational therapy.
Am J Occup Ther  2013 May-Jun
We sought to identify and describe the number, topics, and publishing trends of systematic reviews relevant to occupational therapy indexed in the OTseeker database. We performed a cross-sectional survey of the systematic reviews contained in OTseeker in December 2011. Of the 1,940 systematic reviews indexed in OTseeker, only 53 (2.7%) were published in occupational therapy journals. The most common diagnostic categories were stroke (n = 195, 10.1%) and affective disorders (n = 204, 10.5%). The most common intervention categories were consumer education (n = 644, 33.2%) and psychosocial techniques (n = 571, 29.4%). Only 390 (20.1%) of the 1,940 systematic reviews specifically involved occupational therapy. Occupational therapists need to search broadly to locate relevant systematic reviews or, alternatively, to use databases such as OTseeker. Clarity about the involvement of occupational therapy in reports of future research will improve the ability to identify occupational therapy research for all stakeholders. Finally, occupational therapy practitioners need to read systematic reviews critically to determine whether review conclusions are justified.
23603845	Use of a global metabolic network to curate organismal metabolic networks.
Sci Rep  2013
The difficulty in annotating the vast amounts of biological information poses one of the greatest current challenges in biological research. The number of genomic, proteomic, and metabolomic datasets has increased dramatically over the last two decades, far outstripping the pace of curation efforts. Here, we tackle the challenge of curating metabolic network reconstructions. We predict organismal metabolic networks using sequence homology and a global metabolic network constructed from all available organismal networks. While sequence homology has been a standard to annotate metabolic networks it has been faulted for its lack of predictive power. We show, however, that when homology is used with a global metabolic network one is able to predict organismal metabolic networks that have enhanced network connectivity. Additionally, we compare the annotation behavior of current database curation efforts with our predictions and find that curation efforts are biased towards adding (rather than removing) reactions to organismal networks.
23261646	Data management in large-scale collaborative toxicity studies: how to file experimental data for automated statistical analysis.
Toxicol In Vitro 20121220 2013Jun
High-throughput screening approaches are carried out for the toxicity assessment of a large number of chemical compounds. In such large-scale in vitro toxicity studies several hundred or thousand concentration-response experiments are conducted. The automated evaluation of concentration-response data using statistical analysis scripts saves time and yields more consistent results in comparison to data analysis performed by the use of menu-driven statistical software. Automated statistical analysis requires that concentration-response data are available in a standardised data format across all compounds. To obtain consistent data formats, a standardised data management workflow must be established, including guidelines for data storage, data handling and data extraction. In this paper two procedures for data management within large-scale toxicological projects are proposed. Both procedures are based on Microsoft Excel files as the researcher's primary data format and use a computer programme to automate the handling of data files. The first procedure assumes that data collection has not yet started whereas the second procedure can be used when data files already exist. Successful implementation of the two approaches into the European project ACuteTox is illustrated.
23521146	Nitrogen-doped partially reduced graphene oxide rewritable nonvolatile memory.
ACS Nano 20130401 2013Apr23
As memory materials, two-dimensional (2D) carbon materials such as graphene oxide (GO)-based materials have attracted attention due to a variety of advantageous attributes, including their solution-processability and their potential for highly scalable device fabrication for transistor-based memory and cross-bar memory arrays. In spite of this, the use of GO-based materials has been limited, primarily due to uncontrollable oxygen functional groups. To induce the stable memory effect by ionic charges of a negatively charged carboxylic acid group of partially reduced graphene oxide (PrGO), a positively charged pyridinium N that served as a counterion to the negatively charged carboxylic acid was carefully introduced on the PrGO framework. Partially reduced N-doped graphene oxide (PrGODMF) in dimethylformamide (DMF) behaved as a semiconducting nonvolatile memory material. Its optical energy band gap was 1.7-2.1 eV and contained a sp2 C��?C framework with 45-50% oxygen-functionalized carbon density and 3% doped nitrogen atoms. In particular, rewritable nonvolatile memory characteristics were dependent on the proportion of pyridinum N, and as the proportion of pyridinium N atom decreased, the PrGODMF film lost memory behavior. Polarization of charged PrGODMF containing pyridinium N and carboxylic acid under an electric field produced N-doped PrGODMF memory effects that followed voltage-driven rewrite-read-erase-read processes.
23609696	Optical property study of FePt-C nanocomposite thin film for heat-assisted magnetic recording.
Opt Express  2013Apr22
Optical properties of the FePt-C nanocomposite thin film that was synthesized by sputtering with MgO/NiTa underlayer on glass substrate have been determined by an approach combining spectroscopic ellipsometry and transmission over the wavelength range of 380 - 1700 nm. It was observed that the refractive index is larger than the extinction coefficient, indicating that free electron absorption is not the dominant optical transition in the FePt-C thin film. Compared with FePt thin film, the FePt-C thin film has smaller optical constants, which lead to better optical performance including smaller optical spot on recording media and higher transducer efficiency for heat assisted magnetic recording.
23609699	Sleep/doze controlled dynamic bandwidth allocation algorithms for energy-efficient passive optical networks.
Opt Express  2013Apr22
In this work, we present a comparative study of two just-in-time (JIT) dynamic bandwidth allocation algorithms (DBAs), designed to improve the energy-efficiency of the 10 Gbps Ethernet passive optical networks (10G-EPONs). The algorithms, termed just-in-time with varying polling cycle times (JIT) and just-in-time with fixed polling cycle times (J-FIT), are designed to achieve energy-savings when the idle time of an optical network unit (ONU) is less than the sleep-to-active transition time. This is made possible by a vertical-cavity surface-emitting laser (VCSEL) ONU that can transit into sleep or doze modes during its idle time. We evaluate the performance of the algorithms in terms of polling cycle time, power consumption, percentage of energy-savings, and average delay. The energy-efficiency of a VCSEL ONU that can transition into sleep or doze mode is compared to an always-ON distributed feedback (DFB) laser ONU. Simulation results indicate that both JIT and J-FIT DBA algorithms result in improved energy-efficiency whilst J-FIT performs better in terms of energy-savings at low network loads. The J-FIT DBA however, results in increased average delay in comparison to the JIT DBA. Nonetheless, this increase in average delay is within the acceptable range to support the quality of service (QoS) requirements of the next-generation access networks.
23609713	Storage and retrieval of collective excitations on a long-lived spin transition in a rare-earth ion-doped crystal.
Opt Express  2013Apr22
Robust, long-lived optical quantum memories are important components of many quantum information and communication protocols. We demonstrate coherent generation, storage, and retrieval of excitations on a long-lived spin transition via spontaneous Raman scattering in a rare-earth ion-doped crystal. We further study the time dynamics of the optical correlations in this system. This is the first demonstration of its kind in a solid and an enabling step toward realizing a solid-state quantum repeater.
23617114	Tailoring the EHR for quality.
Hosp Health Netw  2013Mar
Shifting to ICD-10 is more important than ever before. Further delay will hinder how rapidly health care organizations can respond to the changing environment.
22566465	A dual decomposition approach to feature correspondence.
IEEE Trans Pattern Anal Mach Intell  2013Feb
In this paper, we present a new approach for establishing correspondences between sparse image features related by an unknown nonrigid mapping and corrupted by clutter and occlusion, such as points extracted from images of different instances of the same object category. We formulate this matching task as an energy minimization problem by defining an elaborate objective function of the appearance and the spatial arrangement of the features. Optimization of this energy is an instance of graph matching, which is in general an NP-hard problem. We describe a novel graph matching optimization technique, which we refer to as dual decomposition (DD), and demonstrate on a variety of examples that this method outperforms existing graph matching algorithms. In the majority of our examples, DD is able to find the global minimum within a minute. The ability to globally optimize the objective allows us to accurately learn the parameters of our matching model from training examples. We show on several matching tasks that our learned model yields results superior to those of state-of-the-art methods.
22641702	A novel encoding scheme for effective biometric discretization: Linearly Separable Subcode.
IEEE Trans Pattern Anal Mach Intell  2013Feb
Separability in a code is crucial in guaranteeing a decent Hamming-distance separation among the codewords. In multibit biometric discretization where a code is used for quantization-intervals labeling, separability is necessary for preserving distance dissimilarity when feature components are mapped from a discrete space to a Hamming space. In this paper, we examine separability of Binary Reflected Gray Code (BRGC) encoding and reveal its inadequacy in tackling interclass variation during the discrete-to-binary mapping, leading to a tradeoff between classification performance and entropy of binary output. To overcome this drawback, we put forward two encoding schemes exhibiting full-ideal and near-ideal separability capabilities, known as Linearly Separable Subcode (LSSC) and Partially Linearly Separable Subcode (PLSSC), respectively. These encoding schemes convert the conventional entropy-performance tradeoff into an entropy-redundancy tradeoff in the increase of code length. Extensive experimental results vindicate the superiority of our schemes over the existing encoding schemes in discretization performance. This opens up possibilities of achieving much greater classification performance with high output entropy.
22665724	A survey of visualization pipelines.
IEEE Trans Vis Comput Graph  2013Mar
The most common abstraction used by visualization libraries and applications today is what is known as the visualization pipeline. The visualization pipeline provides a mechanism to encapsulate algorithms and then couple them together in a variety of ways. The visualization pipeline has been in existence for over 20 years, and over this time many variations and improvements have been proposed. This paper provides a literature review of the most prevalent features of visualization pipelines and some of the most recent research directions.
22529324	Image transformation based on learning dictionaries across image spaces.
IEEE Trans Pattern Anal Mach Intell  2013Feb
In this paper, we propose a framework of transforming images from a source image space to a target image space, based on learning coupled dictionaries from a training set of paired images. The framework can be used for applications such as image super-resolution and estimation of image intrinsic components (shading and albedo). It is based on a local parametric regression approach, using sparse feature representations over learned coupled dictionaries across the source and target image spaces. After coupled dictionary learning, sparse coefficient vectors of training image patch pairs are partitioned into easily retrievable local clusters. For any test image patch, we can fast index into its closest local cluster and perform a local parametric regression between the learned sparse feature spaces. The obtained sparse representation (together with the learned target space dictionary) provides multiple constraints for each pixel of the target image to be estimated. The final target image is reconstructed based on these constraints. The contributions of our proposed framework are three-fold. 1) We propose a concept of coupled dictionary learning based on coupled sparse coding which requires the sparse coefficient vectors of a pair of corresponding source and target image patches to have the same support, i.e., the same indices of nonzero elements. 2) We devise a space partitioning scheme to divide the high-dimensional but sparse feature space into local clusters. The partitioning facilitates extremely fast retrieval of closest local clusters for query patches. 3) Benefiting from sparse feature-based image transformation, our method is more robust to corrupted input data, and can be considered as a simultaneous image restoration and transformation process. Experiments on intrinsic image estimation and super-resolution demonstrate the effectiveness and efficiency of our proposed method.
23381497	Formal methods for Hopfield-like networks.
Acta Biotheor. 20130205 2013Mar
Building a meaningful model of biological regulatory network is usually done by specifying the components (e.g. the genes) and their interactions, by guessing the values of parameters, by comparing the predicted behaviors to the observed ones, and by modifying in a trial-error process both architecture and parameters in order to reach an optimal fitness. We propose here a different approach to construct and analyze biological models avoiding the trial-error part, where structure and dynamics are represented as formal constraints. We apply the method to Hopfield-like networks, a formalism often used in both neural and regulatory networks modeling. The aim is to characterize automatically the set of all models consistent with all the available knowledge (about structure and behavior). The available knowledge is formalized into formal constraints. The latter are compiled into Boolean formula in conjunctive normal form and then submitted to a Boolean satisfiability solver. This approach allows to formulate a wide range of queries, expressed in a high level language, and possibly integrating formalized intuitions. In order to explore its potential, we use it to find cycles for 3-nodes networks and to determine the flower morphogenesis regulatory network of Arabidopsis thaliana. Applications of this technique are numerous and concern the building of models from data as well as the design of biological networks possessing specified behaviors.
22566472	Visualizing nD point clouds as topological landscape profiles to guide local data analysis.
IEEE Trans Vis Comput Graph  2013Mar
Analyzing high-dimensional point clouds is a classical challenge in visual analytics. Traditional techniques, such as projections or axis-based techniques, suffer from projection artifacts, occlusion, and visual complexity. We propose to split data analysis into two parts to address these shortcomings. First, a structural overview phase abstracts data by its density distribution. This phase performs topological analysis to support accurate and nonoverlapping presentation of the high-dimensional cluster structure as a topological landscape profile. Utilizing a landscape metaphor, it presents clusters and their nesting as hills whose height, width, and shape reflect cluster coherence, size, and stability, respectively. A second local analysis phase utilizes this global structural knowledge to select individual clusters or point sets for further, localized data analysis. Focusing on structural entities significantly reduces visual clutter in established geometric visualizations and permits a clearer, more thorough data analysis. This analysis complements the global topological perspective and enables the user to study subspaces or geometric properties, such as shape.
23542990	DIOS - database of formalized chemotherapeutic regimens.
Stud Health Technol Inform  2013
Chemotherapeutic regimens (CHR) and their administration are routine practice in contemporary oncology. The development of a structured, electronic database of standard CHR can help the faster propagation of information about new CHR and at the same time enable assessment of their adherence in clinical practice. The goal was to develop a standardized way to describe a regimen using XML, fill the database with currently available regimens and develop tools to assess the adherence of the treatment to chosen regimen, compare the dose-intensity and recognize the regimen from existing data on drug administration. The data are being inserted in cooperation with expert oncologists and the database currently contains about 260 CHRs. Such system can be used to enhance decision support systems and interoperability of HIS. The database and tools are available online on the internet.
23542993	Interoperability evaluation case study: an Obstetrics-Gynecology Department and related Information Systems.
Stud Health Technol Inform  2013
The paper presents the steps and metrics for evaluating the interoperability of an Obstetrics-Gynecology Department Information System applied on Bega Clinic Timisoara regarding its readiness for interoperability in relation with similar systems. The developed OGD IS was modeled starting from the Generic Component Model and sends information to other medical units using the HL7 Clinical Document Architecture and Continuity of Care Document standards. The data for evaluation are real, collected between 2009 and 2010 from Bega Clinic Timisoara. The results were relatively good for the investigated data and structure.
23546106	Robust 9-QAM digital recovery for spectrum shaped coherent QPSK signal.
Opt Express  2013Mar25
We propose 9-ary quadrature amplitude modulation (9-QAM) data recovery for polarization multiplexing-quadrature phase shift keying (PM-QPSK) signal in presence of strong filtering to approach Nyquist bandwidth. The decision-directed least radius distance (DD-LRD) algorithm for blind equalization is used for 9-QAM recovery and intersymbol interference (ISI) compression. It shows the robustness under strong filtering to recover 9-QAM signal rather than QPSK. We demonstrate 112 Gb/s spectrum shaped PM-QPSK signal by wavelength selective switch (WSS) in a 25-GHz channel spacing Nyquist wavelength division multiplexing (NWDM). The final equalized signal is detected by maximum likelihood sequence decision (MLSD) for data bit-error-ratio (BER) measurement. Optical signal-to-noise ratio (OSNR) tolerance is improved by 0.5 dB at a BER of 1x10(-3) compared to constant modulus algorithm (CMA) plus post-filter algorithm.
23546440	Key reconciliation for high performance quantum key distribution.
Sci Rep  2013
Quantum Key Distribution is carving its place among the tools used to secure communications. While a difficult technology, it enjoys benefits that set it apart from the rest, the most prominent is its provable security based on the laws of physics. QKD requires not only the mastering of signals at the quantum level, but also a classical processing to extract a secret-key from them. This postprocessing has been customarily studied in terms of the efficiency, a figure of merit that offers a biased view of the performance of real devices. Here we argue that it is the throughput the significant magnitude in practical QKD, specially in the case of high speed devices, where the differences are more marked, and give some examples contrasting the usual postprocessing schemes with new ones from modern coding theory. A good understanding of its implications is very important for the design of modern QKD devices.
23522333	Enhancing the scoping study methodology: a large, inter-professional team's experience with Arksey and O'Malley's framework.
BMC Med Res Methodol 20130323 2013
Scoping studies are increasingly common for broadly searching the literature on a specific topic, yet researchers lack an agreed-upon definition of and framework for the methodology. In 2005, Arksey and O'Malley offered a methodological framework for conducting scoping studies. In their subsequent work, Levac et al. responded to Arksey and O'Malley's call for advances to their framework. Our paper builds on this collective work to further enhance the methodology. This paper begins with a background on what constitutes a scoping study, followed by a discussion about four primary subjects: (1) the types of questions for which Arksey and O'Malley's framework is most appropriate, (2) a contribution to the discussion aimed at enhancing the six steps of Arskey and O'Malley's framework, (3) the strengths and challenges of our experience working with Arksey and O'Malley's framework as a large, inter-professional team, and (4) lessons learned. Our goal in this paper is to add to the discussion encouraged by Arksey and O'Malley to further enhance this methodology. Performing a scoping study using Arksey and O'Malley's framework was a valuable process for our research team even if how it was useful was unexpected. Based on our experience, we recommend researchers be aware of their expectations for how Arksey and O'Malley's framework might be useful in relation to their research question, and remain flexible to clarify concepts and to revise the research question as the team becomes familiar with the literature. Questions portraying comparisons such as between interventions, programs, or approaches seem to be the most suitable to scoping studies. We also suggest assessing the quality of studies and conducting a trial of the method before fully embarking on the charting process in order to ensure consistency. The benefits of engaging a large, inter-professional team such as ours throughout every stage of Arksey and O'Malley's framework far exceed the challenges and we recommend researchers consider the value of such a team. The strengths include breadth and depth of knowledge each team member brings to the study and time efficiencies. In our experience, the most significant challenges presented to our team were those related to consensus and resource limitations. Effective communication is key to the success of a large group. We propose that by clarifying the framework, the purposes of scoping studies are attainable and the definition is enriched.
23554837	Thresholded two-phase test sample representation for outlier rejection in biological recognition.
Comput Math Methods Med 20130311 2013
The two-phase test sample representation (TPTSR) was proposed as a useful classifier for face recognition. However, the TPTSR method is not able to reject the impostor, so it should be modified for real-world applications. This paper introduces a thresholded TPTSR (T-TPTSR) method for complex object recognition with outliers, and two criteria for assessing the performance of outlier rejection and member classification are defined. The performance of the T-TPTSR method is compared with the modified global representation, PCA and LDA methods, respectively. The results show that the T-TPTSR method achieves the best performance among them according to the two criteria.
23556951	Self avoiding paths routing algorithm in scale-free networks.
Chaos  2013Mar
In this paper, we present a new routing algorithm called "the self avoiding paths routing algorithm." Its application to traffic flow in scale-free networks shows a great improvement over the so called "efficient routing" protocol while at the same time maintaining a relatively low average packet travel time. It has the advantage of minimizing path overlapping throughout the network in a self consistent manner with a relatively small number of iterations by maintaining an equilibrated path distribution especially among the hubs. This results in a significant shifting of the critical packet generation rate over which traffic congestion occurs, thus permitting the network to sustain more information packets in the free flow state. The performance of the algorithm is discussed both on a Bara�?basi-Albert network and real autonomous system network data.
23558260	Unifying complexity and information.
Sci Rep  2013
Complex systems, arising in many contexts in the computer, life, social, and physical sciences, have not shared a generally-accepted complexity measure playing a fundamental role as the Shannon entropy H in statistical mechanics. Superficially-conflicting criteria of complexity measurement, i.e. complexity-randomness (C-R) relations, have given rise to a special measure intrinsically adaptable to more than one criterion. However, deep causes of the conflict and the adaptability are not much clear. Here I trace the root of each representative or adaptable measure to its particular universal data-generating or -regenerating model (UDGM or UDRM). A representative measure for deterministic dynamical systems is found as a counterpart of the H for random process, clearly redefining the boundary of different criteria. And a specific UDRM achieving the intrinsic adaptability enables a general information measure that ultimately solves all major disputes. This work encourages a single framework coving deterministic systems, statistical mechanics and real-world living organisms.
23559088	Optimally combining dynamical decoupling and quantum error correction.
Sci Rep  2013
Quantum control and fault-tolerant quantum computing (FTQC) are two of the cornerstones on which the hope of realizing a large-scale quantum computer is pinned, yet only preliminary steps have been taken towards formalizing the interplay between them. Here we explore this interplay using the powerful strategy of dynamical decoupling (DD), and show how it can be seamlessly and optimally integrated with FTQC. To this end we show how to find the optimal decoupling generator set (DGS) for various subspaces relevant to FTQC, and how to simultaneously decouple them. We focus on stabilizer codes, which represent the largest contribution to the size of the DGS, showing that the intuitive choice comprising the stabilizers and logical operators of the code is in fact optimal, i.e., minimizes a natural cost function associated with the length of DD sequences. Our work brings hybrid DD-FTQC schemes, and their potentially considerable advantages, closer to realization.
23496912	The cloud paradigm applied to e-Health.
BMC Med Inform Decis Mak 20130314 2013
Cloud computing is a new paradigm that is changing how enterprises, institutions and people understand, perceive and use current software systems. With this paradigm, the organizations have no need to maintain their own servers, nor host their own software. Instead, everything is moved to the cloud and provided on demand, saving energy, physical space and technical staff. Cloud-based system architectures provide many advantages in terms of scalability, maintainability and massive data processing. We present the design of an e-health cloud system, modelled by an M/M/m queue with QoS capabilities, i.e. maximum waiting time of requests. Detailed results for the model formed by a Jackson network of two M/M/m queues from the queueing theory perspective are presented. These results show a significant performance improvement when the number of servers increases. Platform scalability becomes a critical issue since we aim to provide the system with high Quality of Service (QoS). In this paper we define an architecture capable of adapting itself to different diseases and growing numbers of patients. This platform could be applied to the medical field to greatly enhance the results of those therapies that have an important psychological component, such as addictions and chronic diseases.
23566263	On the efficacy of per-relation basis performance evaluation for PPI extraction and a high-precision rule-based approach.
BMC Med Inform Decis Mak 20130405 2013
Most previous Protein Protein Interaction (PPI) studies evaluated their algorithms' performance based on "per-instance" precision and recall, in which the instances of an interaction relation were evaluated independently. However, we argue that this standard evaluation method should be revisited. In a large corpus, the same relation can be described in various different forms and, in practice, correctly identifying not all but a small subset of them would often suffice to detect the given interaction. In this regard, we propose a more pragmatic "per-relation" basis performance evaluation method instead of the conventional per-instance basis method. In the per-relation basis method, only a subset of a relation's instances needs to be correctly identified to make the relation positive. In this work, we also introduce a new high-precision rule-based PPI extraction algorithm. While virtually all current PPI extraction studies focus on improving F-score, aiming to balance the performance on both precision and recall, in many realistic scenarios involving large corpora, one can benefit more from a high-precision algorithm than a high-recall counterpart. We show that our algorithm not only achieves better per-relation performance than previous solutions but also serves as a good complement to the existing PPI extraction tools. Our algorithm improves the performance of the existing tools through simple pipelining. The significance of this research can be found in that this research brought new perspective to the performance evaluation of PPI extraction studies, which we believe is more important in practice than existing evaluation criteria. Given the new evaluation perspective, we also showed the importance of a high-precision extraction tool and validated the efficacy of our rule-based system as the high-precision tool candidate.
23399513	Why mobile health app overload drives us crazy, and how to restore the sanity.
BMC Med Inform Decis Mak 20130211 2013
Smartphones and tablet computers have become an integral part of our lives. One of their key features is the possibility of installing third-party apps. These apps can be very helpful for improving health and healthcare. However, medical professionals and citizens are currently being overloaded with health apps. Consequently, they will have difficulty with finding the right app, and information and features are fragmented over too many apps, thereby limiting their usefulness. In order to combat health app overload, suppliers of apps need to do three things. One, join the open source movement, so that a few apps can work as gateway to medical information by incorporating information from different sources. Two, standardize content, so that the information provided via apps is readable. And third, in order to prevent information overload from occurring within an app, content should be personalized towards an individual's characteristics and context. Suppliers of medical information and features need to join the open source movement and must make use of standardized medical information formats, in order to allow third parties to create valuable, mobile gateway apps. This can prevent the occurrence of health app overload. By going along in these trends, we can make health apps achieve the impact on healthcare quality and citizens' health many of us envision.
23573172	A soft computing based approach using modified selection strategy for feature reduction of medical systems.
Comput Math Methods Med 20130321 2013
The systems consisting high input spaces require high processing times and memory usage. Most of the attribute selection algorithms have the problems of input dimensions limits and information storage problems. These problems are eliminated by means of developed feature reduction software using new modified selection mechanism with middle region solution candidates adding. The hybrid system software is constructed for reducing the input attributes of the systems with large number of input variables. The designed software also supports the roulette wheel selection mechanism. Linear order crossover is used as the recombination operator. In the genetic algorithm based soft computing methods, locking to the local solutions is also a problem which is eliminated by using developed software. Faster and effective results are obtained in the test procedures. Twelve input variables of the urological system have been reduced to the reducts (reduced input attributes) with seven, six, and five elements. It can be seen from the obtained results that the developed software with modified selection has the advantages in the fields of memory allocation, execution time, classification accuracy, sensitivity, and specificity values when compared with the other reduction algorithms by using the urological test data.
23542422	Towards robust deconvolution of low-dose perfusion CT: sparse perfusion deconvolution using online dictionary learning.
Med Image Anal 20130307 2013May
Computed tomography perfusion (CTP) is an important functional imaging modality in the evaluation of cerebrovascular diseases, particularly in acute stroke and vasospasm. However, the post-processed parametric maps of blood flow tend to be noisy, especially in low-dose CTP, due to the noisy contrast enhancement profile and the oscillatory nature of the results generated by the current computational methods. In this paper, we propose a robust sparse perfusion deconvolution method (SPD) to estimate cerebral blood flow in CTP performed at low radiation dose. We first build a dictionary from high-dose perfusion maps using online dictionary learning and then perform deconvolution-based hemodynamic parameters estimation on the low-dose CTP data. Our method is validated on clinical data of patients with normal and pathological CBF maps. The results show that we achieve superior performance than existing methods, and potentially improve the differentiation between normal and ischemic tissue in the brain.
23571908	Speckle-free digital holographic recording of a diffusely reflecting object.
Opt Express  2013Apr8
We demonstrate holographic recording without speckle noise using the digital holographic technique called optical scanning holography (OSH). First, we record a complex hologram of a diffusely reflecting (DR) object using OSH. The incoherent mode of OSH makes it possible to record the complex hologram without speckle noise. Second, we convert the complex hologram to an off-axis real hologram digitally and finally we reconstruct the real hologram using an amplitude-only spatial light modulator (SLM) without twin-image noise and speckle noise. To the best of our knowledge, this is the first time demonstrating digital holographic recording of a DR object without speckle noise.
23391498	A new algorithm for context-based biomedical diagram similarity estimation.
Bioinformatics 20130207 2013Mar15
Diagrams embedded in the biomedical literature convey rich contents, which often concisely and intuitively highlight key thesis of a research article. Despite their vital importance and informative clues for biomedical literature navigation and retrieval; currently, we miss an effective computational method for automatically understanding and accessing these valuable resources. To address the aforementioned gap, we propose a novel context-based algorithm for estimating the similarity between a pair of biomedical diagrams. The main difference of the proposed algorithm with respect to the existing methods lies in the new algorithm's incorporation of the semantic context associated with diagrams in their source documents into the diagram similarity estimation process. In addition, the new approach also performs a series of advanced image processing and text mining operations to comprehensively extract the semantic content graphically encoded inside diagram images. The new algorithm can be deployed as a reusable component providing a fundamental function for building many advanced, semantic-aware applications on biomedical diagram processing. As a case study, in our experiments, we demonstrate the advantage of the new algorithm for diagram retrieval. A set of biomedical diagram search and ranking experiments were conducted, where the performance of the new method was compared with that of five peer methods. The comparison results demonstrate the performance superiority of the new algorithm with all peer methods with statistical significance.
23442731	PubMed searches: overview and strategies for clinicians.
Nutr Clin Pract 20130226 2013Apr
PubMed is a biomedical and life sciences database maintained by a division of the National Library of Medicine known as the National Center for Biotechnology Information (NCBI). It is a large resource with more than 5600 journals indexed and greater than 22 million total citations. Searches conducted in PubMed provide references that are more specific for the intended topic compared with other popular search engines. Effective PubMed searches allow the clinician to remain current on the latest clinical trials, systematic reviews, and practice guidelines. PubMed continues to evolve by allowing users to create a customized experience through the My NCBI portal, new arrangements and options in search filters, and supporting scholarly projects through exportation of citations to reference managing software. Prepackaged search options available in the Clinical Queries feature also allow users to efficiently search for clinical literature. PubMed also provides information regarding the source journals themselves through the Journals in NCBI Databases link. This article provides an overview of the PubMed database's structure and features as well as strategies for conducting an effective search.
23085139	A system for the extraction and representation of summary of product characteristics content.
Artif Intell Med 20121022 2013Feb
Information about medications is critical in supporting decision-making during the prescription process and thus in improving the safety and quality of care. In this work, we propose a methodology for the automatic recognition of drug-related entities (active ingredient, interaction effects, etc.) in textual drug descriptions, and their further location in a previously developed domain ontology. The summary of product characteristics (SPC) represents the basis of information for health professionals on how to use medicines. However, this information is locked in free-text and, as such, cannot be actively accessed and elaborated by computerized applications. Our approach exploits a combination of machine learning and rule-based methods. It consists of two stages. Initially it learns to classify this information in a structured prediction framework, relying on conditional random fields. The classifier is trained and evaluated using a corpus of about a hundred SPCs. They have been hand-annotated with different semantic labels that have been derived from the domain ontology. At a second stage the extracted entities are added in the domain ontology corresponding concepts as new instances, using a set of rules manually-constructed from the corpus. Our evaluations show that the extraction module exhibits high overall performance, with an average F1-measure of 88% for contraindications and 90% for interactions. SPCs can be exploited to provide structured information for computer-based decision support systems.
23092678	Factors affecting the effectiveness of biomedical document indexing and retrieval based on terminologies.
Artif Intell Med 20121023 2013Feb
The aim of this work is to evaluate a set of indexing and retrieval strategies based on the integration of several biomedical terminologies on the available TREC Genomics collections for an ad hoc information retrieval (IR) task. We propose a multi-terminology based concept extraction approach to selecting best concepts from free text by means of voting techniques. We instantiate this general approach on four terminologies (MeSH, SNOMED, ICD-10 and GO). We particularly focus on the effect of integrating terminologies into a biomedical IR process, and the utility of using voting techniques for combining the extracted concepts from each document in order to provide a list of unique concepts. Experimental studies conducted on the TREC Genomics collections show that our multi-terminology IR approach based on voting techniques are statistically significant compared to the baseline. For example, tested on the 2005 TREC Genomics collection, our multi-terminology based IR approach provides an improvement rate of +6.98% in terms of MAP (mean average precision) (p&lt;0.05) compared to the baseline. In addition, our experimental results show that document expansion using preferred terms in combination with query expansion using terms from top ranked expanded documents improve the biomedical IR effectiveness. We have evaluated several voting models for combining concepts issued from multiple terminologies. Through this study, we presented many factors affecting the effectiveness of biomedical IR system including term weighting, query expansion, and document expansion models. The appropriate combination of those factors could be useful to improve the IR performance.
23273493	The Foundational Model of Anatomy in OWL 2 and its use.
Artif Intell Med 20121228 2013Feb
The objective is to represent the Foundational Model of Anatomy (FMA) in the OWL 2 Web Ontology Language (informally OWL 2), and to use it in a European cross-lingual portal of health terminologies for indexing and searching Web resources. Formalizing the FMA in OWL 2 is essential for semantic interoperability, to improve its design, and to ensure its reliability and correctness, which is particularly important for medical applications. The native FMA was implemented in frames and stored in a MySQL database backend. The main strength of the method is to leverage OWL 2 expressiveness and to rely on the naming conventions of the FMA, to make explicit some implicit semantics, while improving its ontological model and fixing some errors. Doing so, the semantics (meaning) of the formal definitions and axioms are anatomically correct. A flexible tool enables the generation of a new version in OWL 2 at each Protégé FMA update. While it creates by default a 'standard' version of the FMA in OWL 2 (FMA-OWL), many options allow for producing other variants customized to users' applications. Once formalized in OWL 2, it was possible to use an inference engine to check the ontology and detect inconsistencies. Next, the FMA-OWL was used to derive a lightweight FMA terminology for a European cross-lingual portal of terminologies/ontologies for indexing and searching resources. The transformation is mainly based on a reification process. Complete representations of the entire FMA in OWL 1 or OWL 2 are now available. The formalization tool is flexible and easy to use, making it possible to obtain an OWL 2 version for all existing public FMA. A number of errors were detected in the native FMA and several patterns of recurrent errors were identified in the original FMA. This shows how the underlying OWL 2 ontology is essential to ensure that the lightweight derived terminology is reliable. The FMA OWL 2 ontology has been applied to derive an anatomy terminology that is used in a European cross-lingual portal of health terminologies. This portal is daily used by librarians to index Web health resources. In August 2011, 6481 out of 81,450 health resources of CISMeF catalog (http://www.chu-rouen.fr/cismef/--accessed 29.08.12) (7.96%) were indexed with at least one FMA entity. The FMA is a central terminology used to index and search Web resources. To the best of our knowledge, neither a complete representation of the entire FMA in OWL 2, nor an anatomy terminology available in a cross-lingual portal, has been developed to date. The method designed to represent the FMA ontology in OWL 2 presented in this article is general and may be extended to other ontologies. Using a formal ontology for quality assurance and deriving a lightweight terminology for biomedical applications is a general and promising strategy.
23177219	Alternatives to relational database: comparison of NoSQL and XML approaches for clinical data storage.
Comput Methods Programs Biomed 20121121 2013Apr
Clinical data are dynamic in nature, often arranged hierarchically and stored as free text and numbers. Effective management of clinical data and the transformation of the data into structured format for data analysis are therefore challenging issues in electronic health records development. Despite the popularity of relational databases, the scalability of the NoSQL database model and the document-centric data structure of XML databases appear to be promising features for effective clinical data management. In this paper, three database approaches--NoSQL, XML-enabled and native XML--are investigated to evaluate their suitability for structured clinical data. The database query performance is reported, together with our experience in the databases development. The results show that NoSQL database is the best choice for query speed, whereas XML databases are advantageous in terms of scalability, flexibility and extensibility, which are essential to cope with the characteristics of clinical data. While NoSQL and XML technologies are relatively new compared to the conventional relational database, both of them demonstrate potential to become a key database technology for clinical data management as the technology further advances.
23496460	Information-theoretic approach to ground-state phase transitions for two- and three-dimensional frustrated spin systems.
Phys Rev E Stat Nonlin Soft Matter Phys 20130207 2013Feb
The information-theoretic observables entropy (a measure of disorder), excess entropy (a measure of complexity), and multi-information are used to analyze ground-state spin configurations for disordered and frustrated model systems in two and three dimensions. For both model systems, ground-state spin configurations can be obtained in polynomial time via exact combinatorial optimization algorithms, which allowed us to study large systems with high numerical accuracy. Both model systems exhibit a continuous transition from an ordered to a disordered ground state as a model parameter is varied. By using the above information-theoretic observables it is possible to detect changes in the spatial structure of the ground states as the critical point is approached. It is further possible to quantify the scaling behavior of the information-theoretic observables in the vicinity of the critical point. For both model systems considered, the estimates of critical properties for the ground-state phase transitions are in good agreement with existing results reported in the literature.
23496566	Measuring information interactions on the ordinal pattern of stock time series.
Phys Rev E Stat Nonlin Soft Matter Phys 20130208 2013Feb
The interactions among time series as individual components of complex systems can be quantified by measuring to what extent they exchange information among each other. In many applications, one focuses not on the original series but on its ordinal pattern. In such cases, trivial noises appear more likely to be filtered and the abrupt influence of extreme values can be weakened. Cross-sample entropy and inner composition alignment have been introduced as prominent methods to estimate the information interactions of complex systems. In this paper, we modify both methods to detect the interactions among the ordinal pattern of stock return and volatility series, and we try to uncover the information exchanges across sectors in Chinese stock markets.
23496568	Coverage maximization under resource constraints using a nonuniform proliferating random walk.
Phys Rev E Stat Nonlin Soft Matter Phys 20130213 2013Feb
Information management services on networks, such as search and dissemination, play a key role in any large-scale distributed system. One of the most desirable features of these services is the maximization of the coverage, i.e., the number of distinctly visited nodes under constraints of network resources as well as time. However, redundant visits of nodes by different message packets (modeled, e.g., as walkers) initiated by the underlying algorithms for these services cause wastage of network resources. In this work, using results from analytical studies done in the past on a K-random-walk-based algorithm, we identify that redundancy quickly increases with an increase in the density of the walkers. Based on this postulate, we design a very simple distributed algorithm which dynamically estimates the density of the walkers and thereby carefully proliferates walkers in sparse regions. We use extensive computer simulations to test our algorithm in various kinds of network topologies whereby we find it to be performing particularly well in networks that are highly clustered as well as sparse.
23496573	Dynamical traffic light strategy in the Biham-Middleton-Levine model.
Phys Rev E Stat Nonlin Soft Matter Phys 20130220 2013Feb
In this paper, we study dynamical traffic light strategies in the Biham-Middleton-Levine traffic flow model. The strategies use local vehicular information to control urban traffic, which take into account the interaction of vehicles traveling in different directions via considering their dynamical spatial configuration. Simulations find out two strategies, in which local information at nearby sites is used. The two strategies perform much better than the alternating strategy. Under these two strategies, vehicles can self-organize into a new intermediate state with band structure. The analytical solutions of velocity of this state have been presented, which are in good agreement with simulations.
23294509	Seeking health information on the web: positive hypothesis testing.
Int J Med Inform 20130105 2013Apr
The goal of this study is to investigate positive hypothesis testing among consumers of health information when they search the Web. After demonstrating the extent of positive hypothesis testing using Experiment 1, we conduct Experiment 2 to test the effectiveness of two debiasing techniques. A total of 60 undergraduate students searched a tightly controlled online database developed by the authors to test the validity of a hypothesis. The database had four abstracts that confirmed the hypothesis and three abstracts that disconfirmed it. Findings of Experiment 1 showed that majority of participants (85%) exhibited positive hypothesis testing. In Experiment 2, we found that the recommendation technique was not effective in reducing positive hypothesis testing since none of the participants assigned to this server could retrieve disconfirming evidence. Experiment 2 also showed that the incorporation technique successfully reduced positive hypothesis testing since 75% of the participants could retrieve disconfirming evidence. Positive hypothesis testing on the Web is an understudied topic. More studies are needed to validate the effectiveness of the debiasing techniques discussed in this study and develop new techniques. Search engine developers should consider developing new options for users so that both confirming and disconfirming evidence can be presented in search results as users test hypotheses using search engines.
22517427	Integrative Genomics Viewer (IGV): high-performance genomics data visualization and exploration.
Brief. Bioinformatics 20120419 2013Mar
Data visualization is an essential component of genomic data analysis. However, the size and diversity of the data sets produced by today's sequencing and array-based profiling methods present major challenges to visualization tools. The Integrative Genomics Viewer (IGV) is a high-performance viewer that efficiently handles large heterogeneous data sets, while providing a smooth and intuitive user experience at all levels of genome resolution. A key characteristic of IGV is its focus on the integrative nature of genomic studies, with support for both array-based and next-generation sequencing data, and the integration of clinical and phenotypic data. Although IGV is often used to view genomic data from public sources, its primary emphasis is to support researchers who wish to visualize and explore their own data sets or those from colleagues. To that end, IGV supports flexible loading of local and remote data sets, and is optimized to provide high-performance data visualization and exploration on standard desktop systems. IGV is freely available for download from http://www.broadinstitute.org/igv, under a GNU LGPL open-source license.
22764121	A brief introduction to web-based genome browsers.
Brief. Bioinformatics 20120703 2013Mar
Genome browser provides a graphical interface for users to browse, search, retrieve and analyze genomic sequence and annotation data. Web-based genome browsers can be classified into general genome browsers with multiple species and species-specific genome browsers. In this review, we attempt to give an overview for the main functions and features of web-based genome browsers, covering data visualization, retrieval, analysis and customization. To give a brief introduction to the multiple-species genome browser, we describe the user interface and main functions of the Ensembl and UCSC genome browsers using the human alpha-globin gene cluster as an example. We further use the MSU and the Rice-Map genome browsers to show some special features of species-specific genome browser, taking a rice transcription factor gene OsSPL14 as an example.
22908213	The UCSC genome browser and associated tools.
Brief. Bioinformatics 20120820 2013Mar
The UCSC Genome Browser (http://genome.ucsc.edu) is a graphical viewer for genomic data now in its 13th year. Since the early days of the Human Genome Project, it has presented an integrated view of genomic data of many kinds. Now home to assemblies for 58 organisms, the Browser presents visualization of annotations mapped to genomic coordinates. The ability to juxtapose annotations of many types facilitates inquiry-driven data mining. Gene predictions, mRNA alignments, epigenomic data from the ENCODE project, conservation scores from vertebrate whole-genome alignments and variation data may be viewed at any scale from a single base to an entire chromosome. The Browser also includes many other widely used tools, including BLAT, which is useful for alignments from high-throughput sequencing experiments. Private data uploaded as Custom Tracks and Data Hubs in many formats may be displayed alongside the rich compendium of precomputed data in the UCSC database. The Table Browser is a full-featured graphical interface, which allows querying, filtering and intersection of data tables. The Saved Session feature allows users to store and share customized views, enhancing the utility of the system for organizing multiple trains of thought. Binary Alignment/Map (BAM), Variant Call Format and the Personal Genome Single Nucleotide Polymorphisms (SNPs) data formats are useful for visualizing a large sequencing experiment (whole-genome or whole-exome), where the differences between the data set and the reference assembly may be displayed graphically. Support for high-throughput sequencing extends to compact, indexed data formats, such as BAM, bigBed and bigWig, allowing rapid visualization of large datasets from RNA-seq and ChIP-seq experiments via local hosting.
23376193	Using GBrowse 2.0 to visualize and share next-generation sequence data.
Brief. Bioinformatics 20130201 2013Mar
GBrowse is a mature web-based genome browser that is suitable for deployment on both public and private web sites. It supports most of genome browser features, including qualitative and quantitative (wiggle) tracks, track uploading, track sharing, interactive track configuration, semantic zooming and limited smooth track panning. As of version 2.0, GBrowse supports next-generation sequencing (NGS) data by providing for the direct display of SAM and BAM sequence alignment files. SAM/BAM tracks provide semantic zooming and support both local and remote data sources. This article provides step-by-step instructions for configuring GBrowse to display NGS data.
23478811	Oxidation of graphene 'bow tie' nanofuses for permanent, write-once-read-many data storage devices.
Nanotechnology 20130312 2013Apr5
We have fabricated nanoscale fuses from CVD graphene sheets with a 'bow tie' geometry for write-once-read-many data storage applications. The fuses are programmed using thermal oxidation driven by Joule heating. Fuses that were 250 nm wide with 2.5 μm between contact pads were programmed with average voltages and powers of 4.9 V and 2.1 mW, respectively. The required voltages and powers decrease with decreasing fuse sizes. Graphene shows extreme chemical and electronic stability; fuses require temperatures of about 400 °C for oxidation, indicating that they are excellent candidates for permanent data storage. To further demonstrate this stability, fuses were subjected to applied biases in excess of typical read voltages; stable currents were observed when a voltage of 10 V was applied to the devices in the off state and 1 V in the on state for 90 h each.
23375719	Regional heart motion abnormality detection: an information theoretic approach.
Med Image Anal 20130103 2013Apr
Tracking regional heart motion and detecting the corresponding abnormalities play an essential role in the diagnosis of cardiovascular diseases. Based on functional images, which are subject to noise and segmentation/registration inaccuracies, regional heart motion analysis is acknowledged as a difficult problem and, therefore, incorporation of prior knowledge is desirable to enhance accuracy. Given noisy data and a nonlinear dynamic model to describe myocardial motion, an unscented Kalman smoother is proposed in this study to estimate the myocardial points. Due to the similarity between the statistical information of normal and abnormal heart motions, detecting and classifying abnormality is a challenging problem. We use the Shannon's differential entropy of the distributions of potential classifier features to detect and locate regional heart motion abnormality. A naive Bayes classifier algorithm is constructed from the Shannon's differential entropy of different features to automatically detect abnormal functional regions of the myocardium. Using 174 segmented short-axis magnetic resonance cines obtained from 58 subjects (21 normal and 37 abnormal), the proposed method is quantitatively evaluated by comparison with ground truth classifications by radiologists over 928 myocardial segments. The proposed method performed significantly better than other recent methods, and yielded an accuracy of 86.5% (base), 89.4% (mid-cavity) and 84.5% (apex). The overall classification accuracy was 87.1%. Furthermore, standard kappa statistic comparisons between the proposed method and visual wall motion scoring by radiologists showed that the proposed algorithm can yield a kappa measure of 0.73.
23474957	Automated scan prescription for MR imaging of deformed and normal livers.
Magn Reson Med Sci 20130311 2013Mar25
We propose an automated scan prescription to assess normal and deformed livers and demonstrate its efficacy in normal volunteers and in simulated deformed livers. Our automated scan prescription can be used to identify the upper and lower edges of the liver enables in commonly used axial slice positioning. The liver's upper edge is detected by template matching and finally identified by applying an active shape model to a sagittal projection image. The lower edge is detected using a maximum a posteriori (MAP) probability estimate that utilizes statistical information from a region of interest (ROI) placed in the liver. This places no restraints on liver shape and is therefore effective in assessing a deformed liver. Following institutional review and approval, we tested our method in 45 healthy volunteers. We also used clinical information to simulate deformed livers and tested our method with those datasets offline. We could detect the upper edges within an error range of -3 to 6 mm, even without intensity correction for normal volunteers. Similar detection of the lower edges with maximum 21-mm and 7.84-mm standard deviation for normal volunteers confirmed the superior efficacy of our modified approach for deformed livers to that using our previous method. Clinical use required approximately 10 s' computational time on a Core i5 laptop with 2-GB memory. We propose a method for automated scan prescription in magnetic resonance (MR) imaging of the liver and demonstrate the efficacy of our algorithm for evaluating deformed livers within a practical computation time. Detection of liver edges of various shapes by applying the MAP estimate combined with statistical information from the ROI demonstrated the potential clinical utility of this technique.
23224495	Fuzzy ART-based place recognition for visual loop closure detection.
Biol Cybern 20121206 2013Apr
The automatic place recognition problem is one of the key challenges in SLAM approaches for loop closure detection. Most of the appearance-based solutions to this problem share the idea of image feature extraction, memorization, and matching search. The weakness of these solutions is the storage and computational costs which increase drastically with the environment size. In this regard, the major constraints to overcome are the required visual information storage and the complexity of similarity computation. In this paper, a novel formulation is proposed that allows the computation time reduction while no visual information are stored and matched explicitly. The proposed solution relies on the incremental building of a bio-inspired visual memory using a Fuzzy ART network. This network considers the properties discovered in primate brain. The performance evaluation of the proposed method has been conducted using two datasets representing different large scale outdoor environments. The method has been compared with RatSLAM and FAB-MAP approaches and has demonstrated a decreased time and storage costs with broadly comparable precision recall performance.
22934943	A generalized model and high throughput data analysis system for functional modulation of receptor-agonist systems suitable for use in drug discovery.
Comb. Chem. High Throughput Screen.  2013Mar
Positive allosteric modulators (PAMs) of receptors represent a class of pharmacologic agents having the desirable property of acting only in the presence of cognate ligands. Discovery and optimization of the structure activity relationships of PAMs is complicated by the requirement of a second ligand to manifest their action, and by the need to quantify both affinity and intrinsic efficacy. Multivariate regression analysis is a statistical method capable of simultaneously obtaining affinity and intrinsic efficacy parameters from curve fits of multiple agonist dose-response functions generated in the presence of varying concentrations of PAMs. Capitalizing on the advantages of multivariate regression analysis for PAM optimization requires a theoretical framework and a system that facilitates efficient flow of information from data generation through data analysis, storage, and retrieval. We describe here the experimental design, mathematical model and informatics workflow enabling a multivariate regression approach for rapidly obtaining affinity and intrinsic efficacy values for PAMs in a drug discovery setting.
23452484	Radiology education 2.0--on the cusp of change: part 2. eBooks; file sharing and synchronization tools; websites/teaching files; reference management tools and note taking applications.
Acad Radiol  2013Mar
Increasing use of smartphones and handheld computers is accompanied by a rapid growth in the other related industries. Electronic books have revolutionized the centuries-old conventional books and magazines markets and have simplified publishing by reducing the cost and processing time required to create and distribute any given book. We are now able to read, review, store, and share various types of documents via several electronic tools, many of which are available free of charge. Additionally, this electronic revolution has resulted in an explosion of readily available Internet-based educational resources for the residents and has paved the path for educators to reach out to a larger and more diverse student population.
23357004	Registration of the protein with compact disk.
Biosens Bioelectron 20130102 2013May15
CD-based optico-acoustical biosensor (OAB) was used for detection of various types of proteins represented by bovine serum albumin (BSA), heme-containing myoglobin (Mb), monoclonal antibody against viral protein marker of hepatitis B (anti-HBsAg) and membrane-bound cytochrome P450scc (P450scc). We applied standard compact disc reader (CD-ROM) as an optical analyzer and a standard compact disc (CD) as a biochip containing immobilized protein molecules. This biosensor can translate into a digital code the changes of optical signal from the proteins and their complexes immobilized on the CD surface. Then, the digital code is translated into an acoustic series or, in other words, into a "music of proteins". We demonstrate the use of the OAB for direct detection of proteins with different molecular weights, such as BSA, Mb, P450scc, anti-HBsAg with the concentration detection limit (DL) about 10(-7)M. By signal amplification achieved with autometallography, a higher sensitivity level (DL∼10(-9)M) for the detection of myoglobin was obtained. The method of OAB-detection of proteins is cheap: it requires no special equipment like spectrometers, refractometers and other devices. Due to the fact that acoustic series of the protein complexes antigen/antibody differs from that of single proteins, the OAB-detection is of particular interest for rapid assay in yes/no data type and for home diagnostics. Combination of the OAB with a mass spectrometer allowed the detection and identification of the target proteins fished out directly onto a standard CD surface.
23380445	Nonrandomized studies are not always found even when selection criteria for health systems intervention reviews include them: a methodological study.
J Clin Epidemiol 20130204 2013Apr
Systematic reviews within the Cochrane Effective Practice and Organisation of Care Group (EPOC) can include both randomized and nonrandomized study designs. We explored how many EPOC reviews consider and identify nonrandomized studies, and whether the proportion of nonrandomized studies identified is linked to the review topic. We recorded the study designs considered in 65 EPOC reviews. For reviews that considered nonrandomized studies, we calculated the proportion of identified studies that were nonrandomized and explored whether there were differences in the proportion of nonrandomized studies according to the review topic. Fifty-one (78.5%) reviews considered nonrandomized studies. Forty-six of these reviews found nonrandomized studies, but the proportion varied a great deal (median, 33%; interquartile range, 25--50%). Reviews of health care delivery interventions had lower proportions of nonrandomized studies than those of financial and governance interventions. Most EPOC reviews consider nonrandomized studies, but the degree to which they find them varies. As nonrandomized studies are believed to be at higher risk of bias and their inclusion entails a considerable effort, review authors should consider whether the benefits justify the inclusion of these designs. Research should explore whether it is more useful to consider nonrandomized studies in reviews of some intervention types than others.
22350163	A probabilistic approach to spectral graph matching.
IEEE Trans Pattern Anal Mach Intell  2013Jan
Spectral Matching (SM) is a computationally efficient approach to approximate the solution of pairwise matching problems that are np-hard. In this paper, we present a probabilistic interpretation of spectral matching schemes and derive a novel Probabilistic Matching (PM) scheme that is shown to outperform previous approaches. We show that spectral matching can be interpreted as a Maximum Likelihood (ML) estimate of the assignment probabilities and that the Graduated Assignment (GA) algorithm can be cast as a Maximum a Posteriori (MAP) estimator. Based on this analysis, we derive a ranking scheme for spectral matchings based on their reliability, and propose a novel iterative probabilistic matching algorithm that relaxes some of the implicit assumptions used in prior works. We experimentally show our approaches to outperform previous schemes when applied to exhaustive synthetic tests as well as the analysis of real image sequences.
22392704	Affinity learning with diffusion on tensor product graph.
IEEE Trans Pattern Anal Mach Intell  2013Jan
In many applications, we are given a finite set of data points sampled from a data manifold and represented as a graph with edge weights determined by pairwise similarities of the samples. Often the pairwise similarities (which are also called affinities) are unreliable due to noise or due to intrinsic difficulties in estimating similarity values of the samples. As observed in several recent approaches, more reliable similarities can be obtained if the original similarities are diffused in the context of other data points, where the context of each point is a set of points most similar to it. Compared to the existing methods, our approach differs in two main aspects. First, instead of diffusing the similarity information on the original graph, we propose to utilize the tensor product graph (TPG) obtained by the tensor product of the original graph with itself. Since TPG takes into account higher order information, it is not a surprise that we obtain more reliable similarities. However, it comes at the price of higher order computational complexity and storage requirement. The key contribution of the proposed approach is that the information propagation on TPG can be computed with the same computational complexity and the same amount of storage as the propagation on the original graph. We prove that a graph diffusion process on TPG is equivalent to a novel iterative algorithm on the original graph, which is guaranteed to converge. After its convergence we obtain new edge weights that can be interpreted as new, learned affinities. We stress that the affinities are learned in an unsupervised setting. We illustrate the benefits of the proposed approach for data manifolds composed of shapes, images, and image patches on two very different tasks of image retrieval and image segmentation. With learned affinities, we achieve the bull's eye retrieval score of 99.99 percent on the MPEG-7 shape dataset, which is much higher than the state-of-the-art algorithms. When the data- points are image patches, the NCut with the learned affinities not only significantly outperforms the NCut with the original affinities, but it also outperforms state-of-the-art image segmentation methods.
22450817	Estimating information from image colors: an application to digital cameras and natural scenes.
IEEE Trans Pattern Anal Mach Intell  2013Jan
The colors present in an image of a scene provide information about its constituent elements. But the amount of information depends on the imaging conditions and on how information is calculated. This work had two aims. The first was to derive explicitly estimators of the information available and the information retrieved from the color values at each point in images of a scene under different illuminations. The second was to apply these estimators to simulations of images obtained with five sets of sensors used in digital cameras and with the cone photoreceptors of the human eye. Estimates were obtained for 50 hyperspectral images of natural scenes under daylight illuminants with correlated color temperatures 4,000, 6,500, and 25,000 K. Depending on the sensor set, the mean estimated information available across images with the largest illumination difference varied from 15.5 to 18.0 bits and the mean estimated information retrieved after optimal linear processing varied from 13.2 to 15.5 bits (each about 85 percent of the corresponding information available). With the best sensor set, 390 percent more points could be identified per scene than with the worst. Capturing scene information from image colors depends crucially on the choice of camera sensors.
23461001	DNA profiles, computer searches, and the Fourth Amendment.
Duke Law J  2013
Pursuant to federal statutes and to laws in all fifty states, the United States government has assembled a database containing the DNA profiles of over eleven million citizens. Without judicial authorization, the government searches each of these profiles one-hundred thousand times every day, seeking to link database subjects to crimes they are not suspected of committing. Yet, courts and scholars that have addressed DNA databasing have focused their attention almost exclusively on the constitutionality of the government's seizure of the biological samples from which the profiles are generated. This Note fills a gap in the scholarship by examining the Fourth Amendment problems that arise when the government searches its vast DNA database. This Note argues that each attempt to match two DNA profiles constitutes a Fourth Amendment search because each attempted match infringes upon database subjects' expectations of privacy in their biological relationships and physical movements. The Note further argues that database searches are unreasonable as they are currently conducted, and it suggests an adaptation of computer-search procedures to remedy the constitutional deficiency.
23462072	Recruitment to online therapies for depression: pilot cluster randomized controlled trial.
J. Med. Internet Res. 20130305 2013
Raising awareness of online cognitive behavioral therapy (CBT) could benefit many people with depression, but we do not know how purchasing online advertising compares to placing free links from relevant local websites in increasing uptake. To pilot a cluster randomized controlled trial (RCT) comparing purchase of Google AdWords with placing free website links in raising awareness of online CBT resources for depression in order to better understand research design issues. We compared two online interventions with a control without intervention. The pilot RCT had 4 arms, each with 4 British postcode areas: (A) geographically targeted AdWords, (B) adverts placed on local websites by contacting website owners and requesting links be added, (C) both interventions, (D) control. Participants were directed to our research project website linking to two freely available online CBT resource sites (Moodgym and Living Life To The Full (LLTTF)) and two other depression support sites. We used data from (1) AdWords, (2) Google Analytics for our project website and for LLTTF, and (3) research project website. We compared two outcomes: (1) numbers with depression accessing the research project website, and then chose an onward link to one of the two CBT websites, and (2) numbers registering with LLTTF. We documented costs, and explored intervention and assessment methods to make general recommendations to inform researchers aiming to use similar methodologies in future studies. Trying to place local website links appeared much less cost effective than AdWords and although may prove useful for service delivery, was not worth pursuing in the context of the current study design. Our AdWords intervention was effective in recruiting people to the project website but our location targeting "leaked" and was not as geographically specific as claimed. The impact on online CBT was also diluted by offering participants other choices of destinations. Measuring the impact on LLTTF use was difficult as the total number using LLTTF was less than 5% of all users and record linkage across websites was impossible. Confounding activity may have resulted in some increase in registrations in the control arm. Practitioners should consider online advertising to increase uptake of online therapy but need to check its additional value. A cluster RCT using location targeted adverts is feasible and this research design provides the best evidence of cost-effectiveness. Although our British pilot study is limited to online CBT for depression, a cluster RCT with similar design would be appropriate for other online treatments and countries and our recommendations may apply. They include ways of dealing with possible contamination (buffer zones and AdWords techniques), confounding factors (large number of clusters), advertising dose (in proportion to total number of users), record linkage (landing within target website), and length of study (4-6 months). clinicaltrials.gov (Registration No. NCT01469689); http://clinicaltrials.gov/ct2/show/NCT01469689 (Archived by WebCite at http://www.webcitation.org/6EtTthDOp).
22332781	A nationwide evaluation on electronic medication-related information provided by hospital websites.
J Eval Clin Pract 20120214 2013Apr
RATIONAL, AIMS AND OBJECTIVES: The aim of this study was to describe the characteristics of electronic medication-related information (e-MRI) via Internet offered by the hospital settings in Taiwan. The structured Internet search and comprehensive review were performed on the most commonly used search engines in Taiwan. The assessment checklists were developed to describe the characteristics of general, e-MRI in the years 2008 and 2010, and specific digoxin information in 2011 based on the operational definitions derived from other studies. The descriptive analyses and chi-square tests for the retrieved data were performed. With approximately 15% of hospital settings providing general, e-MRI on their websites, their content varied but was not statistically significant, and different among the providers from different levels of hospitals and in different years. More medical centres provided the information with the updated dates and contact approaches than the smaller scale hospitals. Little was found about reference citation and authorships for those general, e-MRI websites. More medical centres created the accesses to search for the individual prescription in the corresponding settings and the specific information about digoxin storage. However, more district hospitals provided the precaution and dosage form information about digoxin. The providers to offer the e-MRI via hospital websites in Taiwan could be more responsible for its update, authorship and evidence. Further, the provision of electronic medication-related information via the Internet should be regularly examined or audited by the neutral personnel or organizations to ensure its quality.
23227930	Medical retrieval and needs of infants with bronchiolitis: an analysis by gestational age.
J Paediatr Child Health 20121211 2013Mar
Viral bronchiolitis is the most common lower respiratory tract infection in children less than 12 months of age. Prematurity is an independent risk factor for disease severity. Many infected infants require hospitalisation and those living in regional centres frequently require transfer to metropolitan hospitals capable of providing assisted ventilation. We reviewed infants with bronchiolitis transported by the Victorian Newborn Emergency Transport Service between January 2003 and June 2007. We compared the clinical presentation and treatment required by infants born preterm with those of their term counterparts. Of the 192 infants transported, 92 were born preterm. Preterm infants were younger at time of transport (mean post-menstrual age 41 weeks vs. 45 weeks) and were more likely to require invasive ventilation (60% vs. 32%, P &lt; 0.001) and to receive a fluid bolus (47% vs. 34%, P = 0.04) when compared with infants who had been born at term. Apnoea, either as a presenting symptom or in combination with respiratory distress, was more common in the preterm group (70% vs. 36%, P &lt; 0.001). Higher illness severity should be anticipated in ex-preterm infants who present with bronchiolitis. Preterm infants with bronchiolitis are more likely to require invasive ventilation and fluid resuscitation than term infants, suggesting the need for a lower threshold for referral and medical retrieval.
23481774	Forward error correction supported 150 Gbit/s error-free wavelength conversion based on cross phase modulation in silicon.
Opt Express  2013Feb11
We build a forward error correction (FEC) module and implement it in an optical signal processing experiment. The experiment consists of two cascaded nonlinear optical signal processes, 160 Gbit/s all optical wavelength conversion based on the cross phase modulation (XPM) in a silicon nanowire and subsequent 160 Gbit/s-to-10 Gbit/s demultiplexing in a highly nonlinear fiber (HNLF). The XPM based all optical wavelength conversion in silicon is achieved by off-center filtering the red shifted sideband on the CW probe. We thoroughly demonstrate and verify that the FEC code operates correctly after the optical signal processing, yielding truly error-free 150 Gbit/s (excl. overhead) optically signal processed data after the two cascaded nonlinear processes.
23481789	Simultaneous detection of 10-Gbit/s QPSK × 2-ch. Fourier-encoded synchronous OCDM signals with digital coherent receiver.
Opt Express  2013Feb11
We experimentally demonstrate the simultaneous detection of 10-Gbit/s quadrature phase shift keying (QPSK) × 2-channel Fourier-encoded synchronous optical code division multiplexing (FE-SOCDM) signals using a digital coherent receiver, for the first time. First, we analytically verify that simultaneous detection can be achieved with an N-point discrete Fourier transform (DFT) using digital signal processing (DSP) because the N-channel Fourier encoding corresponds to an N × N inverse DFT, then the operation is experimentally confirmed. Simultaneous detection of 10-Gbit/s QPSK × 2-channel FE-SOCDM signals is evaluated. The proposed scheme dramatically expands the capability of OCDM systems.
23481823	Self-referential holography and its applications to data storage and phase-to-intensity conversion.
Opt Express  2013Feb11
Holographic recording methods require the use of a reference beam that is coherent with the signal beam carrying the information to be recorded. In this paper, we propose self-referential holography (SRH) for holographic recording without the use of a reference beam. SRH can realize purely one-beam holographic recording by considering the signal beam itself as the reference beam. The readout process in SRH is based on energy transfer by inter-pixel interference in holographic diffraction, which depends on the spatial phase difference between the recorded phase and the readout phase. The phase-modulated recorded signal is converted into an intensity-modulated beam that can be easily detected using a conventional image sensor. SRH can be used effectively for holographic data storage and phase-to-intensity conversion.
23486826	A qualitative case study of LifeGuide: users' experiences of software for developing Internet-based behaviour change interventions.
Health Informatics J  2013Mar
Previously, behavioural scientists seeking to create Internet-based behaviour change interventions have had to rely on computer scientists to actually develop and modify web interventions. The LifeGuide software was designed to enable behavioural researchers to develop and adapt Internet-based behavioural interventions themselves. This article reports a qualitative case study of users' experiences and perceptions of the LifeGuide software. The aim was to explore users' experiences and their perceptions of the benefits and limitations of this approach to intervention development. Twenty LifeGuide users took part in semi-structured interviews and one provided feedback via email. Thematic analysis identified three overarching themes: 'Recognising LifeGuide's potential', 'I'm not a programmer' and 'Knowledge sharing - the future of LifeGuide'. Users valued LifeGuide's potential to allow them to flexibly develop and modify interventions at little cost. However, users noted that their lack of programming experience meant that they needed to learn new skills for using the software, and they varied in the extent to which they felt able to develop interventions without any input from programmers. Respondents saw the potential of using the LifeGuide Community Website to share technical support and examples of intervention components to support their use of LifeGuide.
23485880	Storing and using health data in a virtual private cloud.
J. Med. Internet Res. 20130313 2013
Electronic health records are being adopted at a rapid rate due to increased funding from the US federal government. Health data provide the opportunity to identify possible improvements in health care delivery by applying data mining and statistical methods to the data and will also enable a wide variety of new applications that will be meaningful to patients and medical professionals. Researchers are often granted access to health care data to assist in the data mining process, but HIPAA regulations mandate comprehensive safeguards to protect the data. Often universities (and presumably other research organizations) have an enterprise information technology infrastructure and a research infrastructure. Unfortunately, both of these infrastructures are generally not appropriate for sensitive research data such as HIPAA, as they require special accommodations on the part of the enterprise information technology (or increased security on the part of the research computing environment). Cloud computing, which is a concept that allows organizations to build complex infrastructures on leased resources, is rapidly evolving to the point that it is possible to build sophisticated network architectures with advanced security capabilities. We present a prototype infrastructure in Amazon's Virtual Private Cloud to allow researchers and practitioners to utilize the data in a HIPAA-compliant environment.
23486003	Single-source tumor documentation - reusing oncology data for different purposes.
Onkologie 20130221 2013
We present a path towards single-source tumor documentation established at the Comprehensive Cancer Center Erlangen-Nürnberg (CCC-EN). Our goal was to derive data for cancer quality assurance and certification, cancer registry documentation and cancer research directly from routine care documentation. Therefore, clinical documentation activities were analyzed and a cancer data superset, containing these required elements, was developed. This superset was then split into appropriate clinical documentation packages, and the existing information technology infrastructure was analyzed and adapted to accommodate those documentation packages. A clinical documentation package is the amount of cancer-relevant data that can be captured within a clinical encounter. This grouping of data enables integration into existing clinical documentation workflows. We present examples in which single-source tumor documentation has been successfully established at the CCC-EN. The resulting cancer documentation reference model is described and its transferability to other institutions discussed.
23408979	Potential theory for directed networks.
PLoS ONE 20130211 2013
Uncovering factors underlying the network formation is a long-standing challenge for data mining and network analysis. In particular, the microscopic organizing principles of directed networks are less understood than those of undirected networks. This article proposes a hypothesis named potential theory, which assumes that every directed link corresponds to a decrease of a unit potential and subgraphs with definable potential values for all nodes are preferred. Combining the potential theory with the clustering and homophily mechanisms, it is deduced that the Bi-fan structure consisting of 4 nodes and 4 directed links is the most favored local structure in directed networks. Our hypothesis receives strongly positive supports from extensive experiments on 15 directed networks drawn from disparate fields, as indicated by the most accurate and robust performance of Bi-fan predictor within the link prediction framework. In summary, our main contribution is twofold: (i) We propose a new mechanism for the local organization of directed networks; (ii) We design the corresponding link prediction algorithm, which can not only testify our hypothesis, but also find out direct applications in missing link prediction and friendship recommendation.
23410287	Finite-time erasing of information stored in fermionic bits.
Phys Rev E Stat Nonlin Soft Matter Phys 20130111 2013Jan
We address the issue of minimizing the heat generated when erasing the information stored in an array of quantum dots in finite time. We identify the fundamental limitations and trade-offs involved in this process and analyze how a feedback operation can help improve it.
23410306	Bivariate measure of redundant information.
Phys Rev E Stat Nonlin Soft Matter Phys 20130123 2013Jan
We define a measure of redundant information based on projections in the space of probability distributions. Redundant information between random variables is information that is shared between those variables. But, in contrast to mutual information, redundant information denotes information that is shared about the outcome of a third variable. Formalizing this concept, and being able to measure it, is required for the non-negative decomposition of mutual information into redundant and synergistic information. Previous attempts to formalize redundant or synergistic information struggle to capture some desired properties. We introduce a new formalism for redundant information and prove that it satisfies all the properties necessary outlined in earlier work, as well as an additional criterion that we propose to be necessary to capture redundancy. We also demonstrate the behavior of this new measure for several examples, compare it to previous measures, and apply it to the decomposition of transfer entropy.
23410347	Memory function of turbulent fluctuations in soft-mode turbulence.
Phys Rev E Stat Nonlin Soft Matter Phys 20130123 2013Jan
Modal relaxation dynamics has been observed experimentally to clarify statistical-physical properties of soft-mode turbulence, the spatiotemporal chaos observed in homeotropically aligned nematic liquid crystals. We found a dual structure, dynamical crossover associated with violation of time-reversal invariance, the corresponding time scales satisfying a dynamical scaling law. To specify the origin of the dual structure, the memory function due to nonthermal fluctuations has been defined by a projection-operator method and obtained numerically using experimental results. The results of the memory function suggest that the nonthermal fluctuations can be divided into Markov and non-Markov contributions; the latter is called the turbulent fluctuation (TF). Consequently, the relaxation dynamics is separated into three characteristic stages: bare-friction, early, and late stages. If the dissipation due to TFs dominates over that of the Markov contribution, the bare-friction stage contracts; the early and late stages then configure the dual structure. The memory effect due to TFs results in a time-reversible relaxation at the early stage, and the disappearance of the memory by turbulent mixing leads to a simple exponential relaxation at the late stage. Furthermore, the memory effect due to TFs is shown to originate from characteristic spatial coherency called the patch structure.
22497772	Teleradiology with uncompressed digital mammograms: clinical assessment.
Eur J Radiol 20120411 2013Mar
The purpose of our study was to demonstrate the feasibility of sending uncompressed digital mammograms in a teleradiologic setting without loss of information by comparing image quality, lesion detection, and BI-RADS assessment. CDMAM phantoms were sent bidirectionally to two hospitals via the network. For the clinical aspect of the study, 200 patients were selected based on the BI-RAD system: 50% BI-RADS I and II; and 50% BI-RADS IV and V. Two hundred digital mammograms (800 views) were sent to two different institutions via a teleradiology network. Three readers evaluated those 200 mammography studies at institution 1 where the images originated, and in the two other institutions (institutions 2 and 3) where the images were sent. The readers assessed image quality, lesion detection, and BI-RADS classification. Automatic readout showed that CDMAM image quality was identical before and after transmission. The image quality of the 200 studies (total 600 mammograms) was rated as very good or good in 90-97% before and after transmission. Depending on the institution and the reader, only 2.5-9.5% of all studies were rated as poor. The congruence of the readers with respect to the final BI-RADS assessment ranged from 90% and 91% at institution 1 vs. institution 2, and from 86% to 92% at institution 1 vs. institution 3. The agreement was even higher for conformity of content (BI-RADS I or II and BI-RADS IV or V). Reader agreement in the three different institutions with regard to the detection of masses and calcifications, as well as BI-RADS classification, was very good (κ: 0.775-0.884). Results for interreader agreement were similar. Uncompressed digital mammograms can be transmitted to different institutions with different workstations, without loss of information. The transmission process does not significantly influence image quality, lesion detection, or BI-RADS rating.
23419858	The read-write Linked Data Web.
Philos Trans A Math Phys Eng Sci 20130218 2013Mar28
This paper discusses issues that will affect the future development of the Web, either increasing its power and utility, or alternatively suppressing its development. It argues for the importance of the continued development of the Linked Data Web, and describes the use of linked open data as an important component of that. Second, the paper defends the Web as a read-write medium, and goes on to consider how the read-write Linked Data Web could be achieved.
23428449	Extended pie menus for immersive virtual environments.
IEEE Trans Vis Comput Graph  2013Apr
Pie menus are a well-known technique for interacting with 2D environments and so far a large body of research documents their usage and optimizations. Yet, comparatively little research has been done on the usability of pie menus in immersive virtual environments (IVEs). In this paper we reduce this gap by presenting an implementation and evaluation of an extended hierarchical pie menu system for IVEs that can be operated with a six-degrees-of-freedom input device. Following an iterative development process, we first developed and evaluated a basic hierarchical pie menu system. To better understand how pie menus should be operated in IVEs, we tested this system in a pilot user study with 24 participants and focus on item selection. Regarding the results of the study, the system was tweaked and elements like check boxes, sliders, and color map editors were added to provide extended functionality. An expert review with five experts was performed with the extended pie menus being integrated into an existing VR application to identify potential design issues. Overall results indicated high performance and efficient design.
23445565	Fast probabilistic file fingerprinting for big data.
BMC Genomics 20130215 2013
Biological data acquisition is raising new challenges, both in data analysis and handling. Not only is it proving hard to analyze the data at the rate it is generated today, but simply reading and transferring data files can be prohibitively slow due to their size. This primarily concerns logistics within and between data centers, but is also important for workstation users in the analysis phase. Common usage patterns, such as comparing and transferring files, are proving computationally expensive and are tying down shared resources. We present an efficient method for calculating file uniqueness for large scientific data files, that takes less computational effort than existing techniques. This method, called Probabilistic Fast File Fingerprinting (PFFF), exploits the variation present in biological data and computes file fingerprints by sampling randomly from the file instead of reading it in full. Consequently, it has a flat performance characteristic, correlated with data variation rather than file size. We demonstrate that probabilistic fingerprinting can be as reliable as existing hashing techniques, with provably negligible risk of collisions. We measure the performance of the algorithm on a number of data storage and access technologies, identifying its strengths as well as limitations. Probabilistic fingerprinting may significantly reduce the use of computational resources when comparing very large files. Utilisation of probabilistic fingerprinting techniques can increase the speed of common file-related workflows, both in the data center and for workbench analysis. The implementation of the algorithm is available as an open-source tool named pfff, as a command-line tool as well as a C library. The tool can be downloaded from http://biit.cs.ut.ee/pfff.
23323520	Capturing intra-operative safety information using surgical wikis.
Inform Health Soc Care 20130116 2013Mar
Expert surgeons use a mass of intra-operative information, as well as pre- and post-operative information to complete operations safely. Trainees acquired this intra-operative knowledge at the operating table, now largely diminished by the working time directive. Wikis offer unexplored approaches to capturing and disseminating expert knowledge to further promote safer surgery for the trainee. Grafting an abdominal aortic aneurysm represents a potentially high-risk operation demanding extreme safety measures. Operative details, presented on a surgical wiki in the form of a script and content analysed to classify types of safety information. The intra-operative part of the script contained 2,743 items of essential surgical information, comprising 21 sections, 405 steps and 2,317 items of back-up information; 155 (5.7%) of them were also specific intra-operative safety checks. Best case scenarios consisted of 1,077 items of intra-operative information, 69 of which were safety checks. Worse case and rare scenarios required a further 1,666 items of information, including 86 safety checks. Wikis are relevant to surgical practice specifically as a platform for knowledge sharing and optimising the available operating time of trainees, as a very large amount of minutely detailed information essential for a safe major operation can be captured.
23323596	Semantics-driven modelling of user preferences for information retrieval in the biomedical domain.
Inform Health Soc Care 20130116 2013Mar
A large amount of biomedical and genomic data are currently available on the Internet. However, data are distributed into heterogeneous biological information sources, with little or even no organization. Semantic technologies provide a consistent and reliable basis with which to confront the challenges involved in the organization, manipulation and visualization of data and knowledge. One of the knowledge representation techniques used in semantic processing is the ontology, which is commonly defined as a formal and explicit specification of a shared conceptualization of a domain of interest. The work presented here introduces a set of interoperable algorithms that can use domain and ontological information to improve information-retrieval processes. This work presents an ontology-based information-retrieval system for the biomedical domain. This system, with which some experiments have been carried out that are described in this paper, is based on the use of domain ontologies for the creation and normalization of lightweight ontologies that represent user preferences in a determined domain in order to improve information-retrieval processes.
23193455	Efficient method for content reconstruction with self-embedding.
IEEE Trans Image Process 20121116 2013Mar
This paper presents a new model of the content reconstruction problem in self-embedding systems, based on an erasure communication channel. We explain why such a model is a good fit for this problem, and how it can be practically implemented with the use of digital fountain codes. The proposed method is based on an alternative approach to spreading the reference information over the whole image, which has recently been shown to be of critical importance in the application at hand. Our paper presents a theoretical analysis of the inherent restoration trade-offs. We analytically derive formulas for the reconstruction success bounds, and validate them experimentally with Monte Carlo simulations and a reference image authentication system. We perform an exhaustive reconstruction quality assessment, where the presented reference scheme is compared to five state-of-the-art alternatives in a common evaluation scenario. Our paper leads to important insights on how self-embedding schemes should be constructed to achieve optimal performance. The reference authentication system designed according to the presented principles allows for high-quality reconstruction, regardless of the amount of the tampered content. The average reconstruction quality, measured on 10000 natural images is 37 dB, and is achievable even when 50% of the image area becomes tampered.
23149160	Knowledge-based personalized search engine for the Web-based Human Musculoskeletal System Resources (HMSR) in biomechanics.
J Biomed Inform 20121110 2013Feb
Human musculoskeletal system resources of the human body are valuable for the learning and medical purposes. Internet-based information from conventional search engines such as Google or Yahoo cannot response to the need of useful, accurate, reliable and good-quality human musculoskeletal resources related to medical processes, pathological knowledge and practical expertise. In this present work, an advanced knowledge-based personalized search engine was developed. Our search engine was based on a client-server multi-layer multi-agent architecture and the principle of semantic web services to acquire dynamically accurate and reliable HMSR information by a semantic processing and visualization approach. A security-enhanced mechanism was applied to protect the medical information. A multi-agent crawler was implemented to develop a content-based database of HMSR information. A new semantic-based PageRank score with related mathematical formulas were also defined and implemented. As the results, semantic web service descriptions were presented in OWL, WSDL and OWL-S formats. Operational scenarios with related web-based interfaces for personal computers and mobile devices were presented and analyzed. Functional comparison between our knowledge-based search engine, a conventional search engine and a semantic search engine showed the originality and the robustness of our knowledge-based personalized search engine. In fact, our knowledge-based personalized search engine allows different users such as orthopedic patient and experts or healthcare system managers or medical students to access remotely into useful, accurate, reliable and good-quality HMSR information for their learning and medical purposes.
23229062	Assuring access to data for chemical evaluations.
Environ. Health Perspect. 20121205 2013Feb
A database for studies used for U.S. Environmental Protection Agency (EPA) pesticide and chemical reviews would be an excellent resource for increasing transparency and improving systematic assessments of pesticides and chemicals. There is increased demand for disclosure of raw data from studies used by the U.S. EPA in these reviews. Because the Information Quality Act (IQA) of 2001 provides an avenue for request of raw data, we reviewed all IQA requests to the U.S. EPA in 2002-2012 and the U.S. EPA's responses. We identified other mechanisms to access such data: public access databases, the Freedom of Information Act (FOIA), and reanalysis by a third party. Only two IQA requests to the U.S. EPA were for raw data. Both of these were fulfilled under FOIA, not the IQA. Barriers to the U.S. EPA's proactive collection of all such data include costs to the U.S. EPA and researchers, significant time burdens for researchers, and major regulatory delays. The U.S. EPA regulatory authority in this area is weak, especially for research conducted in the past, not funded by the U.S. government, and/or conducted abroad. The U.S. EPA is also constrained by industry confidential business information (CBI) claims for regulatory testing data under U.S. chemical and pesticide laws. The National Institutes of Health Clinical Trials database systematically collects statistical data about clinical trials but not raw data; this database may be a model for data from studies of chemicals and pesticides. A database that registers studies and obtains systematic sets of parameters and results would be more feasible than a system that attempts to make all raw data available proactively. Such a proposal would not obviate rights under the IQA to obtain raw data at a later point.
23389137	Per-symbol-based digital back-propagation approach for PDM-CO-OFDM transmission systems.
Opt Express  2013Jan28
For polarization-division-multiplexing coherent optical orthogonal frequency division multiplexing (PDM-CO-OFDM) systems, we propose a per-symbol-based digital back-propagation (DBP) approach which, after cyclic prefix removal, conducts DBP for each OFDM symbol. Compared with previous DBP, this new proposal avoids the use of inefficient overlap-and-add operation and saves one fast Fourier transform (FFT) module, therefore simplifying the hardware implementation. Transmitting a 16-QAM, 42.8-Gb/s PDM-CO-OFDM signal over 960-km standard single mode fiber (SSMF), we compare the previous and the proposed DBP approaches with different receiver's sampling rates and different step lengths in each DBP iteration, and found that the proposed DBP can achieve a similar performance as that of the previous DBP while enjoying a simpler implementation. We have also specifically introduced a small self-phase modulation (SPM) model for DBP and demonstrated its feasibility with the same experimental setup.
23389138	25 Tb/s transmission over 5,530 km using 16QAM at 5.2 b/s/Hz spectral efficiency.
Opt Express  2013Jan28
We transmit 250x100G PDM RZ-16QAM channels with 5.2 b/s/Hz spectral efficiency over 5,530 km using single-stage C-band EDFAs equalized to 40 nm. We use single parity check coded modulation and all channels are decoded with no errors after iterative decoding between a MAP decoder and an LDPC based FEC algorithm. We also observe that the optimum power spectral density is nearly independent of SE, signal baud rate or modulation format in a dispersion uncompensated system.
23389171	Free space laser communication experiments from Earth to the Lunar Reconnaissance Orbiter in lunar orbit.
Opt Express  2013Jan28
Laser communication and ranging experiments were successfully conducted from the satellite laser ranging (SLR) station at NASA Goddard Space Flight Center (GSFC) to the Lunar Reconnaissance Orbiter (LRO) in lunar orbit. The experiments used 4096-ary pulse position modulation (PPM) for the laser pulses during one-way LRO Laser Ranging (LR) operations. Reed-Solomon forward error correction codes were used to correct the PPM symbol errors due to atmosphere turbulence and pointing jitter. The signal fading was measured and the results were compared to the model.
23389187	Optical steganography based on amplified spontaneous emission noise.
Opt Express  2013Jan28
We propose and experimentally demonstrate an optical steganography method in which a data signal is transmitted using amplified spontaneous emission (ASE) noise as a carrier. The ASE serving as a carrier for the private signal has an identical frequency spectrum to the existing noise generated by the Erbium doped fiber amplifiers (EDFAs) in the transmission system. The system also carries a conventional data channel that is not private. The so-called "stealth" or private channel is well-hidden within the noise of the system. Phase modulation is used for both the stealth channel and the public channel. Using homodyne detection, the short coherence length of the ASE ensures that the stealth signal can only be recovered if the receiver closely matches the delay-length difference, which is deliberately changed in a dynamic fashion that is only known to the transmitter and its intended receiver.
23389189	Delivery of video-on-demand services using local storages within passive optical networks.
Opt Express  2013Jan28
At present, distributed storage systems have been widely studied to alleviate Internet traffic build-up caused by high-bandwidth, on-demand applications. Distributed storage arrays located locally within the passive optical network were previously proposed to deliver Video-on-Demand services. As an added feature, a popularity-aware caching algorithm was also proposed to dynamically maintain the most popular videos in the storage arrays of such local storages. In this paper, we present a new dynamic bandwidth allocation algorithm to improve Video-on-Demand services over passive optical networks using local storages. The algorithm exploits the use of standard control packets to reduce the time taken for the initial request communication between the customer and the central office, and to maintain the set of popular movies in the local storage. We conduct packet level simulations to perform a comparative analysis of the Quality-of-Service attributes between two passive optical networks, namely the conventional passive optical network and one that is equipped with a local storage. Results from our analysis highlight that strategic placement of a local storage inside the network enables the services to be delivered with improved Quality-of-Service to the customer. We further formulate power consumption models of both architectures to examine the trade-off between enhanced Quality-of-Service performance versus the increased power requirement from implementing a local storage within the network.
23389231	Fast dispersion estimation in coherent optical 16QAM fast OFDM systems.
Opt Express  2013Jan28
Fast channel estimation is crucial to increase the payload efficiency which is of particular importance for optical packet networks. In this paper, we propose a novel least-square based dispersion estimation method in coherent optical fast OFDM (F-OFDM) systems. Additionally, we experimentally demonstrate for the first time a 37.5 Gb/s 16QAM coherent F-OFDM system with 480 km transmission using the proposed scheme. The results show that this method outperforms the conventional channel estimation methods in minimizing the overhead load. A single training symbol can achieve near-optimum channel estimation without any prior information of the transmission distance. This makes optical F-OFDM a very promising scheme for the future burst-mode applications.
23388258	A "Realist Review" approach to e-health: the case of type 2 diabetes in youth.
Stud Health Technol Inform  2013
As e-health technology becomes more ubiquitous in our health and health care environments, a flexible, robust understanding of what works and under what circumstances is needed. Traditional meta-analyses tell us how frequently a technology has worked for previous populations, but not why. Realist Reviews can contribute to understanding why interventions work and by extension how results of past studies can be applied to emerging health challenges. The utility of such a method is considered in e-health interventions to address the serious growing challenge of Type 2 diabetes and metabolic syndrome in young people.
23388262	Methodological approaches to comparing information about bicycle accidents internationally: a case study involving Canada and Germany.
Stud Health Technol Inform  2013
The use of bicycles as a mean of healthy and eco-friendly transportation is currently actively promoted in many industrialized countries. However, the number of severe bicycle accidents rose significantly in Germany and Canada in 2011. In order to identify risk factors for bicycle accidents and possible means of prevention, a study was initiated that analyses bicycle accidents from selected regions in both countries. Due to different healthcare systems and regulations, the data must be selected in different ways in each country before it can be analyzed. Data is collected by means of questionnaires in Germany and using hybrid electronic-paper records in Canada. Using this method, all relevant data can be collected in both countries.
23388263	The development of a standardized software platform to support provincial population-based cancer outcomes units for multiple tumour sites: OaSIS - Outcomes and Surveillance Integration System.
Stud Health Technol Inform  2013
Understanding the impact of treatment policies on patient outcomes is essential in improving all aspects of patient care. The BC Cancer Agency is a provincial program that provides cancer care on a population basis for 4.5 million residents. The Lung and Head &amp;amp; Neck Tumour Groups planned to create a generic yet comprehensive software infrastructure that could be used by all Tumour Groups: the Outcomes and Surveillance Integration System (OaSIS). The primary goal was the development of an integrated database that will amalgamate existing provincial data warehouses of varying datasets and provide the infrastructure to support additional routes of data entry, including clinicians from multiple-disciplines, quality of life and survivorship data from patients, and three dimensional dosimetric information archived from the radiotherapy planning and delivery systems. The primary goal is to be able to capture any data point related to patient characteristics, disease factors, treatment details and survivorship, from the point of diagnosis onwards. Through existing and novel data-mining techniques, OaSIS will support unique population based research activities by promoting collaborative interactions between the research centre, clinical activities at the cancer treatment centres and other institutions. This will also facilitate initiatives to improve patient outcomes, decision support in achieving operational efficiencies and an environment that supports knowledge generation.
23388305	An initial, qualitative investigation of patient-centered education in dentistry.
Stud Health Technol Inform  2013
Patient education plays an important role in the delivery of dental care. Current evidence suggests that the emergence of the Internet and other electronic resources are significantly influencing how patients learn about their healthcare. We conducted a qualitative inquiry using a combination of interviews with patients and clinicians, and direct observation of patient education episodes, to begin identifying requirements for customized, patient-centered approaches to education at the point of care. Most patients in our study felt comfortable with the amount and method of education during the dental visit, but 38% sought additional information on the Internet. Dentists and their team members provided patient education mostly verbally, supported by media such as radiographs, images and models. Electronic means, especially the Internet, were little used. Patient education occupied a significant portion of the time of initial comprehensive examination (29%) and routine (7%) dental visits. A deeper understanding of patient knowledge deficits and information needs will be needed to design effective educational interventions. Patient education should be meaningfully integrated into the workflow shared by dentists, their team members and patients, in order to maximize its outcomes.
23388317	Developing a multivariate electronic medical record integration model for primary health care.
Stud Health Technol Inform  2013
This paper describes the development of a multivariate electronic medical record (EMR) integration model for the primary health care setting. Our working hypothesis is that an integrated EMR is associated with high quality primary health care. Our assumption is that EMR integration should be viewed as a form of complex intervention with multiple interacting components that can impact the quality of care. Depending on how well the EMR is integrated in the practice setting, one can expect a corresponding change in the quality of care as measured through a set of primary health care quality indicators. To test the face validity of this model, a Delphi study is being planned where health care providers and information technology professionals involved with EMR adoption are polled for their feedback. This model has the potential to quantify and explain the factors that influence successful EMR integration to improve primary health care.
23396301	CrossTope: a curate repository of 3D structures of immunogenic peptide: MHC complexes.
Database (Oxford) 20130208 2013
The CrossTope is a highly curate repository of three-dimensional structures of peptide:major histocompatibility complex (MHC) class I complexes (pMHC-I). The complexes hosted by this databank were obtained in protein databases and by large-scale in silico construction of pMHC-I structures, using a new approach developed by our group. At this moment, the database contains 182 'non-redundant' pMHC-I complexes from two human and two murine alleles. A web server provides interface for database query. The user can download (i) structure coordinate files and (ii) topological and charges distribution maps images from the T-cell receptor-interacting surface of pMHC-I complexes. The retrieved structures and maps can be used to cluster similar epitopes in cross-reactivity approaches, to analyse viral escape mutations in a structural level or even to improve the immunogenicity of tumour antigens. Database URL: http://www.crosstope.com.br.
23396322	Using the OntoGene pipeline for the triage task of BioCreative 2012.
Database (Oxford) 20130209 2013
In this article, we describe the architecture of the OntoGene Relation mining pipeline and its application in the triage task of BioCreative 2012. The aim of the task is to support the triage of abstracts relevant to the process of curation of the Comparative Toxicogenomics Database. We use a conventional information retrieval system (Lucene) to provide a baseline ranking, which we then combine with information provided by our relation mining system, in order to achieve an optimized ranking. Our approach additionally delivers domain entities mentioned in each input document as well as candidate relationships, both ranked according to a confidence score computed by the system. This information is presented to the user through an advanced interface aimed at supporting the process of interactive curation. Thanks, in particular, to the high-quality entity recognition, the OntoGene system achieved the best overall results in the task.
23400131	Towards interactive narrative medicine.
Stud Health Technol Inform  2013
Interactive Storytelling technologies have attracted significant interest in the field of simulation and serious gaming for their potential to provide a principled approach to improve user engagement in training scenarios. In this paper, we explore the use of Interactive Storytelling to support Narrative Medicine as a reflective practice. We describe a workflow for the generation of virtual narratives from high-level descriptions of patients' experiences as perceived by physicians, which can help to objectivize such perceptions and support various forms of analysis.
23405048	Expert searcher, teacher, content manager, and patient advocate: an exploratory study of clinical librarian roles.
J Med Libr Assoc  2013Jan
The research explored the roles of practicing clinical librarians embedded in a patient care team. Six clinical librarians from Canada and one from the United States were interviewed to elicit detailed descriptions of their clinical roles and responsibilities and the context in which these were performed. Participants were embedded in a wide range of clinical service areas, working with a diverse complement of health professionals. As clinical librarians, participants wore many hats, including expert searcher, teacher, content manager, and patient advocate. Unique aspects of how these roles played out included a sense of urgency surrounding searching activities, the broad dissemination of responses to clinical questions, and leverage of the roles of expert searcher, teacher, and content manager to advocate for patients. Detailed role descriptions of clinical librarians embedded in patient care teams suggest possible new practices for existing clinical librarians, provide direction for training new librarians working in patient care environments, and raise awareness of the clinical librarian specialty among current and budding health information professionals.
23210850	Multi-aspect candidates for repositioning: data fusion methods using heterogeneous information sources.
Curr. Med. Chem.  2013
Drug repositioning, an innovative therapeutic application of an old drug, has received much attention as a particularly costeffective strategy in drug R&amp;D Recent work has indicated that repositioning can be promoted by utilizing a wide range of information sources, including medicinal chemical, target, mechanism, main and side-effect-related information, and also bibliometric and taxonomical fingerprints, signatures and knowledge bases. This article describes the adaptation of a conceptually novel, more efficient approach for the identification of new possible therapeutic applications of approved drugs and drug candidates, based on a kernel-based data fusion method. This strategy includes (1) the potentially multiple representation of information sources, (2) the automated weighting and statistically optimal combination of information sources, and (3) the automated weighting of parts of the query compounds. The performance was systematically evaluated by using Anatomical Therapeutic Chemical Classification System classes in a cross-validation framework. The results confirmed that kernel-based data fusion can integrate heterogeneous information sources significantly better than standard rank-based fusion can, and this method provides a unique solution for repositioning; it can also be utilized for de novo drug discovery. The advantages of kernel-based data fusion are illustrated with examples and open problems that are particularly relevant for pharmaceutical applications.
23196713	Benchmarking therapeutic drug monitoring software: a review of available computer tools.
Clin Pharmacokinet  2013Jan
Therapeutic drug monitoring (TDM) aims to optimize treatments by individualizing dosage regimens based on the measurement of blood concentrations. Dosage individualization to maintain concentrations within a target range requires pharmacokinetic and clinical capabilities. Bayesian calculations currently represent the gold standard TDM approach but require computation assistance. In recent decades computer programs have been developed to assist clinicians in this assignment. The aim of this survey was to assess and compare computer tools designed to support TDM clinical activities. The literature and the Internet were searched to identify software. All programs were tested on personal computers. Each program was scored against a standardized grid covering pharmacokinetic relevance, user friendliness, computing aspects, interfacing and storage. A weighting factor was applied to each criterion of the grid to account for its relative importance. To assess the robustness of the software, six representative clinical vignettes were processed through each of them. Altogether, 12 software tools were identified, tested and ranked, representing a comprehensive review of the available software. Numbers of drugs handled by the software vary widely (from two to 180), and eight programs offer users the possibility of adding new drug models based on population pharmacokinetic analyses. Bayesian computation to predict dosage adaptation from blood concentration (a posteriori adjustment) is performed by ten tools, while nine are also able to propose a priori dosage regimens, based only on individual patient covariates such as age, sex and bodyweight. Among those applying Bayesian calculation, MM-USC*PACK© uses the non-parametric approach. The top two programs emerging from this benchmark were MwPharm© and TCIWorks. Most other programs evaluated had good potential while being less sophisticated or less user friendly. Programs vary in complexity and might not fit all healthcare settings. Each software tool must therefore be regarded with respect to the individual needs of hospitals or clinicians. Programs should be easy and fast for routine activities, including for non-experienced users. Computer-assisted TDM is gaining growing interest and should further improve, especially in terms of information system interfacing, user friendliness, data storage capability and report generation.
23341759	Visual data mining of biological networks: one size does not fit all.
PLoS Comput. Biol. 20130110 2013
High-throughput technologies produce massive amounts of data. However, individual methods yield data specific to the technique used and biological setup. The integration of such diverse data is necessary for the qualitative analysis of information relevant to hypotheses or discoveries. It is often useful to integrate these datasets using pathways and protein interaction networks to get a broader view of the experiment. The resulting network needs to be able to focus on either the large-scale picture or on the more detailed small-scale subsets, depending on the research question and goals. In this tutorial, we illustrate a workflow useful to integrate, analyze, and visualize data from different sources, and highlight important features of tools to support such analyses.
23302604	A study on PubMed search tag usage pattern: association rule mining of a full-day PubMed query log.
BMC Med Inform Decis Mak 20130109 2013
The practice of evidence-based medicine requires efficient biomedical literature search such as PubMed/MEDLINE. Retrieval performance relies highly on the efficient use of search field tags. The purpose of this study was to analyze PubMed log data in order to understand the usage pattern of search tags by the end user in PubMed/MEDLINE search. A PubMed query log file was obtained from the National Library of Medicine containing anonymous user identification, timestamp, and query text. Inconsistent records were removed from the dataset and the search tags were extracted from the query texts. A total of 2,917,159 queries were selected for this study issued by a total of 613,061 users. The analysis of frequent co-occurrences and usage patterns of the search tags was conducted using an association mining algorithm. The percentage of search tag usage was low (11.38% of the total queries) and only 2.95% of queries contained two or more tags. Three out of four users used no search tag and about two-third of them issued less than four queries. Among the queries containing at least one tagged search term, the average number of search tags was almost half of the number of total search terms. Navigational search tags are more frequently used than informational search tags. While no strong association was observed between informational and navigational tags, six (out of 19) informational tags and six (out of 29) navigational tags showed strong associations in PubMed searches. The low percentage of search tag usage implies that PubMed/MEDLINE users do not utilize the features of PubMed/MEDLINE widely or they are not aware of such features or solely depend on the high recall focused query translation by the PubMed's Automatic Term Mapping. The users need further education and interactive search application for effective use of the search tags in order to fulfill their biomedical information needs from PubMed/MEDLINE.
23358056	Evaluation of metabolic syndrome related health information on internet in Indian context.
Technol Health Care  2013
Metabolic Syndrome (MetS) in India is a major contributor to the global increase in CVD. Lifestyle modification programs have been effective in reducing the burden of MetS. The Objective of our study was to evaluate the quality of MetS related health information on the internet in an Indian context. We used a key term "metabolic syndrome" to retrieve websites from Google, Yahoo and Bing search engines by restricting pages from India during May 2012. Previously validated DISCERN tool was used by the three raters to assess 44 websites. All results have been reported as p-values. The most common topics that were covered in these websites included causes and risk factors of MetS (77.27%). On the contrary medication (13.64%), lab tests (11.36%), type of physical activities (6.82%), prognosis and regular check-ups (4.55%) were the least mentioned topics. The website category .org had higher average DISCERN scores as compared to others categories. Limited information was available related to treatment choices, warning signal and informed decision and hence the need exists for further research to develop evidence based health information portal for MetS in an Indian context.
23014749	Image enhancement using the hypothesis selection filter: theory and application to JPEG decoding.
IEEE Trans Image Process 20120921 2013Mar
We introduce the hypothesis selection filter (HSF) as a new approach for image quality enhancement. We assume that a set of filters has been selected a priori to improve the quality of a distorted image containing regions with different characteristics. At each pixel, HSF uses a locally computed feature vector to predict the relative performance of the filters in estimating the corresponding pixel intensity in the original undistorted image. The prediction result then determines the proportion of each filter used to obtain the final processed output. In this way, the HSF serves as a framework for combining the outputs of a number of different user selected filters, each best suited for a different region of an image. We formulate our scheme in a probabilistic framework where the HSF output is obtained as the Bayesian minimum mean square error estimate of the original image. Maximum likelihood estimates of the model parameters are determined from an offline fully unsupervised training procedure that is derived from the expectation-maximization algorithm. To illustrate how to apply the HSF and to demonstrate its potential, we apply our scheme as a post-processing step to improve the decoding quality of JPEG-encoded document images. The scheme consistently improves the quality of the decoded image over a variety of image content with different characteristics. We show that our scheme results in quantitative improvements over several other state-of-the-art JPEG decoding methods.
23060337	Coaching the exploration and exploitation in active learning for interactive video retrieval.
IEEE Trans Image Process 20121005 2013Mar
Conventional active learning approaches for interactive video/image retrieval usually assume the query distribution is unknown, as it is difficult to estimate with only a limited number of labeled instances available. Thus, it is easy to put the system in a dilemma whether to explore the feature space in uncertain areas for a better understanding of the query distribution or to harvest in certain areas for more relevant instances. In this paper, we propose a novel approach called coached active learning that makes the query distribution predictable through training and, therefore, avoids the risk of searching on a completely unknown space. The estimated distribution, which provides a more global view of the feature space, can be used to schedule not only the timing but also the step sizes of the exploration and the exploitation in a principled way. The results of the experiments on a large-scale data set from TRECVID 2005-2009 validate the efficiency and effectiveness of our approach, which demonstrates an encouraging performance when facing domain-shift, outperforms eight conventional active learning methods, and shows superiority to six state-of-the-art interactive video retrieval systems.
23076045	Nonnegative local coordinate factorization for image representation.
IEEE Trans Image Process 20121012 2013Mar
Recently, nonnegative matrix factorization (NMF) has become increasingly popular for feature extraction in computer vision and pattern recognition. NMF seeks two nonnegative matrices whose product can best approximate the original matrix. The nonnegativity constraints lead to sparse parts-based representations that can be more robust than nonsparse global features. To obtain more accurate control over the sparseness, in this paper, we propose a novel method called nonnegative local coordinate factorization (NLCF) for feature extraction. NLCF adds a local coordinate constraint into the standard NMF objective function. Specifically, we require that the learned basis vectors be as close to the original data points as possible. In this way, each data point can be represented by a linear combination of only a few nearby basis vectors, which naturally leads to sparse representation. Extensive experimental results suggest that the proposed approach provides a better representation and achieves higher accuracy in image clustering.
23144034	Context-dependent logo matching and recognition.
IEEE Trans Image Process 20121022 2013Mar
We contribute, through this paper, to the design of a novel variational framework able to match and recognize multiple instances of multiple reference logos in image archives. Reference logos and test images are seen as constellations of local features (interest points, regions, etc.) and matched by minimizing an energy function mixing: 1) a fidelity term that measures the quality of feature matching, 2) a neighborhood criterion that captures feature co-occurrence/geometry, and 3) a regularization term that controls the smoothness of the matching solution. We also introduce a detection/recognition procedure and study its theoretical consistency. Finally, we show the validity of our method through extensive experiments on the challenging MICC-Logos dataset. Our method overtakes, by 20%, baseline as well as state-of-the-art matching/recognition procedures.
23060318	Toward ubiquitous healthcare services with a novel efficient cloud platform.
IEEE Trans Biomed Eng 20121005 2013Jan
Ubiquitous healthcare services are becoming more and more popular, especially under the urgent demand of the global aging issue. Cloud computing owns the pervasive and on-demand service-oriented natures, which can fit the characteristics of healthcare services very well. However, the abilities in dealing with multimodal, heterogeneous, and nonstationary physiological signals to provide persistent personalized services, meanwhile keeping high concurrent online analysis for public, are challenges to the general cloud. In this paper, we proposed a private cloud platform architecture which includes six layers according to the specific requirements. This platform utilizes message queue as a cloud engine, and each layer thereby achieves relative independence by this loosely coupled means of communications with publish/subscribe mechanism. Furthermore, a plug-in algorithm framework is also presented, and massive semistructure or unstructured medical data are accessed adaptively by this cloud architecture. As the testing results showing, this proposed cloud platform, with robust, stable, and efficient features, can satisfy high concurrent requests from ubiquitous healthcare services.
23256906	CAPER: a chromosome-assembled human proteome browsER.
J. Proteome Res. 20121220 2013Jan4
High-throughput mass spectrometry and antibody-based experiments have begun to produce a large amount of proteomic data sets. Chromosome-based visualization of these data sets and their annotations can help effectively integrate, organize, and analyze them. Therefore, we developed a web-based, user-friendly Chromosome-Assembled human Proteome browsER (CAPER). To display proteomic data sets and related annotations comprehensively, CAPER employs two distinct visualization strategies: track-view for the sequence/site information and the correspondence between proteome, transcriptome, genome, and chromosome and heatmap-view for the qualitative and quantitative functional annotations. CAPER supports data browsing at multiple scales through Google Map-like smooth navigation, zooming, and positioning with chromosomes as the reference coordinate. Both track-view and heatmap-view can mutually switch, providing a high-quality user interface. Taken together, CAPER will greatly facilitate the complete annotation and functional interpretation of the human genome by proteomic approaches, thereby making a significant contribution to the Chromosome-Centric Human Proteome Project and even the human physiology/pathology research. CAPER can be accessed at  http://www.bprc.ac.cn/CAPE .
22985123	The evolution of intraventricular vortex during ejection studied by using vector flow mapping.
Echocardiography 20120918 2013Jan
The purpose of this study was to assess the evolution of intraventricular vortex during left ventricular (LV) ejection. Vector flow mapping was performed in 51 patients with coronary artery disease and LV ejection fraction (EF) &gt;50%, 70 patients with EF &lt;50% (13 with coronary artery disease and 57 with dilated cardiomyopathy), and 62 healthy volunteers. In normals and patients with EF &gt;50%, the intraventricular vortex dissipated quickly during early ejection. In patients with EF &lt;50%, the vortex stayed mainly at apex and persisted for a significantly longer time. The evolution of vortex during ejection was significantly correlated with QRS width, EF, fractional shortening, LV outflow velocity time integral, wall motion score index (WMSI), LV dimensions, left atrial diameter, and diastolic mitral annular velocities. LV end-diastolic short diameter and WMSI were the independent determinants of the duration of vortex (R(2) = 0.482, P &lt; 0.001). End-systolic short diameter and apical WMSI were the independent determinants of duration of vortex corrected for ejection time (R(2) = 0.565, P &lt; 0.001). End-systolic short diameter was the independent determinant of percentage change in vortex area during early ejection (R(2) = 0.355, P &lt; 0.001). End-systolic short diameter and ejection time were the independent determinants of percentage change in vortex flow volume (R(2) = 0.415, P &lt; 0.001). In patients with LV systolic dysfunction, the vortex persists during ejection and stays mainly at apex. The vortex evolution during ejection is closely associated with LV dimensions and functions.
23124262	Significant change or loss of intraoperative monitoring data: a 25-year experience in 12,375 spinal surgeries.
Spine  2013Jan15
Retrospective. The purpose of this study was to report the spectrum of intraoperative events responsible for a loss or significant change in intraoperative monitoring (IOM) data. The efficacy of spinal cord/nerve root monitoring is demonstrated in a large, single institution series of patients, involving all levels of the spinal column (occiput to sacrum) and all spinal surgical procedures. Multimodality IOM included somatosensory-evoked potentials, descending neurogenic-evoked potentials, neurogenic motor-evoked potentials, and spontaneous and triggered electromyography. A total of 12,375 patients who underwent surgery for spinal pathology between January 1985 and December 2010 were reviewed. There were 59.3% female patients (7178) and 40.7% male patients (5197). Procedures by spinal level were as follows: cervical 29.7% (3671), thoracic/thoracolumbar 45.4% (5624), and lumbosacral 24.9% (3080). Age at the time of surgery was as follows: older than 18 years, 72.7% (242/8993) and younger than 18 years, 27.3% (144/3382). A total of 77.8% (9633) patients underwent primary surgical procedures and 22.2% (2742) patients underwent revision surgical procedures. A total of 406 instances of IOM data change/loss occurred in 386 of 12,375 (3.1%) patients. Causes for data degradation/loss included the following: instrumentation (n = 131), positioning (n = 85), correction (n = 56), systemic (n = 49), unknown (n = 24), and focal spinal cord compression (n = 15). Data loss/change was seen in revision (6.1%/167 patients) surgical procedures more commonly than in primary procedures (2.3%/219 patients; P &lt; 0.0001). Data improvement was demonstrated by 88.7% (n = 360) after intervention versus 11.3% (n = 46) with no improvement in IOM data. One patient with improved data after intervention versus 14 with no improvement despite intervention had a permanent neurological deficit (P &lt; 0.0001). IOM data identified 386 (3.1%) patients with loss/degradation of data in 12,375 spinal surgical procedures. Fortunately, in 93.3% of patients, intervention led to data recovery and no neurological deficits. Reduction from a potential (worst-case scenario) 3.1% (386) of patients with significant change/loss of IOM data to a permanent neurological deficit rate of 0.12% (15) patients was achieved (P &lt; 0.0001), thus confirming efficacy of IOM.
23302542	Is the coverage of Google Scholar enough to be used alone for systematic reviews.
BMC Med Inform Decis Mak 20130109 2013
In searches for clinical trials and systematic reviews, it is said that Google Scholar (GS) should never be used in isolation, but in addition to PubMed, Cochrane, and other trusted sources of information. We therefore performed a study to assess the coverage of GS specifically for the studies included in systematic reviews and evaluate if GS was sensitive enough to be used alone for systematic reviews. All the original studies included in 29 systematic reviews published in the Cochrane Database Syst Rev or in the JAMA in 2009 were gathered in a gold standard database. GS was searched for all these studies one by one to assess the percentage of studies which could have been identified by searching only GS. All the 738 original studies included in the gold standard database were retrieved in GS (100%). The coverage of GS for the studies included in the systematic reviews is 100%. If the authors of the 29 systematic reviews had used only GS, no reference would have been missed. With some improvement in the research options, to increase its precision, GS could become the leading bibliographic database in medicine and could be used alone for systematic reviews.
23086836	Accessing and using chemical databases.
Methods Mol. Biol.  2013
Computer-based representation of chemicals makes it possible to organize data in chemical databases-collections of chemical structures and associated properties. Databases are widely used wherever efficient processing of chemical information is needed, including search, storage, retrieval, and dissemination. Structure and functionality of chemical databases are considered. The typical kinds of information found in a chemical database are considered-identification, structural, and associated data. Functionality of chemical databases is presented, with examples of search and access types. More details are included about the OASIS database and platform and the Danish (Q)SAR Database online. Various types of chemical database resources are discussed, together with a list of examples.
22749791	Decision-making in familial database searching: KI alone or not alone?
Forensic Sci Int Genet 20120629 2013Jan
We consider the comparison of hypotheses "parent-child" or "full siblings" against the alternative of "unrelated" for pairs of individuals for whom DNA profiles are available. This is a situation that occurs repeatedly in familial database searching. A decision rule that uses both the kinship index (KI), also known as the likelihood ratio, and the identity-by-state statistic (IBS) was advocated in a recent report as superior to the use of KI alone. Such proposal appears to conflict with the Neyman-Pearson Lemma of statistics, which states that the likelihood ratio alone provides the most powerful criterion for distinguishing between any two simple hypotheses. We therefore performed a simulation study that was two orders of magnitude larger than in the previous report, and our results corroborate the theoretical expectation that KI alone provides a better decision rule than KI combined with IBS.
23230156	An integrated view of data quality in Earth observation.
Philos Trans A Math Phys Eng Sci 20121210 2013Jan28
Data quality is a difficult notion to define precisely, and different communities have different views and understandings of the subject. This causes confusion, a lack of harmonization of data across communities and omission of vital quality information. For some existing data infrastructures, data quality standards cannot address the problem adequately and cannot fulfil all user needs or cover all concepts of data quality. In this study, we discuss some philosophical issues on data quality. We identify actual user needs on data quality, review existing standards and specifications on data quality, and propose an integrated model for data quality in the field of Earth observation (EO). We also propose a practical mechanism for applying the integrated quality information model to a large number of datasets through metadata inheritance. While our data quality management approach is in the domain of EO, we believe that the ideas and methodologies for data quality management can be applied to wider domains and disciplines to facilitate quality-enabled scientific research.
22671134	Control in childbirth: a concept analysis and synthesis.
J Adv Nurs 20120607 2013Jan
To report a concept analysis of control in childbirth. Control has a variety of definitions from a wide range of disciplines. In childbirth, however, the concept is more tenuous and depends on the context. It can be viewed in relationship to a woman's body and labour progression, pain, environment and the ability to request her method of birth. Medline, CINAHL and PsycINFO databases were searched between 1970-2011 using the keywords, 'control', 'childbirth', 'labour' and 'delivery'. Walker and Avant's method of concept analysis was used for this review. In addition, cases were placed before defining attributes as recommended by Risjord. Four attributes of control were identified: decision-making, access to information, personal security and physical functioning. Antecedents include pregnancy and expectations of the birth. Consequences include childbirth satisfaction, childbirth experience, emotional well-being, fulfilment and the transition into motherhood. A model case, contrary case and borderline case are described. Clarifying the definition of control in childbirth and defining its attributes can help inform women and maternity providers throughout the world. This analysis provides clarity to a previously tenuous concept and allows practitioners to better understand the critical relationship between control in childbirth and satisfaction with the childbirth experience. It also has the potential to affect perinatal outcomes and subsequently healthcare costs.
23035717	Determining similarity in histological images using graph-theoretic description and matching methods for content-based image retrieval in medical diagnostics.
Diagn Pathol 20121004 2012
Computer-based analysis of digitalized histological images has been gaining increasing attention, due to their extensive use in research and routine practice. The article aims to contribute towards the description and retrieval of histological images by employing a structural method using graphs. Due to their expressive ability, graphs are considered as a powerful and versatile representation formalism and have obtained a growing consideration especially by the image processing and computer vision community. The article describes a novel method for determining similarity between histological images through graph-theoretic description and matching, for the purpose of content-based retrieval. A higher order (region-based) graph-based representation of breast biopsy images has been attained and a tree-search based inexact graph matching technique has been employed that facilitates the automatic retrieval of images structurally similar to a given image from large databases. The results obtained and evaluation performed demonstrate the effectiveness and superiority of graph-based image retrieval over a common histogram-based technique. The employed graph matching complexity has been reduced compared to the state-of-the-art optimal inexact matching methods by applying a pre-requisite criterion for matching of nodes and a sophisticated design of the estimation function, especially the prognosis function. The proposed method is suitable for the retrieval of similar histological images, as suggested by the experimental and evaluation results obtained in the study. It is intended for the use in Content Based Image Retrieval (CBIR)-requiring applications in the areas of medical diagnostics and research, and can also be generalized for retrieval of different types of complex images. The virtual slide(s) for this article can be found here: http://www.diagnosticpathology.diagnomx.eu/vs/1224798882787923.
23181507	CloudMan as a platform for tool, data, and analysis distribution.
BMC Bioinformatics 20121127 2012
Cloud computing provides an infrastructure that facilitates large scale computational analysis in a scalable, democratized fashion, However, in this context it is difficult to ensure sharing of an analysis environment and associated data in a scalable and precisely reproducible way. CloudMan (usecloudman.org) enables individual researchers to easily deploy, customize, and share their entire cloud analysis environment, including data, tools, and configurations. With the enabled customization and sharing of instances, CloudMan can be used as a platform for collaboration. The presented solution improves accessibility of cloud resources, tools, and data to the level of an individual researcher and contributes toward reproducibility and transparency of research solutions.
23043673	A repository based on a dynamically extensible data model supporting multidisciplinary research in neuroscience.
BMC Med Inform Decis Mak 20121008 2012
Robust, extensible and distributed databases integrating clinical, imaging and molecular data represent a substantial challenge for modern neuroscience. It is even more difficult to provide extensible software environments able to effectively target the rapidly changing data requirements and structures of research experiments. There is an increasing request from the neuroscience community for software tools addressing technical challenges about: (i) supporting researchers in the medical field to carry out data analysis using integrated bioinformatics services and tools; (ii) handling multimodal/multiscale data and metadata, enabling the injection of several different data types according to structured schemas; (iii) providing high extensibility, in order to address different requirements deriving from a large variety of applications simply through a user runtime configuration. A dynamically extensible data structure supporting collaborative multidisciplinary research projects in neuroscience has been defined and implemented. We have considered extensibility issues from two different points of view. First, the improvement of data flexibility has been taken into account. This has been done through the development of a methodology for the dynamic creation and use of data types and related metadata, based on the definition of "meta" data model. This way, users are not constrainted to a set of predefined data and the model can be easily extensible and applicable to different contexts. Second, users have been enabled to easily customize and extend the experimental procedures in order to track each step of acquisition or analysis. This has been achieved through a process-event data structure, a multipurpose taxonomic schema composed by two generic main objects: events and processes. Then, a repository has been built based on such data model and structure, and deployed on distributed resources thanks to a Grid-based approach. Finally, data integration aspects have been addressed by providing the repository application with an efficient dynamic interface designed to enable the user to both easily query the data depending on defined datatypes and view all the data of every patient in an integrated and simple way. The results of our work have been twofold. First, a dynamically extensible data model has been implemented and tested based on a "meta" data-model enabling users to define their own data types independently from the application context. This data model has allowed users to dynamically include additional data types without the need of rebuilding the underlying database. Then a complex process-event data structure has been built, based on this data model, describing patient-centered diagnostic processes and merging information from data and metadata. Second, a repository implementing such a data structure has been deployed on a distributed Data Grid in order to provide scalability both in terms of data input and data storage and to exploit distributed data and computational approaches in order to share resources more efficiently. Moreover, data managing has been made possible through a friendly web interface. The driving principle of not being forced to preconfigured data types has been satisfied. It is up to users to dynamically configure the data model for the given experiment or data acquisition program, thus making it potentially suitable for customized applications. Based on such repository, data managing has been made possible through a friendly web interface. The driving principle of not being forced to preconfigured data types has been satisfied. It is up to users to dynamically configure the data model for the given experiment or data acquisition program, thus making it potentially suitable for customized applications.
23366065	A CAD system for atherosclerotic plaque assessment.
Conf Proc IEEE Eng Med Biol Soc  2012
Recently, several atherosclerotic plaque characterization methods were proposed based on plaque morphology assessed through 2D ultrasound. It is of extreme importance to establish an objective quantification measure which allows the physicians to determine the risk of plaque rupture, and thus, of brain stroke. Having these, sometimes complex, measures easily and quickly assessed might prove invaluable for the physician an patient alike. This paper is a first attempt to incorporate such scores in a user-friendly software platform for Computer-aided Diagnosis. This tool provides a way to objectively and interactively characterize the atherosclerotic plaque, to store relevant patient data and to use several processing tools to outline the plaque and compute different echogenicity measures. Combinations of these features are used to provide two objective measure with clinical significance, known as activity index and enhanced activity index.
23366343	Protocol for cardiac assessment of recreational athletes.
Conf Proc IEEE Eng Med Biol Soc  2012
In this work, the development of a database on physical fitness is presented. As initial population to fill this database, people who practice recreational sports at the Universidad Simon Bolivar (USB) were chosen. The goal was studying individual physical fitness in order to structure exercise routines that gives certain benefits without risking the individual health, promoting a less sedentary way of life. Before the study, a low-cost noninvasive protocol was designed to determine the level of physical fitness. The methodology consisted of four steps: a) A review of existing protocols to propose a set of physical fitness (International Physical Activity Questionnaire (IPAQ)), cardiovascular (heart rate variability, heart rate recovery time and arterial blood pressure), anthropomorphic, aerobic (maximum oxygen consumption) and mood state (Profile of Mood State (POMS)) measurements, which allow sketching a complete profile on the sportsman physical fitness. b) Instrumental data collection. c) Electrocardiographic signal processing. d) Data post-processing using multivariate analysis. The database was composed of 26 subject from USB. Ten subjects were soccer players, ten were mountain climbers and six were sedentary people. Results showed that the heart rate recover time after 2-3 min, IPAQ and maximum oxygen consumption have higher weights for classifying individuals according to their habitual physical activity. Heart rate variability, as well as, POMS did not contribute greatly for discriminating recreational sport from sedentary persons.
23366349	Summarized data to achieve population-wide anonymized wellness measures.
Conf Proc IEEE Eng Med Biol Soc  2012
The growth in smartphone market share has seen the increasing emergence of individuals collecting quantitative wellness data. Beyond the potential health benefits for the individual in regards to managing their own health, the data is highly related to preventative and risk factors for a number of lifestyle related diseases. This data has often been a component of public health data collection and epidemiological studies due to its large impact on the health system with chronic and lifestyle diseases increasingly being a major burden for the health service. However, collection of this kind of information from large segments of the community in a usable fashion has not been specifically explored in previous work. In this paper we discuss some of the technologies that increase the ease and capability of gathering quantitative wellness data via smartphones, how specific and detailed this data needs to be for public health use and the challenges of such anonymized data collection for public health. Additionally, we propose a conceptual architecture that includes the necessary components to support this approach to data collection.
23366403	Automated hand-forearm ergometer data collection system.
Conf Proc IEEE Eng Med Biol Soc  2012
Handgrip contractions are a standard exercise modality to evaluate cardiovascular system performance. Most conventional ergometer systems of this nature are manually controlled, placing a burden on the researcher to guide subject activity while recording the resultant data. This paper presents updates to a hand-forearm ergometer system that automate the control and data-acquisition processes. A LabVIEW virtual instrument serves as the centerpiece for the system, providing the subject/researcher interfaces as well as coordinating data acquisition from both traditional and new sensors. Initial data indicate the viability of the system with regard to its ability to obtain consistent and physiologically meaningful data.
23366405	A portable system for recording neural activity in indoor and outdoor environments.
Conf Proc IEEE Eng Med Biol Soc  2012
We present a self-contained portable USB based device that can amplify and record small bioelectric signals from insects and animals. The system combines a purpose built low noise amplifier with off the shelf components to provide a low cost low power system for recording electrophysiological signals. Using open source software the system is programmed as a simple USB device and can be connected to any USB capable computer for recording data. This simple and universal interface provides the ability to connect to a variety of systems. Open source acquisition software was also written to record signals under the linux operating system. Performance analysis shows that our device is able to record good quality signals both indoors and outdoors and delivers this performance at a very low cost. Compared to larger systems our device provides the additional advantage of portability given that it can fit into a pocket and costs a fraction of large systems used in electrophysiology labs.
23366932	Application of Near-field intra-body communication and spread spectrum technique to vital-sign monitor.
Conf Proc IEEE Eng Med Biol Soc  2012
As a novel vital sign monitor, we have developed wireless ECG monitoring system with Near-field intra-body communication technique. However, communication reliability is not so high because transmission channel is noisy and unstable. In order to improve the problem, we utilize spread spectrum (SS), which is known as robust communication technique even through poor transmission channel. First of all, we evaluated characteristics of human body to SS signal. The results show that SS can be used even through human body. Based on this result, we developed and tested near-field intra-body communication device enhanced by SS. The test result shows that SS can solve the problem mentioned above.
23367041	Motion-based video retrieval with application to computer-assisted retinal surgery.
Conf Proc IEEE Eng Med Biol Soc  2012
In this paper, we address the problem of computer-aided ophthalmic surgery. In particular, a novel Content-Based Video Retrieval (CBVR) system is presented : given a video stream captured by a digital camera monitoring the current surgery, the system retrieves, within digital archives, videos that resemble the current surgery monitoring video. The search results may be used to guide surgeons' decisions, for example, let the surgeon know what a more experienced fellow worker would do in a similar situation. With this goal, we propose to use motion information contained in MPEG- 4 AVC/H.264 video standard to extract features from videos. We propose two approaches, one of which is based on motion histogram created for every frame of a compressed video sequence to extract motion direction and intensity statistics. The other combine segmentation and tracking to extract region displacements between consecutive frames and therefore characterize region trajectories. To compare videos, an extension of the fast dynamic time warping to multidimensional time series was adopted. The system is applied to a dataset of 69 video-recorded retinal surgery steps. Results are promising: the retrieval efficiency is higher than 69%.
23367073	Data-driven modeling of sleep states from EEG.
Conf Proc IEEE Eng Med Biol Soc  2012
Sleep analysis is critical for the diagnosis, treatment, and understanding of sleep disorders. However, the current standards for sleep analysis are widely considered oversimplified and problematic. The ability to automatically annotate different states during a night of sleep in a manner that is more descriptive than current standards, as well as the ability to train these models on a patient-by-patient basis, would provide a complementary approach for sleep analysis. We present a method that discovers latent structure in sleep EEG recordings, by extracting symbols from the continuous EEG signal and learning "topics" for a recording. These sleep topics are derived in a fully automatic and data-driven manner, and can represent the data with mixtures of states. The proposed method allows for identification of states in a patient-specific way, as opposed to the one-size-fits-all approach of the current standard. We demonstrate on a publicly available dataset of 15 sleep recordings that not only do the states discovered by this approach encompass the standard sleep stage structure, they provide additional information about sleep architecture with the potential to provide new insights into sleep disorders.
23367095	Information transfer along the ventral auditory processing stream in the awake macaque.
Conf Proc IEEE Eng Med Biol Soc  2012
Few studies have examined the physiology of the auditory cortical processing streams in the context of information transfer among cortical areas. This study examines information transfer in two cortical areas in the ventral auditory processing stream in an awake macaque. We show conditional information examined over different durations of neural responses provides insight into the time scale and direction of cortical hierarchical processing.
23367269	Distributed PACS using distributed file system with hierarchical meta data servers.
Conf Proc IEEE Eng Med Biol Soc  2012
In this research, we propose a new distributed PACS (Picture Archiving and Communication Systems) which is available to integrate several PACSs that exist in each medical institution. The conventional PACS controls DICOM file into one data-base. On the other hand, in the proposed system, DICOM file is separated into meta data and image data and those are stored individually. Using this mechanism, since file is not always accessed the entire data, some operations such as finding files, changing titles, and so on can be performed in high-speed. At the same time, as distributed file system is utilized, accessing image files can also achieve high-speed access and high fault tolerant. The introduced system has a more significant point. That is the simplicity to integrate several PACSs. In the proposed system, only the meta data servers are integrated and integrated system can be constructed. This system also has the scalability of file access with along to the number of file numbers and file sizes. On the other hand, because meta-data server is integrated, the meta data server is the weakness of this system. To solve this defect, hieratical meta data servers are introduced. Because of this mechanism, not only fault--tolerant ability is increased but scalability of file access is also increased. To discuss the proposed system, the prototype system using Gfarm was implemented. For evaluating the implemented system, file search operating time of Gfarm and NFS were compared.
23104084	To DNA, all information is equal.
Artif DNA PNA XNA 20120701 2012Jul1
Information storage capabilities are key in most aspects of society and the requirement for storage capacity is rapidly expanding. In principle, DNA could be a high-density medium for information storage. Church and coworkers recently demonstrated how binary data can be encoded, stored in, and retrieved from a library of oligonucleotides, increasing by several orders of magnitude the amount and density of manmade information stored in DNA to date. The technology remains in its infancy and important hurdles have yet to be overcome in order to realize its potential. However, DNA may be particularly useful as a storage-medium over long time-scales (centuries), because data-access is compatible with any large-scale DNA-sequencing and -synthesis technology.
23503543	Demand for and availability of online support to stop smoking.
Rev Saude Publica  2012Dec
Estimate the frequency of online searches on the topic of smoking and analyze the quality of online resources available to smokers interested in giving up smoking. Search engines were used to revise searches and online resources related to stopping smoking in Brazil in 2010. The number of searches was determined using analytical tools available on Google Ads; the number and type of sites were determined by replicating the search patterns of internet users. The sites were classified according to content (advertising, library of articles and other). The quality of the sites was analyzed using the Smoking Treatment Scale- Content (STS-C) and the Smoking Treatment Scale - Rating (STS-R). A total of 642,446 searches was carried out. Around a third of the 113 sites encountered were of the 'library' type, i.e. they only contained articles, followed by sites containing clinical advertising (18.6) and professional education (10.6). Thirteen of the sites offered advice on quitting directed at smokers. The majority of the sites did not contain evidence-based information, were not interactive and did not have the possibility of communicating with users after the first contact. Other limitations we came across were a lack of financial disclosure as well as no guarantee of privacy concerning information obtained and no distinction made between editorial content and advertisements. There is a disparity between the high demand for online support in giving up smoking and the scarcity of quality online resources for smokers. It is necessary to develop interactive, customized online resources based on evidence and random clinical testing in order to improve the support available to Brazilian smokers.
23238600	Searching for truth: internet search patterns as a method of investigating online responses to a Russian illicit drug policy debate.
J. Med. Internet Res. 20121213 2012
This is a methodological study investigating the online responses to a national debate over an important health and social problem in Russia. Russia is the largest Internet market in Europe, exceeding Germany in the absolute number of users. However, Russia is unusual in that the main search provider is not Google, but Yandex. This study had two main objectives. First, to validate Yandex search patterns against those provided by Google, and second, to test this method's adequacy for investigating online interest in a 2010 national debate over Russian illicit drug policy. We hoped to learn what search patterns and specific search terms could reveal about the relative importance and geographic distribution of interest in this debate. A national drug debate, centering on the anti-drug campaigner Egor Bychkov, was one of the main Russian domestic news events of 2010. Public interest in this episode was accompanied by increased Internet search. First, we measured the search patterns for 13 search terms related to the Bychkov episode and concurrent domestic events by extracting data from Google Insights for Search (GIFS) and Yandex WordStat (YaW). We conducted Spearman Rank Correlation of GIFS and YaW search data series. Second, we coded all 420 primary posts from Bychkov's personal blog between March 2010 and March 2012 to identify the main themes. Third, we compared GIFS and Yandex policies concerning the public release of search volume data. Finally, we established the relationship between salient drug issues and the Bychkov episode. We found a consistent pattern of strong to moderate positive correlations between Google and Yandex for the terms "Egor Bychkov" (r(s) = 0.88, P &lt; .001), "Bychkov" (r(s) = .78, P &lt; .001) and "Khimki"(r(s) = 0.92, P &lt; .001). Peak search volumes for the Bychkov episode were comparable to other prominent domestic political events during 2010. Monthly search counts were 146,689 for "Bychkov" and 48,084 for "Egor Bychkov", compared to 53,403 for "Khimki" in Yandex. We found Google potentially provides timely search results, whereas Yandex provides more accurate geographic localization. The correlation was moderate to strong between search terms representing the Bychkov episode and terms representing salient drug issues in Yandex-"illicit drug treatment" (r(s) = .90, P &lt; .001), "illicit drugs" (r(s) = .76, P &lt; .001), and "drug addiction" (r(s) = .74, P &lt; .001). Google correlations were weaker or absent-"illicit drug treatment" (r(s) = .12, P = .58), "illicit drugs " (r(s) = -0.29, P = .17), and "drug addiction" (r(s) = .68, P &lt; .001). This study contributes to the methodological literature on the analysis of search patterns for public health. This paper investigated the relationship between Google and Yandex, and contributed to the broader methods literature by highlighting both the potential and limitations of these two search providers. We believe that Yandex Wordstat is a potentially valuable, and underused data source for researchers working on Russian-related illicit drug policy and other public health problems. The Russian Federation, with its large, geographically dispersed, and politically engaged online population presents unique opportunities for studying the evolving influence of the Internet on politics and policy, using low cost methods resilient against potential increases in censorship.
23240333	Issues and challenges for HIS in a small island nation.
Pac Health Dialog  2012Apr
Kiribati is among one of the least developed countries in the world. Every year international agencies and other health stakeholders request information on Kiribati mortality and morbidity, but unfortunately most health data has never been analysed and therefore, health reports have never been formally provided. Despite this, Kiribati has taken important steps forward in improving its health information system (HIS) by prioritising health information in the Ministry of Health's Strategic Action Plan. The main purpose of this case study is to explore the HIS issues and challenges Kiribati faces, actions taken to address these challenges, its next steps, and key messages for other countries in the Pacific.
23242706	Insights from a study by the ESC cardiologists of tomorrow nucleus: the junior cardiologists’ research reveals that the ESC is well regarded by young cardiologists but there is room for improving its appeal.
Eur. Heart J.  2012Dec
This research was performed to provide data on the specific needs and expectations of junior cardiologists across Europe from a professional medical organization characterized by the European Society of Cardiology (ESC). The study was carried out using telephone interviews. The target respondents were based in a wide range of different locations within Europe and were identified by national groups of young cardiologists and trainees. A questionnaire was employed asking about information sources, membership of professional societies and related benefits. A total of 120 interviews were conducted. Websites and journals proved the most popular sources for professional information, consulted by .71 and 68% of respondents, respectively.With regard to the up to date best practice recommendations, guidelines documents were most common, mentioned by 63%. Overall, the ESC resources appeared within highest priority. The two main important tangible benefits expected from membership of professional societies were access to medical information, mostly journals and guidelines, and reduced financial congress requirements. Also, the most significant intangible benefit was networking. The ESC is widely respected by the junior cardiologists and trainees. Its congresses and guidelines are central to respondents’ image of it as a large, well arranged, important, and impressive organization. The ESC is a competently placed institution to further develop its relationship with young cardiologists.
23243730	[On the application of elements of data mining (the detection of useful knowledge in databases) in hygienic research and social-hygiene monitoring].
Gig Sanit  2012 Sep-Oct
In article necessity and possibility of application of Data Mining methods (detection of useful knowledge in databases) in modern hygienic researches and during the decision of practical problems of socially-hygienic monitoring is proved. As illustration for applications of Data Mining technologies for development of administrative decisions in sphere of sanitary-and-epidemiologic well-being of the population of region results of the cluster analysis of a database about medium and population health in districts of the Omsk region are presented.
23117791	LAS: a software platform to support oncological data management.
J Med Syst 20121102 2012Nov
The rapid technological evolution in the biomedical and molecular oncology fields is providing research laboratories with huge amounts of complex and heterogeneous data. Automated systems are needed to manage and analyze this knowledge, allowing the discovery of new information related to tumors and the improvement of medical treatments. This paper presents the Laboratory Assistant Suite (LAS), a software platform with a modular architecture designed to assist researchers throughout diverse laboratory activities. The LAS supports the management and the integration of heterogeneous biomedical data, and provides graphical tools to build complex analyses on integrated data. Furthermore, the LAS interfaces are designed to ease data collection and management even in hostile environments (e.g., in sterile conditions), so as to improve data quality.
23251359	Your relevance feedback is essential: enhancing the learning to rank using the virtual feature based logistic regression.
PLoS ONE 20121210 2012
Information retrieval applications have to publish their output in the form of ranked lists. Such a requirement motivates researchers to develop methods that can automatically learn effective ranking models. Many existing methods usually perform analysis on multidimensional features of query-document pairs directly and don't take users' interactive feedback information into account. They thus incur the high computation overhead and low retrieval performance due to an indefinite query expression. In this paper, we propose a Virtual Feature based Logistic Regression (VFLR) ranking method that conducts the logistic regression on a set of essential but independent variables, called virtual features (VF). They are extracted via the principal component analysis (PCA) method with the user's relevance feedback. We then predict the ranking score of each queried document to produce a ranked list. We systematically evaluate our method using the LETOR 4.0 benchmark datasets. The experimental results demonstrate that the proposal outperforms the state-of-the-art methods in terms of the Mean Average Precision (MAP), the Precision at position k (P@k), and the Normalized Discounted Cumulative Gain at position k (NDCG@k).
23244628	Usability survey of biomedical question answering systems.
Hum. Genomics 20120901 2012
We live in an age of access to more information than ever before. This can be a double-edged sword. Increased access to information allows for more informed and empowered researchers, while information overload becomes an increasingly serious risk. Thus, there is a need for intelligent information retrieval systems that can summarize relevant and reliable textual sources to satisfy a user's query. Question answering is a specialized type of information retrieval with the aim of returning precise short answers to queries posed as natural language questions. We present a review and comparison of three biomedical question answering systems: askHERMES (http://www.askhermes.org/), EAGLi (http://eagl.unige.ch/EAGLi/), and HONQA (http://services.hon.ch/cgi-bin/QA10/qa.pl).
23257057	Patients' uses of information as researchable domains of social     practice.
Health Informatics J  2012Dec
In this article we argue that research into information for patients has to extend beyond            an evaluation of particular information resources to studies of how those resources are            engaged with, made sense of and used in practice. We draw on empirical data collected in            the course of a study of a patient information resource designed for breast cancer            patients in Liverpool and Newcastle in order to demonstrate the limitations of a            restricted focus on information resources alone - namely, that it does not take into            account the specific ways in which information is incorporated within what patients do as            the grounds of 'further inference and action'. Our interest is less in discussing the            strengths and weaknesses of this particular resource than in explicating some neglected            aspects of the commonplace ways in which patients 'work' with information. We conclude by            sketching some broad features of those 'reading' and 'linking' practices, the study of            which, we believe, would help us as researchers to explicate the 'problem of information'            as it is actually encountered and resolved by patients in realworld settings for their own            practical purposes. Taking our lead from ethnomethodological studies and related research            in various fields, we argue patients' uses of information are social practices that can            and should be treated as researchable phenomena.
22553987	Systematic review of methods used in meta-analyses where a primary outcome is an adverse or unintended event.
BMC Med Res Methodol 20120503 2012
Adverse consequences of medical interventions are a source of concern, but clinical trials may lack power to detect elevated rates of such events, while observational studies have inherent limitations. Meta-analysis allows the combination of individual studies, which can increase power and provide stronger evidence relating to adverse events. However, meta-analysis of adverse events has associated methodological challenges. The aim of this study was to systematically identify and review the methodology used in meta-analyses where a primary outcome is an adverse or unintended event, following a therapeutic intervention. Using a collection of reviews identified previously, 166 references including a meta-analysis were selected for review. At least one of the primary outcomes in each review was an adverse or unintended event. The nature of the intervention, source of funding, number of individual meta-analyses performed, number of primary studies included in the review, and use of meta-analytic methods were all recorded. Specific areas of interest relating to the methods used included the choice of outcome metric, methods of dealing with sparse events, heterogeneity, publication bias and use of individual patient data. The 166 included reviews were published between 1994 and 2006. Interventions included drugs and surgery among other interventions. Many of the references being reviewed included multiple meta-analyses with 44.6% (74/166) including more than ten. Randomised trials only were included in 42.2% of meta-analyses (70/166), observational studies only in 33.7% (56/166) and a mix of observational studies and trials in 15.7% (26/166). Sparse data, in the form of zero events in one or both arms where the outcome was a count of events, was found in 64 reviews of two-arm studies, of which 41 (64.1%) had zero events in both arms. Meta-analyses of adverse events data are common and useful in terms of increasing the power to detect an association with an intervention, especially when the events are infrequent. However, with regard to existing meta-analyses, a wide variety of different methods have been employed, often with no evident rationale for using a particular approach. More specifically, the approach to dealing with zero events varies, and guidelines on this issue would be desirable.
23263052	Experimental performance comparison of duobinary and PSBT modulation formats for long-haul 40 Gb/s transmission on G 0.652 fibre.
Opt Express  2012Dec17
Duobinary formats are today considered as being one of the most promising cost-effective solutions for the deployment of 40 Gb/s technology with direct detection on existing 10 Gb/s WDM long-haul (metropolitan and core) transmission infrastructures. Various methods for generating duobinary formats have been developed in the past few years but to our knowledge their respective performances for 40 Gb/s transmission have never been really compared experimentally. Here, we propose to evaluate at 40 Gb/s their respective robustness with respect to the most stringent transmission impairments, namely ASE noise, chromatic dispersion, polarization mode dispersion and nonlinear effects. We demonstrate that, owing to its enhanced resistance to intra-channel nonlinearities as compared to non-return-to-zero, duobinary can permit to reach transmission distances compliant with metropolitan and core applications on G.652 standard single mode fibre when quasi single-channel transmission conditions are met. We show furthermore that shifting optical duobinary filtering from the transmitter output to the receiver input can be of high interest to improve further the system maximum reach. We show also that phase-shaped binary transmission (PSBT) formats are fully compliant with 50-GHz channel spacing and that they are, in terms of transmission performance, as good as partial differential phase shift keying (Partial-DPSK), which is considered by equipment suppliers as the preferential transport solution for deployment of 40 Gb/s technology with direct detection on existing 10 Gb/s WDM metropolitan and core transmission infrastructures.
23263090	Experimental evaluation of efficient routing and distributed spectrum allocation algorithms for GMPLS elastic networks.
Opt Express  2012Dec17
This paper presents and experimentally evaluates efficient strategies for dynamic source/Path Computation Element (PCE) routing with aggregated resource information and advanced distributed spectrum allocation algorithms in Generalized Multi-Protocol Label Switching (GMPLS)-controlled elastic optical networks.
23263118	Experimental demonstration of a format-flexible single-carrier coherent receiver using data-aided digital signal processing.
Opt Express  2012Dec17
We experimentally demonstrate the use of data-aided digital signal processing for format-flexible coherent reception of different 28-GBd PDM and 4D modulated signals in WDM transmission experiments over up to 7680 km SSMF by using the same resource-efficient digital signal processing algorithms for the equalization of all formats. Stable and regular performance in the nonlinear transmission regime is confirmed.
23263137	The robustness of subcarrier-index modulation in 16-QAM CO-OFDM system with 1024-point FFT.
Opt Express  2012Dec17
We present in numerical simulations the robustness of subcarrier index modulation (SIM) OFDM to combat laser phase noise. The ability of using DFB lasers with SIM-OFDM in 16-QAM CO-OFDM system with 1024-point FFT has been verified. Although SIM-OFDM has lower spectral efficiency compared to the conventional CO-OFDM system, it is a good candidate for 16-QAM CO-OFDM system with 1024-point FFT which uses a DFB laser of 1 MHz linewidth. In addition, we show the tolerance of SIM-OFDM for mitigation of fiber nonlinearities in long-haul CO-OFDM system. The simulation results show a significant penalty reduction, essentially that due to SPM.
23263142	Generation of square or hexagonal 16-QAM signals using a dual-drive IQ modulator driven by binary signals.
Opt Express  2012Dec17
We propose a simple square or hexagonal 16-QAM signal generation technique using a commercially available dual-drive IQ modulator driven by four binary electrical signals with properly designed amplitudes. We analytically derive the required driving signal amplitudes for square and hexagonal 16-QAM and characterize its implementation penalty. Polarization-multiplexed (PM)-16-QAM signals at 28 Gbuad are experimentally demonstrated and stable performance is achieved with simple bias control.
22574945	GeVaDSs - decision support system for novel Genetic Vaccine development process.
BMC Bioinformatics 20120510 2012
The lack of a uniform way for qualitative and quantitative evaluation of vaccine candidates under development led us to set up a standardized scheme for vaccine efficacy and safety evaluation. We developed and implemented molecular and immunology methods, and designed support tools for immunization data storage and analyses. Such collection can create a unique opportunity for immunologists to analyse data delivered from their laboratories. We designed and implemented GeVaDSs (Genetic Vaccine Decision Support system) an interactive system for efficient storage, integration, retrieval and representation of data. Moreover, GeVaDSs allows for relevant association and interpretation of data, and thus for knowledge-based generation of testable hypotheses of vaccine responses. GeVaDSs has been tested by several laboratories in Europe, and proved its usefulness in vaccine analysis. Case study of its application is presented in the additional files. The system is available at: http://gevads.cs.put.poznan.pl/preview/(login: viewer, password: password).
22839199	BetaSearch: a new method for querying β-residue motifs.
BMC Res Notes 20120730 2012
Searching for structural motifs across known protein structures can be useful for identifying unrelated proteins with similar function and characterising secondary structures such as β-sheets. This is infeasible using conventional sequence alignment because linear protein sequences do not contain spatial information. β-residue motifs are β-sheet substructures that can be represented as graphs and queried using existing graph indexing methods, however, these approaches are designed for general graphs that do not incorporate the inherent structural constraints of β-sheets and require computationally-expensive filtering and verification procedures. 3D substructure search methods, on the other hand, allow β-residue motifs to be queried in a three-dimensional context but at significant computational costs. We developed a new method for querying β-residue motifs, called BetaSearch, which leverages the natural planar constraints of β-sheets by indexing them as 2D matrices, thus avoiding much of the computational complexities involved with structural and graph querying. BetaSearch exhibits faster filtering, verification, and overall query time than existing graph indexing approaches whilst producing comparable index sizes. Compared to 3D substructure search methods, BetaSearch achieves 33 and 240 times speedups over index-based and pairwise alignment-based approaches, respectively. Furthermore, we have presented case-studies to demonstrate its capability of motif matching in sequentially dissimilar proteins and described a method for using BetaSearch to predict β-strand pairing. We have demonstrated that BetaSearch is a fast method for querying substructure motifs. The improvements in speed over existing approaches make it useful for efficiently performing high-volume exploratory querying of possible protein substructural motifs or conformations. BetaSearch was used to identify a nearly identical β-residue motif between an entirely synthetic (Top7) and a naturally-occurring protein (Charcot-Leyden crystal protein), as well as identifying structural similarities between biotin-binding domains of avidin, streptavidin and the lipocalin gamma subunit of human C8.
23110757	Jointly creating digital abstracts: dealing with synonymy and polysemy.
BMC Res Notes 20121030 2012
Ideally each Life Science article should get a 'structured digital abstract'. This is a structured summary of the paper's findings that is both human-verified and machine-readable. But articles can contain a large variety of information types and contextual details that all need to be reconciled with appropriate names, terms and identifiers, which poses a challenge to any curator. Current approaches mostly use tagging or limited entry-forms for semantic encoding. We implemented a 'controlled language' as a more expressive representation method. We studied how usable this format was for wet-lab-biologists that volunteered as curators. We assessed some issues that arise with the usability of ontologies and other controlled vocabularies, for the encoding of structured information by 'untrained' curators. We take a user-oriented viewpoint, and make recommendations that may prove useful for creating a better curation environment: one that can engage a large community of volunteer curators. Entering information in a biocuration environment could improve in expressiveness and user-friendliness, if curators would be enabled to use synonymous and polysemous terms literally, whereby each term stays linked to an identifier.
23110816	Sagace: a web-based search engine for biomedical databases in Japan.
BMC Res Notes 20121031 2012
In the big data era, biomedical research continues to generate a large amount of data, and the generated information is often stored in a database and made publicly available. Although combining data from multiple databases should accelerate further studies, the current number of life sciences databases is too large to grasp features and contents of each database. We have developed Sagace, a web-based search engine that enables users to retrieve information from a range of biological databases (such as gene expression profiles and proteomics data) and biological resource banks (such as mouse models of disease and cell lines). With Sagace, users can search more than 300 databases in Japan. Sagace offers features tailored to biomedical research, including manually tuned ranking, a faceted navigation to refine search results, and rich snippets constructed with retrieved metadata for each database entry. Sagace will be valuable for experts who are involved in biomedical research and drug development in both academia and industry. Sagace is freely available at http://sagace.nibio.go.jp/en/.
23057584	Repeatability and variation of region-of-interest methods using quantitative diffusion tensor MR imaging of the brain.
BMC Med Imaging 20121011 2012
Diffusion tensor imaging (DTI) is increasingly used in various diseases as a clinical tool for assessing the integrity of the brain's white matter. Reduced fractional anisotropy (FA) and an increased apparent diffusion coefficient (ADC) are nonspecific findings in most pathological processes affecting the brain's parenchyma. At present, there is no gold standard for validating diffusion measures, which are dependent on the scanning protocols, methods of the softwares and observers. Therefore, the normal variation and repeatability effects on commonly-derived measures should be carefully examined. Thirty healthy volunteers (mean age 37.8 years, SD 11.4) underwent DTI of the brain with 3T MRI. Region-of-interest (ROI) -based measurements were calculated at eleven anatomical locations in the pyramidal tracts, corpus callosum and frontobasal area. Two ROI-based methods, the circular method (CM) and the freehand method (FM), were compared. Both methods were also compared by performing measurements on a DTI phantom. The intra- and inter-observer variability (coefficient of variation, or CV%) and repeatability (intra-class correlation coefficient, or ICC) were assessed for FA and ADC values obtained using both ROI methods. The mean FA values for all of the regions were 0.663 with the CM and 0.621 with the FM. For both methods, the FA was highest in the splenium of the corpus callosum. The mean ADC value was 0.727 ×10-3 mm2/s with the CM and 0.747 ×10-3 mm2/s with the FM, and both methods found the ADC to be lowest in the corona radiata. The CV percentages of the derived measures were &lt; 13% with the CM and &lt; 10% with the FM. In most of the regions, the ICCs were excellent or moderate for both methods. With the CM, the highest ICC for FA was in the posterior limb of the internal capsule (0.90), and with the FM, it was in the corona radiata (0.86). For ADC, the highest ICC was found in the genu of the corpus callosum (0.93) with the CM and in the uncinate fasciculus (0.92) with FM. With both ROI-based methods variability was low and repeatability was moderate. The circular method gave higher repeatability, but variation was slightly lower using the freehand method. The circular method can be recommended for the posterior limb of the internal capsule and splenium of the corpus callosum, and the freehand method for the corona radiata.
23110661	Economic analysis of cloud-based desktop virtualization implementation at a hospital.
BMC Med Inform Decis Mak 20121030 2012
Cloud-based desktop virtualization infrastructure (VDI) is known as providing simplified management of application and desktop, efficient management of physical resources, and rapid service deployment, as well as connection to the computer environment at anytime, anywhere with any device. However, the economic validity of investing in the adoption of the system at a hospital has not been established. This study computed the actual investment cost of the hospital-wide VDI implementation at the 910-bed Seoul National University Bundang Hospital in Korea and the resulting effects (i.e., reductions in PC errors and difficulties, application and operating system update time, and account management time). Return on investment (ROI), net present value (NPV), and internal rate of return (IRR) indexes used for corporate investment decision-making were used for the economic analysis of VDI implementation. The results of five-year cost-benefit analysis given for 400 Virtual Machines (VMs; i.e., 1,100 users in the case of SNUBH) showed that the break-even point was reached in the fourth year of the investment. At that point, the ROI was 122.6%, the NPV was approximately US$192,000, and the IRR showed an investment validity of 10.8%. From our sensitivity analysis to changing the number of VMs (in terms of number of users), the greater the number of adopted VMs was the more investable the system was. This study confirms that the emerging VDI can have an economic impact on hospital information system (HIS) operation and utilization in a tertiary hospital setting.
23190475	Bioinformatics clouds for big data manipulation.
Biol. Direct 20121128 2012
As advances in life sciences and information technology bring profound influences on bioinformatics due to its interdisciplinary nature, bioinformatics is experiencing a new leap-forward from in-house computing infrastructure into utility-supplied cloud computing delivered over the Internet, in order to handle the vast quantities of biological data generated by high-throughput experimental technologies. Albeit relatively new, cloud computing promises to address big data storage and analysis issues in the bioinformatics field. Here we review extant cloud-based services in bioinformatics, classify them into Data as a Service (DaaS), Software as a Service (SaaS), Platform as a Service (PaaS), and Infrastructure as a Service (IaaS), and present our perspectives on the adoption of cloud computing in bioinformatics. This article was reviewed by Frank Eisenhaber, Igor Zhulin, and Sandor Pongor.
22248634	A model-based sequence similarity with application to handwritten word spotting.
IEEE Trans Pattern Anal Mach Intell  2012Nov
This paper proposes a novel similarity measure between vector sequences. We work in the framework of model-based approaches, where each sequence is first mapped to a Hidden Markov Model (HMM) and then a measure of similarity is computed between the HMMs. We propose to model sequences with semicontinuous HMMs (SC-HMMs). This is a particular type of HMM whose emission probabilities in each state are mixtures of shared Gaussians. This crucial constraint provides two major benefits. First, the a priori information contained in the common set of Gaussians leads to a more accurate estimate of the HMM parameters. Second, the computation of a similarity between two SC-HMMs can be simplified to a Dynamic Time Warping (DTW) between their mixture weight vectors, which significantly reduces the computational cost. Experiments are carried out on a handwritten word retrieval task in three different datasets-an in-house dataset of real handwritten letters, the George Washington dataset, and the IFN/ENIT dataset of Arabic handwritten words. These experiments show that the proposed similarity outperforms the traditional DTW between the original sequences, and the model-based approach which uses ordinary continuous HMMs. We also show that this increase in accuracy can be traded against a significant reduction of the computational cost.
22231592	Embedding retrieval of articulated geometry models.
IEEE Trans Pattern Anal Mach Intell  2012Nov
Due to the popularity of computer games and animation, research on 3D articulated geometry model retrieval has attracted a lot of attention in recent years. However, most existing works extract high-dimensional features to represent models and suffer from practical limitations. First, misalignment in high-dimensional features may produce unreliable euclidean distances and affect retrieval accuracy. Second, the curse of dimensionality also degrades efficiency. In this paper, we propose an embedding retrieval framework to improve the practicability of these methods. It is based on a manifold learning technique, the Diffusion Map (DM). We project all pairwise distances onto a low-dimensional space. This improves retrieval accuracy because intercluster distances are exaggerated. Then we adapt the Density-Weighted Nyström extension and further propose a novel step to locally align the Nyström embedding to the eigensolver embedding so as to reduce extension error and preserve retrieval accuracy. Finally, we propose a heuristic to handle disconnected manifolds by augmenting the kernel matrix with multiple similarity measures and shortcut edges, and further discuss the choice of DM parameters. We have incorporated two existing matching algorithms for testing. Our experimental results show improvement in precision at high recalls and in speed. Our work provides a robust retrieval framework for the matching of multimedia data that lie on manifolds.
22201053	Human identification using temporal information preserving gait template.
IEEE Trans Pattern Anal Mach Intell  2012Nov
Gait Energy Image (GEI) is an efficient template for human identification by gait. However, such a template loses temporal information in a gait sequence, which is critical to the performance of gait recognition. To address this issue, we develop a novel temporal template, named Chrono-Gait Image (CGI), in this paper. The proposed CGI template first extracts the contour in each gait frame, followed by encoding each of the gait contour images in the same gait sequence with a multichannel mapping function and compositing them to a single CGI. To make the templates robust to a complex surrounding environment, we also propose CGI-based real and synthetic temporal information preserving templates by using different gait periods and contour distortion techniques. Extensive experiments on three benchmark gait databases indicate that, compared with the recently published gait recognition approaches, our CGI-based temporal information preserving approach achieves competitive performance in gait recognition with robustness and efficiency.
22248632	Minimum-distortion isometric shape correspondence using EM algorithm.
IEEE Trans Pattern Anal Mach Intell  2012Nov
We present a purely isometric method that establishes 3D correspondence between two (nearly) isometric shapes. Our method evenly samples high-curvature vertices from the given mesh representations, and then seeks an injective mapping from one vertex set to the other that minimizes the isometric distortion. We formulate the problem of shape correspondence as combinatorial optimization over the domain of all possible mappings, which then reduces in a probabilistic setting to a log-likelihood maximization problem that we solve via the Expectation-Maximization (EM) algorithm. The EM algorithm is initialized in the spectral domain by transforming the sampled vertices via classical Multidimensional Scaling (MDS). Minimization of the isometric distortion, and hence maximization of the log-likelihood function, is then achieved in the original 3D euclidean space, for each iteration of the EM algorithm, in two steps: by first using bipartite perfect matching, and then a greedy optimization algorithm. The optimal mapping obtained at convergence can be one-to-one or many-to-one upon choice. We demonstrate the performance of our method on various isometric (or nearly isometric) pairs of shapes for some of which the ground-truth correspondence is available.
23282463	Compareads: comparing huge metagenomic experiments.
BMC Bioinformatics 20121219 2012
Nowadays, metagenomic sample analyses are mainly achieved by comparing them with a priori knowledge stored in data banks. While powerful, such approaches do not allow to exploit unknown and/or "unculturable" species, for instance estimated at 99% for Bacteria. This work introduces Compareads, a de novo comparative metagenomic approach that returns the reads that are similar between two possibly metagenomic datasets generated by High Throughput Sequencers. One originality of this work consists in its ability to deal with huge datasets. The second main contribution presented in this paper is the design of a probabilistic data structure based on Bloom filters enabling to index millions of reads with a limited memory footprint and a controlled error rate. We show that Compareads enables to retrieve biological information while being able to scale to huge datasets. Its time and memory features make Compareads usable on read sets each composed of more than 100 million Illumina reads in a few hours and consuming 4 GB of memory, and thus usable on today's personal computers. Using a new data structure, Compareads is a practical solution for comparing de novo huge metagenomic samples. Compareads is released under the CeCILL license and can be freely downloaded from http://alcovna.genouest.org/compareads/.
23282094	A de novo next generation genomic sequence assembler based on string graph and MapReduce cloud computing framework.
BMC Genomics 20121213 2012
State-of-the-art high-throughput sequencers, e.g., the Illumina HiSeq series, generate sequencing reads that are longer than 150 bp up to a total of 600 Gbp of data per run. The high-throughput sequencers generate lengthier reads with greater sequencing depth than those generated by previous technologies. Two major challenges exist in using the high-throughput technology for de novo assembly of genomes. First, the amount of physical memory may be insufficient to store the data structure of the assembly algorithm, even for high-end multicore processors. Moreover, the graph-theoretical model used to capture intersection relationships of the reads may contain structural defects that are not well managed by existing assembly algorithms. We developed a distributed genome assembler based on string graphs and MapReduce framework, known as the CloudBrush. The assembler includes a novel edge-adjustment algorithm to detect structural defects by examining the neighboring reads of a specific read for sequencing errors and adjusting the edges of the string graph, if necessary. CloudBrush is evaluated against GAGE benchmarks to compare its assembly quality with the other assemblers. The results show that our assemblies have a moderate N50, a low misassembly rate of misjoins, and indels of &gt; 5 bp. In addition, we have introduced two measures, known as precision and recall, to address the issues of faithfully aligned contigs to target genomes. Compared with the assembly tools used in the GAGE benchmarks, CloudBrush is shown to produce contigs with high precision and recall. We also verified the effectiveness of the edge-adjustment algorithm using simulated datasets and ran CloudBrush on a nematode dataset using a commercial cloud. CloudBrush assembler is available at https://github.com/ice91/CloudBrush.
23282288	GLAD4U: deriving and prioritizing gene lists from PubMed literature.
BMC Genomics 20121217 2012
Answering questions such as "Which genes are related to breast cancer?" usually requires retrieving relevant publications through the PubMed search engine, reading these publications, and creating gene lists. This process is not only time-consuming, but also prone to errors. We report GLAD4U (Gene List Automatically Derived For You), a new, free web-based gene retrieval and prioritization tool. GLAD4U takes advantage of existing resources of the NCBI to ensure computational efficiency. The quality of gene lists created by GLAD4U for three Gene Ontology (GO) terms and three disease terms was assessed using corresponding "gold standard" lists curated in public databases. For all queries, GLAD4U gene lists showed very high recall but low precision, leading to low F-measure. As a comparison, EBIMed's recall was consistently lower than GLAD4U, but its precision was higher. To present the most relevant genes at the top of a list, we studied two prioritization methods based on publication count and the hypergeometric test, and compared the ranked lists and those generated by EBIMed to the gold standards. Both GLAD4U methods outperformed EBIMed for all queries based on a variety of quality metrics. Moreover, the hypergeometric method allowed for a better performance by thresholding genes with low scores. In addition, manual examination suggests that many false-positives could be explained by the incompleteness of the gold standards. The GLAD4U user interface accepts any valid queries for PubMed, and its output page displays the ranked gene list and information associated with each gene, chronologically-ordered supporting publications, along with a summary of the run and links for file export and functional enrichment and protein interaction network analysis. GLAD4U has a high overall recall. Although precision is generally low, the prioritization methods successfully rank truly relevant genes at the top of the lists to facilitate efficient browsing. GLAD4U is simple to use, and its interface can be found at: http://bioinfo.vanderbilt.edu/glad4u.
23286057	Geodesic information flows.
Med Image Comput Comput Assist Interv  2012
Homogenising the availability of manually generated information in large databases has been a key challenge of medical imaging for many years. Due to the time consuming nature of manually segmenting, parcellating and localising landmarks in medical images, these sources of information tend to be scarce and limited to small, and sometimes morphologically similar, subsets of data. In this work we explore a new framework where these sources of information can be propagated to morphologically dissimilar images by diffusing and mapping the information through intermediate steps. The spatially variant data embedding uses the local morphology and intensity similarity between images to diffuse the information only between locally similar images. This framework can thus be used to propagate any information from any group of subject to every other subject in a database with great accuracy. Comparison to state-of-the-art propagation methods showed highly statistically significant (p &lt; 10(-4)) improvements in accuracy when propagating both structural parcelations and brain segmentations geodesically.
23286072	Simplified labeling process for medical image segmentation.
Med Image Comput Comput Assist Interv  2012
Image segmentation plays a crucial role in many medical imaging applications by automatically locating the regions of interest. Typically supervised learning based segmentation methods require a large set of accurately labeled training data. However, thel labeling process is tedious, time consuming and sometimes not necessary. We propose a robust logistic regression algorithm to handle label outliers such that doctors do not need to waste time on precisely labeling images for training set. To validate its effectiveness and efficiency, we conduct carefully designed experiments on cervigram image segmentation while there exist label outliers. Experimental results show that the proposed robust logistic regression algorithms achieve superior performance compared to previous methods, which validates the benefits of the proposed algorithms.
23286116	Neighbourhood approximation forests.
Med Image Comput Comput Assist Interv  2012
Methods that leverage neighbourhood structures in high-dimensional image spaces have recently attracted attention. These approaches extract information from a new image using its "neighbours" in the image space equipped with an application-specific distance. Finding the neighbourhood of a given image is challenging due to large dataset sizes and costly distance evaluations. Furthermore, automatic neighbourhood search for a new image is currently not possible when the distance is based on ground truth annotations. In this article we present a general and efficient solution to these problems. "neighbourhood approximation forests" (NAF) is a supervised learning algorithm that approximates the neighbourhood structure resulting from an arbitrary distance. As NAF uses only image intensities to infer neighbours it can also be applied to distances based on ground truth annotations. We demonstrate NAF in two scenarios: (i) choosing neighbours with respect to a deformation-based distance, and (ii) age prediction from brain MRI. The experiments show NAF's approximation quality, computational advantages and use in different contexts.
23286118	Self-similarity weighted mutual information: a new nonrigid image registration metric.
Med Image Comput Comput Assist Interv  2012
Extending mutual information (MI), which has been widely used as a similarity measure for rigid registration of multi-modal images, to deformable registration is an active field of research. We propose a self-similarity weighted graph-based implementation of alpha-mutual information (alpha-MI) for nonrigid image registration. The new Self Similarity alpha-MI (SeSaMI) metric takes local structures into account and is robust against signal non-stationarity and intensity distortions. We have used SeSaMI as the similarity measure in a regularized cost function with B-spline deformation field. Since the gradient of SeSaMI can be derived analytically, the cost function can be efficiently optimized using stochastic gradient descent. We show that SeSaMI produces a robust and smooth cost function and outperforms the state of the art statistical based similarity metrics in simulation and using data from image-guided neurosurgery.
23286124	Estimation and reduction of target registration error.
Med Image Comput Comput Assist Interv  2012
Fiducial-based registration is often utilized in image guided surgery because of its simplicity and speed. The assessment of target registration error when using this technique, however, is difficult. Although the distribution of the target registration error can be estimated given the fiducial configuration and an estimation of the fiducial localization error, the target registration error for a specific registration is uncorrelated with the fiducial registration error. Fiducial registration error is thus an unreliable predictor of the target registration error for a particular case. In this work, we present a new method to estimate the quality of a fiducial-based registration and show that our measure is correlated to the target registration error and that it can be used to reduce registration error caused by fiducial localization error. This has direct implication on the attainable accuracy of fiducial-based registration methods.
23286126	Initialising groupwise non-rigid registration using multiple parts+geometry models.
Med Image Comput Comput Assist Interv  2012
Groupwise non-rigid registration is an important technique in medical image analysis. Recent studies show that its accuracy can be greatly improved by explicitly providing good initialisation. This is achieved by seeking a sparse correspondence using a parts+geometry model. In this paper we show that a single parts+geometry model is unlikely to establish consistent sparse correspondence for complex objects, and that better initialisation can be achieved using a set of models. We describe how to combine the strengths of multiple models, and demonstrate that the method gives state-of-the-art performance on three datasets, with the most significant improvement on the most challenging.
23286132	Eigenanatomy improves detection power for longitudinal cortical change.
Med Image Comput Comput Assist Interv  2012
We contribute a novel and interpretable dimensionality reduction strategy, eigenanatomy, that is tuned for neuroimaging data. The method approximates the eigendecomposition of an image set with basis functions (the eigenanatomy vectors) that are sparse, unsigned and are anatomically clustered. We employ the eigenanatomy vectors as anatomical predictors to improve detection power in morphometry. Standard voxel-based morphometry (VBM) analyzes imaging data voxel-by-voxel--and follows this with cluster-based or voxel-wise multiple comparisons correction methods to determine significance. Eigenanatomy reverses the standard order of operations by first clustering the voxel data and then using standard linear regression in this reduced dimensionality space. As with traditional region-of-interest (ROI) analysis, this strategy can greatly improve detection power. Our results show that eigenanatomy provides a principled objective function that leads to localized, data-driven regions of interest. These regions improve our ability to quantify biologically plausible rates of cortical change in two distinct forms of neurodegeneration. We detail the algorithm and show experimental evidence of its efficacy.
23286137	Improving accuracy and power with transfer learning using a meta-analytic database.
Med Image Comput Comput Assist Interv  2012
Typical cohorts in brain imaging studies are not large enough for systematic testing of all the information contained in the images. To build testable working hypotheses, investigators thus rely on analysis of previous work, sometimes formalized in a so-called meta-analysis. In brain imaging, this approach underlies the specification of regions of interest (ROIs) that are usually selected on the basis of the coordinates of previously detected effects. In this paper, we propose to use a database of images, rather than coordinates, and frame the problem as transfer learning: learning a discriminant model on a reference task to apply it to a different but related new task. To facilitate statistical analysis of small cohorts, we use a sparse discriminant model that selects predictive voxels on the reference task and thus provides a principled procedure to define ROIs. The benefits of our approach are twofold. First it uses the reference database for prediction, i.e., to provide potential biomarkers in a clinical setting. Second it increases statistical power on the new task. We demonstrate on a set of 18 pairs of functional MRI experimental conditions that our approach gives good prediction. In addition, on a specific transfer situation involving different scanners at different locations, we show that voxel selection based on transfer learning leads to higher detection power on small cohorts.
23286159	Non-local STAPLE: an intensity-driven multi-atlas rater model.
Med Image Comput Comput Assist Interv  2012
Multi-atlas segmentation provides a general purpose, fully automated class of techniques for transferring spatial information from an existing dataset ("atlases") to a previously unseen context ("target") through image registration. The method used to combine information after registration ("label fusion") has a substantial impact on the overall accuracy and robustness. In practice, weighted voting techniques have dramatically outperformed algorithms based on statistical fusion (i.e., algorithms that incorporate rater performance into the estimation process--STAPLE). We posit that a critical limitation of statistical techniques (as generally proposed) is that they fail to incorporate intensity seamlessly into the estimation process and models of observation error. Herein, we propose a novel statistical fusion algorithm, non-local STAPLE, which merges the STAPLE framework with a non-local means perspective. Non-local STAPLE (1) seamlessly integrates intensity into the estimation process, (2) provides a theoretically consistent model of multi-atlas observation error, and (3) largely bypasses the need for group-wise unbiased registrations. We demonstrate significant improvements in two empirical multi-atlas experiments.
23286182	Phase contrast image restoration via dictionary representation of diffraction patterns.
Med Image Comput Comput Assist Interv  2012
The restoration of microscopy images makes the segmentation and detection of cells easier and more reliable, which facilitates automated cell tracking and cell behavior analysis. In this paper, the authors analyze the image formation process of phase contrast images and propose an image restoration method based on the dictionary representation of diffraction patterns. By formulating and solving a min-l1 optimization problem, each pixel is restored into a feature vector corresponding to the dictionary representation. Cells in the images are then segmented by the feature vector clustering. In addition to segmentation, since the feature vectors capture the information on the phase retardation caused by cells, they can be used for cell stage classification between intermitotic and mitotic/apoptotic stages. Experiments on three image sequences demonstrate that the dictionary-based restoration method can restore phase contrast images containing cells with different optical natures and provide promising results on cell stage classification.
23300414	Chapter 13: Mining electronic health records in the genomics era.
PLoS Comput. Biol. 20121227 2012
The combination of improved genomic analysis methods, decreasing genotyping costs, and increasing computing resources has led to an explosion of clinical genomic knowledge in the last decade. Similarly, healthcare systems are increasingly adopting robust electronic health record (EHR) systems that not only can improve health care, but also contain a vast repository of disease and treatment data that could be mined for genomic research. Indeed, institutions are creating EHR-linked DNA biobanks to enable genomic and pharmacogenomic research, using EHR data for phenotypic information. However, EHRs are designed primarily for clinical care, not research, so reuse of clinical EHR data for research purposes can be challenging. Difficulties in use of EHR data include: data availability, missing data, incorrect data, and vast quantities of unstructured narrative text data. Structured information includes billing codes, most laboratory reports, and other variables such as physiologic measurements and demographic information. Significant information, however, remains locked within EHR narrative text documents, including clinical notes and certain categories of test results, such as pathology and radiology reports. For relatively rare observations, combinations of simple free-text searches and billing codes may prove adequate when followed by manual chart review. However, to extract the large cohorts necessary for genome-wide association studies, natural language processing methods to process narrative text data may be needed. Combinations of structured and unstructured textual data can be mined to generate high-validity collections of cases and controls for a given condition. Once high-quality cases and controls are identified, EHR-derived cases can be used for genomic discovery and validation. Since EHR data includes a broad sampling of clinically-relevant phenotypic information, it may enable multiple genomic investigations upon a single set of genotyped individuals. This chapter reviews several examples of phenotype extraction and their application to genetic research, demonstrating a viable future for genomic discovery using EHR-linked data.
23300418	Chapter 10: Mining genome-wide genetic markers.
PLoS Comput. Biol. 20121227 2012
Genome-wide association study (GWAS) aims to discover genetic factors underlying phenotypic traits. The large number of genetic factors poses both computational and statistical challenges. Various computational approaches have been developed for large scale GWAS. In this chapter, we will discuss several widely used computational approaches in GWAS. The following topics will be covered: (1) An introduction to the background of GWAS. (2) The existing computational approaches that are widely used in GWAS. This will cover single-locus, epistasis detection, and machine learning methods that have been recently developed in biology, statistic, and computer science communities. This part will be the main focus of this chapter. (3) The limitations of current approaches and future directions.
23304276	Characterizing the use and contents of free-text family history comments in the Electronic Health Record.
AMIA Annu Symp Proc 20121103 2012
The detailed collection of family history information is becoming increasingly important for patient care and biomedical research. Recent reports have highlighted the need for efforts to better understand collection and use of this information in resources such as the Electronic Health Record (EHR). This two-part study involved characterizing the use and contents of free-text comments within the family history section of an EHR. Based on a manual review of a subset of 11,456 cancer-related family history entries, 20 "reasons for use" were identified and the distribution across these reasons determined. A semi-automated analysis of the 3,358 unique comments associated with these entries was then performed to identify and quantify key categories of information. Implications of this study include guiding efforts for the improved use, collection, and subsequent analysis of family history information in the EHR.
23304279	Meeting the electronic health record "meaningful use" criterion for the HL7 infobutton standard using OpenInfobutton and the Librarian Infobutton Tailoring Environment (LITE).
AMIA Annu Symp Proc 20121103 2012
Infobuttons are clinical decision support tools that use information about the clinical context (institution, user, patient) in which an information need arises to provide direct access to relevant information from knowledge resources. Two freely available resources make infobutton implementation possible for virtually any EHR system. OpenInfobutton is an HL7-compliant system that accepts context parameters from an EHR and, using its knowledge base of resources and information needs, generates a set of links that direct the user to relevant information. The Librarian Infobutton Tailoring Environment (LITE) is a second system that allows institutional librarians to specify which resources should be selected in a given context by OpenInfobutton. This paper describes the steps needed to use LITE to customize OpenInfobutton and to integrate OpenInfobutton into an EHR.
23304280	Syntactic dependency parsers for biomedical-NLP.
AMIA Annu Symp Proc 20121103 2012
Syntactic parsers have made a leap in accuracy and speed in recent years. The high order structural information provided by dependency parsers is useful for a variety of NLP applications. We present a biomedical model for the EasyFirst parser, a fast and accurate parser for creating Stanford Dependencies. We evaluate the models trained in the biomedical domains of EasyFirst and Clear-Parser in a number of task oriented metrics. Both parsers provide stat of the art speed and accuracy in the Genia of over 89%. We show that Clear-Parser excels at tasks relating to negation identification while EasyFirst excels at tasks relating to Named Entities and is more robust to changes in domain.
23304287	Barriers to retrieving patient information from electronic health record data: failure analysis from the TREC Medical Records Track.
AMIA Annu Symp Proc 20121103 2012
Secondary use of electronic health record (EHR) data relies on the ability to retrieve accurate and complete information about desired patient populations. The Text Retrieval Conference (TREC) 2011 Medical Records Track was a challenge evaluation allowing comparison of systems and algorithms to retrieve patients eligible for clinical studies from a corpus of de-identified medical records, grouped by patient visit. Participants retrieved cohorts of patients relevant to 35 different clinical topics, and visits were judged for relevance to each topic. This study identified the most common barriers to identifying specific clinic populations in the test collection. Using the runs from track participants and judged visits, we analyzed the five non-relevant visits most often retrieved and the five relevant visits most often overlooked. Categories were developed iteratively to group the reasons for incorrect retrieval for each of the 35 topics. Reasons fell into nine categories for non-relevant visits and five categories for relevant visits. Non-relevant visits were most often retrieved because they contained a non-relevant reference to the topic terms. Relevant visits were most often infrequently retrieved because they used a synonym for a topic term. This failure analysis provides insight into areas for future improvement in EHR-based retrieval with techniques such as more widespread and complete use of standardized terminology in retrieval and data entry systems.
23304308	An EHR prototype using structured ISO/EN 13606 documents to respond to identified clinical information needs of diabetes specialists: a controlled study on feasibility and impact.
AMIA Annu Symp Proc 20121103 2012
Cross-institutional longitudinal Electronic Health Records (EHR), as introduced in Austria at the moment, increase the challenge of information overload of healthcare professionals. We developed an innovative cross-institutional EHR query prototype that offers extended query options, including searching for specific information items or sets of information items. The available query options were derived from a systematic analysis of information needs of diabetes specialists during patient encounters. The prototype operates in an IHE-XDS-based environment where ISO/EN 13606-structured documents are available. We conducted a controlled study with seven diabetes specialists to assess the feasibility and impact of this EHR query prototype on efficient retrieving of patient information to answer typical clinical questions. The controlled study showed that the specialists were quicker and more successful (measured in percentage of expected information items found) in finding patient information compared to the standard full-document search options. The participants also appreciated the extended query options.
23304309	Evolution in clinical knowledge management strategy at Intermountain Healthcare.
AMIA Annu Symp Proc 20121103 2012
In this manuscript, we present an overview of the clinical knowledge management strategy at Intermountain Healthcare in support of our electronic medical record systems. Intermountain first initiated efforts in developing a centralized enterprise knowledge repository in 2001. Applications developed, areas of emphasis served, and key areas of focus are presented. We also detail historical and current areas of emphasis, in response to business needs.
23304318	Finding and accessing diagrams in biomedical publications.
AMIA Annu Symp Proc 20121103 2012
Complex relationships in biomedical publications are often communicated by diagrams such as bar and line charts, which are a very effective way of summarizing and communicating multi-faceted data sets. Given the ever-increasing amount of published data, we argue that the precise retrieval of such diagrams is of great value for answering specific and otherwise hard-to-meet information needs. To this end, we demonstrate the use of advanced image processing and classification for identifying bar and line charts by the shape and relative location of the different image elements that make up the charts. With recall and precisions of close to 90% for the detection of relevant figures, we discuss the use of this technology in an existing biomedical image search engine, and outline how it enables new forms of literature queries over biomedical relationships that are represented in these charts.
23304326	Extracting temporal information from electronic patient records.
AMIA Annu Symp Proc 20121103 2012
A method for automatic extraction of clinical temporal information would be of significant practical importance for deep medical language understanding, and a key to creating many successful applications, such as medical decision making, medical question and answering, etc. This paper proposes a rich statistical model for extracting temporal information from an extremely noisy clinical corpus. Besides the common linguistic, contextual and semantic features, the highly restricted training sample expansion and the structure distance between the temporal expression &amp; related event expressions are also integrated into a supervised machine-learning approach. The learning method produces almost 80% F- score in the extraction of five temporal classes, and nearly 75% F-score in identifying temporally related events. This process has been integrated into the document-processing component of an implemented clinical question answering system that focuses on answering patient-specific questions (See demonstration at http://hitrl.cs.usyd.edu.au/ICNS/).
23304327	ClinData Express--a metadata driven clinical research data management system for secondary use of clinical data.
AMIA Annu Symp Proc 20121103 2012
Aim to ease the secondary use of clinical data in clinical research, we introduce a metadata driven web-based clinical data management system named ClinData Express. ClinData Express is made up of two parts: 1) m-designer, a standalone software for metadata definition; 2) a web based data warehouse system for data management. With ClinData Express, what the researchers need to do is to define the metadata and data model in the m-designer. The web interface for data collection and specific database for data storage will be automatically generated. The standards used in the system and the data export modular make sure of the data reuse. The system has been tested on seven disease-data collection in Chinese and one form from dbGap. The flexibility of system makes its great potential usage in clinical research. The system is available at http://code.google.com/p/clindataexpress.
23304331	Using SemRep to label semantic relations extracted from clinical text.
AMIA Annu Symp Proc 20121103 2012
In this paper we examined the relationship between semantic relatedness among medical concepts found in clinical reports and biomedical literature. Our objective is to determine whether relations between medical concepts identified from Medline abstracts may be used to inform us as to the nature of the association between medical concepts that appear to be closely related based on their distribution in clinical reports. We used a corpus of 800k inpatient clinical notes as a source of data for determining the strength of association between medical concepts and SemRep database as a source of labeled relations extracted from Medline abstracts. The same pair of medical concepts may be found with more than one predicate type in the SemRep database but often with different frequencies. Our analysis shows that predicate type frequency information obtained from the SemRep database appears to be helpful for labeling semantic relations obtained with measures of semantic relatedness and similarity.
23304354	OPIC: Ontology-driven Patient Information Capturing system for epilepsy.
AMIA Annu Symp Proc 20121103 2012
The widespread use of paper or document-based forms for capturing patient information in various clinical settings, for example in epilepsy centers, is a critical barrier for large-scale, multi-center research studies that require interoperable, consistent, and error-free data collection. This challenge can be addressed by a web-accessible and flexible patient data capture system that is supported by a common terminological system to facilitate data re-usability, sharing, and integration. We present OPIC, an Ontology-driven Patient Information Capture (OPIC) system that uses a domain-specific epilepsy and seizure ontology (EpSO) to (1) support structured entry of multi-modal epilepsy data, (2) proactively ensure quality of data through use of ontology terms in drop-down menus, and (3) identify and index clinically relevant ontology terms in free-text fields to improve accuracy of subsequent analytical queries (e.g. cohort identification). EpSO, modeled using the Web Ontology Language (OWL), conforms to the recommendations of the International League Against Epilepsy (ILAE) classification and terminological commission. OPIC has been developed using agile software engineering methodology for rapid development cycles in close collaboration with domain expert and end users. We report the result from the initial deployment of OPIC at the University Hospitals Case Medical Center (UH CMC) epilepsy monitoring unit (EMU) as part of the NIH-funded project on Sudden Unexpected Death in Epilepsy (SUDEP). Preliminary user evaluation shows that OPIC has achieved its design objectives to be an intuitive patient information capturing system that also reduces the potential for data entry errors and variability in use of epilepsy terms.
23304361	Towards the creation of a visual ontology of biomedical imaging entities.
AMIA Annu Symp Proc 20121103 2012
Image content is frequently the target of biomedical information extraction systems. However, the meaning of this content cannot be easily understood without some associated text. In order to improve the integration of textual and visual information, we are developing a visual ontology for biomedical image retrieval. Our visual ontology maps the appearance of image regions to concepts in an existing textual ontology, thereby inheriting relationships among the visual entities. Such a resource creates a bridge between the visual characteristics of important image regions and their semantic interpretation. We automatically populate our visual ontology by pairing image regions with their associated descriptions. To demonstrate the usefulness of this resource, we have developed a classification method that automatically labels image regions with appropriate concepts based solely on their appearance. Our results for thoracic imaging terms show that our methods are promising first steps towards the creation of a biomedical visual ontology.
23304363	Evaluation of automated term groupings for detecting anaphylactic shock signals for drugs.
AMIA Annu Symp Proc 20121103 2012
Signal detection in pharmacovigilance should take into account all terms related to a medical concept rather than a single term. We built an OWL-DL file with formal definitions of MedDRA and SNOMED-CT concepts and performed two queries, Query 1 and 2, to retrieve narrow and broad terms within the Standard MedDRA Query (SMQ) related to 'anaphylactic shock' and the terms from the High Level Term (HLT) grouping related to 'anaphylaxis'. We compared values of the EB05 (EBGM) statistical test for disproportionality with 50 active ingredients randomly selected in the public version of the FDA pharmacovigilance database. Coefficient of correlation was R(2) = 1.00 between Query 1 and HLT; R(2) = 0.98 between Query 1 and SMQ narrow; R(2) = 0.89 between Query 2 and SMQ Narrow+Broad. Generating automated groupings of terms for signal detection is feasible but requires additional efforts in modeling MedDRA terms in order to improve precision and recall of these groupings.
23304371	Apps to display patient data, making SMART available in the i2b2 platform.
AMIA Annu Symp Proc 20121103 2012
The Substitutable Medical Apps, Reusable Technologies (SMART) project provides a framework of core services to facilitate the use of substitutable health-related web applications. The platform offers a common interface used to "SMART-ready" health IT systems allowing any SMART application to be able to interact with those systems. At Partners Healthcare, we have SMART-enabled the Informatics for Integrating Biology and the Bedside (i2b2) open source analytical platform, enabling the use of SMART applications directly within the i2b2 web client. In i2b2, viewing the patient in an EMR-like view enables a natural-feeling medical review process for each patient.
23304381	Synonym, topic model and predicate-based query expansion for retrieving clinical documents.
AMIA Annu Symp Proc 20121103 2012
We present a study that developed and tested three query expansion methods for the retrieval of clinical documents. Finding relevant documents in a large clinical data warehouse is a challenging task. To address this issue, first, we implemented a synonym expansion strategy that used a few selected vocabularies. Second, we trained a topic model on a large set of clinical documents, which was then used to identify related terms for query expansion. Third, we obtained related terms from a large predicate database derived from Medline abstracts for query expansion. The three expansion methods were tested on a set of clinical notes. All three methods successfully achieved higher average recalls and average F-measures when compared with the baseline method. The average precisions and precision at 10, however, decreased with all expansions. Amongst the three expansion methods, the topic model-based method performed the best in terms of recall and F-measure.
23304389	Hyperdimensional computing approach to word sense disambiguation.
AMIA Annu Symp Proc 20121103 2012
Coping with the ambiguous meanings of words has long been a hurdle for information retrieval and natural language processing systems. This paper presents a new word sense disambiguation approach using high-dimensional binary vectors, which encode meanings of words based on the different contexts in which they occur. In our approach, a randomly constructed vector is assigned to each ambiguous term, and another to each sense of this term. In the context of a sense-annotated training set, a reversible vector transformation is used to combine these vectors, such that both the term and the sense assigned to a context in which the term occurs are encoded into vectors representing the surrounding terms in this context. When a new context is encountered, the information required to disambiguate this term is extracted from the trained semantic vectors for the terms in this context by reversing the vector transformation to recover the correct sense of the term. On repeated experiments using ten-fold cross-validation and a standard test set, we obtained results comparable to the best obtained in previous studies. These results demonstrate the potential of our methodology, and suggest directions for future research.
23304394	Emergency department physician internet use during clinical encounters.
AMIA Annu Symp Proc 20121103 2012
This study explored the Internet log files from emergency department workstations to determine search patterns, compared them to discharge diagnoses, and the emergency medicine curriculum as a way to quantify physician search behaviors. The log files from the computers from January 2006 to March 2010 were mapped to the EM curriculum and compared to discharge diagnoses to explore search terms and website usage by physicians and students. Physicians in the ED averaged 1.35 searches per patient encounter using Google.com and UpToDate.com 83.9% of the time. The most common searches were for drug information (23.1%) by all provider types. The majority of the websites utilized were in the third tier evidence level for evidence-based medicine (EBM). We have shown a need for a readily accessible drug knowledge base within the EMR for decision support as well as easier access to first and second tier EBM evidence.
23304395	An application for monitoring order set usage in a commercial electronic health record.
AMIA Annu Symp Proc 20121103 2012
Organizations that use electronic health records (EHRs) often maintain a considerable amount of clinical content in the form of order sets, documentation templates, and decision support rules. EHR vendors seldom provide analytic tools for customers to maintain such content and monitor its usage. We developed an application for tracking order sets, documentation templates and clinical alerts in a commercial electronic health record. Using the application, we compared trends in order set creation and usage at two academic medical centers over a three-year period. In January 2012, one medical center had 873 order sets available to clinicians; the other had 787. Approximately 50-75 new order sets were added each year at each medical center. We found that 46% of order sets at the first medical center and 39% at the second medical center were unused over the three-year period.
23304423	A study of actions in operative notes.
AMIA Annu Symp Proc 20121103 2012
Operative notes contain rich information about techniques, instruments, and materials used in procedures. To assist development of effective information extraction (IE) techniques for operative notes, we investigated the sublanguage used to describe actions within the operative report 'procedure description' section. Deep parsing results of 362,310 operative notes with an expanded Stanford parser using the SPECIALIST Lexicon resulted in 200 verbs (92% coverage) including 147 action verbs. Nominal action predicates for each action verb were gathered from WordNet, SPECIALIST Lexicon, New Oxford American Dictionary and Stedman's Medical Dictionary. Coverage gaps were seen in existing lexical, domain, and semantic resources (Unified Medical Language System (UMLS) Metathesaurus, SPECIALIST Lexicon, WordNet and FrameNet). Our findings demonstrate the need to construct surgical domain-specific semantic resources for IE from operative notes.
23272771	Search strategies to identify reports on "off-label" drug use in EMBASE.
BMC Med Res Methodol 20121229 2012
Medications are frequently prescribed outside their regulatory approval (off-label) by physicians particularly where appropriate therapies are not available. However, the risk/benefit ratio of drugs in off-label use needs to be critically appraised because it may differ from approved on-label usage. Therefore, an extensive exploration of current evidence on clinical data is well-advised. The objective of this study was to develop a search strategy that facilitates detection of the off-label drug use documents in EMBASE via OvidSP. We constructed two sets of gold standards from relevant records to off-label drug use by a sensitive search of MEDLINE and EMBASE. Search queries, including search words and strings, were conceived based on definition of off-label use of medications as well as text analysis of 500 randomly selected relevant documents. The selected terms were searched in EMBASE (from 1988 to 2011) and their retrieval performance was compared with the gold standards. We developed a sensitivity-maximizing, and a sensitivity- and precision-maximizing search strategy. From 4067 records relevant to off-label drug use in our full gold standard set, 3846 records were retrievable from EMBASE. "off label*.af." was the most sensitive single term (overall sensitivity 77.5%, sensitivity within EMBASE 81.9%, precision 88.1%). The highest sensitive search strategy was achieved by combining 36 search queries with overall sensitivity of 94.0% and precision of 69.5%. An optimal sensitive and precise search strategy was yielded precision 87.4% at the expense of decreasing overall sensitivity to 89.4%. We developed highly sensitive search strategies to enhance the retrieval of studies on off-label drug use in OvidSP EMBASE.
23188338	Staged demodulation and decoding.
Opt Express  2012Oct8
Coding for the phase noise channel is investigated in the paper. Specifically, Wiener's phase noise, which induces memory in the channel, is considered. A general coding principle for channels with memory is the interleaving of two or more codes. The interleaved codes are decoded in sequence, using past decisions to help future decoding. The paper proposes a method based on this principle, and shows its benefits through numerical results obtained by computer simulation. Analysis of the channel capacity given by the proposed method is also worked out in the paper.
23113945	Alzheimer's disease biomarker discovery using in silico literature mining and clinical validation.
J Transl Med 20121031 2012
Alzheimer's Disease (AD) is the most widespread form of dementia in the elderly but despite progress made in recent years towards a mechanistic understanding, there is still an urgent need for disease modification therapy and for early diagnostic tests. Substantial international efforts are being made to discover and validate biomarkers for AD using candidate analytes and various data-driven 'omics' approaches. Cerebrospinal fluid is in many ways the tissue of choice for biomarkers of brain disease but is limited by patient and clinician acceptability, and increasing attention is being paid to the search for blood-based biomarkers. The aim of this study was to use a novel in silico approach to discover a set of candidate biomarkers for AD. We used an in silico literature mining approach to identify potential biomarkers by creating a summarized set of assertional metadata derived from relevant legacy information. We then assessed the validity of this approach using direct assays of the identified biomarkers in plasma by immunodetection methods. Using this in silico approach, we identified 25 biomarker candidates, at least three of which have subsequently been reported to be altered in blood or CSF from AD patients. Two further candidate biomarkers, indicated from the in silico approach, were choline acetyltransferase and urokinase-type plasminogen activator receptor. Using immunodetection, we showed that, in a large sample set, these markers are either altered in disease or correlate with MRI markers of atrophy. These data support as a proof of concept the use of data mining and in silico analyses to derive valid biomarker candidates for AD and, by extension, for other disorders.
23195120	A survey of the neuroscience resource landscape: perspectives from the neuroscience information framework.
Int. Rev. Neurobiol.  2012
The number of available neuroscience resources (databases, tools, materials, and networks) available via the Web continues to expand, particularly in light of newly implemented data sharing policies required by funding agencies and journals. However, the nature of dense, multifaceted neuroscience data and the design of classic search engine systems make efficient, reliable, and relevant discovery of such resources a significant challenge. This challenge is especially pertinent for online databases, whose dynamic content is largely opaque to contemporary search engines. The Neuroscience Information Framework was initiated to address this problem of finding and utilizing neuroscience-relevant resources. Since its first production release in 2008, NIF has been surveying the resource landscape for the neurosciences, identifying relevant resources and working to make them easily discoverable by the neuroscience community. In this chapter, we provide a survey of the resource landscape for neuroscience: what types of resources are available, how many there are, what they contain, and most importantly, ways in which these resources can be utilized by the research community to advance neuroscience research.
23195311	Accessing and mining data from large-scale mouse phenotyping projects.
Int. Rev. Neurobiol.  2012
Comprehensive phenotyping through the International Mouse Phenotyping Consortium (IMPC)-www.mousephenotype.org-will reveal the pleiotropic functions of every gene in the mouse genome and uncover the wider role of genetic loci within diverse biological systems. The informatics challenge will be to develop an infrastructure to acquire the diverse and complex data sets generated from broad-based phenotyping and disseminate these data in an integrated manner to the scientific community. We describe here the current methodologies implemented to capture and disseminate these data, and plans within the Knockout Mouse Phenotyping Project (KOMP2) (http://commonfund.nih.gov/KOMP2/)-funded informatics consortium to scale these implementations to manage the surge in data from the IMPC.
22959458	Proteomic biomarkers for ovarian cancer risk in women with polycystic ovary syndrome: a systematic review and biomarker database integration.
Fertil. Steril. 20120906 2012Dec
To review and identify possible biomarkers for ovarian cancer (OC) in women with polycystic ovary syndrome (PCOS). Systematic literature searches of MEDLINE, EMBASE, and Cochrane using the search terms "proteomics," "proteomic," and "ovarian cancer" or "ovarian carcinoma." Proteomic biomarkers for OC were then integrated with an updated previously published database of all proteomic biomarkers identified to date in patients with PCOS. Academic department of obstetrics and gynecology in the United Kingdom. A total of 180 women identified in the six studies. Tissue samples from women with OC vs. tissue samples from women without OC. Proteomic biomarkers, proteomic technique used, and methodologic quality score. A panel of six biomarkers was overexpressed both in women with OC and in women with PCOS. These biomarkers include calreticulin, fibrinogen-γ, superoxide dismutase, vimentin, malate dehydrogenase, and lamin B2. These biomarkers could help improve our understanding of the links between PCOS and OC and could potentially be used to identify subgroups of women with PCOS at increased risk of OC. More studies are required to further evaluate the role these biomarkers play in women with PCOS and OC.
23062022	Collections of traditional Chinese medical literature as resources for systematic searches.
J Altern Complement Med 20121012 2012Dec
This review evaluates and compares published collections of the traditional literature on Traditional Chinese Medicine in terms of their scope and utility as resources for systematic searches for information of relevance to traditional evidence, clinical research, and drug discovery. Published collections of books and compilation works that contain substantial samples of traditional literature on Chinese herbal medicine were located via internet, library, and bookshop searches. These sources were compared in terms of scope, size, content, and ease of searching. The fourteen included collections varied considerably in scope, format, probity of included material, and accessibility. The largest was Zhong Guo Ben Cao Quan Shu (The Complete Collection of Traditional Texts on Chinese Materia Medica), with 2027 titles; followed by Zhong Hua Yi Dian (Encyclopaedia of Traditional Chinese Medicine), a compact disc (CD) of 1000 full books; and Zhong Yi Fang Ji Da Ci Dian (Great Compendium of Chinese Medical Formulas), which includes extracts derived from 680 books. Zhong Yi Fang Ji Da Ci Dian is an edited collection that is highly accessible because it is well-indexed with respect to herbal formulas and disorders. The most accessible of the large full-text collections is the Zhong Hua Yi Dian CD, which allows electronic searches. However, neither collection provides detailed bibliographic information on their included books. A collection that combines convenient search options with high quality bibliographic data is Si Ku Yi Xue Cong Shu (The Four Treasuries of Medical Works), but having only 104 titles, this is one of the smaller collections. A two-stage process for systematic searches is suggested. Large indexed compendia such as Zhong Yi Fang Ji Da Ci Dian or electronic resources such as Zhong Hua Yi Dian can be used to locate citations, and this can be followed by crossreferencing to authenticated editions of the books to verify the retrieved information.
22587372	CytoITMprobe: a network information flow plugin for Cytoscape.
BMC Res Notes 20120515 2012
Cytoscape is a well-developed flexible platform for visualization, integration and analysis of network data. Apart from the sophisticated graph layout and visualization routines, it hosts numerous user-developed plugins that significantly extend its core functionality. Earlier, we developed a network information flow framework and implemented it as a web application, called ITM Probe. Given a context consisting of one or more user-selected nodes, ITM Probe retrieves other network nodes most related to that context. It requires neither user restriction to subnetwork of interest nor additional and possibly noisy information. However, plugins for Cytoscape with these features do not yet exist. To provide the Cytoscape users the possibility of integrating ITM Probe into their workflows, we developed CytoITMprobe, a new Cytoscape plugin. CytoITMprobe maintains all the desirable features of ITM Probe and adds additional flexibility not achievable through its web service version. It provides access to ITM Probe either through a web server or locally. The input, consisting of a Cytoscape network, together with the desired origins and/or destinations of information and a dissipation coefficient, is specified through a query form. The results are shown as a subnetwork of significant nodes and several summary tables. Users can control the composition and appearance of the subnetwork and interchange their ITM Probe results with other software tools through tab-delimited files. The main strength of CytoITMprobe is its flexibility. It allows the user to specify as input any Cytoscape network, rather than being restricted to the pre-compiled protein-protein interaction networks available through the ITM Probe web service. Users may supply their own edge weights and directionalities. Consequently, as opposed to ITM Probe web service, CytoITMprobe can be applied to many other domains of network-based research beyond protein-networks. It also enables seamless integration of ITM Probe results with other Cytoscape plugins having complementary functionality for data analysis.
22512835	An optimal search filter for retrieving systematic reviews and meta-analyses.
BMC Med Res Methodol 20120418 2012
Health-evidence.ca is an online registry of systematic reviews evaluating the effectiveness of public health interventions. Extensive searching of bibliographic databases is required to keep the registry up to date. However, search filters have been developed to assist in searching the extensive amount of published literature indexed. Search filters can be designed to find literature related to a certain subject (i.e. content-specific filter) or particular study designs (i.e. methodological filter). The objective of this paper is to describe the development and validation of the health-evidence.ca Systematic Review search filter and to compare its performance to other available systematic review filters. This analysis of search filters was conducted in MEDLINE, EMBASE, and CINAHL. The performance of thirty-one search filters in total was assessed. A validation data set of 219 articles indexed between January 2004 and December 2005 was used to evaluate performance on sensitivity, specificity, precision and the number needed to read for each filter. Nineteen of 31 search filters were effective in retrieving a high level of relevant articles (sensitivity scores greater than 85%). The majority achieved a high degree of sensitivity at the expense of precision and yielded large result sets. The main advantage of the health-evidence.ca Systematic Review search filter in comparison to the other filters was that it maintained the same level of sensitivity while reducing the number of articles that needed to be screened. The health-evidence.ca Systematic Review search filter is a useful tool for identifying published systematic reviews, with further screening to identify those evaluating the effectiveness of public health interventions. The filter that narrows the focus saves considerable time and resources during updates of this online resource, without sacrificing sensitivity.
23021932	Where should electronic records for patients be stored?
Int J Med Inform 20120926 2012Dec
The importance of a nationwide health information infrastructure (NHII) is widely recognized. Patient data may be stored where it happens to be created (the distributed or institution-centric model) or in one place for a given patient (the centralized or patient-centric model). Minimal data is available regarding the performance implications of these alternative architectural choices. To help identify the architecture best suited for efficient and complete nationwide health information exchange based on the large-scale operational characteristics of these architectures. We used simulation to study the impact of health care record (data) fragmentation and probability of encounter on transaction volume and data retrieval failure rate as markers of performance for each of the above architectures. Data fragmentation and the probability of encounter directly correlate with transaction volume and are significantly higher for the distributed model when the number of data nodes &gt;4 (p&lt;0.0001). The number of data retrieval failures increases in proportion to fragmentation and is significantly higher for the distributed model when the number of data nodes ≥2 (p&lt;0.0059). In simulation studies, the distributed model scaled poorly in terms of data availability and integrity with a higher failure rate when compared to the centralized model of data storage. Choice of architecture may have implications on the efficiency, usability, and effectiveness of the NHII at the point of care.
22846060	Easy guide to searching for evidence for the busy clinician.
J Paediatr Child Health 20120729 2012Dec
The busy clinician is constantly faced with clinical questions regarding patient care. It is easy to feel overwhelmed by the large amount of health information available electronically. This article offers one easy approach for searching the electronic database. It is intended for the busy clinician who is unsure how to conduct an electronic search. It provides guidance on where to search and how to search using the PICO search method. It also provides a list of useful resources to help clinicians critically appraise the articles found to determine its relevance.
23226495	Reticulamoeba is a long-branched Granofilosean (Cercozoa) that is missing from sequence databases.
PLoS ONE 20121204 2012
We sequenced the 18S ribosomal RNA gene of seven isolates of the enigmatic marine amoeboflagellate Reticulamoeba Grell, which resolved into four genetically distinct Reticulamoeba lineages, two of which correspond to R. gemmipara Grell and R. minor Grell, another with a relatively large cell body forming lacunae, and another that has similarities to both R. minor and R. gemmipara but with a greater propensity to form cell clusters. These lineages together form a long-branched clade that branches within the cercozoan class Granofilosea (phylum Cercozoa), showing phylogenetic affinities with the genus Mesofila. The basic morphology of Reticulamoeba is a roundish or ovoid cell with a more or less irregular outline. Long and branched reticulopodia radiate from the cell. The reticulopodia bear granules that are bidirectionally motile. There is also a biflagellate dispersal stage. Reticulamoeba is frequently observed in coastal marine environmental samples. PCR primers specific to the Reticulamoeba clade confirm that it is a frequent member of benthic marine microbial communities, and is also found in brackish water sediments and freshwater biofilm. However, so far it has not been found in large molecular datasets such as the nucleotide database in NCBI GenBank, metagenomic datasets in Camera, and the marine microbial eukaryote sampling and sequencing consortium BioMarKs, although closely related lineages can be found in some of these datasets using a highly targeted approach. Therefore, although such datasets are very powerful tools in microbial ecology, they may, for several methodological reasons, fail to detect ecologically and evolutionary key lineages.
23214826	Tracking time-varying causality and directionality of information flow using an error reduction ratio test with applications to electroencephalography data.
Phys Rev E Stat Nonlin Soft Matter Phys 20121129 2012Nov
This paper introduces an error reduction ratio-causality (ERR-causality) test that can be used to detect and track causal relationships between two signals. In comparison to the traditional Granger method, one significant advantage of the new ERR-causality test is that it can effectively detect the time-varying direction of linear or nonlinear causality between two signals without fitting a complete model. Another important advantage is that the ERR-causality test can detect both the direction of interactions and estimate the relative time shift between the two signals. Numerical examples are provided to illustrate the effectiveness of the new method together with the determination of the causality between electroencephalograph signals from different cortical sites for patients during an epileptic seizure.
23231275	The second-order differential phase contrast and its retrieval for imaging with x-ray Talbot interferometry.
Med Phys  2012Dec
The x-ray differential phase contrast imaging implemented with the Talbot interferometry has recently been reported to be capable of providing tomographic images corresponding to attenuation-contrast, phase-contrast, and dark-field contrast, simultaneously, from a single set of projection data. The authors believe that, along with small-angle x-ray scattering, the second-order phase derivative Φ(") (s)(x) plays a role in the generation of dark-field contrast. In this paper, the authors derive the analytic formulae to characterize the contribution made by the second-order phase derivative to the dark-field contrast (namely, second-order differential phase contrast) and validate them via computer simulation study. By proposing a practical retrieval method, the authors investigate the potential of second-order differential phase contrast imaging for extensive applications. The theoretical derivation starts at assuming that the refractive index decrement of an object can be decomposed into δ = δ(s) + δ(f), where δ(f) corresponds to the object's fine structures and manifests itself in the dark-field contrast via small-angle scattering. Based on the paraxial Fresnel-Kirchhoff theory, the analytic formulae to characterize the contribution made by δ(s), which corresponds to the object's smooth structures, to the dark-field contrast are derived. Through computer simulation with specially designed numerical phantoms, an x-ray differential phase contrast imaging system implemented with the Talbot interferometry is utilized to evaluate and validate the derived formulae. The same imaging system is also utilized to evaluate and verify the capability of the proposed method to retrieve the second-order differential phase contrast for imaging, as well as its robustness over the dimension of detector cell and the number of steps in grating shifting. Both analytic formulae and computer simulations show that, in addition to small-angle scattering, the contrast generated by the second-order derivative is magnified substantially by the ratio of detector cell dimension over grating period, which plays a significant role in dark-field imaging implemented with the Talbot interferometry. The analytic formulae derived in this work to characterize the second-order differential phase contrast in the dark-field imaging implemented with the Talbot interferometry are of significance, which may initiate more activities in the research and development of x-ray differential phase contrast imaging for extensive preclinical and eventually clinical applications.
23151820	Persuasive system design does matter: a systematic review of adherence to web-based interventions.
J. Med. Internet Res. 20121114 2012
Although web-based interventions for promoting health and health-related behavior can be effective, poor adherence is a common issue that needs to be addressed. Technology as a means to communicate the content in web-based interventions has been neglected in research. Indeed, technology is often seen as a black-box, a mere tool that has no effect or value and serves only as a vehicle to deliver intervention content. In this paper we examine technology from a holistic perspective. We see it as a vital and inseparable aspect of web-based interventions to help explain and understand adherence. This study aims to review the literature on web-based health interventions to investigate whether intervention characteristics and persuasive design affect adherence to a web-based intervention. We conducted a systematic review of studies into web-based health interventions. Per intervention, intervention characteristics, persuasive technology elements and adherence were coded. We performed a multiple regression analysis to investigate whether these variables could predict adherence. We included 101 articles on 83 interventions. The typical web-based intervention is meant to be used once a week, is modular in set-up, is updated once a week, lasts for 10 weeks, includes interaction with the system and a counselor and peers on the web, includes some persuasive technology elements, and about 50% of the participants adhere to the intervention. Regarding persuasive technology, we see that primary task support elements are most commonly employed (mean 2.9 out of a possible 7.0). Dialogue support and social support are less commonly employed (mean 1.5 and 1.2 out of a possible 7.0, respectively). When comparing the interventions of the different health care areas, we find significant differences in intended usage (p=.004), setup (p&lt;.001), updates (p&lt;.001), frequency of interaction with a counselor (p&lt;.001), the system (p=.003) and peers (p=.017), duration (F=6.068, p=.004), adherence (F=4.833, p=.010) and the number of primary task support elements (F=5.631, p=.005). Our final regression model explained 55% of the variance in adherence. In this model, a RCT study as opposed to an observational study, increased interaction with a counselor, more frequent intended usage, more frequent updates and more extensive employment of dialogue support significantly predicted better adherence. Using intervention characteristics and persuasive technology elements, a substantial amount of variance in adherence can be explained. Although there are differences between health care areas on intervention characteristics, health care area per se does not predict adherence. Rather, the differences in technology and interaction predict adherence. The results of this study can be used to make an informed decision about how to design a web-based intervention to which patients are more likely to adhere.
23155773	Extracting epidemiologic exposure and outcome terms from literature using machine learning approaches.
Int J Data Min Bioinform  2012
Much epidemiologic information resides in literature, which is not in a computable format. To extract information and build knowledge bases of epidemiologic studies, we developed a system to extract noun phrases about epidemiologic exposures and outcomes. The system consists of two components: a natural language processing (NLP) engine; a machine learning (ML) based classifier. Four ML algorithms were applied and compared over different feature sets. To evaluate the performance of the system, we manually constructed an annotated dataset. The system achieved the highest F-measure of 82.0% for extracting exposure terms, and 70% for extracting outcome terms.
23155924	The effect of public deposit of scientific articles on readership.
Physiologist  2012Oct
A longitudinal cohort analysis of 3,499 articles published in 12 physiology journals reveals a 14% reduction in full text article downloads when they are made publicly available from the PubMed Central archive. The loss of article readership from the journal website may weaken the ability of the publisher to build communities of interest around the research article, impede the communication of news and events with society members and reduce the perceived value of the journal to institutional subscribers.
22540288	CDKD: a clinical database of kidney diseases.
BMC Nephrol 20120427 2012
The main function of the kidneys is to remove waste products and excess water from the blood. Loss of kidney function leads to various health issues, such as anemia, high blood pressure, bone disease, disorders of cholesterol. The main objective of this database system is to store the personal and laboratory investigatory details of patients with kidney disease. The emphasis is on experimental results relevant to quantitative renal physiology, with a particular focus on data relevant for evaluation of parameters in statistical models of renal function. Clinical database of kidney diseases (CDKD) has been developed with patient confidentiality and data security as a top priority. It can make comparative analysis of one or more parameters of patient's record and includes the information of about whole range of data including demographics, medical history, laboratory test results, vital signs, personal statistics like age and weight. The goal of this database is to make kidney-related physiological data easily available to the scientific community and to maintain &amp; retain patient's record. As a Web based application it permits physician to see, edit and annotate a patient record from anywhere and anytime while maintaining the confidentiality of the personal record. It also allows statistical analysis of all data.
22986592	Levels of evidence in the neurosurgical literature:  more tribulations than trials.
Neurosurgery  2012Dec
The importance of evidence-based medicine has been well documented and supported across various surgical subspecialties. To quantify the levels of evidence across publications in the neurosurgical literature, to assess the change in evidence over time, and to indicate predictive factors of higher-level evidence. We reviewed the levels of evidence across published clinical studies in 3 neurosurgical journals from 2009 to 2010. Randomized trials were evaluated by use of the Detsky quality of reporting scale. Levels-of-evidence data for the same journals in 1999 were obtained from the literature, and regression analysis was performed to identify predictive factors for higher-level evidence. Of 660 eligible articles, 14 (2.1%) were Level I, 54 (8.2%) were level II, 73 (11.1%) were Level III, 287 (43.5%) were level IV, and 232 (35.2%) were level V. The number of level I studies decreased significantly between 1999 and 2010 (3.4% vs. 2.1%, respectively; P = .01). Seven randomized clinical trials were identified, and 1 trial had significant methodological limitations (mean Detsky index = 16.3; SD = 1.8). Publications with larger sample size were significantly associated with higher levels of evidence (levels I and II; odds ratio, 1.7; 95% confidence interval, 1.45-2.05; P = .001). The ratio of higher levels of evidence to lower levels was 0.11. Higher levels of evidence (levels I and II) represent only 1 in 10 neurosurgical clinical papers in the top neurosurgical journals. Increased awareness of the need for better evidence in the field through education and adoption of the levels of evidence may improve the conduct and publication of prospective studies.
23169728	Embedding 3D radiology models in portable document format.
AJR Am J Roentgenol  2012Dec
The purpose of this article is to discuss how to convert cross-sectional images into a 3D model and embed them in a Portable Document Format (PDF) file. Four programs are used: OsiriX, MeshLab, Microsoft PowerPoint, and Adobe Acrobat. Step-by-step instructions are provided. Embedding 3D radiology models into PDF files is a powerful tool that may be used for clinical, educational, and research purposes.
22564405	Recognition of medication information from discharge summaries using ensembles of classifiers.
BMC Med Inform Decis Mak 20120507 2012
Extraction of clinical information such as medications or problems from clinical text is an important task of clinical natural language processing (NLP). Rule-based methods are often used in clinical NLP systems because they are easy to adapt and customize. Recently, supervised machine learning methods have proven to be effective in clinical NLP as well. However, combining different classifiers to further improve the performance of clinical entity recognition systems has not been investigated extensively. Combining classifiers into an ensemble classifier presents both challenges and opportunities to improve performance in such NLP tasks. We investigated ensemble classifiers that used different voting strategies to combine outputs from three individual classifiers: a rule-based system, a support vector machine (SVM) based system, and a conditional random field (CRF) based system. Three voting methods were proposed and evaluated using the annotated data sets from the 2009 i2b2 NLP challenge: simple majority, local SVM-based voting, and local CRF-based voting. Evaluation on 268 manually annotated discharge summaries from the i2b2 challenge showed that the local CRF-based voting method achieved the best F-score of 90.84% (94.11% Precision, 87.81% Recall) for 10-fold cross-validation. We then compared our systems with the first-ranked system in the challenge by using the same training and test sets. Our system based on majority voting achieved a better F-score of 89.65% (93.91% Precision, 85.76% Recall) than the previously reported F-score of 89.19% (93.78% Precision, 85.03% Recall) by the first-ranked system in the challenge. Our experimental results using the 2009 i2b2 challenge datasets showed that ensemble classifiers that combine individual classifiers into a voting system could achieve better performance than a single classifier in recognizing medication information from clinical text. It suggests that simple strategies that can be easily implemented such as majority voting could have the potential to significantly improve clinical entity recognition.
23173665	What is a rapid review? A methodological exploration of rapid reviews in Health Technology Assessments.
Int J Evid Based Healthc  2012Dec
Commissioners of Health Technology Assessments require timely reviews to attain efficacious decisions on healthcare and treatments. In recent years, there has been an emergence of 'rapid reviews' within Health Technology Assessments; however, there is no known published guidance or agreed methodology within recognised systematic review or Health Technology Assessment guidelines. In order to answer the research question 'What is a rapid review and is methodology consistent in rapid reviews of Health Technology Assessments?', a study was undertaken in a sample of rapid review Health Technology Assessments from the Health Technology Assessment database within the Cochrane Library and other specialised Health Technology Assessment databases to investigate similarities and/or differences in rapid review methodology utilised. In a targeted search to obtain a manageable sample of rapid reviews, the Health Technology Assessment database of The Cochrane Library and six international Health Technology Assessment databases were searched to locate rapid review Health Technology Assessments from 2000 onwards. Each rapid review was examined to investigate the individual methodology used for searching, inclusion screening, quality assessment, data extraction and synthesis. Methods of each rapid review were compared to investigate differences and/or similarities in methodologies used, in comparison with recognised methods for systematic reviews. Forty-six full rapid reviews and three extractable summaries of rapid reviews were included. There was a wide diversity of methodology, with some reviews utilising well-established systematic review methods, but many others diversifying in one or more areas, that is searching, inclusion screening, quality assessment, data extraction, synthesis methods, report structure and number of reviewers. There was a significant positive correlation between the number of recommended review methodologies utilised and length of time taken in months. Despite the number of rapid reviews published within Health Technology Assessments over recent years, there is no agreed and tested methodology and it is unclear how rapid reviews differ from systematic reviews. In a sample of Health Technology Assessment rapid reviews from 2000 to 2011, there was a wide diversity of methodology utilised in all aspects of rapid reviews. There is scope for wider research in this area to investigate the diversity of methods in more depth during each stage of the rapid review process, so that eventually recommendations could be made for clear and systematic methods for rapid reviews, thus facilitating equity and credibility of this type of important review methodology.
23113397	[Pediatric surgery 2.0].
Cir Pediatr  2012Apr
New tools from the web are a complete breakthrough in management of information. The aim of this paper is to present different resources in a friendly way, with apps and examples in the different phases of the knowledge management for the paediatric surgeon: search, filter, reception, classification, sharing, collaborative work and publication. We are assisting to a real revolution on how to manage knowledge and information. The main charateristics are: immediateness, social component, growing interaction, and easiness. Every physician has clinical questions and the Internet gives us more and more resources to make searchs easier. Along with them we need electronic resources to filter information of quality and to make easier transfer of knowledge to clinical practice. Cloud computing is on continuous development and makes possible sharing information with differents users and computers. The main feature of the apps from the Intenet is the social component, that makes possible interaction, sharing and collaborative work.
23118800	A rate-distortion-based merging algorithm for compressed image segmentation.
Comput Math Methods Med 20121015 2012
Original images are often compressed for the communication applications. In order to avoid the burden of decompressing computations, it is thus desirable to segment images in the compressed domain directly. This paper presents a simple rate-distortion-based scheme to segment images in the JPEG2000 domain. It is based on a binary arithmetic code table used in the JPEG2000 standard, which is available at both encoder and decoder; thus, there is no need to transmit the segmentation result. Experimental results on the Berkeley image database show that the proposed algorithm is preferable in terms of the running time and the quantitative measures: probabilistic Rand index (PRI) and boundary displacement error (BDE).
22865274	Intelligent image retrieval based on radiology reports.
Eur Radiol 20120804 2012Dec
To create an advanced image retrieval and data-mining system based on in-house radiology reports. Radiology reports are semantically analysed using natural language processing (NLP) techniques and stored in a state-of-the-art search engine. Images referenced by sequence and image number in the reports are retrieved from the picture archiving and communication system (PACS) and stored for later viewing. A web-based front end is used as an interface to query for images and show the results with the retrieved images and report text. Using a comprehensive radiological lexicon for the underlying terminology, the search algorithm also finds results for synonyms, abbreviations and related topics. The test set was 108 manually annotated reports analysed by different system configurations. Best results were achieved using full syntactic and semantic analysis with a precision of 0.929 and recall of 0.952. Operating successfully since October 2010, 258,824 reports have been indexed and a total of 405,146 preview images are stored in the database. Data-mining and NLP techniques provide quick access to a vast repository of images and radiology reports with both high precision and recall values. Consequently, the system has become a valuable tool in daily clinical routine, education and research. Radiology reports can now be analysed using sophisticated natural language-processing techniques. Semantic text analysis is backed by terminology of a radiological lexicon. The search engine includes results for synonyms, abbreviations and compositions. Key images are automatically extracted from radiology reports and fetched from PACS. Such systems help to find diagnoses, improve report quality and save time.
23133323	Teaching about teaching and instruction on instruction: a challenge for health sciences library education.
J Med Libr Assoc  2012Oct
This is a review of the master's-level curricula of the fifty-eight America Library Association-accredited library and information science programs and iSchools for evidence of coursework and content related to library instruction. Special emphasis is placed on the schools and programs that also offer coursework in medical or health sciences librarianship. Fifty-eight school and program websites were reviewed. Course titles and course descriptions for seventy-three separate classes were analyzed. Twenty-three syllabi were examined. All North American library education programs offer at least one course in the general area of library instruction; some programs offer multiple courses. No courses on instruction, however, are focused directly on the specialized area of health sciences librarianship. Master's degree students can take appropriate classes on library instruction, but the medical library profession needs to offer continuing education opportunities for practitioners who want to have specific instruction for the specialized world of the health sciences.
23133325	Best methods for evaluating educational impact: a comparison of the efficacy of commonly used measures of library instruction.
J Med Libr Assoc  2012Oct
Libraries are increasingly called upon to demonstrate student learning outcomes and the tangible benefits of library educational programs. This study reviewed and compared the efficacy of traditionally used measures for assessing library instruction, examining the benefits and drawbacks of assessment measures and exploring the extent to which knowledge, attitudes, and behaviors actually paralleled demonstrated skill levels. An overview of recent literature on the evaluation of information literacy education addressed these questions: (1) What evaluation measures are commonly used for evaluating library instruction? (2) What are the pros and cons of popular evaluation measures? (3) What are the relationships between measures of skills versus measures of attitudes and behavior? Research outcomes were used to identify relationships between measures of attitudes, behaviors, and skills, which are typically gathered via attitudinal surveys, written skills tests, or graded exercises. Results provide useful information about the efficacy of instructional evaluation methods, including showing significant disparities between attitudes, skills, and information usage behaviors. This information can be used by librarians to implement the most appropriate evaluation methods for measuring important variables that accurately demonstrate students' attitudes, behaviors, or skills.
23133326	Is literature search training for medical students and residents effective? a literature review.
J Med Libr Assoc  2012Oct
This literature review examines the effectiveness of literature searching skills instruction for medical students or residents, as determined in studies that either measure learning before and after an intervention or compare test and control groups. The review reports on the instruments used to measure learning and on their reliability and validity, where available. Finally, a summary of learning outcomes is presented. Fifteen studies published between 1998 and 2011 were identified for inclusion in the review. The selected studies all include a description of the intervention, a summary of the test used to measure learning, and the results of the measurement. Instruction generally resulted in improvement in clinical question writing, search strategy construction, article selection, and resource usage. Although the findings of most of the studies indicate that the current instructional methods are effective, the study designs are generally weak, there is little evidence that learning persists over time, and few validated methods of skill measurement have been developed.
23133327	Developing health information literacy: a needs analysis from the perspective of preprofessional health students.
J Med Libr Assoc  2012Oct
The research identified the skills, if any, that health preprofessional students wished to develop after receiving feedback on skill gaps as well as any strategies they intended to use to address these gaps. A qualitative approach was used to elicit students' reflections on building health information literacy skills. First, the students took the Research Readiness Self-Assessment instrument, which measured their health information literacy, and then they received individually tailored feedback about their scores and skill gaps. Second, students completed a post-assessment survey asking how they intended to close identified gaps in their skills on these. Three trained coders analyzed qualitative comments by 181 students and grouped them into themes relating to "what skills to improve" and "how to improve them." Students intended to develop library skills (64% of respondents), Internet skills (63%), and information evaluation skills (63%). Most students reported that they would use library staff members' assistance (55%), but even more respondents (82%) planned to learn the skills by practicing on their own. Getting help from librarians was a much more popular learning strategy than getting assistance from peers (20%) or professors (17%). The study highlighted the importance of providing health preprofessional students with resources to improve skills on their own, remote access to library staff members, and instruction on the complexity of building health literacy skills, while also building relationships among students, librarians, and faculty.
23133328	Aligning library instruction with the needs of basic sciences graduate students: a case study.
J Med Libr Assoc  2012Oct
How can an existing library instruction program be reconfigured to reach basic sciences graduate students and other patrons missed by curriculum-based instruction? The setting is an academic health sciences library that serves both the university and its affiliated teaching hospital. The existing program was redesigned to incorporate a series of seven workshops that encompassed the range of information literacy skills that graduate students in the basic sciences need. In developing the new model, the teaching librarians made changes in pedagogy, technology, marketing, and assessment strategies. Total attendance at the sessions increased substantially in the first 2 years of the new model, increasing from an average of 20 per semester to an average of 124. Survey results provided insight about what patrons wanted to learn and how best to teach it. Modifying the program's content and structure resulted in a program that appealed to the target audience.
23133329	Time to rethink the role of the library in educating doctors: driving information literacy in the clinical environment.
J Med Libr Assoc  2012Oct
Can information literacy (IL) be embedded into the curriculum and clinical environment to facilitate patient care and lifelong learning? The Australian School of Advanced Medicine (ASAM) provides competence-based programs incorporating patient-centred care and lifelong learning. ASAM librarians use outcomes-based educational theory to embed and assess IL into ASAM's educational and clinical environments. A competence-based IL program was developed where learning outcomes were linked to current patients and assessed with checklists. Weekly case presentations included clinicians' literature search strategies, results, and conclusions. Librarians provided support to clinicians' literature searches and assessed their presentations using a checklist. Outcome data showed clinicians' searching skills improved over time; however, advanced MEDLINE searching remained challenging for some. Recommendations are provided. IL learning that takes place in context using measurable outcomes is more meaningful, is enduring, and likely contributes to patient care. Competence-based assessment drives learning in this environment.
23133330	"Information Survival Skills": a medical school elective.
J Med Libr Assoc  2012Oct
The authors developed an elective course to assist students in (1) understanding the changing nature of scholarly communication and online publishing, (2) identifying resources and strategies for searching current best evidence, and (3) demonstrating effective communication of information. The course took place in a medical school in the Southwest. Second- and third-year medical students participated in the course. A pass-fail, undergraduate-level elective was first offered October to December 2006. This 7.5 hour course, developed and co-taught by 2 health sciences library faculty, consisted of hands-on exercises, small group discussion, and didactic lecture. Presenting a medical school elective is one possible outlet for intensive bibliographic instruction. Illustrating the flow of information from creation to management and presentation affords students an opportunity to understand information in context. This elective has been consistently ranked very high in student evaluations and led to new and expanded teaching opportunities.
23134135	Nurse scholars' knowledge and use of electronic theses and dissertations.
Int Nurs Rev 20120521 2012Dec
Electronic theses and dissertations (ETDs) are a valuable resource for nurse scholars worldwide. ETDs and digital libraries offer the potential to radically change the nature and scope of the way in which doctoral research results are presented, disseminated and used. An exploratory study was undertaken to better understand ETD usage and to address areas where there is a need and an opportunity for educational enhancement. The primary objective was to gain an initial understanding of the knowledge and use of ETDs and digital libraries by faculty, graduate students and alumni of graduate programs at schools of nursing. A descriptive online survey design was used. Purposeful sampling of specific schools of nursing was used to identify institutional participants in Australia, New Zealand, the UK and the US. A total of 209 participants completed the online questionnaire. Only 44% of participants reported knowing how to access ETDs in their institutions' digital libraries and only 18% reported knowing how to do so through a national or international digital library. Only 27% had cited an ETD in a publication. The underuse of ETDs was found to be attributable to specific issues rather than general reluctance to use online resources. This is the first international study that has explored awareness and use of ETDs, and ETD digital libraries, with a focus on nursing and has set the stage for future research and development in this field. Results show that most nursing scholars do not use ETDs to their fullest potential.
22876960	A draft genome sequence of Nicotiana benthamiana to enhance molecular plant-microbe biology research.
Mol. Plant Microbe Interact.  2012Dec
Nicotiana benthamiana is a widely used model plant species for the study of fundamental questions in molecular plant-microbe interactions and other areas of plant biology. This popularity derives from its well-characterized susceptibility to diverse pathogens and, especially, its amenability to virus-induced gene silencing and transient protein expression methods. Here, we report the generation of a 63-fold coverage draft genome sequence of N. benthamiana and its availability on the Sol Genomics Network for both BLAST searches and for downloading to local servers. The estimated genome size of N. benthamiana is 3 Gb (gigabases). The current assembly consists of approximately 141,000 scaffolds, spanning 2.6 Gb with 50% of the genome sequence contained within scaffolds &gt;89 kilobases. Of the approximately 16,000 N. benthamiana unigenes available in GenBank, &gt;90% are represented in the assembly. The usefulness of the sequence was demonstrated by the retrieval of N. benthamiana orthologs for 24 immunity-associated genes from other species including Ago2, Ago7, Bak1, Bik1, Crt1, Fls2, Pto, Prf, Rar1, and mitogen-activated protein kinases. The sequence will also be useful for comparative genomics in the Solanaceae family as shown here by the discovery of microsynteny between N. benthamiana and tomato in the region encompassing the Pto and Prf genes.
22705066	Clustering of distinct PACS archives using a cooperative peer-to-peer network.
Comput Methods Programs Biomed 20120615 2012Dec
To face the demanding requirements of the clinical environment, PACS archives need to be resilient and reliable, supporting high availability and fault tolerance. Often, to ensure no data loss, PACS archives retain two copies of images on separate physical machines, using distributed data storage facilities. However, PACS do not take advantage of the various replicas to improve the transfer rates of medical images. This happens mostly because the DICOM standard does not comply with distributed fetching of image fragments while performing a store. Inspired by this unexplored opportunity, we designed and implemented a new solution that takes advantage of the distributed image replicas and, at the same time, respects the DICOM standard. Our strategy brought significant improvements in the exchange rates, load balancing and availability of installed PACS archives. Moreover, the adopted strategy forms a cluster of PACS archives that transparently enables horizontal scaling, facilitates the creation of backups, and gives to healthcare professionals a unified view of the distributed repositories.
23017251	WIMP: web server tool for missing data imputation.
Comput Methods Programs Biomed 20120925 2012Dec
The imputation of unknown or missing data is a crucial task on the analysis of biomedical datasets. There are several situations where it is necessary to classify or identify instances given incomplete vectors, and the existence of missing values can much degrade the performance of the algorithms used for the classification/recognition. The task of learning accurately from incomplete data raises a number of issues some of which have not been completely solved in machine learning applications. In this sense, effective missing value estimation methods are required. Different methods for missing data imputations exist but most of the times the selection of the appropriate technique involves testing several methods, comparing them and choosing the right one. Furthermore, applying these methods, in most cases, is not straightforward, as they involve several technical details, and in particular in cases such as when dealing with microarray datasets, the application of the methods requires huge computational resources. As far as we know, there is not a public software application that can provide the computing capabilities required for carrying the task of data imputation. This paper presents a new public tool for missing data imputation that is attached to a computer cluster in order to execute high computational tasks. The software WIMP (Web IMPutation) is a public available web site where registered users can create, execute, analyze and store their simulations related to missing data imputation.
22934641	Plant virus metagenomics: biodiversity and ecology.
Annu. Rev. Genet. 20120829 2012
Viral metagenomics is the study of viruses in environmental samples, using next generation sequencing that produces very large data sets. For plant viruses, these studies are still relatively new, but are already indicating that our current knowledge grossly underestimates the diversity of these viruses. Some plant virus studies are using thousands of individual plants so that each sequence can be traced back to its precise host. These studies should allow for deeper ecological and evolutionary analyses. The finding of so many new plant viruses that do not cause any obvious symptoms in wild plant hosts certainly changes our perception of viruses and how they interact with their hosts. The major difficulty in these (as in all) metagenomic studies continues to be the need for better bioinformatics tools to decipher the large data sets. The implications of this new information on plant viruses for international agriculture remain to be addressed.
22744959	Life sciences domain analysis model.
J Am Med Inform Assoc 20120628 2012 Nov-Dec
Meaningful exchange of information is a fundamental challenge in collaborative biomedical research. To help address this, the authors developed the Life Sciences Domain Analysis Model (LS DAM), an information model that provides a framework for communication among domain experts and technical teams developing information systems to support biomedical research. The LS DAM is harmonized with the Biomedical Research Integrated Domain Group (BRIDG) model of protocol-driven clinical research. Together, these models can facilitate data exchange for translational research. The content of the LS DAM was driven by analysis of life sciences and translational research scenarios and the concepts in the model are derived from existing information models, reference models and data exchange formats. The model is represented in the Unified Modeling Language and uses ISO 21090 data types. The LS DAM v2.2.1 is comprised of 130 classes and covers several core areas including Experiment, Molecular Biology, Molecular Databases and Specimen. Nearly half of these classes originate from the BRIDG model, emphasizing the semantic harmonization between these models. Validation of the LS DAM against independently derived information models, research scenarios and reference databases supports its general applicability to represent life sciences research. The LS DAM provides unambiguous definitions for concepts required to describe life sciences research. The processes established to achieve consensus among domain experts will be applied in future iterations and may be broadly applicable to other standardization efforts. The LS DAM provides common semantics for life sciences research. Through harmonization with BRIDG, it promotes interoperability in translational science.
22753809	Retail pharmacy staff perceptions of design strengths and weaknesses of electronic prescribing.
J Am Med Inform Assoc 20120629 2012 Nov-Dec
This paper explored pharmacy staff perceptions of the strengths and weaknesses of electronic prescribing (e-prescribing) design in retail pharmacies using the sociotechnical systems framework. This study examined how adoption of e-prescribing technology is affecting clinical practice and patient care. Direct observations and think aloud protocols were used to collect data from seven retail pharmacies. Pharmacists and pharmacy technicians reported strengths of e-prescribing design that facilitated pharmacy work which included: legibility, ease of archiving, quick access to prescriptions and consistency in the format of electronic prescriptions (e-prescriptions). Design weaknesses and potential hazards to patient care associated with e-prescribing systems were due to differences between pharmacy and prescriber computer systems which resulted in the selection of wrong patient or drug (name, directions, dose, strength, formulation, package sizes). There were unique strengths and weaknesses in the design of e-prescriptions peculiar to the three pharmacy computer systems examined in this study. Findings from this study can help inform policy on creating e-prescribing design standards for pharmacy. e-Prescribing system developers can use the results of this study to identify and apply the most usable features of the three main pharmacy computer systems to design systems that support dispensing efficiency and safety. This is the first study to highlight design flaws with e-prescribing in retail pharmacies. The sociotechnical systems framework was useful in providing an indepth understanding of the pharmacist and pharmacy technician's interface with e-prescribing technology. This information can be used by policy makers to create e-prescribing standards for pharmacies.
22923294	The GlycomeAtlas tool for visualizing and querying glycome data.
Bioinformatics 20120824 2012Nov1
The development of glycomics technologies in recent years has produced a sufficient amount of data to begin analyzing the glycan structures present in various organisms and tissues. In particular, glycan profiling using mass spectrometry (MS) and tandem MS has generated a large amount of data that are waiting to be analyzed. The Consortium for Functional Glycomics (CFG) has provided a web resource for obtaining such glycan profiling data easily. Although an interactive spectrum viewer is provided on the website as a Java applet, it is not necessarily easy to search for particular glycans or to find commonalities between different tissues in a single organism, for example. Therefore, to allow users to better take advantage of the valuable glycome data that can be obtained from mass spectra and other leading technologies, we have developed a tool called Glycome Atlas which is pre-loaded with the data from the CFG and is also able to visualize local glycan profiling data for human and mouse. We have developed a tool to allow users to visualize and perform queries of glycome data. This tool, called GlycomeAtlas, is pre-loaded with glycome data as provided by the CFG. Moreover, users can load their own local glycome data into this tool to visualize and perform queries on their own data. This tool is available at the following URL: http://www.rings.t.soka.ac.jp/GlycomeAtlas/GUI.html.
22923300	RIsearch: fast RNA-RNA interaction search using a simplified nearest-neighbor energy model.
Bioinformatics 20120824 2012Nov1
Regulatory, non-coding RNAs often function by forming a duplex with other RNAs. It is therefore of interest to predict putative RNA-RNA duplexes in silico on a genome-wide scale. Current computational methods for predicting these interactions range from fast complementary-based searches to those that take intramolecular binding into account. Together these methods constitute a trade-off between speed and accuracy, while leaving room for improvement within the context of genome-wide screens. A fast pre-filtering of putative duplexes would therefore be desirable. We present RIsearch, an implementation of a simplified Turner energy model for fast computation of hybridization, which significantly reduces runtime while maintaining accuracy. Its time complexity for sequences of lengths m and n is with a much smaller pre-factor than other tools. We show that this energy model is an accurate approximation of the full energy model for near-complementary RNA-RNA duplexes. RIsearch uses a Smith-Waterman-like algorithm using a dinucleotide scoring matrix which approximates the Turner nearest-neighbor energies. We show in benchmarks that we achieve a speed improvement of at least 2.4× compared with RNAplex, the currently fastest method for searching near-complementary regions. RIsearch shows a prediction accuracy similar to RNAplex on two datasets of known bacterial short RNA (sRNA)-messenger RNA (mRNA) and eukaryotic microRNA (miRNA)-mRNA interactions. Using RIsearch as a pre-filter in genome-wide screens reduces the number of binding site candidates reported by miRNA target prediction programs, such as TargetScanS and miRanda, by up to 70%. Likewise, substantial filtering was performed on bacterial RNA-RNA interaction data. The source code for RIsearch is available at: http://rth.dk/resources/risearch.
22923302	HSPIR: a manually annotated heat shock protein information resource.
Bioinformatics 20120824 2012Nov1
Heat shock protein information resource (HSPIR) is a concerted database of six major heat shock proteins (HSPs), namely, Hsp70, Hsp40, Hsp60, Hsp90, Hsp100 and small HSP. The HSPs are essential for the survival of all living organisms, as they protect the conformations of proteins on exposure to various stress conditions. They are a highly conserved group of proteins involved in diverse physiological functions, including de novo folding, disaggregation and protein trafficking. Moreover, their critical role in the control of disease progression made them a prime target of research. Presently, limited information is available on HSPs in reference to their identification and structural classification across genera. To that extent, HSPIR provides manually curated information on sequence, structure, classification, ontology, domain organization, localization and possible biological functions extracted from UniProt, GenBank, Protein Data Bank and the literature. The database offers interactive search with incorporated tools, which enhances the analysis. HSPIR is a reliable resource for researchers exploring structure, function and evolution of HSPs. http://pdslab.biochem.iisc.ernet.in/hspir/
22923306	Exploring spatial patterns of gene expression from fruit fly embryogenesis on the iPhone.
Bioinformatics 20120824 2012Nov1
Mobile technologies provide unique opportunities for ubiquitous distribution of scientific information through user-friendly interfaces. Therefore, we have developed a new FlyExpress mobile application that makes available a growing collection (&gt;100 000) of standardized in situ hybridization images containing spatial patterns of gene expression from Drosophila melanogaster (fruit fly) embryogenesis. Using this application, scientists can visualize and compare expression patterns of &gt;4000 developmentally relevant genes. The FlyExpress app displays the expression patterns of the selected gene for different visual projections (e.g. lateral) and displays them according to their developmental stages, which shows a gene's progression of spatial expression over developmental time. Ultimately, we envision the use of FlyExpress app in the laboratory where scientists may wish to immediately conduct a visual comparison of a known expression pattern with the one observed on the bench top or to display expression patterns of interest during scientific discussions at large. Search "FlyExpress" on the Apple iTunes store.
22945787	Updating annotations with the distributed annotation system and the automated sequence annotation pipeline.
Bioinformatics 20120903 2012Nov1
The integration between BioDAS ProServer and Automated Sequence Annotation Pipeline (ASAP) provides an interface for querying diverse annotation sources, chaining and linking results, and standardizing the output using the Distributed Annotation System (DAS) protocol. This interface allows pipeline plans in ASAP to be integrated into any system using HTTP and also allows the information returned by ASAP to be included in the DAS registry for use in any DAS-aware system. Three example implementations have been developed: the first accesses TRANSFAC information to automatically create gene sets for the Coordinated Gene Activity in Pattern Sets (CoGAPS) algorithm; the second integrates annotations from multiple array platforms and provides unified annotations in an R environment; and the third wraps the UniProt database for integration with the SPICE DAS client. Source code for ASAP 2.7 and the DAS 1.6 interface is available under the GNU public license. Proserver 2.20 is free software available from SourceForge. Scripts for installation and configuration on Linux are provided at our website: http://www.rits.onc.jhmi.edu/dbb/custom/A6/
22945789	JEnsembl: a version-aware Java API to Ensembl data systems.
Bioinformatics 20120903 2012Nov1
The Ensembl Project provides release-specific Perl APIs for efficient high-level programmatic access to data stored in various Ensembl database schema. Although Perl scripts are perfectly suited for processing large volumes of text-based data, Perl is not ideal for developing large-scale software applications nor embedding in graphical interfaces. The provision of a novel Java API would facilitate type-safe, modular, object-orientated development of new Bioinformatics tools with which to access, analyse and visualize Ensembl data. The JEnsembl API implementation provides basic data retrieval and manipulation functionality from the Core, Compara and Variation databases for all species in Ensembl and EnsemblGenomes and is a platform for the development of a richer API to Ensembl datasources. The JEnsembl architecture uses a text-based configuration module to provide evolving, versioned mappings from database schema to code objects. A single installation of the JEnsembl API can therefore simultaneously and transparently connect to current and previous database instances (such as those in the public archive) thus facilitating better analysis repeatability and allowing 'through time' comparative analyses to be performed. Project development, released code libraries, Maven repository and documentation are hosted at SourceForge (http://jensembl.sourceforge.net).
22954629	MaConDa: a publicly accessible mass spectrometry contaminants database.
Bioinformatics 20120906 2012Nov1
Mass spectrometry is widely used in bioanalysis, including the fields of metabolomics and proteomics, to simultaneously measure large numbers of molecules in complex biological samples. Contaminants routinely occur within these samples, for example, originating from the solvents or plasticware. Identification of these contaminants is crucial to enable their removal before data analysis, in particular to maintain the validity of conclusions drawn from uni- and multivariate statistical analyses. Although efforts have been made to report contaminants within mass spectra, this information is fragmented and its accessibility is relatively limited. In response to the needs of the bioanalytical community, here we report the creation of an extensive manually well-annotated database of currently known small molecule contaminants. The Mass spectrometry Contaminants Database (MaConDa) is freely available and accessible through all major browsers or by using the MaConDa web service http://www.maconda.bham.ac.uk.
23088064	Driving value: solving the issue of data overload with an executive dashboard.
Healthc Financ Manage  2012Oct
Many health systems suffer from having too little data to identify significant quality improvement opportunities, while others suffer from the confusion of having too much.
23088695	Evidence-based practice for the busy nurse practitioner: part two: searching for the best evidence to clinical inquiries.
J Am Acad Nurse Pract 20120814 2012Nov
The purpose of this four-part evidence-based practice (EBP) series is to enhance the nurse practitioner's (NP's) EBP skills by reviewing the process of developing a clinical question, searching for the best evidence, and critically appraising and applying the findings. Part two of the series focuses on how to search the published scientific literature for the most relevant studies that will answer a specific clinical question of importance to the NP. Scientific literature review, gray searching, PubMed and other online literature databases and resources, and online EBP websites. Technology has allowed multiple healthcare resources to be available at one's fingertips enabling both NPs and their patients to find answers to clinical questions. EBP databases can be categorized as synthesized/filtered, unfiltered, and background information/expert opinion resources. Learning which database can best answer the clinical inquiry can streamline the search process. For the busy NP, EBP has emerged as an important strategy to maintain valid, accurate, and relevant clinical knowledge. It is expected that this part of the series will enable NPs to identify appropriate databases to answer clinical inquires while refining their search strategy skills, which takes both time and practice.
22826159	Accelerated 3D MERGE carotid imaging using compressed sensing with a hidden Markov tree model.
J Magn Reson Imaging 20120723 2012Nov
To determine the potential for accelerated 3D carotid magnetic resonance imaging (MRI) using wavelet based compressed sensing (CS) with a hidden Markov tree (HMT) model. We retrospectively applied HMT model-based CS and conventional CS to 3D carotid MRI data with 0.7 mm isotropic resolution from six subjects with known carotid stenosis (12 carotids). We applied a wavelet-tree model learned from a training database of carotid images to improve CS reconstruction. Quantitative endpoints such as lumen area, wall area, mean and maximum wall thickness, plaque calcification, and necrotic core area were measured and compared using Bland-Altman analysis along with image quality. Rate-4.5 acceleration with HMT model-based CS provided image quality comparable to that of rate-3 acceleration with conventional CS and fully sampled reference reconstructions. Morphological measurements made on rate-4.5 HMT model-based CS reconstructions were in good agreement with measurements made on fully sampled reference images. There was no significant bias or correlation between mean and difference of measurements when comparing rate 4.5 HMT model-based CS with fully sampled reference images. HMT model-based CS can potentially be used to accelerate clinical carotid MRI by a factor of 4.5 without impacting diagnostic quality or quantitative endpoints.
23095472	Clinical Bioinformatics: challenges and opportunities.
BMC Bioinformatics 20120907 2012
Network Tools and Applications in Biology (NETTAB) Workshops are a series of meetings focused on the most promising and innovative ICT tools and to their usefulness in Bioinformatics. The NETTAB 2011 workshop, held in Pavia, Italy, in October 2011 was aimed at presenting some of the most relevant methods, tools and infrastructures that are nowadays available for Clinical Bioinformatics (CBI), the research field that deals with clinical applications of bioinformatics. In this editorial, the viewpoints and opinions of three world CBI leaders, who have been invited to participate in a panel discussion of the NETTAB workshop on the next challenges and future opportunities of this field, are reported. These include the development of data warehouses and ICT infrastructures for data sharing, the definition of standards for sharing phenotypic data and the implementation of novel tools to implement efficient search computing solutions. Some of the most important design features of a CBI-ICT infrastructure are presented, including data warehousing, modularity and flexibility, open-source development, semantic interoperability, integrated search and retrieval of -omics information. Clinical Bioinformatics goals are ambitious. Many factors, including the availability of high-throughput "-omics" technologies and equipment, the widespread availability of clinical data warehouses and the noteworthy increase in data storage and computational power of the most recent ICT systems, justify research and efforts in this domain, which promises to be a crucial leveraging factor for biomedical research.
23095521	Matching health information seekers' queries to medical terms.
BMC Bioinformatics 20120907 2012
The Internet is a major source of health information but most seekers are not familiar with medical vocabularies. Hence, their searches fail due to bad query formulation. Several methods have been proposed to improve information retrieval: query expansion, syntactic and semantic techniques or knowledge-based methods. However, it would be useful to clean those queries which are misspelled. In this paper, we propose a simple yet efficient method in order to correct misspellings of queries submitted by health information seekers to a medical online search tool. In addition to query normalizations and exact phonetic term matching, we tested two approximate string comparators: the similarity score function of Stoilos and the normalized Levenshtein edit distance. We propose here to combine them to increase the number of matched medical terms in French. We first took a sample of query logs to determine the thresholds and processing times. In the second run, at a greater scale we tested different combinations of query normalizations before or after misspelling correction with the retained thresholds in the first run. According to the total number of suggestions (around 163, the number of the first sample of queries), at a threshold comparator score of 0.3, the normalized Levenshtein edit distance gave the highest F-Measure (88.15%) and at a threshold comparator score of 0.7, the Stoilos function gave the highest F-Measure (84.31%). By combining Levenshtein and Stoilos, the highest F-Measure (80.28%) is obtained with 0.2 and 0.7 thresholds respectively. However, queries are composed by several terms that may be combination of medical terms. The process of query normalization and segmentation is thus required. The highest F-Measure (64.18%) is obtained when this process is realized before spelling-correction. Despite the widely known high performance of the normalized edit distance of Levenshtein, we show in this paper that its combination with the Stoilos algorithm improved the results for misspelling correction of user queries. Accuracy is improved by combining spelling, phoneme-based information and string normalizations and segmentations into medical terms. These encouraging results have enabled the integration of this method into two projects funded by the French National Research Agency-Technologies for Health Care. The first aims to facilitate the coding process of clinical free texts contained in Electronic Health Records and discharge summaries, whereas the second aims at improving information retrieval through Electronic Health Records.
22698889	Analysis of atrial fibrillatory rate during spontaneous episodes of atrial fibrillation in humans using implantable loop recorder electrocardiogram.
J Electrocardiol 20120612 2012 Nov-Dec
Atrial fibrillatory rate (AFR) can predict outcome of interventions for atrial fibrillation (AF); however, AFR behavior at AF onset in humans is poorly described. We studied AFR during spontaneous AF episodes in patients with lone paroxysmal AF who received implantable loop recorders and had AF episodes of 1 hour or more recorded (n = 4). Mean AFR per minute was assessed from continuous implantable loop recorder electrocardiogram using spatiotemporal QRST cancellation and time-frequency analysis. Atrial fibrillatory rate increased from 290 ± 20 to 326 ± 39 fibrillations per minute during the first 3 hours (P&lt;.05) and reached plateau then. Atrial fibrillatory rate beyond the initial 3 hours can, therefore, be considered stable and may be evaluated for prediction of intervention effect.
22683805	Open PHACTS: semantic interoperability for drug discovery.
Drug Discov. Today 20120607 2012Nov
Open PHACTS is a public-private partnership between academia, publishers, small and medium sized enterprises and pharmaceutical companies. The goal of the project is to deliver and sustain an 'open pharmacological space' using and enhancing state-of-the-art semantic web standards and technologies. It is focused on practical and robust applications to solve specific questions in drug discovery research. OPS is intended to facilitate improvements in drug discovery in academia and industry and to support open innovation and in-house non-public drug discovery research. This paper lays out the challenges and how the Open PHACTS project is hoping to address these challenges technically and socially.
23110173	SEED servers: high-performance access to the SEED genomes, annotations, and metabolic models.
PLoS ONE 20121024 2012
The remarkable advance in sequencing technology and the rising interest in medical and environmental microbiology, biotechnology, and synthetic biology resulted in a deluge of published microbial genomes. Yet, genome annotation, comparison, and modeling remain a major bottleneck to the translation of sequence information into biological knowledge, hence computational analysis tools are continuously being developed for rapid genome annotation and interpretation. Among the earliest, most comprehensive resources for prokaryotic genome analysis, the SEED project, initiated in 2003 as an integration of genomic data and analysis tools, now contains &gt;5,000 complete genomes, a constantly updated set of curated annotations embodied in a large and growing collection of encoded subsystems, a derived set of protein families, and hundreds of genome-scale metabolic models. Until recently, however, maintaining current copies of the SEED code and data at remote locations has been a pressing issue. To allow high-performance remote access to the SEED database, we developed the SEED Servers (http://www.theseed.org/servers): four network-based servers intended to expose the data in the underlying relational database, support basic annotation services, offer programmatic access to the capabilities of the RAST annotation server, and provide access to a growing collection of metabolic models that support flux balance analysis. The SEED servers offer open access to regularly updated data, the ability to annotate prokaryotic genomes, the ability to create metabolic reconstructions and detailed models of metabolism, and access to hundreds of existing metabolic models. This work offers and supports a framework upon which other groups can build independent research efforts. Large integrations of genomic data represent one of the major intellectual resources driving research in biology, and programmatic access to the SEED data will provide significant utility to a broad collection of potential users.
23038534	Integrated head design using a nanobeak antenna for thermally assisted magnetic recording.
Opt Express  2012Aug13
We propose a near-field optical transducer using a triangular antenna and a thin film structure (wing) to efficiently generate an optical near-field near a magnetic head. A finite-difference time-domain calculation showed that the near-field was efficiently generated at the apex of the antenna when the dimensions of the wing were optimized for efficient delivery of the surface plasmon excited on the wing to the antenna. The calculated light utilization efficiency (ratio between the absorbed power in the recording medium and the input power) was 8%. The temperature distribution on the medium, magnetic field distribution, and magnetization pattern were calculated; the proposed recording head may be capable of an areal recording density of 2.5 Tb/in.(2).
23037015	Quasi-nonvolatile storage in Ru-doped Bi12SiO20 crystals by two-wavelength holography.
Opt Express  2012Aug27
Prolonged read-out process of a hologram recorded at near infrared with simultaneous green light exposure is measured in Ru-doped Bi12SiO20 crystal. The experimental results are confirmed by numerical simulations, suggesting two different traps involved in the space-charge transport mechanism. In addition, quasi-permanent holographic recording of image with fast updating speed by using two-wavelength recording is demonstrated.
23037434	Two-out-of-two color matching based visual cryptography schemes.
Opt Express  2012Sep24
Visual cryptography which consists in sharing a secret message between transparencies has been extended to color prints. In this paper, we propose a new visual cryptography scheme based on color matching. The stacked printed media reveal a uniformly colored message decoded by the human visual system. In contrast with the previous color visual cryptography schemes, the proposed one enables to share images without pixel expansion and to detect a forgery as the color of the message is kept secret. In order to correctly print the colors on the media and to increase the security of the scheme, we use spectral models developed for color reproduction describing printed colors from an optical point of view.
22925029	Factors influencing consumer adoption of USB-based Personal Health Records in Taiwan.
BMC Health Serv Res 20120827 2012
Usually patients receive healthcare services from multiple hospitals, and consequently their healthcare data are dispersed over many facilities' paper and electronic-based record systems. Therefore, many countries have encouraged the research on data interoperability, access, and patient authorization. This study is an important part of a national project to build an information exchange environment for cross-hospital digital medical records carried out by the Department of Health (DOH) of Taiwan in May 2008. The key objective of the core project is to set up a portable data exchange environment in order to enable people to maintain and own their essential health information.This study is aimed at exploring the factors influencing behavior and adoption of USB-based Personal Health Records (PHR) in Taiwan. Quota sampling was used, and structured questionnaires were distributed to the outpatient department at ten medical centers which participated in the DOH project to establish the information exchange environment across hospitals. A total of 3000 questionnaires were distributed and 1549 responses were collected, out of those 1465 were valid, accumulating the response rate to 48.83%. 1025 out of 1465 respondents had expressed their willingness to apply for the USB-PHR. Detailed analysis of the data reflected that there was a remarkable difference in the "usage intention" between the PHR adopters and non-adopters (χ2 =182.4, p &lt; 0.001). From the result of multivariate logistic regression analyses, we found the key factors affecting patients' adoption pattern were Usage Intention (OR, 9.43, 95%C.I., 5.87-15.16), Perceived Usefulness (OR, 1.60; 95%C.I., 1.11-2.29) and Subjective Norm (OR, 1.47; 95%C.I., 1.21-1.78). Higher Usage Intentions, Perceived Usefulness and Subjective Norm of patients were found to be the key factors influencing PHR adoption. Thus, we suggest that government and hospitals should promote the potential usefulness of PHR, and physicians should encourage patients' to adopt the PHR.
23040967	What really happens to tuberculosis patients classified as lost to follow-up in West Yorkshire?
Euro Surveill. 20120920 2012
Tuberculosis (TB) patients who do not complete treatment pose a potential public health risk. In West Yorkshire, local clinicians suspected that this risk was overestimated by the national Enhanced Tuberculosis Surveillance system. We audited patients who failed to complete treatment and were categorised as lost-to-follow-up (LTFU) between 2004 and 2008, using a combination of hand searching existing records and obtaining additional information from clinicians. In the study period 2,031 TB cases with reported outcome were notified in West Yorkshire, 23% (n=474) did not complete treatment, and 199 (42%) of those were categorised as LTFU 12 months after notification. Of these 199, 49% (n=98) remained LTFU after the audit, 51% (n=101) were re-classified to the following categories: 24% (n=47) transferred abroad, 16% (n=31) recommenced and completed treatment, 6% (n=13) transferred to another clinic in the United Kingdom (UK), and 5% (n=10) died. These patients therefore no-longer posed a public health risk. Further training for clinicians to improve accuracy of outcome reporting has been initiated. Nationally, the collection of treatment outcome data needs to be strengthened and extending the follow-up for treatment outcome monitoring should be considered.
22842049	Creating an infrastructure for high-throughput high-resolution cryogenic electron microscopy.
J. Struct. Biol. 20120725 2012Oct
New instrumentation for three-dimensional electron microscopy is facilitating an increase in the throughput of data collection and reconstruction. The increase in throughput creates bottlenecks in the workflow for storing and processing the image data. Here we describe the creation and quantify the throughput of a high-throughput infrastructure supporting collection of three-dimensional data collection.
23020928	["Veille sanitaire": tools, functions, process of healthcare monitoring in France].
Rev Epidemiol Sante Publique 20120926 2012Oct
In France, the term "veille sanitaire" is widely used to designate healthcare monitoring. It contains, however, a set of concepts that are not shared equally by the entire scientific community. The same is true for activities that are part of it, even if some (surveillance for example) are already well defined. Concepts such as "observation", "vigilance", "alert" for example are not always clear. Furthermore, the use of these words in everyday language maintains this ambiguity. Thus, it seemed necessary to recall these definitions as already used in the literature or legislation texts and to make alternative suggestions. This formalization cannot be carried out without thinking about the structure of "veille sanitaire" and its components. Proposals are provided bringing out concepts of formated "veille" (monitoring) and non-formatted "veille" (monitoring). Definitions, functions, (methods and tools, processes) of these two components are outlined here as well as the cooperative relationship they sustain. The authors have attempted to provide the scientific community with a reference framework useful for exchanging information to promote research and methodological development dedicated to this public health application of epidemiology.
23049785	Research on similarity measurement for texture image retrieval.
PLoS ONE 20120925 2012
A complete texture image retrieval system includes two techniques: texture feature extraction and similarity measurement. Specifically, similarity measurement is a key problem for texture image retrieval study. In this paper, we present an effective similarity measurement formula. The MIT vision texture database, the Brodatz texture database, and the Outex texture database were used to verify the retrieval performance of the proposed similarity measurement method. Dual-tree complex wavelet transform and nonsubsampled contourlet transform were used to extract texture features. Experimental results show that the proposed similarity measurement method achieves better retrieval performance than some existing similarity measurement methods.
22366976	Criteria for the evaluation of a cloud-based hospital information system outsourcing provider.
J Med Syst 20120226 2012Dec
As cloud computing technology has proliferated rapidly worldwide, there has been a trend toward adopting cloud-based hospital information systems (CHISs). This study examines the critical criteria for selecting the CHISs outsourcing provider. The fuzzy Delphi method (FDM) is used to evaluate the primary indicator collected from 188 useable responses at a working hospital in Taiwan. Moreover, the fuzzy analytic hierarchy process (FAHP) is employed to calculate the weights of these criteria and establish a fuzzy multi-criteria model of CHISs outsourcing provider selection from 42 experts. The results indicate that the five most critical criteria related to CHISs outsourcing provider selection are (1) system function, (2) service quality, (3) integration, (4) professionalism, and (5) economics. This study may contribute to understanding how cloud-based hospital systems can reinforce content design and offer a way to compete in the field by developing more appropriate systems.
22399066	A data types profile suitable for use with ISO EN 13606.
J Med Syst 20120308 2012Dec
ISO EN 13606 is a five part International Standard specifying how Electronic Healthcare Record (EHR) information should be communicated between different EHR systems and repositories. Part 1 of the standard defines an information model for representing the EHR information itself, including the representation of types of data value. A later International Standard, ISO 21090:2010, defines a comprehensive set of models for data types needed by all health IT systems. This latter standard is vast, and duplicates some of the functions already handled by ISO EN 13606 part 1. A profile (sub-set) of ISO 21090 would therefore be expected to provide EHR system vendors with a more specially tailored set of data types to implement and avoid the risk of providing more than one modelling option for representing the information properties. This paper describes the process and design decisions made for developing a data types profile for EHR interoperability.
22447202	Security analysis of standards-driven communication protocols for healthcare scenarios.
J Med Syst 20120324 2012Dec
The importance of the Electronic Health Record (EHR), that stores all healthcare-related data belonging to a patient, has been recognised in recent years by governments, institutions and industry. Initiatives like the Integrating the Healthcare Enterprise (IHE) have been developed for the definition of standard methodologies for secure and interoperable EHR exchanges among clinics and hospitals. Using the requisites specified by these initiatives, many large scale projects have been set up for enabling healthcare professionals to handle patients' EHRs. The success of applications developed in these contexts crucially depends on ensuring such security properties as confidentiality, authentication, and authorization. In this paper, we first propose a communication protocol, based on the IHE specifications, for authenticating healthcare professionals and assuring patients' safety. By means of a formal analysis carried out by using the specification language COWS and the model checker CMC, we reveal a security flaw in the protocol thus demonstrating that to simply adopt the international standards does not guarantee the absence of such type of flaws. We then propose how to emend the IHE specifications and modify the protocol accordingly. Finally, we show how to tailor our protocol for application to more critical scenarios with no assumptions on the communication channels. To demonstrate feasibility and effectiveness of our protocols we have fully implemented them.
22492177	Analysis of cloud-based solutions on EHRs systems in different scenarios.
J Med Syst 20120411 2012Dec
Nowadays with the growing of the wireless connections people can access all the resources hosted in the Cloud almost everywhere. In this context, organisms can take advantage of this fact, in terms of e-Health, deploying Cloud-based solutions on e-Health services. In this paper two Cloud-based solutions for different scenarios of Electronic Health Records (EHRs) management system are proposed. We have researched articles published between the years 2005 and 2011 about the implementation of e-Health services based on the Cloud in Medline. In order to analyze the best scenario for the deployment of Cloud Computing two solutions for a large Hospital and a network of Primary Care Health centers have been studied. Economic estimation of the cost of the implementation for both scenarios has been done via the Amazon calculator tool. As a result of this analysis two solutions are suggested depending on the scenario: To deploy a Cloud solution for a large Hospital a typical Cloud solution in which are hired just the needed services has been assumed. On the other hand to work with several Primary Care Centers it's suggested the implementation of a network, which interconnects these centers with just one Cloud environment. Finally it's considered the fact of deploying a hybrid solution: in which EHRs with images will be hosted in the Hospital or Primary Care Centers and the rest of them will be migrated to the Cloud.
22865161	Cloud based emergency health care information service in India.
J Med Syst 20120803 2012Dec
A hospital is a health care organization providing patient treatment by expert physicians, surgeons and equipments. A report from a health care accreditation group says that miscommunication between patients and health care providers is the reason for the gap in providing emergency medical care to people in need. In developing countries, illiteracy is the major key root for deaths resulting from uncertain diseases constituting a serious public health problem. Mentally affected, differently abled and unconscious patients can't communicate about their medical history to the medical practitioners. Also, Medical practitioners can't edit or view DICOM images instantly. Our aim is to provide palm vein pattern recognition based medical record retrieval system, using cloud computing for the above mentioned people. Distributed computing technology is coming in the new forms as Grid computing and Cloud computing. These new forms are assured to bring Information Technology (IT) as a service. In this paper, we have described how these new forms of distributed computing will be helpful for modern health care industries. Cloud Computing is germinating its benefit to industrial sectors especially in medical scenarios. In Cloud Computing, IT-related capabilities and resources are provided as services, via the distributed computing on-demand. This paper is concerned with sprouting software as a service (SaaS) by means of Cloud computing with an aim to bring emergency health care sector in an umbrella with physical secured patient records. In framing the emergency healthcare treatment, the crucial thing considered necessary to decide about patients is their previous health conduct records. Thus a ubiquitous access to appropriate records is essential. Palm vein pattern recognition promises a secured patient record access. Likewise our paper reveals an efficient means to view, edit or transfer the DICOM images instantly which was a challenging task for medical practitioners in the past years. We have developed two services for health care. 1. Cloud based Palm vein recognition system 2. Distributed Medical image processing tools for medical practitioners.
22926919	Secure Dynamic access control scheme of PHR in cloud computing.
J Med Syst 20120828 2012Dec
With the development of information technology and medical technology, medical information has been developed from traditional paper records into electronic medical records, which have now been widely applied. The new-style medical information exchange system "personal health records (PHR)" is gradually developed. PHR is a kind of health records maintained and recorded by individuals. An ideal personal health record could integrate personal medical information from different sources and provide complete and correct personal health and medical summary through the Internet or portable media under the requirements of security and privacy. A lot of personal health records are being utilized. The patient-centered PHR information exchange system allows the public autonomously maintain and manage personal health records. Such management is convenient for storing, accessing, and sharing personal medical records. With the emergence of Cloud computing, PHR service has been transferred to storing data into Cloud servers that the resources could be flexibly utilized and the operation cost can be reduced. Nevertheless, patients would face privacy problem when storing PHR data into Cloud. Besides, it requires a secure protection scheme to encrypt the medical records of each patient for storing PHR into Cloud server. In the encryption process, it would be a challenge to achieve accurately accessing to medical records and corresponding to flexibility and efficiency. A new PHR access control scheme under Cloud computing environments is proposed in this study. With Lagrange interpolation polynomial to establish a secure and effective PHR information access scheme, it allows to accurately access to PHR with security and is suitable for enormous multi-users. Moreover, this scheme also dynamically supports multi-users in Cloud computing environments with personal privacy and offers legal authorities to access to PHR. From security and effectiveness analyses, the proposed PHR access scheme in Cloud computing environments is proven flexible and secure and could effectively correspond to real-time appending and deleting user access authorization and appending and revising PHR records.
22847375	BioGPS and GXD: mouse gene expression data-the benefits and challenges of data integration.
Mamm. Genome 20120731 2012Oct
Mouse gene expression data are complex and voluminous. To maximize the utility of these data, they must be made readily accessible through databases, and those resources need to place the expression data in the larger biological context. Here we describe two community resources that approach these problems in different but complementary ways: BioGPS and the Mouse Gene Expression Database (GXD). BioGPS connects its large and homogeneous microarray gene expression reference data sets via plugins with a heterogeneous collection of external gene centric resources, thus casting a wide but loose net. GXD acquires different types of expression data from many sources and integrates these data tightly with other types of data in the Mouse Genome Informatics (MGI) resource, with a strong emphasis on consistency checks and manual curation. We describe and contrast the "loose" and "tight" data integration strategies employed by BioGPS and GXD, respectively, and discuss the challenges and benefits of data integration. BioGPS is freely available at http://biogps.org . GXD is freely available through the MGI web site ( www.informatics.jax.org ) or directly at www.informatics.jax.org/expression.shtml .
22672435	Glomerular disease search filters for Pubmed, Ovid Medline, and Embase: a development and validation study.
BMC Med Inform Decis Mak 20120606 2012
Tools to enhance physician searches of Medline and other bibliographic databases have potential to improve the application of new knowledge in patient care. This is particularly true for articles about glomerular disease, which are published across multiple disciplines and are often difficult to track down. Our objective was to develop and test search filters for PubMed, Ovid Medline, and Embase that allow physicians to search within a subset of the database to retrieve articles relevant to glomerular disease. We used a diagnostic test assessment framework with development and validation phases. We read a total of 22,992 full text articles for relevance and assigned them to the development or validation set to define the reference standard. We then used combinations of search terms to develop 997,298 unique glomerular disease filters. Outcome measures for each filter included sensitivity, specificity, precision, and accuracy. We selected optimal sensitive and specific search filters for each database and applied them to the validation set to test performance. High performance filters achieved at least 93.8% sensitivity and specificity in the development set. Filters optimized for sensitivity reached at least 96.7% sensitivity and filters optimized for specificity reached at least 98.4% specificity. Performance of these filters was consistent in the validation set and similar among all three databases. PubMed, Ovid Medline, and Embase can be filtered for articles relevant to glomerular disease in a reliable manner. These filters can now be used to facilitate physician searching.
23017886	A hierarchical method for molecular docking using cloud computing.
Bioorg. Med. Chem. Lett. 20120914 2012Nov1
Discovering small molecules that interact with protein targets will be a key part of future drug discovery efforts. Molecular docking of drug-like molecules is likely to be valuable in this field; however, the great number of such molecules makes the potential size of this task enormous. In this paper, a method to screen small molecular databases using cloud computing is proposed. This method is called the hierarchical method for molecular docking and can be completed in a relatively short period of time. In this method, the optimization of molecular docking is divided into two subproblems based on the different effects on the protein-ligand interaction energy. An adaptive genetic algorithm is developed to solve the optimization problem and a new docking program (FlexGAsDock) based on the hierarchical docking method has been developed. The implementation of docking on a cloud computing platform is then discussed. The docking results show that this method can be conveniently used for the efficient molecular design of drugs.
21397354	Searching biosignal databases by content and context: Research Oriented Integration System for ECG Signals (ROISES).
Comput Methods Programs Biomed 20110311 2012Nov
Technological advances in textile, biosensor and electrocardiography domain induced the wide spread use of bio-signal acquisition devices leading to the generation of massive bio-signal datasets. Among the most popular bio-signals, electrocardiogram (ECG) possesses the longest tradition in bio-signal monitoring and recording, being a strong and relatively robust signal. As research resources are fostered, research community promotes the need to extract new knowledge from bio-signals towards the adoption of new medical procedures. However, integrated access, query and management of ECGs are impeded by the diversity and heterogeneity of bio-signal storage data formats. In this scope, the proposed work introduces a new methodology for the unified access to bio-signal databases and the accompanying metadata. It allows decoupling information retrieval from actual underlying datasource structures and enables transparent content and context based searching from multiple data resources. Our approach is based on the definition of an interactive global ontology which manipulates the similarities and the differences of the underlying sources to either establish similarity mappings or enrich its terminological structure. We also introduce ROISES (Research Oriented Integration System for ECG Signals), for the definition of complex content based queries against the diverse bio-signal data sources.
21975083	Generic integration of content-based image retrieval in computer-aided diagnosis.
Comput Methods Programs Biomed 20111005 2012Nov
Content-based image retrieval (CBIR) offers approved benefits for computer-aided diagnosis (CAD), but is still not well established in radiological routine yet. An essential factor is the integration gap between CBIR systems and clinical information systems. The international initiative Integrating the Healthcare Enterprise (IHE) aims at improving interoperability of medical computer systems. We took into account deficiencies in IHE compliance of current picture archiving and communication systems (PACS), and developed an intermediate integration scheme based on the IHE post-processing workflow integration profile (PWF) adapted to CBIR in CAD. The Image Retrieval in Medical Applications (IRMA) framework was used to apply our integration scheme exemplarily, resulting in the application called IRMAcon. The novel IRMAcon scheme provides a generic, convenient and reliable integration of CBIR systems into clinical systems and workflows. Based on the IHE PWF and designed to grow at a pace with the IHE compliance of the particular PACS, it provides sustainability and fosters CBIR in CAD.
22894688	Mining chemical reactions using neighborhood behavior and condensed graphs of reactions approaches.
J Chem Inf Model 20120904 2012Sep24
This work addresses the problem of similarity search and classification of chemical reactions using Neighborhood Behavior (NB) and Condensed Graphs of Reaction (CGR) approaches. The CGR formalism represents chemical reactions as a classical molecular graph with dynamic bonds, enabling descriptor calculations on this graph. Different types of the ISIDA fragment descriptors generated for CGRs in combination with two metrics--Tanimoto and Euclidean--were considered as chemical spaces, to serve for reaction dissimilarity scoring. The NB method has been used to select an optimal combination of descriptors which distinguish different types of chemical reactions in a database containing 8544 reactions of 9 classes. Relevance of NB analysis has been validated in generic (multiclass) similarity search and in clustering with Self-Organizing Maps (SOM). NB-compliant sets of descriptors were shown to display enhanced mapping propensities, allowing the construction of better Self-Organizing Maps and similarity searches (NB and classical similarity search criteria--AUC ROC--correlate at a level of 0.7). The analysis of the SOM clusters proved chemically meaningful CGR substructures representing specific reaction signatures.
23005184	Time-dependent wave selection for information processing in excitable media.
Phys Rev E Stat Nonlin Soft Matter Phys 20120626 2012Jun
We demonstrate an improved technique for implementing logic circuits in light-sensitive chemical excitable media. The technique makes use of the constant-speed propagation of waves along defined channels in an excitable medium based on the Belousov-Zhabotinsky reaction, along with the mutual annihilation of colliding waves. What distinguishes this work from previous work in this area is that regions where channels meet at a junction can periodically alternate between permitting the propagation of waves and blocking them. These valvelike areas are used to select waves based on the length of time that it takes waves to propagate from one valve to another. In an experimental implementation, the channels that make up the circuit layout are projected by a digital projector connected to a computer. Excitable channels are projected as dark areas and unexcitable regions as light areas. Valves alternate between dark and light: Every valve has the same period and phase, with a 50% duty cycle. This scheme can be used to make logic gates based on combinations of or and and-not operations, with few geometrical constraints. Because there are few geometrical constraints, compact circuits can be implemented. Experimental results from an implementation of a four-bit input, two-bit output integer square root circuit are given.
23005375	Extraction of stochastic dynamics from time series.
Phys Rev E Stat Nonlin Soft Matter Phys 20120711 2012Jul
We present a method for the reconstruction of the dynamics of processes with discrete time. The time series from such a system is described by a stochastic recurrence equation, the continuous form of which is known as the Langevin equation. The deterministic f and stochastic g components of the stochastic equation are directly extracted from the measurement data with the assumption that the noise has finite moments and has a zero mean and a unit variance. No other information about the noise distribution is needed. This is contrary to the usual Langevin description, in which the additional assumption that the noise is Gaussian (δ-correlated) distributed as necessary. We test the method using one dimensional deterministic systems (the tent and logistic maps) with Gaussian and with Gumbel noise. In addition, results for human heart rate variability are presented as an example of the application of our method to real data. The differences between cardiological cases can be observed in the properties of the deterministic part f and of the reconstructed noise distribution.
23005828	Information storage, loop motifs, and clustered structure in complex networks.
Phys Rev E Stat Nonlin Soft Matter Phys 20120815 2012Aug
We use a standard discrete-time linear Gaussian model to analyze the information storage capability of individual nodes in complex networks, given the network structure and link weights. In particular, we investigate the role of two- and three-node motifs in contributing to local information storage. We show analytically that directed feedback and feedforward loop motifs are the dominant contributors to information storage capability, with their weighted motif counts locally positively correlated to storage capability. We also reveal the direct local relationship between clustering coefficient(s) and information storage. These results explain the dynamical importance of clustered structure and offer an explanation for the prevalence of these motifs in biological and artificial networks.
23010824	Contact alternatives and the internet.
Dermatitis  2012 Sep-Oct
The Internet has dramatically changed the way we live our lives and the way we practice medicine over the past 20 years. The amount of information that is available, the speed at which new information is shared and disseminated, and the availability of that information to anyone, anywhere have all grown exponentially. This rapid growth presents 2 major challenges: (1) among the vast amount of information available, one must be able to find the specific information needed, at the moment it is needed, without knowing for certain if it even exists; (2) the accuracy and "currentness" of the information must be assessed once it is found.
22950686	Developing open source, self-contained disease surveillance software applications for use in resource-limited settings.
BMC Med Inform Decis Mak 20120906 2012
Emerging public health threats often originate in resource-limited countries. In recognition of this fact, the World Health Organization issued revised International Health Regulations in 2005, which call for significantly increased reporting and response capabilities for all signatory nations. Electronic biosurveillance systems can improve the timeliness of public health data collection, aid in the early detection of and response to disease outbreaks, and enhance situational awareness. As components of its Suite for Automated Global bioSurveillance (SAGES) program, The Johns Hopkins University Applied Physics Laboratory developed two open-source, electronic biosurveillance systems for use in resource-limited settings. OpenESSENCE provides web-based data entry, analysis, and reporting. ESSENCE Desktop Edition provides similar capabilities for settings without internet access. Both systems may be configured to collect data using locally available cell phone technologies. ESSENCE Desktop Edition has been deployed for two years in the Republic of the Philippines. Local health clinics have rapidly adopted the new technology to provide daily reporting, thus eliminating the two-to-three week data lag of the previous paper-based system. OpenESSENCE and ESSENCE Desktop Edition are two open-source software products with the capability of significantly improving disease surveillance in a wide range of resource-limited settings. These products, and other emerging surveillance technologies, can assist resource-limited countries compliance with the revised International Health Regulations.
23010857	Multi-GPU Jacobian accelerated computing for soft-field tomography.
Physiol Meas 20120926 2012Oct
Image reconstruction in soft-field tomography is based on an inverse problem formulation, where a forward model is fitted to the data. In medical applications, where the anatomy presents complex shapes, it is common to use finite element models (FEMs) to represent the volume of interest and solve a partial differential equation that models the physics of the system. Over the last decade, there has been a shifting interest from 2D modeling to 3D modeling, as the underlying physics of most problems are 3D. Although the increased computational power of modern computers allows working with much larger FEM models, the computational time required to reconstruct 3D images on a fine 3D FEM model can be significant, on the order of hours. For example, in electrical impedance tomography (EIT) applications using a dense 3D FEM mesh with half a million elements, a single reconstruction iteration takes approximately 15-20 min with optimized routines running on a modern multi-core PC. It is desirable to accelerate image reconstruction to enable researchers to more easily and rapidly explore data and reconstruction parameters. Furthermore, providing high-speed reconstructions is essential for some promising clinical application of EIT. For 3D problems, 70% of the computing time is spent building the Jacobian matrix, and 25% of the time in forward solving. In this work, we focus on accelerating the Jacobian computation by using single and multiple GPUs. First, we discuss an optimized implementation on a modern multi-core PC architecture and show how computing time is bounded by the CPU-to-memory bandwidth; this factor limits the rate at which data can be fetched by the CPU. Gains associated with the use of multiple CPU cores are minimal, since data operands cannot be fetched fast enough to saturate the processing power of even a single CPU core. GPUs have much faster memory bandwidths compared to CPUs and better parallelism. We are able to obtain acceleration factors of 20 times on a single NVIDIA S1070 GPU, and of 50 times on four GPUs, bringing the Jacobian computing time for a fine 3D mesh from 12 min to 14 s. We regard this as an important step toward gaining interactive reconstruction times in 3D imaging, particularly when coupled in the future with acceleration of the forward problem. While we demonstrate results for EIT, these results apply to any soft-field imaging modality where the Jacobian matrix is computed with the adjoint method.
22903519	Next-generation digital information storage in DNA.
Science 20120816 2012Sep28
Digital information is accumulating at an astounding rate, straining our ability to store and archive it. DNA is among the most dense and stable information media known. The development of new technologies in both DNA synthesis and sequencing make DNA an increasingly feasible digital storage medium. We developed a strategy to encode arbitrary digital information in DNA, wrote a 5.27-megabit book using DNA microchips, and read the book by using next-generation DNA sequencing.
22901028	"Prescribing sunshine": a national, cross-sectional survey of 1,089 New Zealand general practitioners regarding their sun exposure and vitamin D perceptions, and advice provided to patients.
BMC Fam Pract 20120817 2012
The health effects of ultraviolet radiation vary according to wavelength, timing and pattern of exposure, personal characteristics and practices. Negative effects include skin cancers, eye diseases and immune suppression; positive effects primarily relate to endogenous vitamin D production which protects against bone disease. Drafting comprehensive guidelines regarding appropriate sun protective behaviours and vitamin D sufficiency is challenging. Advice given by general practitioners is potentially influential because they are widely respected. A survey instrument was developed, pre-tested and provided to practising GP's, either by on-line link or mailed, reply paid hard-copy. Odds ratios, differences in means, or ratios of geometric means from regression models are reported for potential predictor variables with 95% confidence intervals. Data (demographic, training, practicing, information accessing, confidence in vitamin D knowledge) suitable for analysis were obtained from 1,089 GPs (32% participation). Many (43%) were 'not at all confident' about their vitamin D knowledge. Recent information led 29% to recommend less sun protection during winter months and 10% less all year. Confidence was positively associated with non-'Western' medical training, information sources read and practising in a metropolitan centre with a medical school. Reading the Melanoma Clinical Practice Guidelines was associated with lower estimates of the amount of summer sun exposure required to obtain adequate vitamin D. Increasing years in practice was negatively associated with provision of recommended advice about summer and winter sun protection. Greater concern about vitamin D than skin cancer was expressed by females and those in practice longer. Concern about the potentially negative impact of skin cancer prevention on vitamin D status may undermine appropriate sun protective recommendations. Reading some educational resources was associated with confidence about vitamin D knowledge and a perception that significantly less summer sun exposure was required for those with high sun sensitivity to achieve adequate vitamin D, suggesting a potentially positive impact of such resources. Education could be targeted towards groups least likely to promote existing recommendations. Authoritative guidelines about vitamin D and sun protection would be a valued resource among GPs. Study findings are potentially valuable to help guide public policy and target interventions.
22814199	Integration of critically appraised topics into evidence-based physical therapist practice.
J Orthop Sports Phys Ther 20120719 2012Oct
Physical therapists frequently encounter situations that require complex differential-diagnosis decisions and the ability to consistently screen for serious pathology that may mimic a musculoskeletal complaint. By applying the evidence-based-practice process to diagnosis, screening, and referral, physical therapists can identify diagnostic and screening strategies that positively influence clinical decisions. A critically appraised topic document (a standard 1-page summary of the literature appraisal and clinical relevance in response to a specific clinical question) is a valuable tool in evidence-based practice. The creation of a critically appraised topic makes the educational process cumulative instead of duplicative, allowing the individual clinician to assimilate and consolidate knowledge after a search effort and improving search and appraisal skills. The purpose of this clinical commentary is as follows: (1) to describe the clinical reasoning process of 3 orthopaedic physical therapists that led to the development of specific clinical questions related to screening for nonmusculoskeletal pathology, (2) to describe the search and triage strategy that led each physical therapist to the current best evidence needed to rule out nonmusculoskeletal pathology in the patient, and (3) to discuss the advantages and disadvantages of a critically appraised topic, the implementation of this process, and the tailoring of search strategies to find diagnostic and screening strategies.
22902541	Minireview: progress and challenges in proteomics data management, sharing, and integration.
Mol. Endocrinol. 20120817 2012Oct
The proteome represents the identity, expression levels, interacting partners, and posttranslational modifications of proteins expressed within any given cell. Proteomic studies aim to census the quantitative and qualitative factors regulating the biological relationships of proteins acting in concert as functional cellular networks. In the field of endocrinology, proteomics has been of considerable value in determining the function and mechanism of action of endocrine signaling molecules in the cell membrane, cytoplasm, and nucleus and for the discovery of proteins as candidates for clinical biomarkers. The volume of data that can be generated by proteomics methodologies, up to gigabytes of data within a few hours, brings with it its own logistical hurdles and presents significant challenges to realizing the full potential of these datasets. In this minireview, we describe selected current proteomics methodologies and their application in basic and translational endocrinology before focusing on mass spectrometry as a model for current progress and challenges in data analysis, management, sharing, and integration.
23028267	Predicting the extension of biomedical ontologies.
PLoS Comput. Biol. 20120913 2012
Developing and extending a biomedical ontology is a very demanding task that can never be considered complete given our ever-evolving understanding of the life sciences. Extension in particular can benefit from the automation of some of its steps, thus releasing experts to focus on harder tasks. Here we present a strategy to support the automation of change capturing within ontology extension where the need for new concepts or relations is identified. Our strategy is based on predicting areas of an ontology that will undergo extension in a future version by applying supervised learning over features of previous ontology versions. We used the Gene Ontology as our test bed and obtained encouraging results with average f-measure reaching 0.79 for a subset of biological process terms. Our strategy was also able to outperform state of the art change capturing methods. In addition we have identified several issues concerning prediction of ontology evolution, and have delineated a general framework for ontology extension prediction. Our strategy can be applied to any biomedical ontology with versioning, to help focus either manual or semi-automated extension methods on areas of the ontology that need extension.
23028676	mMass as a software tool for the annotation of cyclic peptide tandem mass spectra.
PLoS ONE 20120913 2012
Natural or synthetic cyclic peptides often possess pronounced bioactivity. Their mass spectrometric characterization is difficult due to the predominant occurrence of non-proteinogenic monomers and the complex fragmentation patterns observed. Even though several software tools for cyclic peptide tandem mass spectra annotation have been published, these tools are still unable to annotate a majority of the signals observed in experimentally obtained mass spectra. They are thus not suitable for extensive mass spectrometric characterization of these compounds. This lack of advanced and user-friendly software tools has motivated us to extend the fragmentation module of a freely available open-source software, mMass (http://www.mmass.org), to allow for cyclic peptide tandem mass spectra annotation and interpretation. The resulting software has been tested on several cyanobacterial and other naturally occurring peptides. It has been found to be superior to other currently available tools concerning both usability and annotation extensiveness. Thus it is highly useful for accelerating the structure confirmation and elucidation of cyclic as well as linear peptides and depsipeptides.
23029317	Patterns of information-seeking for cancer on the internet: an analysis of real world data.
PLoS ONE 20120921 2012
Although traditionally the primary information sources for cancer patients have been the treating medical team, patients and their relatives increasingly turn to the Internet, though this source may be misleading and confusing. We assess Internet searching patterns to understand the information needs of cancer patients and their acquaintances, as well as to discern their underlying psychological states. We screened 232,681 anonymous users who initiated cancer-specific queries on the Yahoo Web search engine over three months, and selected for study users with high levels of interest in this topic. Searches were partitioned by expected survival for the disease being searched. We compared the search patterns of anonymous users and their contacts. Users seeking information on aggressive malignancies exhibited shorter search periods, focusing on disease- and treatment-related information. Users seeking knowledge regarding more indolent tumors searched for longer periods, alternated between different subjects, and demonstrated a high interest in topics such as support groups. Acquaintances searched for longer periods than the proband user when seeking information on aggressive (compared to indolent) cancers. Information needs can be modeled as transitioning between five discrete states, each with a unique signature representing the type of information of interest to the user. Thus, early phases of information-seeking for cancer follow a specific dynamic pattern. Areas of interest are disease dependent and vary between probands and their contacts. These patterns can be used by physicians and medical Web site authors to tailor information to the needs of patients and family members.
23030728	Intraoral digital radiography: elements of effective imaging.
Compend Contin Educ Dent  2012Oct
Intraoral digital imaging has evolved from an experimental and sometimes disparaged technique in the mid 1980s to a reliable and ubiquitously used technology today. There are many advantages for use of digital radiographic techniques in dentistry, one of the chief ones being patient dose reduction. However, as important as dose reduction is for safe and effective radiography, practicing dentists would also like to understand the fundamental differences between digital system configurations so they may be able to make an informed choice as to which system best fits their needs. In addition, there has been considerable debate on the following topics: sensor technology; factors associated with image display; optimum techniques for image manipulation; and image storage, retrieval, and archiving. This article provides insight into these and other elements of effective imaging in intraoral digital imaging.
22651257	Knowledge translation of research findings.
Implement Sci 20120531 2012
One of the most consistent findings from clinical and health services research is the failure to translate research into practice and policy. As a result of these evidence-practice and policy gaps, patients fail to benefit optimally from advances in healthcare and are exposed to unnecessary risks of iatrogenic harms, and healthcare systems are exposed to unnecessary expenditure resulting in significant opportunity costs. Over the last decade, there has been increasing international policy and research attention on how to reduce the evidence-practice and policy gap. In this paper, we summarise the current concepts and evidence to guide knowledge translation activities, defined as T2 research (the translation of new clinical knowledge into improved health). We structure the article around five key questions: what should be transferred; to whom should research knowledge be transferred; by whom should research knowledge be transferred; how should research knowledge be transferred; and, with what effect should research knowledge be transferred? We suggest that the basic unit of knowledge translation should usually be up-to-date systematic reviews or other syntheses of research findings. Knowledge translators need to identify the key messages for different target audiences and to fashion these in language and knowledge translation products that are easily assimilated by different audiences. The relative importance of knowledge translation to different target audiences will vary by the type of research and appropriate endpoints of knowledge translation may vary across different stakeholder groups. There are a large number of planned knowledge translation models, derived from different disciplinary, contextual (i.e., setting), and target audience viewpoints. Most of these suggest that planned knowledge translation for healthcare professionals and consumers is more likely to be successful if the choice of knowledge translation strategy is informed by an assessment of the likely barriers and facilitators. Although our evidence on the likely effectiveness of different strategies to overcome specific barriers remains incomplete, there is a range of informative systematic reviews of interventions aimed at healthcare professionals and consumers (i.e., patients, family members, and informal carers) and of factors important to research use by policy makers. There is a substantial (if incomplete) evidence base to guide choice of knowledge translation activities targeting healthcare professionals and consumers. The evidence base on the effects of different knowledge translation approaches targeting healthcare policy makers and senior managers is much weaker but there are a profusion of innovative approaches that warrant further evaluation.
22969272	Anatomy of biometric passports.
J. Biomed. Biotechnol. 20120826 2012
Travelling is becoming available for more and more people. Millions of people are on a way every day. That is why a better control over global human transfer and a more reliable identity check is desired. A recent trend in a field of personal identification documents is to use RFID (Radio Frequency Identification) technology and biometrics, especially (but not only) in passports. This paper provides an insight into the electronic passports (also called e-passport or ePassport) implementation chosen in the Czech Republic. Such a summary is needed for further studies of biometric passports implementation security and biometric passports analysis. A separate description of the Czech solution is a prerequisite for a planned analysis, because of the uniqueness of each implementation. (Each country can choose the implementation details within a range specified by the ICAO (International Civil Aviation Organisation); moreover, specific security mechanisms are optional and can be omitted).
22969273	A privacy-preserved analytical method for ehealth database with minimized information loss.
J. Biomed. Biotechnol. 20120830 2012
Digitizing medical information is an emerging trend that employs information and communication technology (ICT) to manage health records, diagnostic reports, and other medical data more effectively, in order to improve the overall quality of medical services. However, medical information is highly confidential and involves private information, even legitimate access to data raises privacy concerns. Medical records provide health information on an as-needed basis for diagnosis and treatment, and the information is also important for medical research and other health management applications. Traditional privacy risk management systems have focused on reducing reidentification risk, and they do not consider information loss. In addition, such systems cannot identify and isolate data that carries high risk of privacy violations. This paper proposes the Hiatus Tailor (HT) system, which ensures low re-identification risk for medical records, while providing more authenticated information to database users and identifying high-risk data in the database for better system management. The experimental results demonstrate that the HT system achieves much lower information loss than traditional risk management methods, with the same risk of re-identification.
22973733	[Improving data warehouse environments for efficient analysis of long time-series data].
Rinsho Byori  2012Jul
Medical records contain enormous amounts of data. It is important to extract useful evidence from such data and feedback to clinical medicine. Evidence-based medicine (EBM) was introduced in the 1990s and has been widely used for more than 20 years, however, hospital information system environments that take advantage of the ideas of EBM have not yet been established. Recently, the numbers of medical institutions with multilateral search systems for the medical records stored in data warehouses (DWHs) have been increasing, but these institutions' systems cannot deal fully with issues such as data reliability and high-dimensional, high-speed searches. DWHs can control long time-series data. Although, the measurement methods and analytical equipment used have been modified and improved with advances in testing techniques, this may have induced shifting and/or fragmentation of these types of data. Furthermore, database design has to be flexible to satisfy the various demands of information retrieval; systems must therefore have the structures to deal with such demands. We report here our new system infrastructure, which exchanges data in order to absorb the data shifting associated with changes in the testing methods. The system enables the preparation of DWH environments that can be used to seamlessly analyze long time-series data, record in knowledge databases the results of comprehensive analyses of institutions' characteristics of laboratory diagnoses, and use the data in education, research and clinical practice.
22809317	Detecting causality from online psychiatric texts using inter-sentential language patterns.
BMC Med Inform Decis Mak 20120718 2012
Online psychiatric texts are natural language texts expressing depressive problems, published by Internet users via community-based web services such as web forums, message boards and blogs. Understanding the cause-effect relations embedded in these psychiatric texts can provide insight into the authors' problems, thus increasing the effectiveness of online psychiatric services. Previous studies have proposed the use of word pairs extracted from a set of sentence pairs to identify cause-effect relations between sentences. A word pair is made up of two words, with one coming from the cause text span and the other from the effect text span. Analysis of the relationship between these words can be used to capture individual word associations between cause and effect sentences. For instance, (broke up, life) and (boyfriend, meaningless) are two word pairs extracted from the sentence pair: "I broke up with my boyfriend. Life is now meaningless to me". The major limitation of word pairs is that individual words in sentences usually cannot reflect the exact meaning of the cause and effect events, and thus may produce semantically incomplete word pairs, as the previous examples show. Therefore, this study proposes the use of inter-sentential language patterns such as ≪broke up, boyfriend&gt;, &lt;life, meaningless≫ to detect causality between sentences. The inter-sentential language patterns can capture associations among multiple words within and between sentences, thus can provide more precise information than word pairs. To acquire inter-sentential language patterns, we develop a text mining framework by extending the classical association rule mining algorithm such that it can discover frequently co-occurring patterns across the sentence boundary. Performance was evaluated on a corpus of texts collected from PsychPark (http://www.psychpark.org), a virtual psychiatric clinic maintained by a group of volunteer professionals from the Taiwan Association of Mental Health Informatics. Experimental results show that the use of inter-sentential language patterns outperformed the use of word pairs proposed in previous studies. This study demonstrates the acquisition of inter-sentential language patterns for causality detection from online psychiatric texts. Such semantically more complete and precise features can improve causality detection performance.
22976360	Evidence-based medicine and primary care: keeping up is hard to do.
Mt. Sinai J. Med.  2012 Sep-Oct
Primary-care physicians feel pressure to be knowledgeable, efficient, comprehensive, and compassionate while delivering evidence-based medical care. Incorporating evidence-based medicine into practice requires training in the skills of finding and applying good evidence to patients, and, increasingly, infrastructure that supports the incorporation of evidence into electronic health records. Physicians cite many barriers to the use of evidence-based medicine in practice. In this review, we examine evidence of the value of evidence-based medicine in clinical practice, discuss the interface of evidence and shared decision-making, suggest tools and approaches for incorporating evidence-based medicine into practice, and discuss the impact of recent health insurance reform on expectations and incentives for physicians with respect to evidence-based practice.
22721565	A robust data treatment approach for fuel cells system analysis.
ISA Trans 20120620 2012Nov
This paper describes the implementation of a practical approach for fuel cells system data analysis. A number of data treatment techniques such as data management and treatment, data synchronization, and data reconciliation are introduced and discussed in order to solve the issues raised in the practical case. These techniques are integrated in a software environment which provides user a fast, efficient, and rational electrochemical investigation. The performance of the approach is illustrated using an industrial fuel cell stack test system.
22749424	The benefits of introducing electronic health records in residential aged care facilities: a multiple case study.
Int J Med Inform 20120628 2012Oct
Information and communications technology solutions have been introduced into the residential aged care system in order to improve the effectiveness and efficiency of aged care, however to date, the actual benefits have not been systematically analysed. The aim of this study was to identify the benefits of electronic health records (EHR) in residential aged care services and to examine how the benefits have been achieved. A qualitative interview study was conducted in nine residential aged care facilities (RACFs) belonging to three organisations in the Australian Capital Territory (ACT), New South Wales (NSW) and Queensland, Australia. A longitudinal investigation after the implementation of the aged care EHR systems was conducted at two data points: January 2009 to December 2009 and December 2010 to February 2011. Semi-structured interviews were conducted with 110 care staff members selected through theoretical sampling, representing all levels of care staff who worked in those facilities. Three categories of benefits were perceived by the care staff members according to who gain the benefits: the benefits to individual care staff members, to residents and to the RACFs. The benefits to individual care staff members include an improvement of documentation efficiency, information and knowledge growth as well as empowering the staff; the benefits to residents are an improvement in the quality of individual residents' health records, the higher quality of care and smoother communication between the residents and aged care staff; the RACFs gain an increased ability to manage information and acquire funding, an increase in their ability to control the care quality and improvements in the working environment and educational benefits. Three factors leading to these benefits were examined: the nature of the aged care EHR systems in comparison with paper-based records; the way the systems were used by the staff and one benefit that could lead to another. In this study, EHR systems were perceived to have substantial benefits for care staff, residents and the aged care organisations introducing the systems. The benefits were derived from the nature of the aged care EHR systems, staff members' continuous use of the systems, and one benefit led to the other.
22981028	[Information on legal issues in health technologies: methodological proposal to identify them in a systematic and comprehensible manner].
Z Evid Fortbild Qual Gesundhwes 20120623 2012
Rules and regulations form the framework of Health Technology Assessments. Legal issues are directly associated with the technology (as patents/licenses) or to the patients and their basic rights (as autonomy). In order to identify the regulations of interest as well as the relevant publications in a systematic and transparent way a specific methodological approach is required. In the absence of adapted methods, our objective was to develop a methodological approach to the systematic retrieval of information on legal issues. No publications on adapted methods could be identified. We therefore developed a procedure following the workflow of information retrieval for effectiveness assessments. This workflow consists of 8 steps: 0. pre-search: identification of the relevant rules, regulations and patient-related issues, 1. translation of the search question, 2. concept building, 3. identification of synonyms, 4. selection of relevant information sources, 5. design of the search strategies, 6. execution and quality check, 7. saving the results and reporting. There are numerous publications on legal issues associated with health technologies. Specifically adapted procedures are qualified to identify them in a systematic and transparent manner using the appropriate sensitivity and precision. A wider application seems to be reasonable in order to further test its practicality against more topics and to modify the proposed method if indicated.
22776564	Estimating the re-identification risk of clinical data sets.
BMC Med Inform Decis Mak 20120709 2012
De-identification is a common way to protect patient privacy when disclosing clinical data for secondary purposes, such as research. One type of attack that de-identification protects against is linking the disclosed patient data with public and semi-public registries. Uniqueness is a commonly used measure of re-identification risk under this attack. If uniqueness can be measured accurately then the risk from this kind of attack can be managed. In practice, it is often not possible to measure uniqueness directly, therefore it must be estimated. We evaluated the accuracy of uniqueness estimators on clinically relevant data sets. Four candidate estimators were identified because they were evaluated in the past and found to have good accuracy or because they were new and not evaluated comparatively before: the Zayatz estimator, slide negative binomial estimator, Pitman's estimator, and mu-argus. A Monte Carlo simulation was performed to evaluate the uniqueness estimators on six clinically relevant data sets. We varied the sampling fraction and the uniqueness in the population (the value being estimated). The median relative error and inter-quartile range of the uniqueness estimates was measured across 1000 runs. There was no single estimator that performed well across all of the conditions. We developed a decision rule which selected between the Pitman, slide negative binomial and Zayatz estimators depending on the sampling fraction and the difference between estimates. This decision rule had the best consistent median relative error across multiple conditions and data sets. This study identified an accurate decision rule that can be used by health privacy researchers and disclosure control professionals to estimate uniqueness in clinical data sets. The decision rule provides a reliable way to measure re-identification risk.
22849591	Querying phenotype-genotype relationships on patient datasets using semantic web technology: the example of Cerebrotendinous xanthomatosis.
BMC Med Inform Decis Mak 20120731 2012
Semantic Web technology can considerably catalyze translational genetics and genomics research in medicine, where the interchange of information between basic research and clinical levels becomes crucial. This exchange involves mapping abstract phenotype descriptions from research resources, such as knowledge databases and catalogs, to unstructured datasets produced through experimental methods and clinical practice. This is especially true for the construction of mutation databases. This paper presents a way of harmonizing abstract phenotype descriptions with patient data from clinical practice, and querying this dataset about relationships between phenotypes and genetic variants, at different levels of abstraction. Due to the current availability of ontological and terminological resources that have already reached some consensus in biomedicine, a reuse-based ontology engineering approach was followed. The proposed approach uses the Ontology Web Language (OWL) to represent the phenotype ontology and the patient model, the Semantic Web Rule Language (SWRL) to bridge the gap between phenotype descriptions and clinical data, and the Semantic Query Web Rule Language (SQWRL) to query relevant phenotype-genotype bidirectional relationships. The work tests the use of semantic web technology in the biomedical research domain named cerebrotendinous xanthomatosis (CTX), using a real dataset and ontologies. A framework to query relevant phenotype-genotype bidirectional relationships is provided. Phenotype descriptions and patient data were harmonized by defining 28 Horn-like rules in terms of the OWL concepts. In total, 24 patterns of SWQRL queries were designed following the initial list of competency questions. As the approach is based on OWL, the semantic of the framework adapts the standard logical model of an open world assumption. This work demonstrates how semantic web technologies can be used to support flexible representation and computational inference mechanisms required to query patient datasets at different levels of abstraction. The open world assumption is especially good for describing only partially known phenotype-genotype relationships, in a way that is easily extensible. In future, this type of approach could offer researchers a valuable resource to infer new data from patient data for statistical analysis in translational research. In conclusion, phenotype description formalization and mapping to clinical data are two key elements for interchanging knowledge between basic and clinical research.
22547459	On combining multiple features for cartoon character retrieval and clip synthesis.
IEEE Trans Syst Man Cybern B Cybern 20120425 2012Oct
How do we retrieve cartoon characters accurately? Or how to synthesize new cartoon clips smoothly and efficiently from the cartoon library? Both questions are important for animators and cartoon enthusiasts to design and create new cartoons by utilizing existing cartoon materials. The first key issue to answer those questions is to find a proper representation that describes the cartoon character effectively. In this paper, we consider multiple features from different views, i.e., color histogram, Hausdorff edge feature, and skeleton feature, to represent cartoon characters with different colors, shapes, and gestures. Each visual feature reflects a unique characteristic of a cartoon character, and they are complementary to each other for retrieval and synthesis. However, how to combine the three visual features is the second key issue of our application. By simply concatenating them into a long vector, it will end up with the so-called "curse of dimensionality," let alone their heterogeneity embedded in different visual feature spaces. Here, we introduce a semisupervised multiview subspace learning (semi-MSL) algorithm, to encode different features in a unified space. Specifically, under the patch alignment framework, semi-MSL uses the discriminative information from labeled cartoon characters in the construction of local patches where the manifold structure revealed by unlabeled cartoon characters is utilized to capture the geometric distribution. The experimental evaluations based on both cartoon character retrieval and clip synthesis demonstrate the effectiveness of the proposed method for cartoon application. Moreover, additional results of content-based image retrieval on benchmark data suggest the generality of semi-MSL for other applications.
22967952	Ranking Gene Ontology terms for predicting non-classical secretory proteins in eukaryotes and prokaryotes.
J. Theor. Biol. 20120808 2012Nov7
Protein secretion is an important biological process for both eukaryotes and prokaryotes. Several sequence-based methods mainly rely on utilizing various types of complementary features to design accurate classifiers for predicting non-classical secretory proteins. Gene Ontology (GO) terms are increasing informative in predicting protein functions. However, the number of used GO terms is often very large. For example, there are 60,020 GO terms used in the prediction method Euk-mPLoc 2.0 for subcellular localization. This study proposes a novel approach to identify a small set of m top-ranked GO terms served as the only type of input features to design a support vector machine (SVM) based method Sec-GO to predict non-classical secretory proteins in both eukaryotes and prokaryotes. To evaluate the Sec-GO method, two existing methods and their used datasets are adopted for performance comparisons. The Sec-GO method using m=436 GO terms yields an independent test accuracy of 96.7% on mammalian proteins, much better than the existing method SPRED (82.2%) which uses frequencies of tri-peptides and short peptides, secondary structure, and physicochemical properties as input features of a random forest classifier. Furthermore, when applying to Gram-positive bacterial proteins, the Sec-GO with m=158 GO terms has a test accuracy of 94.5%, superior to NClassG+ (90.0%) which uses SVM with several feature types, comprising amino acid composition, di-peptides, physicochemical properties and the position specific weighting matrix. Analysis of the distribution of secretory proteins in a GO database indicates the percentage of the non-classical secretory proteins annotated by GO is larger than that of classical secretory proteins in both eukaryotes and prokaryotes. Of the m top-ranked GO features, the top-four GO terms are all annotated by such subcellular locations as GO:0005576 (Extracellular region). Additionally, the method Sec-GO is easily implemented and its web tool of prediction is available at iclab.life.nctu.edu.tw/secgo.
22234836	An interactive system for computer-aided diagnosis of breast masses.
J Digit Imaging  2012Oct
Although mammography is the only clinically accepted imaging modality for screening the general population to detect breast cancer, interpreting mammograms is difficult with lower sensitivity and specificity. To provide radiologists "a visual aid" in interpreting mammograms, we developed and tested an interactive system for computer-aided detection and diagnosis (CAD) of mass-like cancers. Using this system, an observer can view CAD-cued mass regions depicted on one image and then query any suspicious regions (either cued or not cued by CAD). CAD scheme automatically segments the suspicious region or accepts manually defined region and computes a set of image features. Using content-based image retrieval (CBIR) algorithm, CAD searches for a set of reference images depicting "abnormalities" similar to the queried region. Based on image retrieval results and a decision algorithm, a classification score is assigned to the queried region. In this study, a reference database with 1,800 malignant mass regions and 1,800 benign and CAD-generated false-positive regions was used. A modified CBIR algorithm with a new function of stretching the attributes in the multi-dimensional space and decision scheme was optimized using a genetic algorithm. Using a leave-one-out testing method to classify suspicious mass regions, we compared the classification performance using two CBIR algorithms with either equally weighted or optimally stretched attributes. Using the modified CBIR algorithm, the area under receiver operating characteristic curve was significantly increased from 0.865 ± 0.006 to 0.897 ± 0.005 (p &lt; 0.001). This study demonstrated the feasibility of developing an interactive CAD system with a large reference database and achieving improved performance.
22546983	Development of a next-generation automated DICOM processing system in a PACS-less research environment.
J Digit Imaging  2012Oct
The use of clinical imaging modalities within the pharmaceutical research space provides value and challenges. Typical clinical settings will utilize a Picture Archive and Communication System (PACS) to transmit and manage Digital Imaging and Communications in Medicine (DICOM) images generated by clinical imaging systems. However, a PACS is complex and provides many features that are not required within a research setting, making it difficult to generate a business case and determine the return on investment. We have developed a next-generation DICOM processing system using open-source software, commodity server hardware such as Apple Xserve®, high-performance network-attached storage (NAS), and in-house-developed preprocessing programs. DICOM-transmitted files are arranged in a flat file folder hierarchy easily accessible via our downstream analysis tools and a standard file browser. This next-generation system had a minimal construction cost due to the reuse of all the components from our first-generation system with the addition of a second server for a few thousand dollars. Performance metrics were gathered and the system was found to be highly scalable, performed significantly better than the first-generation system, is modular, has satisfactory image integrity, and is easier to maintain than the first-generation system. The resulting system is also portable across platforms and utilizes minimal hardware resources, allowing for easier upgrades and migration to smaller form factors at the hardware end-of-life. This system has been in production successfully for 8 months and services five clinical instruments and three pre-clinical instruments. This system has provided us with the necessary DICOM C-Store functionality, eliminating the need for a clinical PACS for day-to-day image processing.
22992644	Engaging nurses in research utilization.
J Nurses Staff Dev  2012 Sep-Oct
Research skills education is needed for nurses at all levels: novice, intermediate, and advanced. Nurse educators can help novice nurse researchers develop skills such as performing literature searches and critiquing research articles, which are necessary to develop and update clinical practice guidelines and implement evidence-based practice. The purpose of this article is to describe an innovative approach to encourage nurses to perform literature searches and critique research articles as a means to eventually engage in evidence-based practice.
22994069	[How to increase the visibility of your scientific publication on the Internet?].
Med Pr  2012
New technologies compel the scientists to change their thinking about dissemination of their own publications. In the time of rapid information exchange scientific papers should be available to the largest possible audience, it is therefore of vital importance to prepare them properly. The authors present the ways how to optimize the visibility of scientific publications on the Internet by the use of appropriate choice of words in the title, summary and key words. They discuss the mechanisms of indexing in scientific search engines and bibliographic-abstract databases showing at the same time how to use them in drawing up the paper. They also give an example of two summaries of the same article. The first one comprises the elements of academic search engine optimization (ASEO) while the other one does not follow these principles in the article description.
22728304	Detecting outliers in high-dimensional neuroimaging datasets with robust covariance estimators.
Med Image Anal 20120524 2012Oct
Medical imaging datasets often contain deviant observations, the so-called outliers, due to acquisition or preprocessing artifacts or resulting from large intrinsic inter-subject variability. These can undermine the statistical procedures used in group studies as the latter assume that the cohorts are composed of homogeneous samples with anatomical or functional features clustered around a central mode. The effects of outlying subjects can be mitigated by detecting and removing them with explicit statistical control. With the emergence of large medical imaging databases, exhaustive data screening is no longer possible, and automated outlier detection methods are currently gaining interest. The datasets used in medical imaging are often high-dimensional and strongly correlated. The outlier detection procedure should therefore rely on high-dimensional statistical multivariate models. However, state-of-the-art procedures, based on the Minimum Covariance Determinant (MCD) estimator, are not well-suited for such high-dimensional settings. In this work, we introduce regularization in the MCD framework and investigate different regularization schemes. We carry out extensive simulations to provide backing for practical choices in absence of ground truth knowledge. We demonstrate on functional neuroimaging datasets that outlier detection can be performed with small sample sizes and improves group studies.
22959839	Deformable segmentation via sparse representation and dictionary learning.
Med Image Anal 20120823 2012Oct
"Shape" and "appearance", the two pillars of a deformable model, complement each other in object segmentation. In many medical imaging applications, while the low-level appearance information is weak or mis-leading, shape priors play a more important role to guide a correct segmentation, thanks to the strong shape characteristics of biological structures. Recently a novel shape prior modeling method has been proposed based on sparse learning theory. Instead of learning a generative shape model, shape priors are incorporated on-the-fly through the sparse shape composition (SSC). SSC is robust to non-Gaussian errors and still preserves individual shape characteristics even when such characteristics is not statistically significant. Although it seems straightforward to incorporate SSC into a deformable segmentation framework as shape priors, the large-scale sparse optimization of SSC has low runtime efficiency, which cannot satisfy clinical requirements. In this paper, we design two strategies to decrease the computational complexity of SSC, making a robust, accurate and efficient deformable segmentation system. (1) When the shape repository contains a large number of instances, which is often the case in 2D problems, K-SVD is used to learn a more compact but still informative shape dictionary. (2) If the derived shape instance has a large number of vertices, which often appears in 3D problems, an affinity propagation method is used to partition the surface into small sub-regions, on which the sparse shape composition is performed locally. Both strategies dramatically decrease the scale of the sparse optimization problem and hence speed up the algorithm. Our method is applied on a diverse set of biomedical image analysis problems. Compared to the original SSC, these two newly-proposed modules not only significant reduce the computational complexity, but also improve the overall accuracy.
22829486	Beyond PICO: the SPIDER tool for qualitative evidence synthesis.
Qual Health Res 20120724 2012Oct
Standardized systematic search strategies facilitate rigor in research. Current search tools focus on retrieval of quantitative research. In this article we address issues relating to using existing search strategy tools, most typically the PICO (Population, Intervention, Comparison, Outcome) formulation for defining key elements of a review question, when searching for qualitative and mixed methods research studies. An alternative search strategy tool for qualitative/mixed methods research is outlined: SPIDER (Sample, Phenomenon of Interest, Design, Evaluation, Research type). We used both the SPIDER and PICO search strategy tools with a qualitative research question. We have used the SPIDER tool to advance thinking beyond PICO in its suitable application to qualitative and mixed methods research. However, we have highlighted once more the need for improved indexing of qualitative articles in databases. To constitute a viable alternative to PICO, SPIDER needs to be refined and tested on a wider range of topics.
22932220	Designing a public square for research computing.
Sci Transl Med  2012Aug29
A set of principles is proposed for sponsors and developers of research computing applications that can increase the likelihood of successful adoption by researchers.
22937081	Local-based semantic navigation on a networked representation of information.
PLoS ONE 20120824 2012
The size and complexity of actual networked systems hinders the access to a global knowledge of their structure. This fact pushes the problem of navigation to suboptimal solutions, one of them being the extraction of a coherent map of the topology on which navigation takes place. In this paper, we present a Markov chain based algorithm to tag networked terms according only to their topological features. The resulting tagging is used to compute similarity between terms, providing a map of the networked information. This map supports local-based navigation techniques driven by similarity. We compare the efficiency of the resulting paths according to their length compared to that of the shortest path. Additionally we claim that the path steps towards the destination are semantically coherent. To illustrate the algorithm performance we provide some results from the Simple English Wikipedia, which amounts to several thousand of pages. The simplest greedy strategy yields over an 80% of average success rate. Furthermore, the resulting content-coherent paths most often have a cost between one- and threefold compared to shortest-path lengths.
22941959	Interfaces to PeptideAtlas: a case study of standard data access systems.
Brief. Bioinformatics 20111122 2012Sep
Access to public data sets is important to the scientific community as a resource to develop new experiments or validate new data. Projects such as the PeptideAtlas, Ensembl and The Cancer Genome Atlas (TCGA) offer both access to public data and a repository to share their own data. Access to these data sets is often provided through a web page form and a web service API. Access technologies based on web protocols (e.g. http) have been in use for over a decade and are widely adopted across the industry for a variety of functions (e.g. search, commercial transactions, and social media). Each architecture adapts these technologies to provide users with tools to access and share data. Both commonly used web service technologies (e.g. REST and SOAP), and custom-built solutions over HTTP are utilized in providing access to research data. Providing multiple access points ensures that the community can access the data in the simplest and most effective manner for their particular needs. This article examines three common access mechanisms for web accessible data: BioMart, caBIG, and Google Data Sources. These are illustrated by implementing each over the PeptideAtlas repository and reviewed for their suitability based on specific usages common to research. BioMart, Google Data Sources, and caBIG are each suitable for certain uses. The tradeoffs made in the development of the technology are dependent on the uses each was designed for (e.g. security versus speed). This means that an understanding of specific requirements and tradeoffs is necessary before selecting the access technology.
22611296	A mass spectrometry proteomics data management platform.
Mol. Cell Proteomics 20120518 2012Sep
Mass spectrometry-based proteomics is increasingly being used in biomedical research. These experiments typically generate a large volume of highly complex data, and the volume and complexity are only increasing with time. There exist many software pipelines for analyzing these data (each typically with its own file formats), and as technology improves, these file formats change and new formats are developed. Files produced from these myriad software programs may accumulate on hard disks or tape drives over time, with older files being rendered progressively more obsolete and unusable with each successive technical advancement and data format change. Although initiatives exist to standardize the file formats used in proteomics, they do not address the core failings of a file-based data management system: (1) files are typically poorly annotated experimentally, (2) files are "organically" distributed across laboratory file systems in an ad hoc manner, (3) files formats become obsolete, and (4) searching the data and comparing and contrasting results across separate experiments is very inefficient (if possible at all). Here we present a relational database architecture and accompanying web application dubbed Mass Spectrometry Data Platform that is designed to address the failings of the file-based mass spectrometry data management approach. The database is designed such that the output of disparate software pipelines may be imported into a core set of unified tables, with these core tables being extended to support data generated by specific pipelines. Because the data are unified, they may be queried, viewed, and compared across multiple experiments using a common web interface. Mass Spectrometry Data Platform is open source and freely available at http://code.google.com/p/msdapl/.
22941983	Federated queries for comparative effectiveness research: performance analysis.
Stud Health Technol Inform  2012
This paper presents a study of the performance of federated queries implemented in a system that simulates the architecture proposed for the Scalable Architecture for Federated Translational Inquiries Network (SAFTINet). Performance tests were conducted using both physical hardware and virtual machines within the test laboratory of the Center for High Performance Computing at the University of Utah. Tests were performed on SAFTINet networks ranging from 4 to 32 nodes with databases containing synthetic data for several million patients. The results show that the caGrid FQE (Federated Query Engine) is capable and suitable for comparative effectiveness research (CER) federated queries given its nearly linear scalability as partner nodes increase in number. The results presented here are also important for the specification of the hardware required to run a CER grid.
22941984	A system architecture for sharing de-identified, research-ready brain scans and health information across clinical imaging centers.
Stud Health Technol Inform  2012
Progress in our understanding of brain disorders increasingly relies on the costly collection of large standardized brain magnetic resonance imaging (MRI) data sets. Moreover, the clinical interpretation of brain scans benefits from compare and contrast analyses of scans from patients with similar, and sometimes rare, demographic, diagnostic, and treatment status. A solution to both needs is to acquire standardized, research-ready clinical brain scans and to build the information technology infrastructure to share such scans, along with other pertinent information, across hospitals. This paper describes the design, deployment, and operation of a federated imaging system that captures and shares standardized, de-identified clinical brain images in a federation across multiple institutions. In addition to describing innovative aspects of the system architecture and our initial testing of the deployed infrastructure, we also describe the Standardized Imaging Protocol (SIP) developed for the project and our interactions with the Institutional Review Board (IRB) regarding handling patient data in the federated environment.
22941986	eLab: bringing together people, data and methods to enhance knowledge discovery in healthcare settings.
Stud Health Technol Inform  2012
The discovery of knowledge from raw data is a multistage process, that typical requires collaboration between experts from disparate disciplines, and the application of a range of methods tailored to the research question. The aim of the eLab is to provide a web-based environment for health professionals and researchers to access health datasets, share knowledge and expertise and to apply methods for analysis and visualization of the results. The eLab is built around the core concept of the Research Object as the mechanism for preserving, reusing and disseminating the knowledge discovery process. The possible range of applications of the eLab is vast, and so the consideration of the trade off between specificity and generality is an important one, that is reflected in the requirements. The architecture and implementation of the eLab is described, and we report on the deployment of eLabs for applications in primary care, long-term conditions management, bariatric surgery and public health.
22941994	Tutorial on academic high-performance cloud computing.
Stud Health Technol Inform  2012
This documents shortly describes the background and structure of the academic high-performance cloud computing tutorial at the Healthgrid conference.
22942000	Desktop Cloud Visualization: the new technology to remote access 3D interactive applications in the Cloud.
Stud Health Technol Inform  2012
In the proposed demonstration we will present DCV (Desktop Cloud Visualization): a unique technology that allows users to remote access 2D and 3D interactive applications over a standard network. This allows geographically dispersed doctors work collaboratively and to acquire anatomical or pathological images and visualize them for further investigations.
22942003	Science gateways for semantic-web-based life science applications.
Stud Health Technol Inform  2012
In this paper we present the architecture of a framework for building Science Gateways supporting official standards both for user authentication and authorization and for middleware-independent job and data management. Two use cases of the customization of the Science Gateway framework for Semantic-Web-based life science applications are also described.
22942004	Opening new gateways to workflows for life scientists.
Stud Health Technol Inform  2012
The combination of highly complex biology problems and varying IT skills among life scientists poses a unique challenge in designing bioinformatics programs. The set of tools and initiatives described in this work shows new ways of making life science workflows more accessible to the community. Our aim is to help bioinformaticians help biologists. We present how to make Taverna workflows available from within Galaxy, both widely used bioinformatics platforms. Calling Galaxy tools from Taverna is also discussed. In addition, we describe a web application that allows a user to run arbitrary Taverna workflows by only using a web browser.
22942005	Workflow-enhanced conformational analysis of guanidine zinc complexes via a science gateway.
Stud Health Technol Inform  2012
The new science gateway MoSGrid (Molecular Simulation Grid) enables users to submit and process molecular simulation studies on a large scale. A conformational analysis of guanidine zinc complexes, which are active catalysts in the ring-opening polymerization of lactide, is presented as an example. Such a large-scale quantum chemical study is enabled by workflow technologies. Two times 40 conformers have been generated, for two guanidine zinc complexes. Their structures were optimized using Gaussian03 and the energies processed within the quantum chemistry portlet of the MoSGrid portal. All meta- and post-processing steps have been performed in this portlet. All workflow features are implemented via WS-PGRADE and submitted to UNICORE.
22942006	Application repository and science gateway for running molecular docking and dynamics simulations.
Stud Health Technol Inform  2012
Molecular docking and dynamics studies are of considerable importance in a range of disciplines including molecular biology, drug design, environmental studies, psychology, etc. Using in silico tools to support or even to substitute wet laboratory work could help better focusing the laboratory experiments resulting not only in considerable saving of resources but also increasing the number of molecules and scenarios investigated. There are several software packages that support in silico modeling. However, these tools require lot of compute resources and special technical knowledge. As a result, many bio-scientists cannot use them. The paper describes a science gateway based solution which provides access to Distributed Computing Infrastructures such as clouds, desktop and service grids. This environment enables bio-scientists to execute simple molecular modeling scenarios or build more complex use-cases from existing building blocks while hiding the technical details of the infrastructure. Four scenarios have been defined and deconstructed in order to identify common building blocks supporting a large number of complex use-cases. A reference implementation for the first scenario regarding the impact on indicator species of pharmaceuticals released into water courses has been implemented on the EDGI infrastructure, demonstrating the feasibility of the approach.
22942007	Structure simulation with calculated NMR parameters - integrating COSMOS into the CCPN framework.
Stud Health Technol Inform  2012
The Collaborative Computing Project for NMR (CCPN) has build a software framework consisting of the CCPN data model (with APIs) for NMR related data, the CcpNmr Analysis program and additional tools like CcpNmr FormatConverter. The open architecture allows for the integration of external software to extend the abilities of the CCPN framework with additional calculation methods. Recently, we have carried out the first steps for integrating our software Computer Simulation of Molecular Structures (COSMOS) into the CCPN framework. The COSMOS-NMR force field unites quantum chemical routines for the calculation of molecular properties with a molecular mechanics force field yielding the relative molecular energies. COSMOS-NMR allows introducing NMR parameters as constraints into molecular mechanics calculations. The resulting infrastructure will be made available for the NMR community. As a first application we have tested the evaluation of calculated protein structures using COSMOS-derived 13C Cα and Cβ chemical shifts. In this paper we give an overview of the methodology and a roadmap for future developments and applications.
22942008	Web-based interactive visualization in a Grid-enabled neuroimaging application using HTML5.
Stud Health Technol Inform  2012
Interactive visualization and correction of intermediate results are required in many medical image analysis pipelines. To allow certain interaction in the remote execution of compute- and data-intensive applications, new features of HTML5 are used. They allow for transparent integration of user interaction into Grid- or Cloud-enabled scientific workflows. Both 2D and 3D visualization and data manipulation can be performed through a scientific gateway without the need to install specific software or web browser plugins. The possibilities of web-based visualization are presented along the FreeSurfer-pipeline, a popular compute- and data-intensive software tool for quantitative neuroimaging.
22942009	The Einstein Genome Gateway using WASP - a high throughput multi-layered life sciences portal for XSEDE.
Stud Health Technol Inform  2012
Massively-parallel sequencing (MPS) technologies and their diverse applications in genomics and epigenomics research have yielded enormous new insights into the physiology and pathophysiology of the human genome. The biggest hurdle remains the magnitude and diversity of the datasets generated, compromising our ability to manage, organize, process and ultimately analyse data. The Wiki-based Automated Sequence Processor (WASP), developed at the Albert Einstein College of Medicine (hereafter Einstein), uniquely manages to tightly couple the sequencing platform, the sequencing assay, sample metadata and the automated workflows deployed on a heterogeneous high performance computing cluster infrastructure that yield sequenced, quality-controlled and 'mapped' sequence data, all within the one operating environment accessible by a web-based GUI interface. WASP at Einstein processes 4-6 TB of data per week and since its production cycle commenced it has processed ~ 1 PB of data overall and has revolutionized user interactivity with these new genomic technologies, who remain blissfully unaware of the data storage, management and most importantly processing services they request. The abstraction of such computational complexity for the user in effect makes WASP an ideal middleware solution, and an appropriate basis for the development of a grid-enabled resource - the Einstein Genome Gateway - as part of the Extreme Science and Engineering Discovery Environment (XSEDE) program. In this paper we discuss the existing WASP system, its proposed middleware role, and its planned interaction with XSEDE to form the Einstein Genome Gateway.
22942011	Integrated support for neuroscience research: from study design to publication.
Stud Health Technol Inform  2012
Computational neuroscience is a new field of research in which neurodegenerative diseases are studied with the aid of new imaging techniques and computation facilities. Researchers with different expertise collaborate in these studies. A study requires scalable computational and storage capacity and information management facilities to succeed. Many virtual laboratories are proposed and developed to facilitate these studies, however most of them cover only the parts related to the computational data processing. In this paper we describe and analyse the phases of the computational neuroscience studies including the actors, the tasks they perform, and the characteristics of each phase. Based on these we identify the required properties and functionalities of a virtual laboratory that supports the actors and their tasks throughout the complete study.
22942012	iBRAIN2: automated analysis and data handling for RNAi screens.
Stud Health Technol Inform  2012
We report on the implementation of a software suite dedicated to the management and analysis of large scale RNAi High Content Screening (HCS). We describe the requirements identified amongst our different users, the supported data flow, and the implemented software. Our system is already supporting productively three different laboratories operating in distinct IT infrastructures. The system was already used to analyze hundreds of RNAi HCS plates.
22942047	Standards and solutions for architecture based, ontology driven and individualized pervasive health.
Stud Health Technol Inform  2012
Based on the long-term work of scientific institutions and SDOs dedicated to system architectures, an interoperability framework is presented to help navigation through existing, emerging and even future standards for comprehensive interoperability of intelligent health and social care services. HL7 artifacts as well as work products of competing organizations are classified and semi-formally interrelated. The methodology is proven in many international standard development and health information systems implementation projects.
22942048	Mobile health apps - from singular to collaborative.
Stud Health Technol Inform  2012
Mobile health apps are proliferating, but they fail to deliver on a key patient and caregiver requirement - the ability to collaborate using key phone features while leveraging existing web services. Typically, mobile health apps are for single use, proprietary, and deliver closed-world solutions. By making use of web services, both open and proprietary, mobile health apps can be created to support the caregiver network in the community. The full value of telehealth will only be achieved when the spectrum of trusted health care services (preventive, promotion, curative, and rehabilitative) is delivered to the collaborating network of caregivers.
22942049	Architectural approach for semantic EHR systems development based on Detailed Clinical Models.
Stud Health Technol Inform  2012
The integrative approach to health information in general and the development of pHealth systems in particular, require an integrated approach of formally modeled system architectures. Detailed Clinical Models (DCM) is one of the most promising modeling efforts for clinical concept representation in EHR system architectures. Although the feasibility of DCM modeling methodology has been demonstrated through examples, there is no formal, generic and automatic modeling transformation technique to ensure a semantic lossless transformation of clinical concepts expressed in DCM to either clinical concept representations based on ISO 13606/openEHR Archetypes or HL7 Templates. The objective of this paper is to propose a generic model transformation method and tooling for transforming DCM Clinical Concepts into ISO/EN 13606/openEHR Archetypes or HL7 Template models. The automation of the transformation process is supported by Model Driven-Development (MDD) transformation mechanisms and tools. The availability of processes, techniques and tooling for automatic DCM transformation would enable the development of intelligent, adaptive information systems as demanded for pHealth solutions.
22942050	Adaptive intelligent systems for pHealth - an architectural approach.
Stud Health Technol Inform  2012
Health systems around the globe, especially in developing countries, are facing the challenge of delivering effective, safe, and high quality public health and individualized health services independent of time and location, and with minimum of allocated resources (pHealth). In this context, health promotion and health education services are very important, especially in primary care settings. The objective of this paper is to describe the architecture of an adaptive intelligent system mainly developed to support education and training of citizens, but also of health professionals. The proposed architecture describes a system consisting of several agents that cooperatively interact to find and process tutoring materials to disseminate them to users (multi-agent system). A prototype is being implemented which includes medical students from the Medical Faculty at University of Cauca (Colombia). In the experimental process, the student´s learning style - detected with the Bayesian Model - is compared against the learning style obtained from a questioner (manual approach).
22942051	Architectural analysis of clinical ontologies for pHealth interoperability.
Stud Health Technol Inform  2012
Comprehensive interoperability between eHealth/pHealth systems requires properly represented shared knowledge. Formal ontologies allow specifying the semantics of health knowledge representation in a well-defined and unambiguous manner. The objective of this paper is to formally analyze - from a system-theoretical architectural perspective - existing clinical ontologies. The paper defines important ontology requirements for semantically interoperable pHealth/eHealth systems. Then, based on those requirements, 17 criteria are defined and used for analyzing 129 clinical ontologies. Statistical results confirm that most ontologies do not meet the defined criteria. OBO foundry defines a good approach to meet all defined criteria, but it does not cover yet the clinical domain as a whole. SNOMED CT was found the more comprehensive one, despite several restrictions.
22762949	The database search problem: a question of rational decision making.
Forensic Sci. Int. 20120703 2012Oct10
This paper applies probability and decision theory in the graphical interface of an influence diagram to study the formal requirements of rationality which justify the individualization of a person found through a database search. The decision-theoretic part of the analysis studies the parameters that a rational decision maker would use to individualize the selected person. The modeling part (in the form of an influence diagram) clarifies the relationships between this decision and the ingredients that make up the database search problem, i.e., the results of the database search and the different pairs of propositions describing whether an individual is at the source of the crime stain. These analyses evaluate the desirability associated with the decision of 'individualizing' (and 'not individualizing'). They point out that this decision is a function of (i) the probability that the individual in question is, in fact, at the source of the crime stain (i.e., the state of nature), and (ii) the decision maker's preferences among the possible consequences of the decision (i.e., the decision maker's loss function). We discuss the relevance and argumentative implications of these insights with respect to recent comments in specialized literature, which suggest points of view that are opposed to the results of our study.
22946032	Recent advances in nuclear magnetic resonance quantum information processing.
Philos Trans A Math Phys Eng Sci  2012Oct13
Quantum information processors have the potential to drastically change the way we communicate and process information. Nuclear magnetic resonance (NMR) has been one of the first experimental implementations of quantum information processing (QIP) and continues to be an excellent testbed to develop new QIP techniques. We review the recent progress made in NMR QIP, focusing on decoupling, pulse engineering and indirect nuclear control. These advances have enhanced the capabilities of NMR QIP, and have useful applications in both traditional NMR and other QIP architectures.
22946033	Implementing quantum logic gates with gradient ascent pulse engineering: principles and practicalities.
Philos Trans A Math Phys Eng Sci  2012Oct13
We briefly describe the use of gradient ascent pulse engineering (GRAPE) pulses to implement quantum logic gates in nuclear magnetic resonance quantum computers, and discuss a range of simple extensions to the core technique. We then consider a range of difficulties that can arise in practical implementations of GRAPE sequences, reflecting non-idealities in the experimental systems used.
22946034	Control aspects of quantum computing using pure and mixed states.
Philos Trans A Math Phys Eng Sci  2012Oct13
Steering quantum dynamics such that the target states solve classically hard problems is paramount to quantum simulation and computation. And beyond, quantum control is also essential to pave the way to quantum technologies. Here, important control techniques are reviewed and presented in a unified frame covering quantum computational gate synthesis and spectroscopic state transfer alike. We emphasize that it does not matter whether the quantum states of interest are pure or not. While pure states underly the design of quantum circuits, ensemble mixtures of quantum states can be exploited in a more recent class of algorithms: it is illustrated by characterizing the Jones polynomial in order to distinguish between different (classes of) knots. Further applications include Josephson elements, cavity grids, ion traps and nitrogen vacancy centres in scenarios of closed as well as open quantum systems.
22676626	The data management of a phase III efficacy trial of an 11-valent pneumococcal conjugate vaccine and related satellite studies conducted in the Philippines.
BMC Res Notes 20120607 2012
A large phase III placebo-controlled, randomized efficacy trial of an investigational 11-valent pneumococcal conjugate vaccine against pneumonia in children less than 2 years of age was conducted in the Philippines from July 2000 to December 2004. Clinical data from 12,194 children who were given either study vaccine or placebo was collected from birth up to two years of age for the occurrence of radiologically proven pneumonia as the primary endpoint, and for clinical pneumonia and invasive pneumococcal disease as the secondary endpoints. Several tertiary endpoints were also explored. Along the core trial, several satellite studies on herd immunity, cost-effectiveness of the study vaccine, acute otitis media, and wheezing were conducted. We describe here in detail how the relevant clinical records were managed and how quality control procedures were implemented to ensure that valid data were obtained respectively for the core trial and for the satellite studies. We discuss how the task was achieved, what the challenges were and what might have been done differently. There were several factors that made the task of data management doable and efficient. First, a pre-trial data management system was available. Secondly, local committed statisticians, programmers and support staff were available and partly familiar to clinical trials. Thirdly, the personnel had undergone training during trial and grew with the task they were supposed to do. Thus the knowledge needed to develop and operate clinical data system was fully transferred to local staff. Current Controlled Trials ISRCTN62323832.
22776079	Concept annotation in the CRAFT corpus.
BMC Bioinformatics 20120709 2012
Manually annotated corpora are critical for the training and evaluation of automated methods to identify concepts in biomedical text. This paper presents the concept annotations of the Colorado Richly Annotated Full-Text (CRAFT) Corpus, a collection of 97 full-length, open-access biomedical journal articles that have been annotated both semantically and syntactically to serve as a research resource for the biomedical natural-language-processing (NLP) community. CRAFT identifies all mentions of nearly all concepts from nine prominent biomedical ontologies and terminologies: the Cell Type Ontology, the Chemical Entities of Biological Interest ontology, the NCBI Taxonomy, the Protein Ontology, the Sequence Ontology, the entries of the Entrez Gene database, and the three subontologies of the Gene Ontology. The first public release includes the annotations for 67 of the 97 articles, reserving two sets of 15 articles for future text-mining competitions (after which these too will be released). Concept annotations were created based on a single set of guidelines, which has enabled us to achieve consistently high interannotator agreement. As the initial 67-article release contains more than 560,000 tokens (and the full set more than 790,000 tokens), our corpus is among the largest gold-standard annotated biomedical corpora. Unlike most others, the journal articles that comprise the corpus are drawn from diverse biomedical disciplines and are marked up in their entirety. Additionally, with a concept-annotation count of nearly 100,000 in the 67-article subset (and more than 140,000 in the full collection), the scale of conceptual markup is also among the largest of comparable corpora. The concept annotations of the CRAFT Corpus have the potential to significantly advance biomedical text mining by providing a high-quality gold standard for NLP systems. The corpus, annotation guidelines, and other associated resources are freely available at http://bionlp-corpora.sourceforge.net/CRAFT/index.shtml.
22402197	Active learning strategies for the deduplication of electronic patient data using classification trees.
J Biomed Inform 20120228 2012Oct
Supervised record linkage methods often require a clerical review to gain informative training data. Active learning means to actively prompt the user to label data with special characteristics in order to minimise the review costs. We conducted an empirical evaluation to investigate whether a simple active learning strategy using binary comparison patterns is sufficient or if string metrics together with a more sophisticated algorithm are necessary to achieve high accuracies with a small training set. Based on medical registry data with different numbers of attributes, we used active learning to acquire training sets for classification trees, which were then used to classify the remaining data. Active learning for binary patterns means that every distinct comparison pattern represents a stratum from which one item is sampled. Active learning for patterns consisting of the Levenshtein string metric values uses an iterative process where the most informative and representative examples are added to the training set. In this context, we extended the active learning strategy by Sarawagi and Bhamidipaty (2002). On the original data set, active learning based on binary comparison patterns leads to the best results. When dropping four or six attributes, using string metrics leads to better results. In both cases, not more than 200 manually reviewed training examples are necessary. In record linkage applications where only forename, name and birthday are available as attributes, we suggest the sophisticated active learning strategy based on string metrics in order to achieve highly accurate results. We recommend the simple strategy if more attributes are available, as in our study. In both cases, active learning significantly reduces the amount of manual involvement in training data selection compared to usual record linkage settings.
22901087	Modeling and mining term association for improving biomedical information retrieval performance.
BMC Bioinformatics 20120611 2012
The growth of the biomedical information requires most information retrieval systems to provide short and specific answers in response to complex user queries. Semantic information in the form of free text that is structured in a way makes it straightforward for humans to read but more difficult for computers to interpret automatically and search efficiently. One of the reasons is that most traditional information retrieval models assume terms are conditionally independent given a document/passage. Therefore, we are motivated to consider term associations within different contexts to help the models understand semantic information and use it for improving biomedical information retrieval performance. We propose a term association approach to discover term associations among the keywords from a query. The experiments are conducted on the TREC 2004-2007 Genomics data sets and the TREC 2004 HARD data set. The proposed approach is promising and achieves superiority over the baselines and the GSP results. The parameter settings and different indices are investigated that the sentence-based index produces the best results in terms of the document-level, the word-based index for the best results in terms of the passage-level and the paragraph-based index for the best results in terms of the passage2-level. Furthermore, the best term association results always come from the best baseline. The tuning number k in the proposed recursive re-ranking algorithm is discussed and locally optimized to be 10. First, modelling term association for improving biomedical information retrieval using factor analysis, is one of the major contributions in our work. Second, the experiments confirm that term association considering co-occurrence and dependency among the keywords can produce better results than the baselines treating the keywords independently. Third, the baselines are re-ranked according to the importance and reliance of latent factors behind term associations. These latent factors are decided by the proposed model and their term appearances in the first round retrieved passages.
22634118	O' surgery case log data, where art thou?
J. Am. Coll. Surg. 20120526 2012Sep
The American College of Surgeons (ACS) Case Log represents a data system that satisfies the American Board of Surgery (ABS) Maintenance of Certification (MOC) program, yet has broad data fields for surgical subspecialties. Using the ACS Case Log, we have developed a method of data capture, categorization, and reporting of acute care surgery fellows' experiences. In July 2010, our acute care surgery fellowship required our fellows to log their clinical experiences into the ACS Case Log. Cases were entered similar to billable documentation rules. Keywords were entered that specified institutional services and/or resuscitation types. These data were exported in comma separated value format, deidentified, structured by Current Procedural Terminology (CPT) codes relevant to acute care surgery, and substratified by fellow and/or fellow year. Fifteen report types were created consisting of operative experience by service, procedure by major category (cardiothoracic, vascular, solid organ, abdominal wall, hollow viscus, and soft tissue), total resuscitations, ultrasound, airway, ICU services, basic neurosurgery, and basic orthopaedics. Results are viewable via a secure Web application, accessible nationally, and exportable to many formats. Using the ACS Case Log satisfies the ABS MOC program requirements and provides a method for monitoring and reporting acute care surgery fellow experiences. This system is flexible to accommodate the needs of surgical subspecialties and their training programs. As documentation requirements expand, efficient clinical documentation is a must for the busy surgeon. Although, our data entry and processing method has the immediate capacity for acute care surgery fellowships nationwide, multiple larger decisions regarding national case log systems should be encouraged.
22570233	Role of the apparent diffusion coefficient in the differential diagnosis of gastric wall thickening.
J Magn Reson Imaging 20120508 2012Sep
To evaluate the role of the apparent diffusion coefficient (ADC) measurement made using diffusion-weighted magnetic resonance imaging (DWMRI) in the differential diagnosis of benign and malignant gastric wall thickening. Axial T2-weighted and DWMRI at b 600 and b 1000 s/mm(2) gradients were performed in 94 patients (44 patients with gastric malignancy and 50 patients with benign gastric diseases) with gastric wall thickening which was detected by multidetector computed tomography (MDCT). The ADC values of the gastric lesions and healthy gastric walls in patients with gastric malignancies and in patients with benign gastric diseases were used in the differential diagnosis of benign and malignant lesions of the stomach. The mean ADC values were lower in patients with gastric malignancies (1.62 ± 0.57 and 1.40 ± 0.33 at b 600 and b 1000, respectively) compared to those with healthy gastric walls (2.95 ± 0.59 and 2.18 ± 0.48) and benign gastric diseases (3.08 ± 0.52 and 2.34 ± 0.42) at b 600 and b 1000 gradients (P &lt; 0.0001). The ADC measurement on DWMRI may be used to differentiate between benign and malignant gastric diseases.
22903655	MRI temporal acceleration techniques.
J Magn Reson Imaging  2012Sep
In recent years, there has been an explosive growth of magnetic resonance imaging (MRI) techniques that allow faster scan speed by exploiting temporal or spatiotemporal redundancy of the images. These techniques improve the performance of dynamic imaging significantly across multiple clinical applications, including cardiac functional examinations, perfusion imaging, blood flow assessment, contrast-enhanced angiography, functional MRI, and interventional imaging, among others. The scan acceleration permits higher spatial resolution, increased temporal resolution, shorter scan duration, or a combination of these benefits. Along with the exciting developments is a dizzying proliferation of acronyms and variations of the techniques. The present review attempts to summarize this rapidly growing topic and presents conceptual frameworks to understand these techniques in terms of their underlying mechanics and connections. Techniques from view sharing, keyhole, k-t, to compressed sensing are covered.
22641155	Inclusive sharing of mass spectrometry imaging data requires a converter for all.
J Proteomics 20120526 2012Aug30
With continued efforts towards a single MSI data format, data conversion routines must be made universally available. The benefits of a common imaging format, imzML, are slowly becoming more widely appreciated but the format remains to be used by only a small proportion of imaging groups. Increased awareness amongst researchers and continued support from major MS vendors in providing tools for converting proprietary formats into imzML are likely to result in a rapidly increasing uptake of the format. It is important that this does not lead to the exclusion of researchers using older or unsupported instruments. We describe an open source converter, imzMLConverter, to ensure against this. We propose that proprietary formats should first be converted to mzML using one of the widely available converters, such as msconvert and then use imzMLConverter to convert mzML to imzML. This will allow a wider audience to benefit from the imzML format immediately.
22842151	imzML--a common data format for the flexible exchange and processing of mass spectrometry imaging data.
J Proteomics 20120726 2012Aug30
The application of mass spectrometry imaging (MS imaging) is rapidly growing with a constantly increasing number of different instrumental systems and software tools. The data format imzML was developed to allow the flexible and efficient exchange of MS imaging data between different instruments and data analysis software. imzML data is divided in two files which are linked by a universally unique identifier (UUID). Experimental details are stored in an XML file which is based on the HUPO-PSI format mzML. Information is provided in the form of a 'controlled vocabulary' (CV) in order to unequivocally describe the parameters and to avoid redundancy in nomenclature. Mass spectral data are stored in a binary file in order to allow efficient storage. imzML is supported by a growing number of software tools. Users will be no longer limited to proprietary software, but are able to use the processing software best suited for a specific question or application. MS imaging data from different instruments can be converted to imzML and displayed with identical parameters in one software package for easier comparison. All technical details necessary to implement imzML and additional background information is available at www.imzml.org.
22916163	Identification of additional trials in prospective trial registers for Cochrane systematic reviews.
PLoS ONE 20120815 2012
Publication and selective outcome reporting bias are a threat to the validity of systematic reviews. Extensive searching for additional trials in prospective trial registers could reduce this problem. We have evaluated how authors of Cochrane systematic reviews currently make use of trial registers as an additional source for the identification of potentially eligible trials. We included 210 systematic Cochrane reviews of interventions published between 2008 and 2010 of which the protocol was first published in 2008. When prospective trial registers were searched we recorded the names of the register(s), the authors' motive(s) and if they yielded any extra trials. In 80 reviews (38.1%) the authors had searched in one or more prospective trial register(s) of which 55% had searched in overlapping search portals and individual registers. Most frequently assessed were the MetaRegister (66.3%) and Clinicaltrials.gov (60%) which is in sharp contrast of other registers or portals like the WHO ICTRP Search Portal (20%). Reported motives to use registers were to identify ongoing trials (83.3%), to identify unpublished outcomes or trials (23.5%), to identify recently published trials (11.8%), or to identify any relevant trial (3.9%).In 28 reviews (35%) the authors had selected (ongoing) trials identified in trial registers as potentially eligible. Trial registers as an additional source of information are gaining acknowledgement amongst Cochrane reviewers. Nevertheless, searches seem to be inefficient as overlapping databases are frequently consulted, while the WHO ICTRP Search Portal that includes the data from all approved registers worldwide is being underused. Moreover, the emphasis is now on the identification of ongoing trials, although the prospective registers offer a broader potential. Further familiarity of registers and guidance how to search and to report will help to implement this as a common method and utilize the full potential of prospective trial registers for systematic reviews.
22759239	Qualitative evaluation of a diabetes electronic decision support tool: views of users.
BMC Med Inform Decis Mak 20120703 2012
Quality care of type 2 diabetes is complex and requires systematic use of clinical data to monitor care processes and outcomes. An electronic decision support (EDS) tool for the management of type 2 diabetes in primary care was developed by the Australian Pharmaceutical Alliance. The aim of this qualitative study was to evaluate the uptake and use of the EDS tool as well as to describe the impact of the EDS tool on the primary care consultation for diabetes from the perspectives of general practitioners and practice nurses. This was a qualitative study of telephone interviews. General Practitioners and Practice Nurses from four Divisions of General Practice who had used the EDS tool for a minimum of six weeks were invited to participate. Semi-structured interviews were conducted and the interview transcripts were coded and thematically analysed using NVivo 8 software. In total 15 General Practitioners and 2 Practice Nurses completed the interviews. The most commonly used feature of the EDS tool was the summary side bar; its major function was to provide an overview of clinical information and a prompt or reminder to diabetes care. It also assisted communication and served an educational role as a visual aide in the consultation. Some participants thought the tool resulted in longer consultations. There were a range of barriers to use related to the design and functionality of the tool and to the primary care context. The EDS tool shows promise as a way of summarising information about patients' diabetes state, reminder of required diabetes care and an aide to patient education.
22743228	VAT: a computational framework to functionally annotate variants in personal genomes within a cloud-computing environment.
Bioinformatics 20120628 2012Sep1
The functional annotation of variants obtained through sequencing projects is generally assumed to be a simple intersection of genomic coordinates with genomic features. However, complexities arise for several reasons, including the differential effects of a variant on alternatively spliced transcripts, as well as the difficulty in assessing the impact of small insertions/deletions and large structural variants. Taking these factors into consideration, we developed the Variant Annotation Tool (VAT) to functionally annotate variants from multiple personal genomes at the transcript level as well as obtain summary statistics across genes and individuals. VAT also allows visualization of the effects of different variants, integrates allele frequencies and genotype data from the underlying individuals and facilitates comparative analysis between different groups of individuals. VAT can either be run through a command-line interface or as a web application. Finally, in order to enable on-demand access and to minimize unnecessary transfers of large data files, VAT can be run as a virtual machine in a cloud-computing environment. VAT is implemented in C and PHP. The VAT web service, Amazon Machine Image, source code and detailed documentation are available at vat.gersteinlab.org.
22789588	MyMiner: a web application for computer-assisted biocuration and text annotation.
Bioinformatics 20120712 2012Sep1
The exponential growth of scientific literature has resulted in a massive amount of unstructured natural language data that cannot be directly handled by means of bioinformatics tools. Such tools generally require structured data, often generated through a cumbersome process of manual literature curation. Herein, we present MyMiner, a free and user-friendly text annotation tool aimed to assist in carrying out the main biocuration tasks and to provide labelled data for the development of text mining systems. MyMiner allows easy classification and labelling of textual data according to user-specified classes as well as predefined biological entities. The usefulness and efficiency of this application have been tested for a range of real-life annotation scenarios of various research topics. http://myminer.armi.monash.edu.au.
22922203	BIND - an algorithm for loss-less compression of nucleotide sequence data.
J. Biosci.  2012Sep
Recent advances in DNA sequencing technologies have enabled the current generation of life science researchers to probe deeper into the genomic blueprint. The amount of data generated by these technologies has been increasing exponentially since the last decade. Storage, archival and dissemination of such huge data sets require efficient solutions, both from the hardware as well as software perspective. The present paper describes BIND-an algorithm specialized for compressing nucleotide sequence data. By adopting a unique 'block-length' encoding for representing binary data (as a key step), BIND achieves significant compression gains as compared to the widely used general purpose compression algorithms (gzip, bzip2 and lzma). Moreover, in contrast to implementations of existing specialized genomic compression approaches, the implementation of BIND is enabled to handle non-ATGC and lowercase characters. This makes BIND a loss-less compression approach that is suitable for practical use. More importantly, validation results of BIND (with real-world data sets) indicate reasonable speeds of compression and decompression that can be achieved with minimal processor/ memory usage. BIND is available for download at http://metagenomics.atc.tcs.com/compression/BIND. No license is required for academic or non-profit use.
21822675	Directional binary wavelet patterns for biomedical image indexing and retrieval.
J Med Syst 20110806 2012Oct
A new algorithm for medical image retrieval is presented in the paper. An 8-bit grayscale image is divided into eight binary bit-planes, and then binary wavelet transform (BWT) which is similar to the lifting scheme in real wavelet transform (RWT) is performed on each bitplane to extract the multi-resolution binary images. The local binary pattern (LBP) features are extracted from the resultant BWT sub-bands. Three experiments have been carried out for proving the effectiveness of the proposed algorithm. Out of which two are meant for medical image retrieval and one for face retrieval. It is further mentioned that the database considered for three experiments are OASIS magnetic resonance imaging (MRI) database, NEMA computer tomography (CT) database and PolyU-NIRFD face database. The results after investigation shows a significant improvement in terms of their evaluation measures as compared to LBP and LBP with Gabor transform.
22327385	Effective management of medical information through a novel blind watermarking technique.
J Med Syst 20120212 2012Oct
Medical Data Management (MDM) domain consists of various issues of medical information like authentication, security, privacy, retrieval and storage etc. Medical Image Watermarking (MIW) techniques have recently emerged as a leading technology to solve the problems associated with MDM. This paper proposes a blind, Contourlet Transform (CNT) based MIW scheme, robust to high JPEG and JPEG2000 compression and simultaneously capable of addressing a range of MDM issues like medical information security, content authentication, safe archiving and controlled access retrieval etc. It also provides a way for effective data communication along with automated medical personnel teaching. The original medical image is first decomposed by CNT. The Low pass subband is used to embed the watermark in such a way that enables the proposed method to extract the embedded watermark in a blind manner. Inverse CNT is then applied to get the watermarked image. Extensive experiments were carried out and the performance of the proposed scheme is evaluated through both subjective and quantitative measures. The experimental results and comparisons, confirm the effectiveness and efficiency of the proposed technique in the MDM paradigm.
22351166	A secure EHR system based on hybrid clouds.
J Med Syst 20120221 2012Oct
Consequently, application services rendering remote medical services and electronic health record (EHR) have become a hot topic and stimulating increased interest in studying this subject in recent years. Information and communication technologies have been applied to the medical services and healthcare area for a number of years to resolve problems in medical management. Sharing EHR information can provide professional medical programs with consultancy, evaluation, and tracing services can certainly improve accessibility to the public receiving medical services or medical information at remote sites. With the widespread use of EHR, building a secure EHR sharing environment has attracted a lot of attention in both healthcare industry and academic community. Cloud computing paradigm is one of the popular healthIT infrastructures for facilitating EHR sharing and EHR integration. In this paper, we propose an EHR sharing and integration system in healthcare clouds and analyze the arising security and privacy issues in access and management of EHRs.
22771201	Proactive screening for depression through metaphorical and automatic text analysis.
Artif Intell Med 20120706 2012Sep
Proactive and automatic screening for depression is a challenge facing the public health system. This paper describes a system for addressing the above challenge. The system implementing the methodology--Pedesis--harvests the Web for metaphorical relations in which depression is embedded and extracts the relevant conceptual domains describing it. This information is used by human experts for the construction of a "depression lexicon". The lexicon is used to automatically evaluate the level of depression in texts or whether the text is dealing with depression as a topic. Tested on three corpora of questions addressed to a mental health site the system provides 9% improvement in prediction whether the question is dealing with depression. Tested on a corpus of Blogs, the system provides 84.2% correct classification rate (p&lt;.001) whether a post includes signs of depression. By comparing the system's prediction to the judgment of human experts we achieved an average 78% precision and 76% recall. Depression can be automatically screened in texts and the mental health system may benefit from this screening ability.
22925387	Health information seeking in the information society.
Health Info Libr J  2012Sep
This article is the second student contribution to the Dissertations into Practice feature. It reports on a study that investigated the everyday health information-seeking practices of a small group of the 'general public' and the implications for information-seeking theory and health information provision. The first student article, about the implementation of radio frequency identification (RFID) in a hospital library, was very different, and the two articles illustrate the broad spectrum of possible subjects for the Dissertations into Practice feature. This study was conducted in summer 2011 by Abir Mukherjee for his MSc dissertation in the Library and Information Sciences programme at City University London. Further information and copies of the full dissertation may be obtained from Abir Mukherjee or David Bawden. AM.
22139974	Diffusion-prepared fast imaging with steady-state free precession (DP-FISP): a rapid diffusion MRI technique at 7 T.
Magn Reson Med 20111202 2012Sep
Diffusion MRI is a useful imaging technique with many clinical applications. Many diffusion MRI studies have utilized echo-planar imaging (EPI) acquisition techniques. In this study, we have developed a rapid diffusion-prepared fast imaging with steady-state free precession MRI acquisition for a preclinical 7T scanner providing diffusion-weighted images in less than 500 ms and diffusion tensor imaging assessments in ∼1 min with minimal image artifacts in comparison with EPI. Phantom apparent diffusion coefficient (ADC) and fractional anisotropy (FA) assessments obtained from the diffusion-prepared fast imaging with steady-state free precession (DP-FISP) acquisition resulted in good agreement with EPI and spin echo diffusion methods. The mean apparent diffusion coefficient was 2.0 × 10(-3) mm(2) /s, 1.90 × 10(-3) mm(2) /s, and 1.97 × 10(-3) mm(2) /s for DP-FISP, diffusion-weighted spin echo, and diffusion-weighted EPI, respectively. The mean fractional anisotropy was 0.073, 0.072, and 0.070 for diffusion-prepared fast imaging with steady-state free precession, diffusion-weighted spin echo, and diffusion-weighted EPI, respectively. Initial in vivo studies show reasonable ADC values in a normal mouse brain and polycystic rat kidneys.
22743197	Automatic sleep staging using fMRI functional connectivity data.
Neuroimage 20120626 2012Oct15
Recent EEG-fMRI studies have shown that different stages of sleep are associated with changes in both brain activity and functional connectivity. These results raise the concern that lack of vigilance measures in resting state experiments may introduce confounds and contamination due to subjects falling asleep inside the scanner. In this study we present a method to perform automatic sleep staging using only fMRI functional connectivity data, thus providing vigilance information while circumventing the technical demands of simultaneous recording of EEG, the gold standard for sleep scoring. The features to classify are the linear correlation values between 20 cortical regions identified using independent component analysis and two regions in the bilateral thalamus. The method is based on the construction of binary support vector machine classifiers discriminating between all pairs of sleep stages and the subsequent combination of them into multiclass classifiers. Different multiclass schemes and kernels are explored. After parameter optimization through 5-fold cross validation we achieve accuracies over 0.8 in the binary problem with functional connectivities obtained for epochs as short as 60s. The multiclass classifier generalizes well to two independent datasets (accuracies over 0.8 in both sets) and can be efficiently applied to any dataset using a sliding window procedure. Modeling vigilance states in resting state analysis will avoid confounded inferences and facilitate the study of vigilance states themselves. We thus consider the method introduced in this study a novel and practical contribution for monitoring vigilance levels inside an MRI scanner without the need of extra recordings other than fMRI BOLD signals.
22925786	Consequences of "going digital" for pathology professionals - entering the cloud.
Stud Health Technol Inform  2012
New opportunities and the adoption of digital technologies will transform the way pathology professionals and services work. Many areas of our daily life as well as medical professions have experienced this change already which has resulted in a paradigm shift in many activities. Pathology is an image-based discipline, therefore, arrival of digital imaging into this domain promises major shift in our work and required mentality. Recognizing the physical and digital duality of the pathology workflow, we can prepare for the imminent increase of the digital component, synergize and enjoy its benefits. Development of a new generation of laboratory information systems along with seamless integration of digital imaging, decision-support, and knowledge databases will enable pathologists to work in a distributed environment. The paradigm of "cloud pathology" is proposed as an ultimate vision of digital pathology workstations plugged into the integrated multidisciplinary patient care systems.
22925797	Adaptive clustering of image database (ACID) as an efficient tool for improving retrieval in a CBIR system.
Stud Health Technol Inform  2012
The paper describes a content-based image retrieval (CBIR) system with relevance feedback (RF). Instead of standard relevance feedback procedure, an adaptive clustering of image database (ACID) according to particular subjective needs is introduced in our system. Images labeled by the user as relevant are collected in clusters, and their representative members are used in further searching procedure instead of all images contained in the database. By this way, some history of previous retrieving is embedded into a searching process enabling faster and more subjective retrieval. Moreover, clusters are adaptively updated after each retrieving session, following actual user's needs. The efficiency of the proposed ACID system is tested with images from Corel and MIT datasets.
22925801	Emerging trends: grid technology in pathology.
Stud Health Technol Inform  2012
Grid technology has enabled clustering and access to, and interaction among, a wide variety of geographically distributed resources such as supercomputers, storage systems, data sources, instruments as well as special devices and services, realizing network-centric operations. Their main applications include large scale computational and data intensive problems in science and engineering. Grids are likely to have a deep impact on health related applications. Moreover, they seem to be suitable for tissue-based diagnosis. They offer a powerful tool to deal with current challenges in many biomedical domains involving complex anatomical and physiological modeling of structures from images or large image databases assembling and analysis. This chapter analyzes the general structures and functions of a Grid environment implemented for tissue-based diagnosis on digital images. Moreover, it presents a Grid middleware implemented by the authors for diagnostic pathology applications. The chapter is a review of the work done as part of the European COST project EUROTELEPATH.
22583488	3DMolNavi: a web-based retrieval and navigation tool for flexible molecular shape comparison.
BMC Bioinformatics 20120514 2012
Many molecules of interest are flexible and undergo significant shape deformation as part of their function, but most existing methods of molecular shape comparison treat them as rigid shapes, which may lead to incorrect measure of the shape similarity of flexible molecules. Currently, there still is a limited effort in retrieval and navigation for flexible molecular shape comparison, which would improve data retrieval by helping users locate the desirable molecule in a convenient way. To address this issue, we develop a web-based retrieval and navigation tool, named 3DMolNavi, for flexible molecular shape comparison. This tool is based on the histogram of Inner Distance Shape Signature (IDSS) for fast retrieving molecules that are similar to a query molecule, and uses dimensionality reduction to navigate the retrieved results in 2D and 3D spaces. We tested 3DMolNavi in the Database of Macromolecular Movements (MolMovDB) and CATH. Compared to other shape descriptors, it achieves good performance and retrieval results for different classes of flexible molecules. The advantages of 3DMolNavi, over other existing softwares, are to integrate retrieval for flexible molecular shape comparison and enhance navigation for user's interaction. 3DMolNavi can be accessed via https://engineering.purdue.edu/PRECISE/3dmolnavi/index.html.
22859022	Color code identification in coded structured light.
Appl Opt  2012Aug1
Color code is widely employed in coded structured light to reconstruct the three-dimensional shape of objects. Before determining the correspondence, a very important step is to identify the color code. Until now, the lack of an effective evaluation standard has hindered the progress in this unsupervised classification. In this paper, we propose a framework based on the benchmark to explore the new frontier. Two basic facets of the color code identification are discussed, including color feature selection and clustering algorithm design. First, we adopt analysis methods to evaluate the performance of different color features, and the order of these color features in the discriminating power is concluded after a large number of experiments. Second, in order to overcome the drawback of K-means, a decision-directed method is introduced to find the initial centroids. Quantitative comparisons affirm that our method is robust with high accuracy, and it can find or closely approach the global peak.
22326733	Comp Plan: A computer program to generate dose and radiobiological metrics from dose-volume histogram files.
Med Dosim 20120210 2012Autumn
Treatment planning studies often require the calculation of a large number of dose and radiobiological metrics. To streamline these calculations, a computer program called Comp Plan was developed using MATLAB. Comp Plan calculates common metrics, including equivalent uniform dose, tumor control probability, and normal tissue complication probability from dose-volume histogram data. The dose and radiobiological metrics can be calculated for the original data or for an adjusted fraction size using the linear quadratic model. A homogeneous boost dose can be added to a given structure if desired. The final output is written to an Excel file in a format convenient for further statistical analysis. Comp Plan was verified by independent calculations. A lung treatment planning study comparing 45 plans for 7 structures using up to 6 metrics for each structure was successfully analyzed within approximately 5 minutes with Comp Plan. The code is freely available from the authors on request.
22860048	Robustness and information propagation in attractors of Random Boolean Networks.
PLoS ONE 20120730 2012
Attractors represent the long-term behaviors of Random Boolean Networks. We study how the amount of information propagated between the nodes when on an attractor, as quantified by the average pairwise mutual information (I(A)), relates to the robustness of the attractor to perturbations (R(A)). We find that the dynamical regime of the network affects the relationship between I(A) and R(A). In the ordered and chaotic regimes, I(A) is anti-correlated with R(A), implying that attractors that are highly robust to perturbations have necessarily limited information propagation. Between order and chaos (for so-called "critical" networks) these quantities are uncorrelated. Finite size effects cause this behavior to be visible for a range of networks, from having a sensitivity of 1 to the point where I(A) is maximized. In this region, the two quantities are weakly correlated and attractors can be almost arbitrarily robust to perturbations without restricting the propagation of information in the network.
22868778	System architecture for intraoperative ultrasound registration in image-based medical navigation.
Biomed Tech (Berl)  2012Aug
Medical navigation systems for orthopedic surgery are becoming more and more important with the increasing proportion of older people in the population, and hence the increasing incidence of diseases of the musculoskeletal system. The central problem for such systems is the exact transformation of the preoperatively acquired datasets to the coordinate system of the patient's body, which is crucial for the accuracy of navigation. Our approach, based on the use of intraoperative ultrasound for image registration, is capable of robustly registering bone structures for different applications, e.g., at the spine or the knee. Nevertheless, this new procedure demands additional steps of preparation of preoperative data. To increase the clinical acceptance of this procedure, it is useful to automate most of the data processing steps. In this article, we present the architecture of our system with focus on the automation of the data processing steps. In terms of accuracy, a mean target registration error of 0.68 mm was achieved for automatically segmented and registered phantom data where the reference transformation was obtained by performing point-based registration using artificial structures. As the overall accuracy for subject data cannot be determined non-invasively, automatic segmentation and registration were judged by visual inspection and precision, which showed a promising result of 1.76 mm standard deviation for 100 registration trials based on automatic segmentation of magnetic resonance imaging data of the spine.
22783909	Uncovering text mining: a survey of current work on web-based epidemic intelligence.
Glob Public Health 20120711 2012
Real world pandemics such as SARS 2002 as well as popular fiction like the movie Contagion graphically depict the health threat of a global pandemic and the key role of epidemic intelligence (EI). While EI relies heavily on established indicator sources a new class of methods based on event alerting from unstructured digital Internet media is rapidly becoming acknowledged within the public health community. At the heart of automated information gathering systems is a technology called text mining. My contribution here is to provide an overview of the role that text mining technology plays in detecting epidemics and to synthesise my existing research on the BioCaster project.
22711793	Identifying aberrant pathways through integrated analysis of knowledge in pharmacogenomics.
Bioinformatics 20120617 2012Aug15
Many complex diseases are the result of abnormal pathway functions instead of single abnormalities. Disease diagnosis and intervention strategies must target these pathways while minimizing the interference with normal physiological processes. Large-scale identification of disease pathways and chemicals that may be used to perturb them requires the integration of information about drugs, genes, diseases and pathways. This information is currently distributed over several pharmacogenomics databases. An integrated analysis of the information in these databases can reveal disease pathways and facilitate novel biomedical analyses. We demonstrate how to integrate pharmacogenomics databases through integration of the biomedical ontologies that are used as meta-data in these databases. The additional background knowledge in these ontologies can then be used to enable novel analyses. We identify disease pathways using a novel multi-ontology enrichment analysis over the Human Disease Ontology, and we identify significant associations between chemicals and pathways using an enrichment analysis over a chemical ontology. The drug-pathway and disease-pathway associations are a valuable resource for research in disease and drug mechanisms and can be used to improve computational drug repurposing. http://pharmgkb-owl.googlecode.com
22743224	GO-Elite: a flexible solution for pathway and ontology over-representation.
Bioinformatics 20120627 2012Aug15
We introduce GO-Elite, a flexible and powerful pathway analysis tool for a wide array of species, identifiers (IDs), pathways, ontologies and gene sets. In addition to the Gene Ontology (GO), GO-Elite allows the user to perform over-representation analysis on any structured ontology annotations, pathway database or biological IDs (e.g. gene, protein or metabolite). GO-Elite exploits the structured nature of biological ontologies to report a minimal set of non-overlapping terms. The results can be visualized on WikiPathways or as networks. Built-in support is provided for over 60 species and 50 ID systems, covering gene, disease and phenotype ontologies, multiple pathway databases, biomarkers, and transcription factor and microRNA targets. GO-Elite is available as a web interface, GenMAPP-CS plugin and as a cross-platform application. http://www.genmapp.org/go_elite
22753780	AutoBind: automatic extraction of protein-ligand-binding affinity data from biological literature.
Bioinformatics 20120702 2012Aug15
Determination of the binding affinity of a protein-ligand complex is important to quantitatively specify whether a particular small molecule will bind to the target protein. Besides, collection of comprehensive datasets for protein-ligand complexes and their corresponding binding affinities is crucial in developing accurate scoring functions for the prediction of the binding affinities of previously unknown protein-ligand complexes. In the past decades, several databases of protein-ligand-binding affinities have been created via visual extraction from literature. However, such approaches are time-consuming and most of these databases are updated only a few times per year. Hence, there is an immediate demand for an automatic extraction method with high precision for binding affinity collection. We have created a new database of protein-ligand-binding affinity data, AutoBind, based on automatic information retrieval. We first compiled a collection of 1586 articles where the binding affinities have been marked manually. Based on this annotated collection, we designed four sentence patterns that are used to scan full-text articles as well as a scoring function to rank the sentences that match our patterns. The proposed sentence patterns can effectively identify the binding affinities in full-text articles. Our assessment shows that AutoBind achieved 84.22% precision and 79.07% recall on the testing corpus. Currently, 13 616 protein-ligand complexes and the corresponding binding affinities have been deposited in AutoBind from 17 221 articles. AutoBind is automatically updated on a monthly basis, and it is freely available at http://autobind.csie.ncku.edu.tw/ and http://autobind.mc.ntu.edu.tw/. All of the deposited binding affinities have been refined and approved manually before being released.
22873933	Why do nursing homes close? An analysis of newspaper articles.
Soc Work Public Health  2012
Using Non-numerical Unstructured Data Indexing Searching and Theorizing (NUD'IST) software to extract and examine keywords from text, the authors explored the phenomenon of nursing home closure through an analysis of 30 major-market newspapers over a period of 66 months (January 1, 1999 to June 1, 2005). Newspaper articles typically represent a careful analysis of staff impressions via interviews, managerial perspectives, and financial records review. There is a current reliance on the synthesis of information from large regulatory databases such as the Online Survey Certification And Reporting database, the California Office of Statewide Healthcare Planning and Development database, and Area Resource Files. Although such databases permit the construction of studies capable of revealing some reasons for nursing home closure, they are hampered by the confines of the data entered. Using our analysis of newspaper articles, the authors are able to add further to their understanding of nursing home closures.
22874149	Migration path for structured documentation systems including standardized medical device data.
Stud Health Technol Inform  2012
A standardized end-to-end solution has been implemented with the aim of supporting the semantic integration of clinical content in institution spanning applications. The approach outlined is a proof-of-concept design. It has shown that the standards chosen are suitable to integrate device data into forms, to document the results consistently and finally enable semantic interoperability. In detail the implementation includes a standardized device interface, a standardized representation of data entry forms and enables the communication of structured data via HL7 CDA. Because the proposed method applies a combination of standards semantic interoperability and the possibility of a contextual interpretation at each stage can be ensured.
22874150	Quality requirements for EHR archetypes.
Stud Health Technol Inform  2012
The realisation of semantic interoperability, in which any EHR data may be communicated between heterogeneous systems and fully understood by computers as well as people on receipt, is a challenging goal. Despite the use of standardised generic models for the EHR and standard terminology systems, too much optionality and variability exists in how particular clinical entries may be represented. Clinical archetypes provide a means of defining how generic models should be shaped and bound to terminology for specific kinds of clinical data. However, these will only contribute to semantic interoperability if libraries of archetypes can be built up consistently. This requires the establishment of design principles, editorial and governance policies, and further research to develop ways for archetype authors to structure clinical data and to use terminology consistently. Drawing on several years of work within communities of practice developing archetypes and implementing systems from them, this paper presents quality requirements for the development of archetypes. Clinical engagement on a wide scale is also needed to help grow libraries of good quality archetypes that can be certified. Vendor and eHealth programme engagement is needed to validate such archetypes and achieve safe, meaningful exchange of EHR data between systems.
22874151	Using archetypes for defining CDA templates.
Stud Health Technol Inform  2012
While HL7 CDA is a widely adopted standard for the documentation of clinical information, the archetype approach proposed by CEN/ISO 13606 and openEHR is gaining recognition as a means of describing domain models and medical knowledge. This paper describes our efforts in combining both standards. Using archetypes as an alternative for defining CDA templates permit new possibilities all based on the formal nature of archetypes and their ability to merge into the same artifact medical knowledge and technical requirements for semantic interoperability of electronic health records. We describe the process followed for the normalization of existing legacy data in a hospital environment, from the importation of the HL7 CDA model into an archetype editor, the definition of CDA archetypes and the application of those archetypes to obtain normalized CDA data instances.
22874154	An information artifact ontology perspective on data collections and associated representational artifacts.
Stud Health Technol Inform  2012
Biomedical data collections are typically compiled on the basis of assessment instruments and associated terminologies and their data structure explained by means of data dictionaries. The Information Artifact Ontology (IAO) is an attempt to give a realism-based account of the essence of information entities and how components of such entities relate to each other and to that what they are information about. Changes in the taxonomy and the definitions of the IAO, most importantly the addition of the terms 'representational artifact' and 'representational unit', are proposed to make the IAO a useful tool to clarify formally the distinctions and commonalities between data collections and associated artifacts that are compiled independently from each other, yet cover the same domain.
22874156	Formal specification of an ontology-based service for EHR interoperability.
Stud Health Technol Inform  2012
The objective of this paper is to describe by a Platform Independent Model, the formal specification of an ontology-based service for electronic health records interoperability. The GCM is used as a framework for the service's architectural design. The formal specification of the service is an extension of the OMG CTS 2 specification. A review of mapping approaches is also provided. The paper describes the service' information and computation models, including the mapping process workflow. The platform specific implementation (Platform Specific Model) is provided as a set of WSDL interfaces. The specification includes ontology mapping algorithms and tools needed.
22874157	Mapping SNOMED CT to ICD-10.
Stud Health Technol Inform  2012
A collaboration between the International Health Terminology Standards Development Organisation (IHTSDO®) and the World Health Organization (WHO) has resulted in a priority set of cross maps from SNOMED CT® to ICD-10® to support the epidemiological, statistical and administrative reporting needs of the IHTSDO member countries, WHO Collaborating Centres, and other interested parties. Overseen by the Joint Advisory Group (JAG), approximately 20,000 SNOMED CT concepts have been mapped to ICD-10 using a stand-alone mapping tool. The IHTSDO Map Special Interest Group (MapSIG) developed the mapping heuristics and established the validation process in conjunction with the JAG. Mapping team personnel were selected and then required to participate in a training session using the heuristics and tool. Quality metrics were used to assess the training program. An independent validation of cross map content was conducted under the supervision of the American Health Information Management Association. Lessons learned are being incorporated into the plans to complete the mapping of the remaining SNOMED CT concepts to ICD-10.
22874172	IRMA Code II: unique annotation of medical images for access and retrieval.
Stud Health Technol Inform  2012
Content-based image retrieval (CBIR) provides novel options to access large repositories of medical images, in particular for storing, querying and reporting. This requires a revisit of nomenclatures for image classification such as DICOM, SNOMED, and RadLex. For instance, DICOM defines only about 20 concept terms for body regions, which partly overlap. This is insufficient to access the visual image characteristics. In 2002, the Image Retrieval in Medical Applications (IRMA) project proposed a mono-hierarchic, multi-axial coding scheme called IRMA Code. It was used in the Cross Language Evaluation Forum (ImageCLEF) annotation tasks. Ten years of experience have discovered several weak points. In this paper, we propose eight axes of three levels in hierarchy for (A) anatomy, (B) biological system, (C) configuration, (D) direction, (E) equipment, (F) finding, (G) generation, and (H) human maneuver as well as additional flags for age class, body side, contrast agent, ethnicity, finding certainty, gender, quality, and scanned film, which are captured in form of another axis (I). Using a tag-based notation IRMA Code II supports multiple selection coding within one axis, which is required for the new main categories.
22874173	Method for mapping the French CCAM terminology to the UMLS metathesaurus.
Stud Health Technol Inform  2012
The French coding system of surgical procedures, the Classification Commune des Actes Médicaux (CCAM), is used in France for DRG databases and fee for services payment. Mapping between CCAM and other clinical procedures terminologies by the means of UMLS metathesaurus is essential in order to increase semantic interoperability between different healthcare terminologies and between different case mix systems. In a previous work the CISMeF team used an automatic approach to map CCAM descriptors to the French part of the UMLS metathesaurus. In another way for the French funded research project InterSTIS, we performed a mapping using MetaMap based on the top level semantic structure descriptors of anatomy and action of CCAM translated from French to English. This paper aims to present this new approach and to compare the results with the previous one. The combination of both approaches significantly improved the coverage of the mapping to 68 % for both descriptors and 95 % for at least one descriptor.
22874175	The issue of building generative terminologies for automatic medical data aggregation.
Stud Health Technol Inform  2012
This work addresses the problem of medical terminologies building. Starting from a set of elementary knowledge, a multi-hierarchical terminology is generated according to formal principles that are based on explicit classificatory points of view. The experimentation has been carried out on kidney diseases. Hierarchical conceptual structures are generated starting from Conceptual Graph based definition of kidney disease, a hierarchy of defining types and a set of organizing rules. The generated hierarchy includes new potential disease concepts with different level of likeliness. This "ready to use" generated multi-hierarchical terminology can be used for coding, includes new potential terms for the future and, due to the formalization of classificatory principles, offers improved opportunities of diseases instances aggregation. These features are looking very promising in the context of epidemiological observation tools (registries, long term cohort studies). They offer mean to meet two critical requirements: the statistical continuity in data analysis and the sustainable management of the underlying terminological system.
22874184	Development and representation of health indicators with thematic maps.
Stud Health Technol Inform  2012
Italian Local Health Care Agencies (ASLs) have the role of managing the public healthcare resources in their area of competence. To this end, the ASL of Pavia has implemented a data warehouse, which collects and integrates health data of more than 500,000 people since 2004. We have exploited such data repository to compute a variety of yearly health indicators, which have been represented on thematic maps of the area. Thanks to a Web-based application, the ASL decision-makers can monitor the area with a fine-grained spatial detail, dissecting the epidemiological, economical and pharmaceutical factors underlying citizens' health and patients' care. The implemented tool is currently up-and-running and has been evaluated with a usability questionnaire on a small number of users.
22874185	Efficient medical information retrieval in encrypted Electronic Health Records.
Stud Health Technol Inform  2012
The recent development of eHealth platforms across the world, whose main objective is to centralize patient's healthcare information to ensure the best continuity of care, requires the development of advanced tools and techniques for supporting health professionals in retrieving relevant information in this vast quantity of data. However, for preserving patient's privacy, some countries decided to de-identify and encrypt data contained in the shared Electronic Health Records, which reinforces the complexity of proposing efficient medical information retrieval approach. In this paper, we describe an original approach exploiting standards metadata as well as knowledge organizing systems to overcome the barriers of data encryption for improving the results of medical information retrieval in centralized and encrypted Electronic Health Records. This is done through the exploitation of semantic properties provided by knowledge organizing systems, which enable query expansion. Furthermore, we provide an overview of the approach together with illustrating examples and a discussion on the advantages and limitations of the provided framework.
22874192	A "meta"-perspective on "bit rot" of biomedical research data.
Stud Health Technol Inform  2012
Research data management (RDM) is an important topic for biomedical research due to the issue of "bit rot". RDM aims to implement access to reliable digital data for local and distributed research groups. A key aspect for the understanding of data is the use of metadata. This understanding has been investigated on the basis of two use cases of the DFG project LABIMI/F: RDM for genome data and biomedical image data. The results show that metadata can improve research not only for others but also for the researcher himself. However, RDM is still far from integrating all biomedical data. In addition, RDM is not (yet) a valid approach for clinical trial data management.
22874193	A semantic approach for digital long-term preservation of electronic health documents.
Stud Health Technol Inform  2012
Long-term preservation of electronic patient health information is a key issue for life-long electronic health records, however, it is poorly implemented in healthcare institutions and little attention is given to problems like obsolescence of formats and EHR applications or changing regulations, which jeopardize reusability of information after decades of preservation. We present in this paper an ontology driven approach to digital preservation and related metadata management which seems to be superior to conventional concepts of the digital library world.
22874194	Automated realtime data import for the i2b2 clinical data warehouse: introducing the HL7 ETL cell.
Stud Health Technol Inform  2012
Clinical data warehouses are used to consolidate all available clinical data from one or multiple organizations. They represent an important source for clinical research, quality management and controlling. Since its introduction, the data warehouse i2b2 gathered a large user base in the research community. Yet, little work has been done on the process of importing clinical data into data warehouses using existing standards. In this article, we present a novel approach of utilizing the clinical integration server as data source, commonly available in most hospitals. As information is transmitted through the integration server, the standardized HL7 message is immediately parsed and inserted into the data warehouse. Evaluation of import speeds suggest feasibility of the provided solution for real-time processing of HL7 messages. By using the presented approach of standardized data import, i2b2 can be used as a plug and play data warehouse, without the hurdle of customized import for every clinical information system or electronic medical record. The provided solution is available for download at http://sourceforge.net/projects/histream/.
22874197	TEDIS, Pervasive Developmental Disorder' patients information system, preliminary results.
Stud Health Technol Inform  2012
TEDIS, an information system dedicated to patients with Pervasive Developmental Disorder (PDD) was tested. Results focused on the process of behavioural changes among physicians and health professionals with regard to structured organized patient information.The experiment encouraged changes in professionals' habits for further documenting and systematizing patient information collection. TEDIS' project federated professionals for developing methods for a systematic and exhaustive patient data management, in a longitudinal and cross-domain perspective, for improving knowledge and health care management.
22874207	eHealth interoperability evaluation using a maturity model.
Stud Health Technol Inform  2012
To further improve individual health and well-being, access to high quality and safe services, eHealth interoperability is a fundamental prerequisite. A mature interoperability between health systems will support health services organization and delivery, and improve citizens' awareness of how to prevent disease and preserve good health. Within this context, health institutions have to solve interoperability problems or prevent them to appear, and if possible avoid them before they occur by adapting good practices toward interoperability. This paper proposes an evaluation of the potential health interoperability using the MMEI methodology (Maturity Model for Enterprise Interoperability). It discusses how the MMEI model can be used to help institutions to avoid interoperability problems. A use case for a particular hospital is more closely examined.
22874216	Using video observation to gain insight into complex clinical work practices.
Stud Health Technol Inform  2012
Experience shows that the precondition for development of successful health-information-technologies (HIT) is a thorough insight into clinical work practices. In contemporary clinical work practices, clinical work and health information technology are closely integrated. Research within Virtual Centre for Health Informatics at Aalborg University, Denmark have during recent years focused on video observation to supplementing traditional ethnographical research methods in providing insight into complex clinical work practices. The objective of this paper is to argue for the potentials of the video observation method to inform and to improve HIT development compared to traditional ethnographic methods. Based on several studies conducted within the healthcare sector, we find, that the video observation method is superior to other ethnographical research methods when it comes to rapidly disclosing the complexity in clinical sociomaterial work practices. We also find that the video techniques used in the healthcare context allows us to revisit the field of observation through the data, to broaden our initial focus and to share data with both the clinical staff involved and other researchers. Hence, it provides us a more in depth insight in the complex clinical sociomaterial work practices than when observing by the use of pen and paper.
22874233	Interoperability design of personal health information import service.
Stud Health Technol Inform  2012
Availability of personal health information for individual use from professional patient records is an important success factor for personal health information management (PHIM) solutions such as personal health records. In this paper we focus on this crucial part of personal wellbeing information management splutions and report the interoperability design of personal information import service. Key requirements as well as design factors for interfaces between PHRs and EPRs are discussed. Open standards, low implementation threshold and the acknowledgement of local market and conventions are emphasized in the design.
22874235	Accuracy of clinical data entry when using a computerized decision support system: a case study with OncoDoc2.
Stud Health Technol Inform  2012
Some studies suggest that the implementation of health information technology (HIT) introduces unpredicted and unintended consequences including e-iatrogenesis. OncoDoc2 is a guideline-based clinical decision support system (CDSS) applied to the management of breast cancer. The system is used by answering closed-ended questions in order to document patient data while navigating through the knowledge base until the best patient-specific recommended treatments are obtained. OncoDoc2 has been used by three hospitals in real clinical settings and for genuine patients. We analysed 394 navigations, recorded on a 10-month period, which correspond to 6,025 data entries. The data entry error rate is 4.2%, spread over 52% of incorrect navigations (N-). However, the overall compliance rate of clinical decisions with guidelines significantly increased from 72.8% (without CDSS) to 87.3% (with CDSS). Although this increase is lowered because of N- navigations (compliance rates are respectively 95% and 80% for N+ and N- navigations), the benefits of HIT outweighted its disadvantages in our study.
22874243	Establishing end-to-end security in a nationwide network for telecooperation.
Stud Health Technol Inform  2012
Telecooperation is used to support care for trauma patients by facilitating a mutual exchange of treatment and image data in use-cases such as emergency consultation, second-opinion, transfer, rehabilitation and out-patient aftertreatment. To comply with data protection legislation a two-factor authentication using ownership and knowledge has been implemented to assure personalized access rights. End-to-end security is achieved by symmetric encryption in combination with external trusted services which provide the symmetric key solely at runtime. Telecooperation partners may be chosen at departmental level but only individuals of that department, as a result of checking the organizational assignments maintained by LDAP services, are granted access. Data protection officers of a federal state have accepted the data protection means. The telecooperation platform is in routine operation and designed to serve for up to 800 trauma centers in Germany, organized in more than 50 trauma networks.
22874246	Key-linked on-line databases for clinical research.
Stud Health Technol Inform  2012
Separating patient identification data from clinical data and/or information about biomaterial samples is an effective data protection measure, especially in clinical research employing "on-line", i.e., web-based, data capture. In this paper, we show that this specialised technique can be generalised into a network architecture of interconnected on-line databases potentially serving a variety of purposes. The basic idea of this approach consists of maintaining logical links, i.e., common record keys, between corresponding data structures in pairs of databases while keeping the actual key values hidden from clients. For client systems, simultaneous access to corresponding records is mediated by temporary access tokens. At the relational level, these links are represented by arbitrary unique record keys common to both databases. This architecture allows for integration of related data in different databases without replicating or permanently sharing this data in one place. Each participating on-line database can determine the degree of integration by specifying linkage keys only for those data structures that may be logically connected to other data. Logical links can de designed for specific use cases. In addition, each database controls user access by enforcing its own authorisation scheme. Another advantage is that individual database owners retain considerable leeway in adapting to changing local requirements without compromising the integration into the network. Beyond protecting individual subject identification data, this architecture permits splitting a cooperatively used data pool to achieve many kinds of objectives. Application examples could be clinical registries needing subject contact information for follow-up, biomaterial banks with or without genetic information, and automatic or assisted integration of data from electronic medical records into research data.
22874248	The Electronic Healthcare Record for Clinical Research (EHR4CR) information model and terminology.
Stud Health Technol Inform  2012
A major barrier to repurposing routinely collected data for clinical research is the heterogeneity of healthcare information systems. Electronic Healthcare Record for Clinical Research (EHR4CR) is a European platform designed to improve the efficiency of conducting clinical trials. In this paper, we propose an initial architecture of the EHR4CR Semantic Interoperability Framework. We used a model-driven engineering approach to build a reference HL7-based multidimensional model bound to a set of reference clinical terminologies acting as a global as view model. We then conducted an evaluation of its expressiveness for patient eligibility. The EHR4CR information model consists in one fact table dedicated to clinical statement and 4 dimensions. The EHR4CR terminology integrates reference terminologies used in patient care (e.g LOINC, ICD-10, SNOMED CT, etc). We used the Object Constraint Language (OCL) to represent patterns of eligibility criteria as constraints on the EHR4CR model to be further transformed in SQL statements executed on different clinical data warehouses.
22874249	A DICOM architecture for clinicians and researchers.
Stud Health Technol Inform  2012
Over the last years there has been a strong trend of publishing health data in anonymized format in order to make it available for research. This is also true for medical imaging where the DICOM standard is the predominant data format and network protocol. This paper proposes an extension to any DICOM networking infrastructure that permits sharing of medical images in an anonymized way. Standard DICOM software is utilized on client and server side. While offering researchers access to all images in anonymous format, the architecture enables authorized clinicians to access the same images including their original patient information (name, institution, etc.). Identifying parts and anonymous parts of the image data are stored to geologically different databases. Together with sophisticated network protocols, patient privacy is fully preserved.
22874253	Designing and implementing a biobanking IT framework for multiple research scenarios.
Stud Health Technol Inform  2012
This paper presents a biobanking IT framework, comprising a set of integrated biobanking information technology components. It provides adaptable and scalable IT support for varying biobanking scenarios, workflows and projects, while avoiding redundancy in data and technology. Feasibility of this approach is illustrated by implementations for four different biobanking projects at Erlangen University Hospital and with cooperating partners in Münster and Lübeck.
22874254	Interoperability in clinical research: from metadata registries to semantically annotated CDISC ODM.
Stud Health Technol Inform  2012
Planning case report forms for data capture in clinical trials is a labor-insensitive and not formalized process. These CRFs are often neither standardized nor using defined data elements. Metadata registries as the NCI caDSR provide the capability to create forms based on common data elements. However, an exchange of these forms into clinical trial management systems through a standardized format like CDISC ODM is currently not offered. Thus, our objectives were to develop a mapping model between NCI forms and ODM. We analyzed 3012 NCI forms and included common data elements regarding their frequency and uniqueness. In this paper, we have created a mapping model between both formats and identified limitations in the conversion process: Semantic codes requested from the caDSR registry did not allow a proper mapping to ODM items and information like the number of module repetitions got lost. Summarized, it can be stated that our mapping model is feasible. However, mapping of semantic concepts in ODM needs to be specified more precisely.
22874255	Requirements for semantic biobanks.
Stud Health Technol Inform  2012
World-wide availability of biobank samples is a great desideratum for biomedical researchers. We describe the use case of biobank information retrieval that requires the semantic descriptions of biobank samples and of clinical information. In addition we sketch the foundations of an ontology for biobanks, as a basis on which distributed biobank indexing and retrieval systems can be built. We advocate that a detailed and robust representation of this kind of information improves and allows complex queries that will certainly arise to explore the full potential of biobanks.
22874256	A semantic model for multimodal data mining in healthcare information systems.
Stud Health Technol Inform  2012
Electronic health records (EHRs) are representative examples of multimodal/multisource data collections; including measurements, images and free texts. The diversity of such information sources and the increasing amounts of medical data produced by healthcare institutes annually, pose significant challenges in data mining. In this paper we present a novel semantic model that describes knowledge extracted from the lowest-level of a data mining process, where information is represented by multiple features i.e. measurements or numerical descriptors extracted from measurements, images, texts or other medical data, forming multidimensional feature spaces. Knowledge collected by manual annotation or extracted by unsupervised data mining from one or more feature spaces is modeled through generalized qualitative spatial semantics. This model enables a unified representation of knowledge across multimodal data repositories. It contributes to bridging the semantic gap, by enabling direct links between low-level features and higher-level concepts e.g. describing body parts, anatomies and pathological findings. The proposed model has been developed in web ontology language based on description logics (OWL-DL) and can be applied to a variety of data mining tasks in medical informatics. It utility is demonstrated for automatic annotation of medical data.
22874263	Reusable data in public health data-bases-problems encountered in Danish Children's Database.
Stud Health Technol Inform  2012
Denmark have unique health informatics databases e.g. "The Children's Database", which since 2009 holds data on all Danish children from birth until 17 years of age. In the current set-up a number of potential sources of errors exist - both technical and human-which means that the data is flawed. This gives rise to erroneous statistics and makes the data unsuitable for research purposes. In order to make the data usable, it is necessary to develop new methods for validating the data generation process at the municipal/regional/national level. In the present ongoing research project, two research areas are combined: Public Health Informatics and Computer Science, and both ethnographic as well as system engineering research methods are used. The project is expected to generate new generic methods and knowledge about electronic data collection and transmission in different social contexts and by different social groups and thus to be of international importance, since this is sparsely documented in the Public Health Informatics perspective. This paper presents the preliminary results, which indicate that health information technology used ought to be subject for redesign, where a thorough insight into the work practices should be point of departure.
22874265	Standardizing intensive care device data to enable secondary usages.
Stud Health Technol Inform  2012
To represent medical device observations in a format that is consumable by clinical software, standards like HL7v3 and ISO/IEEE 11073 should be used jointly. This is demonstrated in a project with Dräger Medical GmbH focusing on their Patient Data Management System (PDMS) in intensive care, called Integrated Care Manager (ICM). Patient and device data of interest should be mapped to suitable formats to enable data exchange and decision support. Instead of mapping device data to target formats bilaterally we use a generic HL7v3 Refined Message Information Model (RMIM) with device specific parts adapted to ISO/IEEE 11073 DIM. The generality of the underlying model (based on Yuksel et al. [1]) allows the flexible inclusion of IEEE 11073 conformant device models of interest on the one hand and the generation of needed artifacts for secondary usages on the other hand, e.g. HL7 V2 messages, HL7 CDA documents like the Personal Health Monitoring Report (PHMR) or web services. Hence, once the medical device data are obtained in the RMIM format, it can quite easily be transformed into HL7-based standard interfaces through XSL transformations because these interfaces all have their building blocks from the same RIM. From there data can be accessed uniformly, e.g. as needed by Dräger´s decision support system SmartCare [2] for automated control and optimization of weaning from mechanical ventilation.
22874266	Why is clinical information not reused?
Stud Health Technol Inform  2012
Reuse of clinical information plays a key role in the vision of a health sector with comprehensive semantic and pragmatic interoperability. Several papers have dealt with the secondary reuse of information, e.g. for statistics or research, while the primary reuse - clinical information reused in a clinical setting - has received less attention. On the basis of a qualitative literature review, this paper creates a categorised overview of the different causes to refrain from reuse of clinical information in clinical settings. The categorisation contributes to a greater understanding of failing reuse of clinical information in clinical settings, and it can probably be used in designing, evaluating and optimising clinical information systems. Further, it is speculated that the categorisation can be used in the process of identifying the concepts that constitute the context of clinical information.
22874268	Archetype based search in an IHE XDS environment.
Stud Health Technol Inform  2012
To prevent information overload of physicians when accessing EHRs we introduce a method to extend the IHE XDS profile metadata-based search towards a content-based search. Detailed queries are created based on predefined information needs mapped to ISO/EN 13606 Archetypes. They are aggregated to a metadata-based query to retrieve all relevant documents, which are then analyzed for the desired contents. The results are presented in a tabular form. The content-based search in IHE-XDS could be implemented efficiently and was found helpful by the evaluating physicians.
22874269	Concepts for a personal health record.
Stud Health Technol Inform  2012
Healthcare is about information. It is usually assumed that personal health information exists primarily for professional's use but well informed patients motivate better informed professionals. A longitudinal health record containing a patient's medical history has been the holy grail of healthcare. Personal Electronic Health Records (P-EHR) hold the potential to transform healthcare by providing a complete set of patient managed information. We present a portable P-EHR's functionalities from the patient's perspective.
22874270	Enhancing the many-to-many relations across IHE document sharing communities.
Stud Health Technol Inform  2012
The Integrating Healthcare Enterprise (IHE) initiative is an ongoing project aiming to enable true inter-site interoperability in the health IT field. IHE is a work in progress and many challenges need to be overcome before the healthcare Institutions may share patient clinical records transparently and effortless. Configuring, deploying and testing an IHE document sharing community requires a significant effort to plan and maintain the supporting IT infrastructure. With the new paradigm of cloud computing is now possible to launch software devices on demand and paying accordantly to the usage. This paper presents a framework designed with purpose of expediting the creation of IHE document sharing communities. It provides semi-ready templates of sharing communities that will be customized according the community needs. The framework is a meeting point of the healthcare institutions, creating a favourable environment that might converge in new inter-institutional professional relationships and eventually the creation of new Affinity Domains.
22874271	Standardized EHR interoperability - preliminary results of a German pilot project using the archetype methodology.
Stud Health Technol Inform  2012
The mobility of doctors and patients asks for multilingualism of electronic health record (EHR) systems: Doctors might face language problems using foreign medical information systems; people working abroad ask for continuous care which requires the treating physician to consult and understand the patient's health record. To address these linguistic and interoperability issues a solution is being developed that is based on widely acclaimed standards. Medical concepts that are derived from ASTM CCR define an interface model (based on ISO 13606). A server manages the data exchange between heterogeneous systems based on the interface model. It provides web services (automatic) and web forms (manual) and performs a transformation from the legacy scheme to the common structure. Furthermore, the server provides rich visualization capabilities (e.g. language-switch, custom charts etc.) which are useful for those EHR systems that don't provide these features.
22874272	The FBE development project: toward flexible electronic standards-based bio-psycho-social individual records.
Stud Health Technol Inform  2012
Under the ARCHITRAVE programme aimed at redesigning the regional health and social information system, the alpha version of a new web application was developed using the International Classification of Functioning, Disability and Health (ICF) and other medical terminology systems as a basis for a flexible electronic standards-based bio-psycho-social record. The web application was developed in order to collect information according to a multiaxial assessment framework consistent with the model of functioning adopted by the ICF. The web application translates information collected in natural language into ICF and releases outputs at different stages of the assessment process useful in evaluating clinical and social outcomes, distinguishing between functioning and disability in the same functioning profile and planning reasonable adaptations to overcome disability. The alpha version works in Italian and was adapted to the Italian welfare system/services/policies, but an international version working in other languages/welfare systems can be designed. The first field trial is ongoing in the Friuli Venezia Giulia Region, implementing the regional Health and Social Action Plan 2010-2012.
22874276	Health data collecting and sharing: case studies of Czech e-health applications.
Stud Health Technol Inform  2012
The paper shows the importance of e-health applications for electronic healthcare development. It describes several e-health applications for health data collecting and sharing that are running in the Czech Republic. These are IZIP system, electronic health record MUDR and K4CARE project applications. The e3-health concept is considered as a tool for judging e-health applications in different healthcare settings.
22874277	Ontology-based reusable clinical document template production system.
Stud Health Technol Inform  2012
Clinical documents embody professional clinical knowledge. This paper shows an effective clinical document template (CDT) production system that uses a clinical description entity (CDE) model, a CDE ontology, and a knowledge management system called STEP that manages ontology-based clinical description entities. The ontology represents CDEs and their inter-relations, and the STEP system stores and manages CDE ontology-based information regarding CDTs. The system also provides Web Services interfaces for search and reasoning over clinical entities. The system was populated with entities and relations extracted from 35 CDTs that were used in admission, discharge, and progress reports, as well as those used in nursing and operation functions. A clinical document template editor is shown that uses STEP.
22874286	Organizing data quality assessment of shifting biomedical data.
Stud Health Technol Inform  2012
Low biomedical Data Quality (DQ) leads into poor decisions which may affect the care process or the result of evidence-based studies. Most of the current approaches for DQ leave unattended the shifting behaviour of data underlying concepts and its relation to DQ. There is also no agreement on a common set of DQ dimensions and how they interact and relate to these shifts. In this paper we propose an organization of biomedical DQ assessment based on these concepts, identifying characteristics and requirements which will facilitate future research. As a result, we define the Data Quality Vector compiling a unified set of DQ dimensions (completeness, consistency, duplicity, correctness, timeliness, spatial stability, contextualization, predictive value and reliability), as the foundations to the further development of DQ assessment algorithms and platforms.
22874289	The role of electronic checklists - case study on MRI-safety.
Stud Health Technol Inform  2012
Checklists can be used to improve and standardize safety critical processes and their communication. The introduction of potentially harmful medical technology and equipment has created additional requirements for the safe delivery of health care. We have studied the implementation of an electronic checklist to ensure the safety of patients scheduled for Magnetic Resonance Imaging examinations. Through a combination of observations and semi-structured interviews we investigated how health care workers in a Norwegian University hospital dealt with variations in checklist compliance, missing and lack of information. The checklist provided different functionality for the different users, ranging from a memory/attention support to a standardized form of communication on safety matters. However, the rigidity afforded by the electronic implementation, showed some serious drawbacks over the prior, simpler, paper-based versions.
22874292	Recognition and privacy preservation of paper-based health records.
Stud Health Technol Inform  2012
While the digitization of medical data within electronic health records has been introduced in some areas, massive amounts of paper-based health records are still produced on a daily basis. This data has to be stored for decades due to legal reasons but is of no benefit for research organizations, as the unstructured medical data in paper-based health records cannot be efficiently used for clinical studies. This paper presents a system for the recognition and privacy preservation of personal data in paper-based health records with the aim to provide clinical studies with medical data gained from existing paper-based health records.
22874293	Towards human-centric visual access control for clinical data management.
Stud Health Technol Inform  2012
We propose a novel human-centric, visual, and context-aware access control (AC) system for distributed clinical data management and health information systems. Human-centricity in this context means that medical staff should be able to configure AC rules, both in a timesaving and reliable manner. Since medical data often includes (meta-) information about a patient, it is essential that an AC system includes the patient into the AC process. To cater for the strong security needs in the medical domain, both the AC policy creation by medical staff as well as the patient-interaction feature need to be taken into account. While traditional AC systems offer sufficient security in theory, they lack in comfort and flexibility and as a result find no widespread acceptance with non tech-savvy users. Distributed medical institutions could enormously benefit from the opportunity of dynamic AC configuration at an end-user level while adhering to legal, ethical or other privacy requirements. Hence, this paper presents a human-centric visual AC model for medical data, addressing usability, information security and patient interaction.
22874294	Watermarking as a traceability standard.
Stud Health Technol Inform  2012
The exponential increase in the number of electronic document exchanges in healthcare has considerably increased the risk of document drop-out or address errors. It may therefore be important to know to whom the information belongs and who produced it. This becomes a major concern when the document has been involved in processes leading to the choice of therapy and eventually in cases where patients seek damages for medical malpractice. Watermarking, which is the embedding of security elements, such as a digital signature, within a document, can help to ensure that a digital document is reliable. However, at the same time, questions arise about the validity of watermarking-based evidence. In this paper, beyond the technical aspects, we discuss the worldwide legal acceptability of watermarking and the need for its recognition as a standard according to technical characteristics that the CEN and ISO need to agree on.
22874296	Identifying types and causes of errors in mortality data in a clinical registry using multiple information systems.
Stud Health Technol Inform  2012
Errors may occur in the registration of in-hospital mortality, making it less reliable as a quality indicator. We assessed the types of errors made in in-hospital mortality registration in the clinical quality registry National Intensive Care Evaluation (NICE) by comparing its mortality data to data from a national insurance claims database. Subsequently, we performed site visits at eleven Intensive Care Units (ICUs) to investigate the number, types and causes of errors made in in-hospital mortality registration. A total of 255 errors were found in the NICE registry. Two different types of software malfunction accounted for almost 80% of the errors. The remaining 20% were five types of manual transcription errors and human failures to record outcome data. Clinical registries should be aware of the possible existence of errors in recorded outcome data and understand their causes. In order to prevent errors, we recommend to thoroughly verify the software that is used in the registration process.
22874308	Experiences in the creation of an electromyography database to help hand amputated persons.
Stud Health Technol Inform  2012
Currently, trans-radial amputees can only perform a few simple movements with prosthetic hands. This is mainly due to low control capabilities and the long training time that is required to learn controlling them with surface electromyography (sEMG). This is in contrast with recent advances in mechatronics, thanks to which mechanical hands have multiple degrees of freedom and in some cases force control. To help improve the situation, we are building the NinaPro (Non-Invasive Adaptive Prosthetics) database, a database of about 50 hand and wrist movements recorded from several healthy and currently very few amputated persons that will help the community to test and improve sEMG-based natural control systems for prosthetic hands. In this paper we describe the experimental experiences and practical aspects related to the data acquisition.
22874311	Online health information search: what struggles and empowers the users? Results of an online survey.
Stud Health Technol Inform  2012
The most popular mean of searching for online health content is a general search engine for all domains of interest. Being general implies on one hand that the search engine is not tailored to the needs which are particular to the medical and on another hand that health domain and health-specific queries may not always return adequate and adapted results. The aim of our study was to identify difficulties and preferences in online health information search encountered by members of the general public. The survey in four languages was online from the 9th of March until the 27th of April, 2011. 385 answers were collected, representing mostly the opinions of highly educated users, mostly from France and Spain. The most important characteristics of a search engine are relevance and trustworthiness of results. The results currently retrieved do not fulfil these requirements. The ideal representation of the information will be a categorization of the results into different groups. Medical dictionaries/thesauruses, suggested relevant topics, image searches and spelling corrections are regarded as helpful tools. There is a need to work towards better customized solutions which provide users with the trustworthy information of high quality specific to his/her case in a user-friendly environment which would eventually lead to making appropriate health decisions.
22874312	Finding online health-related information: usability issues of health portals.
Stud Health Technol Inform  2012
As Internet and computers become widespread, health portals offering online health-related information become more popular. The most important point for health portals is presenting reliable and valid information. Besides, portal needs to be usable to be able to serve information to users effectively. This study aims to determine usability issues emerging when health-related information is searched on a health portal. User-based usability tests are conducted and eye movement analyses are used in addition to traditional performance measures. Results revealed that users prefer systematic, simple and consistent designs offering interactive tools. Moreover, content and partitions needs to be shaped according to the medical knowledge of target users.
22874314	Tracking changes in search behaviour at a health web site.
Stud Health Technol Inform  2012
Nowadays, the internet is used as a means to provide the public with official information on many different topics, including health related matters and care providers. In this work we have studied a search log from the official Swedish health web site 1177.se for patterns of search behaviour over time. To improve the analysis, we mapped the queries to UMLS semantic types and MeSH categories. Our analysis shows that, as expected, diseases and health care activities are the ones of most interest, but also a clear increased interest in geographical locations in the setting of health care providers. We also note a change over time in which kinds of diseases are of interest. Finally, we conclude that this type of analysis may be useful in studies of what health related topics matter to the public, but also for design and follow-up of public information campaigns.
22874315	Trustworthiness and relevance in web-based clinical question answering.
Stud Health Technol Inform  2012
Question answering systems try to give precise answers to a user's question posed in natural language. It is of utmost importance that the answers returned are relevant to the user's question. For clinical QA, the trustworthiness of answers is another important issue. Limiting the document collection to certified websites helps to improve the trustworthiness of answers. On the other hand, limited document collections are known to harm the relevancy of answers. We show, however, in a comparative evaluation, that promoting trustworthiness has no negative effect on the relevance of the retrieved answers in our clinical QA system. On the contrary, the answers found are in general more relevant.
22874326	Proposal of an architecture for the national integration of Electronic Health Records: a semi-centralized approach.
Stud Health Technol Inform  2012
This paper proposes a novel architecture for the national integration of Electronic Health Records (EHRs), the semi-centralized approach, in which summarized EHRs are maintained centrally at a nation-wide system with references to their comprehensive versions at their original locations on the various healthcare providers' databases. The idea is to allow the clinicians to have an idea of what is included inside the patient's EHRs at each healthcare provider's database and to have a general view of the patient's medical history, and when needed to retrieve the complete EHR of the patient from a remote healthcare providers' systems. A high level system architecture needed to integrate EHRs from various sources on a nation-wide basis using the proposed semi-centralized approach is described. Best practices and essential requirements are the central to the evolution of the approach taken.
22874344	Flexible medical image management using service-oriented architecture.
Stud Health Technol Inform  2012
Management of medical images increasingly involves the need for integration with a variety of information systems. To address this need, we developed Content Management Offering (CMO), a platform for medical image management supporting interoperability through compliance with standards. CMO is based on the principles of service-oriented architecture, implemented with emphasis on three areas: clarity of business process definition, consolidation of service configuration management, and system scalability. Owing to the flexibility of this platform, a small team is able to accommodate requirements of customers varying in scale and in business needs. We describe two deployments of CMO, highlighting the platform's value to customers. CMO represents a flexible approach to medical image management, which can be applied to a variety of information technology challenges in healthcare and life sciences organizations.
22874347	Do worklists work? A quantitative approach to the assessment of an Electronic Patient Record function.
Stud Health Technol Inform  2012
In this study, a quantitative approach was used to assess an Electronic Patient Record worklist function introduced to prevent radiology reports from being overlooked by the responsible clinicians. The function reduced the rate of overlooked reports, but was not able to eliminate it. Not all reports were identified by the automatic worklist function. Clinicians did not use the worklists to detect new reports. Our results suggest that this was not the result of insufficient user training or user errors, but rather that the worklists function did not comply with the way clinical work was organised and performed. Quantitative methods as used in this study are suggested as supplementary to the traditional qualitative methods.
22874348	Log analysis to understand medical professionals' image searching behaviour.
Stud Health Technol Inform  2012
This paper reports on the analysis of the query logs of a visual medical information retrieval system that provides access to radiology resources. Our analysis shows that, despite sharing similarities with general Web search and also with biomedical text search, query formulation and query modification when searching for visual biomedical information have unique characteristics that need to be taken into account in order to enhance the effectiveness of the search support offered by such systems. Typical information needs of medical professionals searching radiology resources are also identified with the goal to create realistic search tasks for a medical image retrieval evaluation benchmark.
22874357	Development of Pain flowsheet based on electronic nursing record system.
Stud Health Technol Inform  2012
Pain assessment and control is the most important issue in medical fields. To manage patient pain effectively with high quality, we need a Pain flowsheet to share patient information of pain between staffs. Nurses already input more information into their electronic nursing record. We focus how to integrate each data and show appropriately without nurse's duplicated work. Finally we develop the Pain flowsheet based on nursing record, that includes the pattern of pain, influential factors, how to implement previously, what to do pain control. This study shows that well designed structured nursing records is an essential basement to advance medical process. In addition, we expect to develop more helpful functions through how to reorganize and combine each data.
22874360	INCA - Individual Nomad Clinical Assistant - supporting nurses with mobile devices.
Stud Health Technol Inform  2012
A completely structured nursing record has been deployed in the 8 hospitals of the University hospitals of Geneva. Even with laptops, the access to the records restrains nurses' mobility during their bedside work. It has lead to a strong demand for mobile devices. There are several papers showing that mobile computers can lead to increase time for data acquisition, increased errors and omissions. Thus, there are important challenges at developing these tools, while respecting the mobile paradigm and the needs for qualitative and efficient acquisition. A simple translation of user interfaces from usual computers is not recommended. After evaluating various user interfaces with users in real conditions, we propose a solution that eases the selection of patients, the navigation into the various screens, and provides a very clear list of tasks to achieve for nurses. The article exposes the difficulties to adapt an existing tool on mobile devices. Despite these difficulties, by organizing smartly the displayed information, we produced a tool with similar functionalities but better adapted to the user.
22874362	A system-theoretical, architecture-based approach to ontology management.
Stud Health Technol Inform  2012
Comprehensive interoperability between distributed eHealth/pHealth environments requires that the systems involved are based on a common architectural framework and share common knowledge. The paper deals with the representation of systems by related ontologies. Therefore, the architectural principles ruling the system design and the interrelations of its components also rule the design of those ontologies and their management as exemplified.
22874366	Representing clinical communication knowledge through database management system integration.
Stud Health Technol Inform  2012
Clinical communication failures are considered the leading cause of medical errors [1]. The complexity of the clinical culture and the significant variance in training and education levels form a challenge to enhancing communication within the clinical team. In order to improve communication, a comprehensive understanding of the overall communication process in health care is required. In an attempt to further understand clinical communication, we conducted a thorough methodology literature review to identify strengths and limitations of previous approaches [2]. Our research proposes a new data collection method to study the clinical communication activities among Intensive Care Unit (ICU) clinical teams with a primary focus on the attending physician. In this paper, we present the first ICU communication instrument, and, we introduce the use of database management system to aid in discovering patterns and associations within our ICU communications data repository.
22874367	Semantic enrichment of medical forms - semi-automated coding of ODM-elements via web services.
Stud Health Technol Inform  2012
Semantic interoperability is an unsolved problem which occurs while working with medical forms from different information systems or institutions. Standards like ODM or CDA assure structural homogenization but in order to compare elements from different data models it is necessary to use semantic concepts and codes on an item level of those structures. We developed and implemented a web-based tool which enables a domain expert to perform semi-automated coding of ODM-files. For each item it is possible to inquire web services which result in unique concept codes without leaving the context of the document. Although it was not feasible to perform a totally automated coding we have implemented a dialog based method to perform an efficient coding of all data elements in the context of the whole document. The proportion of codable items was comparable to results from previous studies.
22874368	Conducting requirements analyses for research using routinely collected health data: a model driven approach.
Stud Health Technol Inform  2012
Medical research increasingly requires the linkage of data from different sources. Conducting a requirements analysis for a new application is an established part of software engineering, but rarely reported in the biomedical literature; and no generic approaches have been published as to how to link heterogeneous health data. Literature review, followed by a consensus process to define how requirements for research, using, multiple data sources might be modeled. We have developed a requirements analysis: i-ScheDULEs - The first components of the modeling process are indexing and create a rich picture of the research study. Secondly, we developed a series of reference models of progressive complexity: Data flow diagrams (DFD) to define data requirements; unified modeling language (UML) use case diagrams to capture study specific and governance requirements; and finally, business process models, using business process modeling notation (BPMN). These requirements and their associated models should become part of research study protocols.
22874369	Classification of ischaemic episodes with ST/HR diagrams.
Stud Health Technol Inform  2012
Coronary artery disease is the developed world's premier cause of mortality and the most probable cause of myocardial ischaemia. More advanced diagnostic tests aside, in electrocardiogram (ECG) analysis it manifests itself as a ST segment deviation, targeted by both exercise ECG and ambulatory ECG. In ambulatory ECG, besides ischaemic ST segment deviation episodes there are also non-ischaemic heart rate related episodes which aggravate real ischaemia detection. We present methods to transform the features developed for the heart rate adjustment of ST segment depression in exercise ECG for use in ambulatory ECG. We use annotations provided by the Long-Term ST Database to plot the ST/HR diagrams and then estimate the overall and maximal slopes of the diagrams in the exercise and recovery phase for each ST segment deviation episode. We also estimate the angle at the extrema of the ST/HR diagrams. Statistical analysis shows that ischaemic ST segment deviation episodes have significantly steeper overall and maximal slopes than heart rate related episodes, which indicates the explored features' utility for distinguishing between the two types of episodes. This makes the proposed features very useful in automated ECG analysis.
22874372	Statistical disclosure limitation of health data based on Pk-anonymity.
Stud Health Technol Inform  2012
The Act for the Protection of Personal Information in Japan considers as personal information any quasi-identifier that may be used to obtain information that identifies individuals through comparisons with datasets. Studies using health records are not widely conducted because of the concern regarding the safety of anonymized health records. To increase the safety of such records, we used the Pk-anonymity method. In this method, attributes are probabilistically randomized and then reconstructions are performed on the basis of statistical information from perturbed data. Hence, it is expected to provide more precise statistics and more reliably preserve privacy than the traditional "k-anonymity" method. We anonymized health records, performed cross tabulation, and assessed the error rate using original data. This study shows that the Pk-anonymity method can be used to perform safety statistical disclosures with low error rates, even in small cases.
22874375	CARDIO-i2b2: integrating arrhythmogenic disease data in i2b2.
Stud Health Technol Inform  2012
The CARDIO-i2b2 project is an initiative to customize the i2b2 bioinformatics tool with the aim to integrate clinical and research data in order to support translational research in cardiology. In this work we describe the implementation and the customization of i2b2 to manage the data of arrhytmogenic disease patients collected at the Fondazione Salvatore Maugeri of Pavia in a joint project with the NYU Langone Medical Center (New York, USA). The i2b2 clinical research chart data warehouse is populated with the data obtained by the research database called TRIAD. The research infrastructure is extended by the development of new plug-ins for the i2b2 web client application able to properly select and export phenotypic data and to perform data analysis.
22874378	IT behind a platform for Translational Cancer Research - concept and objectives.
Stud Health Technol Inform  2012
The German Consortium for Translational Cancer Research (DKTK) and the Rhine-Main Translational Cancer Research Network (RM-TCRN) are designed to exploit large population cohorts of cancer patients for the purpose of bio-banking, clinical trials, and clinical cancer registration. Hence, the success of these platforms is heavily dependent on the close interlinking of clinical data from cancer patients, information from study registries, and data from bio-banking systems of different laboratories and scientific institutions. This article referring to the poster discusses the main challenges of the platforms from an information technology point of view, legal and data security issues, and outlines an integrative IT-concept concerning a decentralized, distributed search approach where data management and search is in compliance with existing legislative rules.
22874380	A clinical research analytics toolkit for cohort study.
Stud Health Technol Inform  2012
This paper presents a clinical informatics toolkit that can assist physicians to conduct cohort studies effectively and efficiently. The toolkit has three key features: 1) support of procedures defined in epidemiology, 2) recommendation of statistical methods in data analysis, and 3) automatic generation of research reports. On one hand, our system can help physicians control research quality by leveraging the integrated knowledge of epidemiology and medical statistics; on the other hand, it can improve productivity by reducing the complexities for physicians during their cohort studies.
22874388	Concise healthcare-associated infection reporting and benchmarking with minimal staff resources.
Stud Health Technol Inform  2012
We report on intelligent information technology tools that produce fully-automated surveillance reports of high precision for 12 intensive care units (ICUs) without relevant time expenditure of infection control or ICU staff. This is accomplished by MONI-ICU, a computerized system for automated identification and continuous monitoring of ICU-associated infections, which makes surveillance data readily accessible and presents them in easily perceptible reporting format.
22874396	ICF machine: a web-based system for collection of ICF data.
Stud Health Technol Inform  2012
The International Classification of Functioning, Disability and Health (ICF) is a WHO classification for health and health-related issues. In order to foster ICF application in information systems, we devised an implementation profile in ClaML (Classification Markup Language) that allows for representation of ICF subsets and we developed a web-based system for collecting ICF data based on from their ClaML representation. The implementation profile and the application have been tested on 17 subsets, which have been translated into ClaML and then submitted to the web application, to produce test documents.
22874397	'Onco alerts' to support acute oncology services.
Stud Health Technol Inform  2012
The National Chemotherapy Advisory Group report has recommended that all hospitals in the UK with an Emergency Department should establish an acute oncology (AO) service. Acute oncology, when implemented at a clinical network level, presents significant challenges for informatics, including the requirement for 'onco alerts' - automated notification of admission of potential cancer patients, whose diagnosis may not be recorded on the admitting hospital's IT systems. In this short paper we present a case study and describe an approach to supporting the development of AO services with cross-organisational information systems.
22874399	Study of Iranian breast cancer registration via established online system during 2011.
Stud Health Technol Inform  2012
The present paper reports the results of a project aimed at providing Iran with a an online registry for breast cancer patients. A new ad hoc system/software has been developed in order to collect patients' data from the first visit to follow-up visits, including tests and Herceptin based therapy. The system has been designed to address both healthcare personnel needs, but also decision makers' information needs. After having identified in the country the laboratories performing the specific tests and the pharmacies that distribute Hercepting, the developed software have been installed in order to build the operational network representing the input points for the register. In 8 months of usage the results are: • Total number of Patients: 2240 inhabitant • Total number of doctors have participated in the program: 229 persons • Number of provinces contributing to the project: 17 provinces out of 30.  Online registry of breast cancer is country's requirement because this system not only delivers services for laboratories, pharmacies and all physicians for their patients' follow up and monitoring, but also provides ability to access and analyze collected data for managers and experts.
22874402	Development of an electronic nursing records system based on information models and clinical practice guidelines.
Stud Health Technol Inform  2012
The purpose of this study was to test the feasibility of an electronic nursing records system for perinatal care that is based on information models and clinical practice guidelines in perinatal care. We first generated 799 nursing statements describing nursing assessment, diagnoses, interventions, and outcomes using the entities, attributes, and value sets of detailed clinical models for perinatal care that we developed in a previous study. We then extracted 506 detailed recommendations from clinical practice guidelines. Finally, we created sets of nursing statements to be used for nursing documentation by grouping nursing statements based on these detailed recommendations. A prototype electronic nursing records system providing nurses with detailed recommendations for nursing practice and sets of nursing statements based on the detailed recommendations to guide nursing documentation was developed and evaluated.
22874405	An example of a multi-professional process-oriented structured documentation bound to SNOMED CT.
Stud Health Technol Inform  2012
Structured code-based documentation, i.e. templates which restrain the user with predetermined terms/phrases bound to terminologies, offers opportunities for advanced types of retrieval and guides the user in multiple ways; to act in accordance with evidence, for decision support and to achieve adequate documentation for the condition in question. This type of documentation is especially appropriate in health care processes which are nearly the same every time. A template for documentation of the family planning process (abortion) was elaborated at a Swedish hospital. It uses both structured elements and free-text and covers all information needed in the process. The predetermined terms and phrases were bound to SNOMED CT concepts. After the template has been completed, it forms the basis for a customary free-text note in the record. The structured information is also stored in its original form and can be used for different kinds of advanced data retrieval. The documentation is completed during the visit and there is no need for additional secretarial work. The implementation has reduced the total time used for documentation, reporting and follow up and shows that process-oriented structured documentation bound to SNOMED CT improves the documentation, supports advanced retrieval of data and reduces resource utilization.
22127956	Coherent information structure in complex computation.
Theory Biosci. 20111130 2012Sep
We have recently presented a framework for the information dynamics of distributed computation that locally identifies the component operations of information storage, transfer, and modification. We have observed that while these component operations exist to some extent in all types of computation, complex computation is distinguished in having coherent structure in its local information dynamics profiles. In this article, we conjecture that coherent information structure is a defining feature of complex computation, particularly in biological systems or artificially evolved computation that solves human-understandable tasks. We present a methodology for studying coherent information structure, consisting of state-space diagrams of the local information dynamics and a measure of structure in these diagrams. The methodology identifies both clear and "hidden" coherent structure in complex computation, most notably reconciling conflicting interpretations of the complexity of the Elementary Cellular Automata rule 22.
22656866	iPixel: a visual content-based and semantic search engine for retrieving digitized mammograms by using collective intelligence.
Inform Health Soc Care 20120601 2012Sep
Nowadays, traditional search engines such as Google, Yahoo and Bing facilitate the retrieval of information in the format of images, but the results are not always useful for the users. This is mainly due to two problems: (1) the semantic keywords are not taken into consideration and (2) it is not always possible to establish a query using the image features. This issue has been covered in different domains in order to develop content-based image retrieval (CBIR) systems. The expert community has focussed their attention on the healthcare domain, where a lot of visual information for medical analysis is available. This paper provides a solution called iPixel Visual Search Engine, which involves semantics and content issues in order to search for digitized mammograms. iPixel offers the possibility of retrieving mammogram features using collective intelligence and implementing a CBIR algorithm. Our proposal compares not only features with similar semantic meaning, but also visual features. In this sense, the comparisons are made in different ways: by the number of regions per image, by maximum and minimum size of regions per image and by average intensity level of each region. iPixel Visual Search Engine supports the medical community in differential diagnoses related to the diseases of the breast. The iPixel Visual Search Engine has been validated by experts in the healthcare domain, such as radiologists, in addition to experts in digital image analysis.
22437075	Leveraging medical thesauri and physician feedback for improving medical literature retrieval for case queries.
J Am Med Inform Assoc 20120321 2012 Sep-Oct
This paper presents a study of methods for medical literature retrieval for case queries, in which the goal is to retrieve literature articles similar to a given patient case. In particular, it focuses on analyzing the performance of state-of-the-art general retrieval methods and improving them by the use of medical thesauri and physician feedback. The Kullback-Leibler divergence retrieval model with Dirichlet smoothing is used as the state-of-the-art general retrieval method. Pseudorelevance feedback and term weighing methods are proposed by leveraging MeSH and UMLS thesauri. Evaluation is performed on a test collection recently created for the ImageCLEF medical case retrieval challenge. Experimental results show that a well-tuned state-of-the-art general retrieval model achieves a mean average precision of 0.2754, but the performance can be improved by over 40% to 0.3980, through the proposed methods. The results over the ImageCLEF test collection, which is currently the best collection available for the task, are encouraging. There are, however, limitations due to small evaluation set size. The analysis shows that further refinement of the methods is necessary before they can be really useful in a clinical setting. Medical case-based literature retrieval is a critical search application that presents a number of unique challenges. This analysis shows that the state-of-the-art general retrieval models are reasonably good for the task, but the performance can be significantly improved by developing new task-specific retrieval models that incorporate medical thesauri and physician feedback.
22511018	Protecting count queries in study design.
J Am Med Inform Assoc 20120417 2012 Sep-Oct
Today's clinical research institutions provide tools for researchers to query their data warehouses for counts of patients. To protect patient privacy, counts are perturbed before reporting; this compromises their utility for increased privacy. The goal of this study is to extend current query answer systems to guarantee a quantifiable level of privacy and allow users to tailor perturbations to maximize the usefulness according to their needs. A perturbation mechanism was designed in which users are given options with respect to scale and direction of the perturbation. The mechanism translates the true count, user preferences, and a privacy level within administrator-specified bounds into a probability distribution from which the perturbed count is drawn. Users can significantly impact the scale and direction of the count perturbation and can receive more accurate final cohort estimates. Strong and semantically meaningful differential privacy is guaranteed, providing for a unified privacy accounting system that can support role-based trust levels. This study provides an open source web-enabled tool to investigate visually and numerically the interaction between system parameters, including required privacy level and user preference settings. Quantifying privacy allows system administrators to provide users with a privacy budget and to monitor its expenditure, enabling users to control the inevitable loss of utility. While current measures of privacy are conservative, this system can take advantage of future advances in privacy measurement. The system provides new ways of trading off privacy and utility that are not provided in current study design systems.
22879806	Improving information retrieval using Medical Subject Headings Concepts: a test case on rare and chronic diseases.
J Med Libr Assoc  2012Jul
As more scientific work is published, it is important to improve access to the biomedical literature. Since 2000, when Medical Subject Headings (MeSH) Concepts were introduced, the MeSH Thesaurus has been concept based. Nevertheless, information retrieval is still performed at the MeSH Descriptor or Supplementary Concept level. The study assesses the benefit of using MeSH Concepts for indexing and information retrieval. Three sets of queries were built for thirty-two rare diseases and twenty-two chronic diseases: (1) using PubMed Automatic Term Mapping (ATM), (2) using Catalog and Index of French-language Health Internet (CISMeF) ATM, and (3) extrapolating the MEDLINE citations that should be indexed with a MeSH Concept. Type 3 queries retrieve significantly fewer results than type 1 or type 2 queries (about 18,000 citations versus 200,000 for rare diseases; about 300,000 citations versus 2,000,000 for chronic diseases). CISMeF ATM also provides better precision than PubMed ATM for both disease categories. Using MeSH Concept indexing instead of ATM is theoretically possible to improve retrieval performance with the current indexing policy. However, using MeSH Concept information retrieval and indexing rules would be a fundamentally better approach. These modifications have already been implemented in the CISMeF search engine.
22879808	Teaching evidence-based medicine literature searching skills to medical students during the clinical years: a randomized controlled trial.
J Med Libr Assoc  2012Jul
Constructing an answerable question and effectively searching the medical literature are key steps in practicing evidence-based medicine (EBM). This study aimed to identify the effectiveness of delivering a single workshop in EBM literature searching skills to medical students entering their first clinical years of study. A randomized controlled trial was conducted with third-year undergraduate medical students. Participants were randomized to participate in a formal workshop in EBM literature searching skills, with EBM literature searching skills and perceived competency in EBM measured at one-week post-intervention via the Fresno tool and Clinical Effectiveness and Evidence-Based Practice Questionnaire. A total of 121 participants were enrolled in the study, with 97 followed-up post-intervention. There was no statistical mean difference in EBM literature searching skills between the 2 groups (mean difference = 0.007 (P = 0.99)). Students attending the EBM workshop were significantly more confident in their ability to construct clinical questions and had greater perceived awareness of information resources. A single EBM workshop did not result in statistically significant changes in literature searching skills. Teaching and reinforcing EBM literature searching skills during both preclinical and clinical years may result in increased student confidence, which may facilitate student use of EBM skills as future clinicians.
22879809	The Internet and health information: differences in pet owners based on age, gender, and education.
J Med Libr Assoc  2012Jul
The research assessed the attitudes and behaviors of pet owners pertaining to online search behavior for pet health information. A survey was conducted with a random sample of pet owners drawn from two US metropolitan areas and surrounding cities. Participating clinics were chosen randomly, and each participating clinic was asked to distribute 100 surveys to their clients until all surveys were disbursed. Although some perceptions and behaviors surrounding the use of the Internet for pet health information differ based on gender, age, or education level of pet owners, there are many aspects in which there are no differences based on these demographics. Results of the study suggest that closer examination of the common perception that gender, age, or education level has an effect on Internet behavior as it relates to veterinary medicine is required. Recommendations are made pertaining to the growing presence of the Internet and its impact on veterinary medicine.
22879810	Health information seekers in Japan: a snapshot of needs, behavior, and recognition in 2008.
J Med Libr Assoc  2012Jul
The purpose of this study was to explore the latest information-seeking behavior among health care consumers in Japan and to compare these behaviors with those recorded in similar surveys administered in Japan and the United States after 2000. The authors conducted a randomized, population-based, door-to-door survey in 2008. A total of 1,200 Japanese adults over 15 years of age completed the questionnaire. The results from 1,189 valid responses indicated that slightly more than half the number of participants had actively sought health information during the previous 2 years. Most seekers looked for information on a specific disease. "Physicians" remained the respondents' first choice as an information source, while "Internet" has gained greater popularity as a resource since the previous survey in 2000. Half the number of participants stated that they were willing to read academic or professional medical journal articles if written in Japanese and provided free of charge. The evidence indicates that Japanese health care consumers are now proactively seeking health information. These consumers feel reassured by the information they can access and would like to read clinical research in their native language.
22573485	Ontological phenotype standards for neurogenetics.
Hum. Mutat. 20120702 2012Sep
Neurological disorders comprise one of the largest groups of human diseases. Due to the myriad symptoms and the extreme degree of clinical variability characteristic of many neurological diseases, the differential diagnosis process is extremely challenging. Even though most neurogenetic diseases are individually rare, collectively, the subgroup of neurogenetic disorders is large, comprising more than 2,400 different disorders. Recently, increasing efforts have been undertaken to unravel the molecular basis of neurogenetic diseases and to correlate pathogenetic mechanisms with clinical signs and symptoms. In order to enable computer-based analyses, the systematic representation of the neurological phenotype is of major importance. We demonstrate how the Human Phenotype Ontology (HPO) can be incorporated into these efforts by providing a systematic semantic representation of phenotypic abnormalities encountered in human genetic diseases. The combination of the HPO together with the Orphanet disease classification represents a promising resource for automated disease classification, performing computational clustering and analysis of the neurogenetic phenome. Furthermore, standardized representations of neurologic phenotypic abnormalities employing the HPO link neurological phenotypic abnormalities to anatomical and functional entities represented in other biomedical ontologies through the semantic references provided by the HPO.
22753137	ALSoD: A user-friendly online bioinformatics tool for amyotrophic lateral sclerosis genetics.
Hum. Mutat. 20120716 2012Sep
Amyotrophic lateral sclerosis (ALS) is the commonest adult onset motor neuron disease, with a peak age of onset in the seventh decade. With advances in genetic technology, there is an enormous increase in the volume of genetic data produced, and a corresponding need for storage, analysis, and interpretation, particularly as our understanding of the relationships between genotype and phenotype mature. Here, we present a system to enable this in the form of the ALS Online Database (ALSoD at http://alsod.iop.kcl.ac.uk), a freely available database that has been transformed from a single gene storage facility recording mutations in the SOD1 gene to a multigene ALS bioinformatics repository and analytical instrument combining genotype, phenotype, and geographical information with associated analysis tools. These include a comparison tool to evaluate genes side by side or jointly with user configurable features, a pathogenicity prediction tool using a combination of computational approaches to distinguish variants with nonfunctional characteristics from disease-associated mutations with more dangerous consequences, and a credibility tool to enable ALS researchers to objectively assess the evidence for gene causation in ALS. Furthermore, integration of external tools, systems for feedback, annotation by users, and two-way links to collaborators hosting complementary databases further enhance the functionality of ALSoD.
22890789	Databases for neurogenetics: introduction, overview, and challenges.
Hum. Mutat.  2012Sep
The importance for research and clinical utility of mutation databases, as well as the issues and difficulties entailed in their construction, is discussed within the Human Variome Project. While general principles and standards can apply to most human diseases, some specific questions arise when dealing with the nature of genetic neurological disorders. So far, publically accessible mutation databases exist for only about half of the genes causing neurogenetic disorders; and a considerable work is clearly still needed to optimize their content. The current landscape, main challenges, some potential solutions, and future perspectives on genetic databases for disorders of the nervous system are reviewed in this special issue of Human Mutation on neurogenetics.
22036696	Using LOINC to link 10 terminology standards to one unified standard in a specialized domain.
J Biomed Inform 20111019 2012Aug
Despite the existence of multiple standards for the coding of biomedical data and the known benefits of doing so, there remain a myriad of biomedical information domain spaces that are essentially un-coded and unstandardized. Perhaps a worse situation is when the same or similar information in a given domain is coded to a variety of different standards. Such is the case with cephalometrics - standardized measurements of angles and distances between specified landmarks on X-ray film used for orthodontic treatment planning and a variety of research applications. We describe how we unified the existing cephalometric definitions from 10 existing cephalometric standards to one unifying terminology set using an existing standard (LOINC). Using our example of an open and web-based orthodontic case file system, we describe how this work benefited our project and discuss how adopting or expanding established standards can benefit other similar projects in specialized domains.
22226933	Implementations of the HL7 Context-Aware Knowledge Retrieval ("Infobutton") Standard: challenges, strengths, limitations, and uptake.
J Biomed Inform 20120102 2012Aug
To support clinical decision-making, computerized information retrieval tools known as "infobuttons" deliver contextually-relevant knowledge resources into clinical information systems. The Health Level Seven International (HL7) Context-Aware Knowledge Retrieval (Infobutton) Standard specifies a standard mechanism to enable infobuttons on a large scale. To examine the experience of organizations in the course of implementing the HL7 Infobutton Standard. Cross-sectional online survey and in-depth phone interviews. A total of 17 organizations participated in the study. Analysis of the in-depth interviews revealed 20 recurrent themes. Implementers underscored the benefits, simplicity, and flexibility of the HL7 Infobutton Standard. Yet, participants voiced the need for easier access to standard specifications and improved guidance to beginners. Implementers predicted that the Infobutton Standard will be widely or at least fairly well adopted in the next 5 years, but uptake will depend largely on adoption among electronic health record (EHR) vendors. To accelerate EHR adoption of the Infobutton Standard, implementers recommended HL7-compliant infobutton capabilities to be included in the United States Meaningful Use Certification Criteria for EHR systems. Opinions and predictions should be interpreted with caution, since all the participant organizations have successfully implemented the standard and over half of the organizations were actively engaged in the development of the standard. Overall, implementers reported a very positive experience with the HL7 Infobutton Standard. Despite indications of increasing uptake, measures should be taken to stimulate adoption of the Infobutton Standard among EHR vendors. Widespread adoption of the Infobutton Standard has the potential to bring contextually relevant clinical decision support content into the healthcare provider workflow.
22734939	Intersectoral interagency partnerships to promote financial capability in older people.
J Interprof Care 20120626 2012Sep
From the second quarter of 2008, the UK economy entered a period of economic decline. Older people are particularly vulnerable during these times. To promote ways in which older people can be better supported to maintain their financial well-being, this study explored the sources older people utilize to keep themselves financially informed. Interviews with older people (n = 28) showed that older people access trusted sources of information (e.g. healthcare professionals) rather than specialist financial information providers (e.g. financial advisors) which highlighted the need for interagency working between financial services in the private, public and voluntary sectors. An example of how such interagency partnerships might be achieved in practice is presented with some recommendations on directions for future research into interagency working that spans public, private and voluntary sectors.
22819996	Searching the scientific literature: implications for quantitative and qualitative reviews.
Clin Psychol Rev 20120707 2012Aug
Literature reviews are an essential step in the research process and are included in all empirical and review articles. Electronic databases are commonly used to gather this literature. However, several factors can affect the extent to which relevant articles are retrieved, influencing future research and conclusions drawn. The current project examined articles obtained by comparable search strategies in two electronic archives using an exemplar search to illustrate factors that authors should consider when designing their own search strategies. Specifically, literature searches were conducted in PsycINFO and PubMed targeting review articles on two exemplar disorders (bipolar disorder and attention deficit/hyperactivity disorder) and issues of classification and/or differential diagnosis. Articles were coded for relevance and characteristics of article content. The two search engines yielded significantly different proportions of relevant articles overall and by disorder. Keywords differed across search engines for the relevant articles identified. Based on these results, it is recommended that when gathering literature for review papers, multiple search engines should be used, and search syntax and strategies be tailored to the unique capabilities of particular engines. For meta-analyses and systematic reviews, authors may consider reporting the extent to which different archives or sources yielded relevant articles for their particular review.
22706349	MScreen: an integrated compound management and high-throughput screening data storage and analysis system.
J Biomol Screen 20120615 2012Sep
High-throughput screening (HTS) has historically been used by the pharmaceutical industry to rapidly test hundreds of thousands of compounds to identify potential drug candidates. More recently, academic groups have used HTS to identify new chemical probes or small interfering RNA (siRNA) that can serve as experimental tools to examine the biology or physiology of novel proteins, processes, or interactions. HTS presents a significant challenge with the vast and complex nature of data generated. This report describes MScreen, a Web-based, open-source cheminformatics application for chemical library and siRNA plate management, primary HTS and dose-response data handling, structure search, and administrative functions. Each project in MScreen can be secured with passwords or shared in an open-information environment that enables collaborators to easily compare data from many screens, providing a useful means to identify compounds with desired selectivity. Unique features include compound, substance, mixture, and siRNA plate creation and formatting; automated dose-response fitting and quality control (QC); and user, target, and assay method administration. MScreen provides an effective means to facilitate HTS information handling and analysis in the academic setting so that users can efficiently view their screening data and evaluate results for follow-up.
22429558	A simple and versatile data acquisition system for software coincidence and pulse-height discrimination in 4πβ-γ coincidence experiments.
Appl Radiat Isot 20120301 2012Sep
A simple but versatile data acquisition system for software coincidence experiments is described, in which any time stamping and live time controller are not provided. Signals from β- and γ-channels are fed to separately two fast ADCs (16 bits, 25 MHz clock maximum) via variable delay circuits and pulse-height stretchers, and also to pulse-height discriminators. The discriminating level was set to just above the electronic noise. Two ADCs were controlled with a common clock signal, and triggered simultaneously by the logic OR pulses from both discriminators. Paired digital signals for each sampling were sent to buffer memories connected to main PC with a FIFO (First-In, First-Out) pipe via USB. After data acquisition in list mode, various processing including pulse-height analyses was performed using MS-Excel (version 2007 and later). The usefulness of this system was demonstrated for 4πβ(PS)-4πγ coincidence measurements of (60)Co, (134)Cs and (152)Eu. Possibilities of other extended applications will be touched upon.
22436449	Measurements of the half-life of 214Po and 218Rn using digital electronics.
Appl Radiat Isot 20120306 2012Sep
The half-lives of (214)Po and (218)Rn have been measured. The radionuclides were produced in the decay of a (230)U source and the emitted alpha-particles were measured in nearly-2π geometry with an ion-implanted planar silicon detector. The data acquisition was performed with a digitiser operated in list mode, saving the energy and time of detection (10 ns precision timestamp) of each event. The half-lives were deduced from the time differences between the alpha-decays populating the nuclide of interest and those corresponding to its decay. Different methods were applied, based on delayed coincidence counting and time-interval distribution analysis. The resulting half-lives are 33.75 (15) ms for (218)Rn and 164.2 (6) μs for (214)Po, both in agreement with some of the literature values, and obtained with higher precision in this work.
22659238	Population-ethnic group specific genome variation allele frequency data: a querying and visualization journey.
Genomics 20120530 2012Aug
National/ethnic mutation databases aim to document the genetic heterogeneity in various populations and ethnic groups worldwide. We have previously reported the development and upgrade of FINDbase (www.findbase.org), a database recording causative mutations and pharmacogenomic marker allele frequencies in various populations around the globe. Although this database has recently been upgraded, we continuously try to enhance its functionality by providing more advanced visualization tools that would further assist effective data querying and comparisons. We are currently experimenting in various visualization techniques on the existing FINDbase causative mutation data collection aiming to provide a dynamic research tool for the worldwide scientific community. We have developed an interactive web-based application for population-based mutation data retrieval. It supports sophisticated data exploration allowing users to apply advanced filtering criteria upon a set of multiple views of the underlying data collection and enables browsing the relationships between individual datasets in a novel and meaningful way.
22140210	Adoption of a wiki within a large internal medicine residency program: a 3-year experience.
J Am Med Inform Assoc 20111202 2012 Jul-Aug
To describe the creation and evaluate the use of a wiki by medical residents, and to determine if a wiki would be a useful tool for improving the experience, efficiency, and education of housestaff. In 2008, a team of medical residents built a wiki containing institutional knowledge and reference information using Microsoft SharePoint. We tracked visit data for 3 years, and performed an audit of page views and updates in the second year. We evaluated the attitudes of medical residents toward the wiki using a survey. Users accessed the wiki 23,218, 35,094, and 40,545 times in each of three successive academic years from 2008 to 2011. In the year two audit, 85 users made a total of 1082 updates to 176 pages and of these, 91 were new page creations by 17 users. Forty-eight percent of residents edited a page. All housestaff felt the wiki improved their ability to complete tasks, and 90%, 89%, and 57% reported that the wiki improved their experience, efficiency, and education, respectively, when surveyed in academic year 2009-2010. A wiki is a useful and popular tool for organizing administrative and educational content for residents. Housestaff felt strongly that the wiki improved their workflow, but a smaller educational impact was observed. Nearly half of the housestaff edited the wiki, suggesting broad buy-in among the residents. A wiki is a feasible and useful tool for improving information retrieval for house officers.
22249967	Validity of electronic health record-derived quality measurement for performance monitoring.
J Am Med Inform Assoc 20120116 2012 Jul-Aug
Since 2007, New York City's primary care information project has assisted over 3000 providers to adopt and use a prevention-oriented electronic health record (EHR). Participating practices were taught to re-adjust their workflows to use the EHR built-in population health monitoring tools, including automated quality measures, patient registries and a clinical decision support system. Practices received a comprehensive suite of technical assistance, which included quality improvement, EHR customization and configuration, privacy and security training, and revenue cycle optimization. These services were aimed at helping providers understand how to use their EHR to track and improve the quality of care delivered to patients. Retrospective electronic chart reviews of 4081 patient records across 57 practices were analyzed to determine the validity of EHR-derived quality measures and documented preventive services. Results from this study show that workflow and documentation habits have a profound impact on EHR-derived quality measures. Compared with the manual review of electronic charts, EHR-derived measures can undercount practice performance, with a disproportionately negative impact on the number of patients captured as receiving a clinical preventive service or meeting a recommended treatment goal. This study provides a cautionary note in using EHR-derived measurement for public reporting of provider performance or use for payment.
22268218	Shifts in the architecture of the Nationwide Health Information Network.
J Am Med Inform Assoc 20120121 2012 Jul-Aug
In the midst of a US $30 billion USD investment in the Nationwide Health Information Network (NwHIN) and electronic health records systems, a significant change in the architecture of the NwHIN is taking place. Prior to 2010, the focus of information exchange in the NwHIN was the Regional Health Information Organization (RHIO). Since 2010, the Office of the National Coordinator (ONC) has been sponsoring policies that promote an internet-like architecture that encourages point to-point information exchange and private health information exchange networks. The net effect of these activities is to undercut the limited business model for RHIOs, decreasing the likelihood of their success, while making the NwHIN dependent on nascent technologies for community level functions such as record locator services. These changes may impact the health of patients and communities. Independent, scientifically focused debate is needed on the wisdom of ONC's proposed changes in its strategy for the NwHIN.
22298565	A system for coreference resolution for the clinical narrative.
J Am Med Inform Assoc 20120131 2012 Jul-Aug
To research computational methods for coreference resolution in the clinical narrative and build a system implementing the best methods. The Ontology Development and Information Extraction corpus annotated for coreference relations consists of 7214 coreferential markables, forming 5992 pairs and 1304 chains. We trained classifiers with semantic, syntactic, and surface features pruned by feature selection. For the three system components--for the resolution of relative pronouns, personal pronouns, and noun phrases--we experimented with support vector machines with linear and radial basis function (RBF) kernels, decision trees, and perceptrons. Evaluation of algorithms and varied feature sets was performed using standard metrics. The best performing combination is support vector machines with an RBF kernel and all features (MUC score=0.352, B(3)=0.690, CEAF=0.486, BLANC=0.596) outperforming a traditional decision tree baseline. The application showed good performance similar to performance on general English text. The main error source was sentence distances exceeding a window of 10 sentences between markables. A possible solution to this problem is hinted at by the fact that coreferent markables sometimes occurred in predictable (although distant) note sections. Another system limitation is failure to fully utilize synonymy and ontological knowledge. Future work will investigate additional ways to incorporate syntactic features into the coreference problem. We investigated computational methods for coreference resolution in the clinical narrative. The best methods are released as modules of the open source Clinical Text Analysis and Knowledge Extraction System and Ontology Development and Information Extraction platforms.
22427539	The SMART Platform: early experience enabling substitutable applications for electronic health records.
J Am Med Inform Assoc 20120317 2012 Jul-Aug
The Substitutable Medical Applications, Reusable Technologies (SMART) Platforms project seeks to develop a health information technology platform with substitutable applications (apps) constructed around core services. The authors believe this is a promising approach to driving down healthcare costs, supporting standards evolution, accommodating differences in care workflow, fostering competition in the market, and accelerating innovation. The Office of the National Coordinator for Health Information Technology, through the Strategic Health IT Advanced Research Projects (SHARP) Program, funds the project. The SMART team has focused on enabling the property of substitutability through an app programming interface leveraging web standards, presenting predictable data payloads, and abstracting away many details of enterprise health information technology systems. Containers--health information technology systems, such as electronic health records (EHR), personally controlled health records, and health information exchanges that use the SMART app programming interface or a portion of it--marshal data sources and present data simply, reliably, and consistently to apps. The SMART team has completed the first phase of the project (a) defining an app programming interface, (b) developing containers, and (c) producing a set of charter apps that showcase the system capabilities. A focal point of this phase was the SMART Apps Challenge, publicized by the White House, using http://www.challenge.gov website, and generating 15 app submissions with diverse functionality. Key strategic decisions must be made about the most effective market for further disseminating SMART: existing market-leading EHR vendors, new entrants into the EHR market, or other stakeholders such as health information exchanges.
22431555	Presence of key findings in the medical record prior to a documented high-risk diagnosis.
J Am Med Inform Assoc 20120319 2012 Jul-Aug
Failure or delay in diagnosis is a common preventable source of error. The authors sought to determine the frequency with which high-information clinical findings (HIFs) suggestive of a high-risk diagnosis (HRD) appear in the medical record before HRD documentation. A knowledge base from a diagnostic decision support system was used to identify HIFs for selected HRDs: lumbar disc disease, myocardial infarction, appendicitis, and colon, breast, lung, ovarian and bladder carcinomas. Two physicians reviewed at least 20 patient records retrieved from a research patient data registry for each of these eight HRDs and for age- and gender-compatible controls. Records were searched for HIFs in visit notes that were created before the HRD was established in the electronic record and in general medical visit notes for controls. 25% of records reviewed (61/243) contained HIFs in notes before the HRD was established. The mean duration between HIFs first occurring in the record and time of diagnosis ranged from 19 days for breast cancer to 2 years for bladder cancer. In three of the eight HRDs, HIFs were much less likely in control patients without the HRD. In many records of patients with an HRD, HIFs were present before the HRD was established. Reasons for delay include non-compliance with recommended follow-up, unusual presentation of a disease, and system errors (eg, lack of laboratory follow-up). The presence of HIFs in clinical records suggests a potential role for the integration of diagnostic decision support into the clinical workflow to provide reminder alerts to improve the diagnostic focus.
22828830	FPGA-based reconfigurable processor for ultrafast interlaced ultrasound and photoacoustic imaging.
IEEE Trans Ultrason Ferroelectr Freq Control  2012Jul
In this paper, we report, to the best of our knowledge, a unique field-programmable gate array (FPGA)-based reconfigurable processor for real-time interlaced co-registered ultrasound and photoacoustic imaging and its application in imaging tumor dynamic response. The FPGA is used to control, acquire, store, delay-and-sum, and transfer the data for real-time co-registered imaging. The FPGA controls the ultrasound transmission and ultrasound and photoacoustic data acquisition process of a customized 16-channel module that contains all of the necessary analog and digital circuits. The 16-channel module is one of multiple modules plugged into a motherboard; their beamformed outputs are made available for a digital signal processor (DSP) to access using an external memory interface (EMIF). The FPGA performs a key role through ultrafast reconfiguration and adaptation of its structure to allow real-time switching between the two imaging modes, including transmission control, laser synchronization, internal memory structure, beamforming, and EMIF structure and memory size. It performs another role by parallel accessing of internal memories and multi-thread processing to reduce the transfer of data and the processing load on the DSP. Furthermore, because the laser will be pulsing even during ultrasound pulse-echo acquisition, the FPGA ensures that the laser pulses are far enough from the pulse-echo acquisitions by appropriate time-division multiplexing (TDM). A co-registered ultrasound and photoacoustic imaging system consisting of four FPGA modules (64-channels) is constructed, and its performance is demonstrated using phantom targets and in vivo mouse tumor models.
22828831	System-on-chip design for ultrasonic target detection using split-spectrum processing  and neural networks.
IEEE Trans Ultrason Ferroelectr Freq Control  2012Jul
Ultrasonic detection and characterization of targets concealed by scattering noise is remarkably challenging. In this study, a neural network (NN) coupled to split-spectrum processing (SSP) is examined for target echo visibility enhancement using experimental measurements with input signal-to-noise ratio around 0 dB. The SSP-NN target detection system is trainable and consequently is capable of improving the target-to-clutter ratio by an average of 40 dB. The proposed system is exceptionally robust and outperforms the conventional techniques such as minimum, median, average, geometric mean, and polarity threshold detectors. For realtime imaging applications, a field-programmable gate array (FPGA)-based hardware platform is designed for system-onchip (SoC) realization of the SSP-NN target detection system. This platform is a hardware/software co-design system using parallel and pipelined multiplications and additions for highspeed operation and high computational throughput.
22828832	A novel biomimetic sonarhead using beamforming technology to mimic bat echolocation.
IEEE Trans Ultrason Ferroelectr Freq Control  2012Jul
A novel biomimetic sonarhead has been developed to allow researchers of bat echolocation behavior and biomimetic sonar to perform experiments with a system similar to the bat¿s sensory system. The bat's echolocation-related transfer function (ERTF) is implemented using an array of receivers to implement the head-related transfer function (HRTF), and an array of emitters mounted on a cylindrical manifold to implement the emission pattern of the bat. The complete system is controlled by a field-programmable gate array (FPGA) based embedded system connected through a USB interface.
22828833	A reconfigurable and programmable FPGA-based system for nonstandard ultrasound methods.
IEEE Trans Ultrason Ferroelectr Freq Control  2012Jul
The availability of programmable and reconfigurable ultrasound (US) research platforms may have a considerable impact on the advancement of ultrasound systems technology; indeed, they allow novel transmission strategies or challenging processing methods to be tested and experimentally refined. In this paper, the ULtrasound Advanced Open Platform (ULA-OP), recently developed in our University laboratory, is shown to be a flexible tool that can be easily adapted to a wide range of applications. Five nonstandard working modalities are illustrated. Vector Doppler and quasi-static elastography applications emphasize the real-time potential and versatility of the system. Flow-mediated dilation, pulse compression, and high-frame-rate imaging highlight the flexibility of data access at different points in the reception chain. For each modality, the role played by the onboard programmable devices is discussed. Experimental results are reported, indicating the relative performance of the system for each application.
22828834	A single FPGA-based portable ultrasound imaging system for point-of-care applications.
IEEE Trans Ultrason Ferroelectr Freq Control  2012Jul
We present a cost-effective portable ultrasound system based on a single field-programmable gate array (FPGA) for point-of-care applications. In the portable ultrasound system developed, all the ultrasound signal and image processing modules, including an effective 32-channel receive beamformer with pseudo-dynamic focusing, are embedded in an FPGA chip. For overall system control, a mobile processor running Linux at 667 MHz is used. The scan-converted ultrasound image data from the FPGA are directly transferred to the system controller via external direct memory access without a video processing unit. The potable ultrasound system developed can provide real-time B-mode imaging with a maximum frame rate of 30, and it has a battery life of approximately 1.5 h. These results indicate that the single FPGA-based portable ultrasound system developed is able to meet the processing requirements in medical ultrasound imaging while providing improved flexibility for adapting to emerging POC applications.
22828835	Embedded Doppler system for industrial in-line rheometry.
IEEE Trans Ultrason Ferroelectr Freq Control  2012Jul
Rheological fluid behavior characterization is crucial for the industrial production of cosmetics, food, pharmaceutics, adhesive, sealants, etc. For example, the measurement of specific rheological features at every step of the production chain is critical for product quality control. Such measurements are often limited to laboratory tests on product specimens because of technical difficulties. In this work, we present an embedded system suitable for in-line rheometric evaluation of highly filled polyurethane-based adhesives. This system includes an ultrasound front-end and a digital signal processing section integrated in a low-cost field-programmable gate array. The system measures the real-time velocity profile developed in the pipe by the fluid, employing a Doppler multigate technique. The high-resolution velocity profile, combined with a pressure drop measurement, allows an accurate evaluation of the flow consistency index, K, and the flow behavior index, n, of the interrogated fluid.
22828836	An ultrasonic imaging system based on a new SAFT approach and a GPU beamformer.
IEEE Trans Ultrason Ferroelectr Freq Control  2012Jul
The design of newer ultrasonic imaging systems attempts to obtain low-cost, small-sized devices with reduced power consumption that are capable of reaching high frame rates with high image quality. In this regard, synthetic aperture techniques have been very useful. They reduce hardware requirements and accelerate information capture. However, the beamforming process is still very slow, limiting the overall speed of the system. Recently, general-purpose computing on graphics processing unit techniques have been proposed as a way to accelerate image composition. They provide excellent computing power with which a very large volume of data can easily and quickly be processed. This paper describes a new system architecture that merges both principles. Thus, using a minimum-redundancy synthetic aperture technique to acquire the signals (2R-SAFT), and a graphics processing unit as a beamformer, we have developed a new scanner with full dynamic focusing, both on emission and reception, that attains real-time imaging with very few resources.
22828838	Vibro-acoustography beam formation with reconfigurable arrays.
IEEE Trans Ultrason Ferroelectr Freq Control  2012Jul
In this work, we present a numerical study of the use of reconfigurable arrays (RCA) for vibro-acoustography (VA) beam formation. A parametric study of the aperture selection, number of channels, number of elements, focal distance, and steering parameters is presented to show the feasibility and evaluate the performance of VA imaging based on RCA. The transducer aperture was based on two concentric arrays driven by two continuous-wave or toneburst signals at slightly different frequencies. The mathematical model considers a homogeneous, isotropic, inviscid medium. The pointspread function of the system is calculated based on angular spectrum methods using the Fresnel approximation for rectangular sources. Simulations considering arrays with 50 x 50 to 200 x 200 elements with number of channels varying in the range of 32 to 128 are evaluated to identify the best configuration for VA. Advantages of two-dimensional and RCA arrays and aspects related to clinical importance of the RCA implementation in VA, such as spatial resolution, image frame rate, and commercial machine implementation, are discussed. It is concluded that RCA transducers can produce spatial resolution similar to confocal transducers and steering is possible in the elevational and azimuthal planes. Optimal settings for number of elements, number of channels, maximum steering, and focal distance are suggested for VA clinical applications. Furthermore, an optimization for beam steering based on the channel assignment is proposed for balancing the contribution of the two waves in the steered focus.
22828839	An FPGA-based open platform for ultrasound biomicroscopy.
IEEE Trans Ultrason Ferroelectr Freq Control  2012Jul
Ultrasound biomicroscopy (UBM) has been extensively applied to preclinical studies in small animal models. Individual animal study is unique and requires different utilization of the UBM system to accommodate different transducer characteristics, data acquisition strategies, signal processing, and image reconstruction methods. There is a demand for a flexible and open UBM platform to allow users to customize the system for various studies and have full access to experimental data. This paper presents the development of an open UBM platform (center frequency 20 to 80 MHz) for various preclinical studies. The platform design was based on a field-programmable gate array (FPGA) embedded in a printed circuit board to achieve B-mode imaging and directional pulsed-wave Doppler. Instead of hardware circuitry, most functions of the platform, such as filtering, envelope detection, and scan conversion, were achieved by FPGA programs; thus, the system architecture could be easily modified for specific applications. In addition, a novel digital quadrature demodulation algorithm was implemented for fast and accurate Doppler profiling. Finally, test results showed that the platform could offer a minimum detectable signal of 25 μV, allowing a 51 dB dynamic range at 47 dB gain, and real-time imaging at more than 500 frames/s. Phantom and in vivo imaging experiments were conducted and the results demonstrated good system performance.
22828841	Conformal ultrasound imaging system for anatomical breast inspection.
IEEE Trans Ultrason Ferroelectr Freq Control  2012Jul
Ultrasound tomography has considerable potential as a means of breast cancer detection because it reduces the operator-dependency observed in echography. A half-ring transducer array was designed based on breast anatomy, to obtain reflectivity images of the ductolobular structures using tomographic reconstruction procedures. The 3-MHz transducer array comprises 1024 elements set in a 190-degree circular arc with a radius of 100 mm. The front-end electronics incorporate 32 independent parallel transmit/receive channels and a 32-to-1024 multiplexer unit. The transmit and receive circuitries have a variable sampling frequency of up to 80 MHz and 12-bit precision. Arbitrary waveforms are synthesized to improve the signal-to-noise ratio and to increase the spatial resolution when working with low-contrast objects. The setup was calibrated with academic objects and a needle hydrophone to develop the data correction tools and specify the properties of the system. The backscattering field was recorded using a restricted aperture, and tomographic acquisitions were performed with a pair of 0.08-mm-diameter steel wires, a low-contrast 2-D breast phantom, and a breast-shaped phantom containing inclusions. Data were processed with dedicated correction tools and a pulse compression technique. Objects were reconstructed using the elliptical back-projection algorithm.
22828842	Ultrasonic scanner for in vivo measurement of cancellous bone properties from backscattered data.
IEEE Trans Ultrason Ferroelectr Freq Control  2012Jul
A dedicated ultrasonic scanner for acquiring RF echoes backscattered from the trabecular bone was developed. The design of device is based on the goal of minimizing of custom electronics and computations executed solely on the main computer processor and the graphics card. The electronic encoder-digitizer module executing all of the transmission and reception functions is based on a single low-cost field programmable gate array (FPGA). The scanner is equipped with a mechanical sector-scan probe with a concave transducer with 50 mm focal length, center frequency of 1.5 MHz and 60% bandwidth at -6 dB. The example of femoral neck bone examination shows that the scanner can provide ultrasonic data from deeply located bones with the ultrasound penetrating the trabecular bone up to a depth of 20 mm. It is also shown that the RF echo data acquired with the scanner allow for the estimation of attenuation coefficient and frequency dependence of backscattering coefficient of trabecular bone. The values of the calculated parameters are in the range of corresponding in vitro data from the literature but their variation is relatively high.
22828844	Implementation of a versatile research data acquisition system using a commercially available medical ultrasound scanner.
IEEE Trans Ultrason Ferroelectr Freq Control  2012Jul
This paper describes the design and implementation of a versatile, open-architecture research data acquisition system using a commercially available medical ultrasound scanner. The open architecture will allow researchers and clinicians to rapidly develop applications and move them relatively easy to the clinic. The system consists of a standard PC equipped with a camera link and an ultrasound scanner equipped with a research interface. The ultrasound scanner is an easy-to-use imaging device that is capable of generating high-quality images. In addition to supporting the acquisition of multiple data types, such as B-mode, M-mode, pulsed Doppler, and color flow imaging, the machine provides users with full control over imaging parameters such as transmit level, excitation waveform, beam angle, and focal depth. Beamformed RF data can be acquired from regions of interest throughout the image plane and stored to a file with a simple button press. For clinical trials and investigational purposes, when an identical image plane is desired for both an experimental and a reference data set, interleaved data can be captured. This form of data acquisition allows switching between multiple setups while maintaining identical transducer, scanner, region of interest, and recording time. Data acquisition is controlled through a graphical user interface running on the PC. This program implements an interface for third-party software to interact with the application. A software development toolkit is developed to give researchers and clinicians the ability to utilize third-party software for data analysis and flexible manipulation of control parameters. Because of the advantages of speed of acquisition and clinical benefit, research projects have successfully used the system to test and implement their customized solutions for different applications. Three examples of system use are presented in this paper: evaluation of synthetic aperture sequential beamformation, transverse oscillation for blood velocity estimation, and acquisition of spectral velocity data for evaluating aortic aneurysms.
22828845	Front-end receiver electronics for a matrix transducer for 3-D transesophageal echocardiography.
IEEE Trans Ultrason Ferroelectr Freq Control  2012Jul
There is a clear clinical need for creating 3-D images of the heart. One promising technique is the use of transesophageal echocardiography (TEE). To enable 3-D TEE, we are developing a miniature ultrasound probe containing a matrix piezoelectric transducer with more than 2000 elements. Because a gastroscopic tube cannot accommodate the cables needed to connect all transducer elements directly to an imaging system, a major challenge is to locally reduce the number of channels, while maintaining a sufficient signal-to-noise ratio. This can be achieved by using front-end receiver electronics bonded to the transducers to provide appropriate signal conditioning in the tip of the probe. This paper presents the design of such electronics, realizing time-gain compensation (TGC) and micro-beamforming using simple, low-power circuits. Prototypes of TGC amplifiers and micro-beamforming cells have been fabricated in 0.35-μm CMOS technology. These prototype chips have been combined on a printed circuit board (PCB) to form an ultrasound-receiver system capable of reading and combining the signals of three transducer elements. Experimental results show that this design is a suitable candidate for 3-D TEE.
22828846	An FPGA-based ultrasound imaging system using capacitive micromachined ultrasonic transducers.
IEEE Trans Ultrason Ferroelectr Freq Control  2012Jul
We report the design and experimental results of a field-programmable gate array (FPGA)-based real-time ultrasound imaging system that uses a 16-element phased-array capacitive micromachined ultrasonic transducer fabricated using a fusion bonding process. The imaging system consists of the transducer, discrete analog components situated on a custom-made circuit board, the FPGA, and a monitor. The FPGA program consists of five functional blocks: a main counter, transmit and receive beamformer, receive signal pre-processing, envelope detection, and display. No dedicated digital signal processor or personal computer is required for the imaging system. An experiment is carried out to obtain the sector B-scan of a 4-wire target. The ultrasound imaging system demonstrates the possibility of an integrated system-in-a-package solution.
22828849	A look-up-table digital predistortion technique for high-voltage power amplifiers in ultrasonic applications.
IEEE Trans Ultrason Ferroelectr Freq Control  2012Jul
In this paper, we present a digital predistortion technique to improve the linearity and power efficiency of a high-voltage class-AB power amplifier (PA) for ultrasound transmitters. The system is composed of a digital-to-analog converter (DAC), an analog-to-digital converter (ADC), and a field-programmable gate array (FPGA) in which the digital predistortion (DPD) algorithm is implemented. The DPD algorithm updates the error, which is the difference between the ideal signal and the attenuated distorted output signal, in the look-up table (LUT) memory during each cycle of a sinusoidal signal using the least-mean-square (LMS) algorithm. On the next signal cycle, the error data are used to equalize the signal with negative harmonic components to cancel the amplifier's nonlinear response. The algorithm also includes a linear interpolation method applied to the windowed sinusoidal signals for the B-mode and Doppler modes. The measurement test bench uses an arbitrary function generator as the DAC to generate the input signal, an oscilloscope as the ADC to capture the output waveform, and software to implement the DPD algorithm. The measurement results show that the proposed system is able to reduce the second-order harmonic distortion (HD2) by 20 dB and the third-order harmonic distortion (HD3) by 14.5 dB, while at the same time improving the power efficiency by 18%.
22833496	Mining the pharmacogenomics literature--a survey of the state of the art.
Brief. Bioinformatics  2012Jul
This article surveys efforts on text mining of the pharmacogenomics literature, mainly from the period 2008 to 2011. Pharmacogenomics (or pharmacogenetics) is the field that studies how human genetic variation impacts drug response. Therefore, publications span the intersection of research in genotypes, phenotypes and pharmacology, a topic that has increasingly become a focus of active research in recent years. This survey covers efforts dealing with the automatic recognition of relevant named entities (e.g. genes, gene variants and proteins, diseases and other pathological phenomena, drugs and other chemicals relevant for medical treatment), as well as various forms of relations between them. A wide range of text genres is considered, such as scientific publications (abstracts, as well as full texts), patent texts and clinical narratives. We also discuss infrastructure and resources needed for advanced text analytics, e.g. document corpora annotated with corresponding semantic metadata (gold standards and training data), biomedical terminologies and ontologies providing domain-specific background knowledge at different levels of formality and specificity, software architectures for building complex and scalable text analytics pipelines and Web services grounded to them, as well as comprehensive ways to disseminate and interact with the typically huge amounts of semiformal knowledge structures extracted by text mining tools. Finally, we consider some of the novel applications that have already been developed in the field of pharmacogenomic text mining and point out perspectives for future research.
22221111	The manipulation of drugs to obtain the required dose: systematic review.
J Adv Nurs 20120103 2012Sep
To describe the development of a systematic review protocol that maps the evidence relating to drug manipulations conducted to obtain the required dose. This process included defining a search strategy and methods to assess the quality and to synthesize the evidence retrieved. Economic constraints mean that marketed formulations may not meet the needs of all patients. Consequently, it is sometimes necessary to manipulate marketed products with the aim of obtaining the required dose. Most clinical practice appears to be guided by ad hoc approaches and informal literature reviews. This systematic review protocol has been designed to identify the evidence available on drug manipulation. The review aims to identify what evidence is available and where the gaps appear in the current evidence. This report describes the challenges of developing a systematic review in an area that potentially involves many drugs and considers outcomes other than effectiveness. In particular, searches required the use of non-specific terms and the iterative development of a complex search strategy. The development of quality assessment criteria is also described. Funding commenced in April 2009. The systematic review described here will capture a broad selection of research about drug manipulations and may also be of interest to those conducting reviews in broad remit subject areas that are not easy to define using accepted terminology.
22830766	Resolution evaluation of MR images reconstructed by iterative thresholding algorithms for compressed sensing.
Med Phys  2012Jul
Magnetic resonance imaging systems usually feature linear and shift-invariant (stationary) transform characteristics. The point spread function or equivalently the modulation transfer function may thus be used for an objective quality assessment of imaging modalities. The recently introduced theory of compressed sensing, however, incorporates nonlinear and nonstationary reconstruction algorithms into the magnetic resonance imaging process which prohibits the usage of the classical point spread function and therefore the according evaluation. In this work, a local point spread function concept was applied to assess the quality of magnetic resonance images which were reconstructed by an iterative soft thresholding algorithm for compressed sensing. The width of the main lobe of the local point spread function was used to perform studies on the spatial and temporal resolution properties of both numerical phantom and in vivo images. The impact of k-space sampling patterns as well as additional sparsifying transforms on the local spatial image resolution was investigated. In addition, the local temporal resolution of image series, which were reconstructed by exploiting spatiotemporal sparsity, was determined. Finally, the dependency of the local resolution on the thresholding parameter of the algorithm was examined. The sampling patterns as well as the additional sparsifying transform showed a distinct impact on the local image resolution of the phantom image. The reconstructions, which were using x-f-space as a sparse transform domain showed slight temporal blurring for dynamic parts of the imaged object. The local image resolution had a dependence on the thresholding parameter, which allowed for optimizing the reconstruction. Local point spread functions enable the evaluation of the local spatial and temporal resolution of images reconstructed with the nonlinear and nonstationary iterative soft thresholding algorithm. By determining the influence of thresholding parameter and sampling pattern chosen on this model-based reconstruction, the method allows selecting appropriate acquisition parameters and thus improving the results.
21383410	Human motion retrieval from hand-drawn sketch.
IEEE Trans Vis Comput Graph  2012May
The rapid growth of motion capture data increases the importance of motion retrieval. The majority of the existing motion retrieval approaches are based on a labor-intensive step in which the user browses and selects a desired query motion clip from the large motion clip database. In this work, a novel sketching interface for defining the query is presented. This simple approach allows users to define the required motion by sketching several motion strokes over a drawn character, which requires less effort and extends the users’ expressiveness. To support the real-time interface, a specialized encoding of the motions and the hand-drawn query is required. Here, we introduce a novel hierarchical encoding scheme based on a set of orthonormal spherical harmonic (SH) basis functions, which provides a compact representation, and avoids the CPU/processing intensive stage of temporal alignment used by previous solutions. Experimental results show that the proposed approach can well retrieve the motions, and is capable of retrieve logically and numerically similar motions, which is superior to previous approaches. The user study shows that the proposed system can be a useful tool to input motion query if the users are familiar with it. Finally, an application of generating a 3D animation from a hand-drawn comics strip is demonstrated.
22848998	When humans are the exception: cross-species databases at the interface of biological and clinical research.
Soc Stud Sci  2012Apr
Cross-species comparison has long been regarded as a stepping-stone for medical research, enabling the discovery and testing of prospective treatments before they undergo clinical trial on humans. Post-genomic medicine has made cross-species comparison crucial in another respect: the 'community databases' developed to collect and disseminate data on model organisms are now often used as a template for the dissemination of data on humans and as a tool for comparing results of medical significance across the human-animal boundary. This paper identifies and discusses four key problems encountered by database curators when integrating human and non-human data within the same database: (1) picking criteria for what counts as reliable evidence, (2) selecting metadata, (3) standardising and describing research materials and (4) choosing nomenclature to classify data. An analysis of these hurdles reveals epistemic disagreement and controversies underlying cross-species comparisons, which in turn highlight important differences in the experimental cultures of biologists and clinicians trying to make sense of these data. By considering database development through the eyes of curators, this study casts new light on the complex conjunctions of biological and clinical practice, model organisms and human subjects, and material and virtual sources of evidence--thus emphasizing the fragmented, localized and inherently translational nature of biomedicine.
22850636	Roles and applications of biomedical ontologies in experimental animal science.
Exp. Anim.  2012
A huge amount of experimental data from past studies has played a vital role in the development of new knowledge and technologies in biomedical science. The importance of computational technologies for the reuse of data, data integration, and knowledge discoveries has also increased, providing means of processing large amounts of data. In recent years, information technologies related to "ontologies" have played more significant roles in the standardization, integration, and knowledge representation of biomedical information. This review paper outlines the history of data integration in biomedical science and its recent trends in relation to the field of experimental animal science.
22853299	Converting an intranet site to the cloud: using CampusGuides to refresh a library portal.
Med Ref Serv Q  2012
After a major redesign project in 2002, Mayo Clinic Libraries' heavily used intranet portal remained largely static. Library staff were unable to make substantive design changes or introduce tools that would make the content more dynamic. CampusGuides offered a practical, user-friendly, web-based solution to add dynamic content to the library site. A task force was formed both to establish design and style guidelines that would integrate with the library site and to plan the conversion of content to CampusGuides. Converting intranet site content to CampusGuides gave the task force the opportunity to examine, re-imagine, and revitalize site content.
22853305	An introduction to web scale discovery systems.
Med Ref Serv Q  2012
This article explores the basic principles of web-scale discovery systems and how they are being implemented in libraries. "Web scale discovery" refers to a class of products that index a vast number of resources in a wide variety formats and allow users to search for content in the physical collection, print and electronic journal collections, and other resources from a single search box. Search results are displayed in a manner similar to Internet searches, in a relevance ranked list with links to online content. The advantages and disadvantages of these systems are discussed, and a list of popular discovery products is provided. A list of library websites with discovery systems currently implemented is also provided.
22768818	[Clinical Research VII. Systematic search: how to look for medical documents].
Rev Med Inst Mex Seguro Soc  2012 Jan-Feb
In the process of responding to questions generated during medical care, the number of articles appearing in the search is so vast that a strategy must be considered. This article describes the process to find and select the information to help us respond the needs of our patients. The judgment of the quality and relevance of the answer depends on each reader. Initially you have to look in places where there is medical arbitration for publications, reasons why we recommend PubMed. Start the search once the acronym PICO breakdown, where P is for patients, I intervention, C comparator and O outcome or result; these words are used as MeSH (Medical Subject Headings) and are linked with Boolean terms (and, or, and not). The acronym PICO shares components with the classical model of the Architecture of the Research described by Dr. Alvan R. Feinstein. A good search should participate in the response to our question in the first 20 articles, if it does not happen, the search must be more specific with the use of filters.
22503730	An implicit approach to deal with periodically repeated medical data.
Artif Intell Med 20120413 2012Jul
Temporal information plays a crucial role in medicine, so that in medical informatics there is an increasing awareness that suitable database approaches are needed to store and support it. Specifically, a great amount of clinical data (e.g., therapeutic data) are periodically repeated. Although an explicit treatment is possible in most cases, it causes severe storage and disk I/O problems. In this paper, we propose an innovative approach to cope with periodic relational medical data in an implicit way. We propose a new data model, representing periodic data in a compact (implicit) way, which is a consistent extension of TSQL2 consensus approach. Then, we identify some important types of temporal queries, and present query answering algorithms to answer them. Finally, we also run experiments to evaluate our approach. The experiments show that our approach outperforms current explicit approaches, especially as regard disk I/O. We have provided an implicit approach to periodic data with is a consistent extension of TSQL2 (and which is thus grant interoperable with it), and we have experimentally proven that it outperforms current explicit approaches.
22776692	P2P watch: personal health information detection in peer-to-peer file-sharing networks.
J. Med. Internet Res. 20120709 2012
Users of peer-to-peer (P2P) file-sharing networks risk the inadvertent disclosure of personal health information (PHI). In addition to potentially causing harm to the affected individuals, this can heighten the risk of data breaches for health information custodians. Automated PHI detection tools that crawl the P2P networks can identify PHI and alert custodians. While there has been previous work on the detection of personal information in electronic health records, there has been a dearth of research on the automated detection of PHI in heterogeneous user files. To build a system that accurately detects PHI in files sent through P2P file-sharing networks. The system, which we call P2P Watch, uses a pipeline of text processing techniques to automatically detect PHI in files exchanged through P2P networks. P2P Watch processes unstructured texts regardless of the file format, document type, and content. We developed P2P Watch to extract and analyze PHI in text files exchanged on P2P networks. We labeled texts as PHI if they contained identifiable information about a person (eg, name and date of birth) and specifics of the person's health (eg, diagnosis, prescriptions, and medical procedures). We evaluated the system's performance through its efficiency and effectiveness on 3924 files gathered from three P2P networks. P2P Watch successfully processed 3924 P2P files of unknown content. A manual examination of 1578 randomly selected files marked by the system as non-PHI confirmed that these files indeed did not contain PHI, making the false-negative detection rate equal to zero. Of 57 files marked by the system as PHI, all contained both personally identifiable information and health information: 11 files were PHI disclosures, and 46 files contained organizational materials such as unfilled insurance forms, job applications by medical professionals, and essays. PHI can be successfully detected in free-form textual files exchanged through P2P networks. Once the files with PHI are detected, affected individuals or data custodians can be alerted to take remedial action.
22778598	Personalized health care system with virtual reality rehabilitation and appropriate information for seniors.
Sensors (Basel) 20120430 2012
The concept of the information society is now a common one, as opposed to the industrial society that dominated the economy during the last years. It is assumed that all sectors should have access to information and reap its benefits. Elderly people are, in this respect, a major challenge, due to their lack of interest in technological progress and their lack of knowledge regarding the potential benefits that information society technologies might have on their lives. The Naviga Project (An Open and Adaptable Platform for the Elderly and Persons with Disability to Access the Information Society) is a European effort, whose main goal is to design and develop a technological platform allowing elder people and persons with disability to access the internet and the information society. Naviga also allows the creation of services targeted to social networks, mind training and personalized health care. In this paper we focus on the health care and information services designed on the project, the technological platform developed and details of two representative elements, the virtual reality hand rehabilitation and the health information intelligent system.
22646023	OLSVis: an animated, interactive visual browser for bio-ontologies.
BMC Bioinformatics 20120710 2012
More than one million terms from biomedical ontologies and controlled vocabularies are available through the Ontology Lookup Service (OLS). Although OLS provides ample possibility for querying and browsing terms, the visualization of parts of the ontology graphs is rather limited and inflexible. We created the OLSVis web application, a visualiser for browsing all ontologies available in the OLS database. OLSVis shows customisable subgraphs of the OLS ontologies. Subgraphs are animated via a real-time force-based layout algorithm which is fully interactive: each time the user makes a change, e.g. browsing to a new term, hiding, adding, or dragging terms, the algorithm performs smooth and only essential reorganisations of the graph. This assures an optimal viewing experience, because subsequent screen layouts are not grossly altered, and users can easily navigate through the graph. URL: http://ols.wordvis.com The OLSVis web application provides a user-friendly tool to visualise ontologies from the OLS repository. It broadens the possibilities to investigate and select ontology subgraphs through a smooth visualisation method.
22533507	Identification of methicillin-resistant Staphylococcus aureus within the nation's Veterans Affairs medical centers using natural language processing.
BMC Med Inform Decis Mak 20120711 2012
Accurate information is needed to direct healthcare systems' efforts to control methicillin-resistant Staphylococcus aureus (MRSA). Assembling complete and correct microbiology data is vital to understanding and addressing the multiple drug-resistant organisms in our hospitals. Herein, we describe a system that securely gathers microbiology data from the Department of Veterans Affairs (VA) network of databases. Using natural language processing methods, we applied an information extraction process to extract organisms and susceptibilities from the free-text data. We then validated the extraction against independently derived electronic data and expert annotation. We estimate that the collected microbiology data are 98.5% complete and that methicillin-resistant Staphylococcus aureus was extracted accurately 99.7% of the time. Applying natural language processing methods to microbiology records appears to be a promising way to extract accurate and useful nosocomial pathogen surveillance data. Both scientific inquiry and the data's reliability will be dependent on the surveillance system's capability to compare from multiple sources and circumvent systematic error. The dataset constructed and methods used for this investigation could contribute to a comprehensive infectious disease surveillance system or other pressing needs.
22678953	Paperless medical physics QA in radiation therapy.
Australas Phys Eng Sci Med 20120608 2012Jun
Physics quality assurance (QA) is an integral part of a medical physicist's role in the radiotherapy centre. Management of physics QA documents is an issue with a long-term accumulation. Storage space, archive administration and paper consumption are just some of the difficulties faced by physicists. Plotting trends and drawing meaningful conclusions from these results can be challenging using traditional QA methods. Remote checking of QA within a hospital network can also be problematic. The aim of this project is introduce a paperless QA system that will provide solutions to many of these issues.
22190144	Improved encoding strategy for CPMG-based Bloch-Siegert B(1)(+) mapping.
Magn Reson Med 20111221 2012Aug
Bloch-Siegert (BS) based B(1)(+) mapping methods use off-resonant pulses to encode quantitative B(1)(+) information into the signal phase. It was recently shown that the principle behind BS-based B(1)(+) mapping can be expanded from spin echo (BS-SE) and gradient-echo (BS-FLASH) based BS B(1)(+) mapping to methods such as Carr, Purcell, Meiboom, Gill (CPMG)-based turbo-spin echo (BS-CPMG-TSE) and multi-spin echo (BS-CPMG-MSE) imaging. If CPMG conditions are preserved, BS-CPMG-TSE allows fast acquisition of the B(1)(+) information and BS-CPMG-MSE enables simultaneous mapping of B(1)(+), M(0), and T(2). To date, however, two separate MRI experiments must be performed to enable the calculation of B(1)(+) maps. This study investigated a modified encoding strategy for CPMG BS-based methods to overcome this limitation. By applying a "bipolar" off-resonant BS pulse before the refocusing pulse train, the needed phase information was able to be encoded into different echo images of one echo train. Thus, this technique allowed simultaneous B(1)(+) and T(2) mapping in a single BS-CPMG-MSE experiment. To allow single-shot B(1)(+) mapping, this method was also applied to turbo-spin echo imaging. Furthermore, the presented modification intrinsically minimizes phase-based image artifacts in BS-CPMG-TSE experiments.
22734722	Genome research in the cloud.
OMICS 20120626 2012 Jul-Aug
High-throughput genome research has long been associated with bioinformatics, as it assists genome sequencing and annotation projects. Along with databases, to store, properly manage, and retrieve biological data, a large number of computational tools have been developed to decode biological information from this data. However, with the advent of next-generation sequencing (NGS) technology the sequence data starts generating at a pace never before seen. Consequently researchers are facing a threat as they are experiencing a potential shortage of storage space and tools to analyze the data. Moreover, the voluminous data increases traffic in the network by uploading and downloading large data sets, and thus consume much of the network's available bandwidth. All of these obstacles have led to the solution in the form of cloud computing.
21399913	Intelligent personal health record: experience and open issues.
J Med Syst 20110312 2012Aug
Web-based personal health records (PHRs) are under massive deployment. To improve PHR's capability and usability, we previously proposed the concept of intelligent PHR (iPHR). By introducing and extending expert system technology and Web search technology into the PHR domain, iPHR can automatically provide users with personalized healthcare information to facilitate their daily activities of living. Our iPHR system currently provides three functions: guided search for disease information, recommendation of home nursing activities, and recommendation of home medical products. This paper discusses our experience with iPHR as well as the open issues, including both enhancements to the existing functions and potential new functions. We outline some preliminary solutions, whereas a main purpose of this paper is to stimulate future research work in the area of consumer health informatics.
22802077	CellMiner: a web-based suite of genomic and pharmacologic tools to explore transcript and drug patterns in the NCI-60 cell line set.
Cancer Res.  2012Jul15
High-throughput and high-content databases are increasingly important resources in molecular medicine, systems biology, and pharmacology. However, the information usually resides in unwieldy databases, limiting ready data analysis and integration. One resource that offers substantial potential for improvement in this regard is the NCI-60 cell line database compiled by the U.S. National Cancer Institute, which has been extensively characterized across numerous genomic and pharmacologic response platforms. In this report, we introduce a CellMiner (http://discover.nci.nih.gov/cellminer/) web application designed to improve the use of this extensive database. CellMiner tools allowed rapid data retrieval of transcripts for 22,379 genes and 360 microRNAs along with activity reports for 20,503 chemical compounds including 102 drugs approved by the U.S. Food and Drug Administration. Converting these differential levels into quantitative patterns across the NCI-60 clarified data organization and cross-comparisons using a novel pattern match tool. Data queries for potential relationships among parameters can be conducted in an iterative manner specific to user interests and expertise. Examples of the in silico discovery process afforded by CellMiner were provided for multidrug resistance analyses and doxorubicin activity; identification of colon-specific genes, microRNAs, and drugs; microRNAs related to the miR-17-92 cluster; and drug identification patterns matched to erlotinib, gefitinib, afatinib, and lapatinib. CellMiner greatly broadens applications of the extensive NCI-60 database for discovery by creating web-based processes that are rapid, flexible, and readily applied by users without bioinformatics expertise.
21256617	Efficient data management in a large-scale epidemiology research project.
Comput Methods Programs Biomed 20110121 2012Sep
This article describes the concept of a "Central Data Management" (CDM) and its implementation within the large-scale population-based medical research project "Personalized Medicine". The CDM can be summarized as a conjunction of data capturing, data integration, data storage, data refinement, and data transfer. A wide spectrum of reliable "Extract Transform Load" (ETL) software for automatic integration of data as well as "electronic Case Report Forms" (eCRFs) was developed, in order to integrate decentralized and heterogeneously captured data. Due to the high sensitivity of the captured data, high system resource availability, data privacy, data security and quality assurance are of utmost importance. A complex data model was developed and implemented using an Oracle database in high availability cluster mode in order to integrate different types of participant-related data. Intelligent data capturing and storage mechanisms are improving the quality of data. Data privacy is ensured by a multi-layered role/right system for access control and de-identification of identifying data. A well defined backup process prevents data loss. Over the period of one and a half year, the CDM has captured a wide variety of data in the magnitude of approximately 5terabytes without experiencing any critical incidents of system breakdown or loss of data. The aim of this article is to demonstrate one possible way of establishing a Central Data Management in large-scale medical and epidemiological studies.
22609187	EXP-PAC: providing comparative analysis and storage of next generation gene expression data.
Genomics 20120515 2012Jul
Microarrays and more recently RNA sequencing has led to an increase in available gene expression data. How to manage and store this data is becoming a key issue. In response we have developed EXP-PAC, a web based software package for storage, management and analysis of gene expression and sequence data. Unique to this package is SQL based querying of gene expression data sets, distributed normalization of raw gene expression data and analysis of gene expression data across experiments and species. This package has been populated with lactation data in the international milk genomic consortium web portal (http://milkgenomics.org/). Source code is also available which can be hosted on a Windows, Linux or Mac APACHE server connected to a private or public network (http://mamsap.it.deakin.edu.au/~pcc/Release/EXP_PAC.html).
22733677	Health information literacy in everyday life: a study of Finns aged 65-79 years.
Health Informatics J  2012Jun
This article examines the health information literacy of elderly Finns. The results are based on a survey conducted in January 2011. The questionnaire was distributed to 1000 persons that were randomly drawn from the Finnish Population Register. The respondents were aged 65-79 years (mean age 70 years) and lived in the Turku region in Finland. A total of 281 questionnaires (28%) were returned. χ(2) analyses were used to find possible relationships between demographic factors, as well as interest, seeking activity, current self-rated health and different dimensions of health information literacy, including needs, seeking and use of health-related information. Significant relationships were found between education level, interest in health information, seeking activity, self-rated current health and dimensions of health information literacy. Some categories of elderly people are more vulnerable regarding obtaining and use of health information: those with lower levels of education, those with poor health, and those who are not interested in and active at seeking information. For people who are found in any of these categories, it is important that available health-related information is understandable and can be accessed without too much effort-something that information providers should take into account.
22733678	Multifaceted determinants of online non-prescription drug information seeking and the impact on consumers' use of purchase channels.
Health Informatics J  2012Jun
The growing importance of the Internet as an information and purchasing channel is drawing widespread attention from marketing decision makers. Nevertheless, the relevance of the Internet to the so-called self-medication market in Germany has been paid barely enough attention. Our study aims to contribute insights concerning the penetration of the Internet in this market, as well as to give an overview of the critical determinants of Internet use for non-prescription drug information seeking, such as the accessibility of professional information, trust in health professionals' opinion and the ability to search online, as well as the perceived usefulness and credibility of online non-prescription drug information. Furthermore, we demonstrate that the preferred use of the Internet as a non-prescription drug information source positively influences the choice of unconventional purchase channels for non-prescription drugs and negatively affects the use of stationary pharmacies.
22733638	Impact of knowledge resources linked to an electronic health record on frequency of unnecessary tests and treatments.
J Contin Educ Health Prof  2012Spring
Electronic knowledge resources have the potential to rapidly provide answers to clinicians' questions. We sought to determine clinicians' reasons for searching these resources, the rate of finding relevant information, and the perceived clinical impact of the information they retrieved. We asked general internists, family physicians, and clinical nurse practitioners to complete the Information Assessment Method (IAM) survey after searching 1 of 2 electronic knowledge resources linked in the electronic health record. IAM stimulates reflection on the relevance, cognitive impact, use, and potential health outcomes of retrieved clinical information. Forty-two clinicians rated 502 searches (mean 12, range 1-48) and reported finding information 75% (n = 375) of the time. The most common reasons for searching were to address a clinical question (411, 82%) and for curiosity (75, 15%). In 68% of the rated searches (341), participants indicated they would use the retrieved information for at least 1 patient. In 31% (157) of rated searches, clinicians expected the retrieved information to benefit the patient by avoiding an unnecessary or inappropriate treatment, diagnostic procedure, or preventive intervention. Searches in electronic knowledge resources frequently yield relevant information that may benefit the patient by, for example, avoiding an inappropriate diagnostic procedure or treatment. Knowing that searches for answers to clinical questions can result in patient health benefits should intensify efforts to encourage clinicians to pursue answers to their questions.
22733641	Feasibility of a knowledge translation CME program: Courriels Cochrane.
J Contin Educ Health Prof  2012Spring
Systematic literature reviews provide best evidence, but are underused by clinicians. Thus, integrating Cochrane reviews into continuing medical education (CME) is challenging. We designed a pilot CME program where summaries of Cochrane reviews (Courriels Cochrane) were disseminated by e-mail. Program participants automatically received CME credit for each Courriel Cochrane they rated. The feasibility of this program is reported (delivery, participation, and participant evaluation). We recruited French-speaking physicians through the Canadian Medical Association. Program delivery and participation were documented. Participants rated the informational value of Courriels Cochrane using the Information Assessment Method (IAM), which documented their reflective learning (relevance, cognitive impact, use for a patient, expected health benefits). IAM responses were aggregated and analyzed. The program was delivered as planned. Thirty Courriels Cochrane were delivered to 985 physicians, and 127 (12.9%) completed at least one IAM questionnaire. Out of 1109 Courriels Cochrane ratings, 973 (87.7%) conta-ined 1 or more types of positive cognitive impact, while 835 (75.3%) were clinically relevant. Participants reported the use of information for a patient and expected health benefits in 595 (53.7%) and 569 (51.3%) ratings, respectively. Program delivery required partnering with 5 organizations. Participants valued Courriels Cochrane. IAM ratings documented their reflective learning. The aggregation of IAM ratings documented 3 levels of CME outcomes: participation, learning, and performance. This evaluation study demonstrates the feasibility of the Courriels Cochrane as an approach to further disseminate Cochrane systematic literature reviews to clinicians and document self-reported knowledge translation associated with Cochrane reviews.
22732953	[Linked Data as a tool in the nutrition domain].
Nutr Hosp  2012 Mar-Apr
Currently, there is a huge amount of information available on Internet that can neither be interpreted nor used by software agents. This fact poses a serious drawback to the potential of tools that deal with data on the current Web. Nevertheless, in recent times, advances in the domain of Semantic Web make possible the development of a new generation of smart applications capable of creating added-value services for the final user. This work shows the technical challenges that must be faced in the area of nutrition in order to transform one or several oldfashion sources of raw data into a web repository based on semantic technologies and linked with external and publicly available data on Internet. This approach makes possible for automatic tools to operate on the top of this information providing new functionalities highly interesting in the domain of public health, such as the automatic generation of menus for children or intelligent dietetic assistants, among others. This article explains the process to create such information support applying the guidelines of the Linked Data initiative and provides insights into the use of tools to make the most of this technology for its adoption in related use cases and environments.
22736059	IDPredictor: predict database links in biomedical database.
J Integr Bioinform 20120626 2012
Knowledge found in biomedical databases, in particular in Web information systems, is a major bioinformatics resource. In general, this biological knowledge is worldwide represented in a network of databases. These data is spread among thousands of databases, which overlap in content, but differ substantially with respect to content detail, interface, formats and data structure. To support a functional annotation of lab data, such as protein sequences, metabolites or DNA sequences as well as a semi-automated data exploration in information retrieval environments, an integrated view to databases is essential. Search engines have the potential of assisting in data retrieval from these structured sources, but fall short of providing a comprehensive knowledge except out of the interlinked databases. A prerequisite of supporting the concept of an integrated data view is to acquire insights into cross-references among database entities. This issue is being hampered by the fact, that only a fraction of all possible cross-references are explicitely tagged in the particular biomedical informations systems. In this work, we investigate to what extend an automated construction of an integrated data network is possible. We propose a method that predicts and extracts cross-references from multiple life science databases and possible referenced data targets. We study the retrieval quality of our method and report on first, promising results. The method is implemented as the tool IDPredictor, which is published under the DOI 10.5447/IPK/2012/4 and is freely available using the URL: http://dx.doi.org/10.5447/IPK/2012/4.
22706334	Efficiently mining protein interaction dependencies from large text corpora.
Integr Biol (Camb) 20120615 2012Jul
Biochemical research has yielded an extensive amount of information about dependencies between protein interactions, as generated by allosteric regulations, steric hindrance and other mechanisms. Collectively, this information is valuable for understanding large intracellular protein networks. However, this information is sparsely distributed among millions of publications and documented as freely styled text meant for manual reading. Here we develop a computational approach for extracting information about interaction dependencies from large numbers of publications. First, keyword-based tokenization reduces full papers to short strings, facilitating an efficient search for patterns that are likely to indicate descriptions of interaction dependencies. Sentences that match such patterns are extracted, thereby reducing the amount of text to be read by human curators. Application of this approach to the integrin adhesome network extracted from 59,933 papers 208 short statements, close to half of which indeed describe interaction dependencies. We visualize the obtained hypernetwork of dependencies and illustrate that these dependencies confine the feasible mechanisms of adhesion sites assembly and generate testable hypotheses about their switchability.
21968917	Exploring context and content links in social media: a latent space method.
IEEE Trans Pattern Anal Mach Intell  2012May
Social media networks contain both content and context-specific information. Most existing methods work with either of the two for the purpose of multimedia mining and retrieval. In reality, both content and context information are rich sources of information for mining, and the full power of mining and processing algorithms can be realized only with the use of a combination of the two. This paper proposes a new algorithm which mines both context and content links in social media networks to discover the underlying latent semantic space. This mapping of the multimedia objects into latent feature vectors enables the use of any off-the-shelf multimedia retrieval algorithms. Compared to the state-of-the-art latent methods in multimedia analysis, this algorithm effectively solves the problem of sparse context links by mining the geometric structure underlying the content links between multimedia objects. Specifically for multimedia annotation, we show that an effective algorithm can be developed to directly construct annotation models by simultaneously leveraging both context and content information based on latent structure between correlated semantic concepts. We conduct experiments on the Flickr data set, which contains user tags linked with images. We illustrate the advantages of our approach over the state-of-the-art multimedia retrieval techniques.
22745177	Analysis of high-throughput plant image data with the information system IAP.
J Integr Bioinform 20120629 2012
This work presents a sophisticated information system, the Integrated Analysis Platform (IAP), an approach supporting large-scale image analysis for different species and imaging systems. In its current form, IAP supports the investigation of Maize, Barley and Arabidopsis plants based on images obtained in different spectra.    Several components of the IAP system, which are described in this work, cover the complete end-to-end pipeline, starting with the image transfer from the imaging infrastructure, (grid distributed) image analysis, data management for raw data and analysis results, to the automated generation of experiment reports.
22520449	The selection of search sources influences the findings of a systematic review of people's views: a case study in public health.
BMC Med Res Methodol 20120420 2012
For systematic reviews providing evidence for policy decisions in specific geographical regions, there is a need to minimise regional bias when seeking out relevant research studies. Studies on people's views tend to be dispersed across a range of bibliographic databases and other search sources. It is recognised that a comprehensive literature search can provide unique evidence not found from a focused search; however, the geographical focus of databases as a potential source of bias on the findings of a research review is less clear. This case study describes search source selection for research about people's views and how supplementary searches designed to redress geographical bias influenced the findings of a systematic review. Our research questions are: a) what was the impact of search methods employed to redress potential database selection bias on the overall findings of the review? and b) how did each search source contribute to the identification of all the research studies included in the review? The contribution of 25 search sources in locating 28 studies included within a systematic review on UK children's views of body size, shape and weight was analysed retrospectively. The impact of utilising seven search sources chosen to identify UK-based literature on the review's findings was assessed. Over a sixth (5 out of 28) of the studies were located only through supplementary searches of three sources. These five studies were of a disproportionally high quality compared with the other studies in the review. The retrieval of these studies added direction, detail and strength to the overall findings of the review. All studies in the review were located within 21 search sources. Precision for 21 sources ranged from 0.21% to 1.64%. For reducing geographical bias and increasing the coverage and context-specificity of systematic reviews of people's perspectives and experiences, searching that is sensitive and aimed at reducing geographical bias in database sources is recommended.
22752950	Preparing a Minimum Information about a Flow Cytometry Experiment (MIFlowCyt) compliant manuscript using the International Society for Advancement of Cytometry (ISAC) FCS file repository (FlowRepository.org).
Curr Protoc Cytom  2012Jul
FlowRepository.org is a Web-based flow cytometry data repository provided by the International Society for Advancement of Cytometry (ISAC). It supports storage, annotation, analysis, and sharing of flow cytometry datasets. A fundamental tenet of scientific research is that published results should be open to independent validation and refutation. With FlowRepository, researchers can annotate their datasets in compliance with the Minimum Information about a Flow Cytometry Experiment (MIFlowCyt) standard, thus greatly facilitating third-party interpretation of their data. In this unit, we will mainly focus on the deposition, sharing, and annotation of flow cytometry data.
22753580	Thoughts on concept analysis: multiple approaches, one result.
Nurs Sci Q  2012Jul
In this essay, I share my thoughts about concept analysis. Emphasis is placed on a nursing conceptual model as the starting point to provide an intellectual context for the concept analysis. The difference between a concept and the application of the concept also is emphasized.
22759456	Overview of the ID, EPI and REL tasks of BioNLP Shared Task 2011.
BMC Bioinformatics 20120626 2012
We present the preparation, resources, results and analysis of three tasks of the BioNLP Shared Task 2011: the main tasks on Infectious Diseases (ID) and Epigenetics and Post-translational Modifications (EPI), and the supporting task on Entity Relations (REL). The two main tasks represent extensions of the event extraction model introduced in the BioNLP Shared Task 2009 (ST'09) to two new areas of biomedical scientific literature, each motivated by the needs of specific biocuration tasks. The ID task concerns the molecular mechanisms of infection, virulence and resistance, focusing in particular on the functions of a class of signaling systems that are ubiquitous in bacteria. The EPI task is dedicated to the extraction of statements regarding chemical modifications of DNA and proteins, with particular emphasis on changes relating to the epigenetic control of gene expression. By contrast to these two application-oriented main tasks, the REL task seeks to support extraction in general by separating challenges relating to part-of relations into a subproblem that can be addressed by independent systems. Seven groups participated in each of the two main tasks and four groups in the supporting task. The participating systems indicated advances in the capability of event extraction methods and demonstrated generalization in many aspects: from abstracts to full texts, from previously considered subdomains to new ones, and from the ST'09 extraction targets to other entities and events. The highest performance achieved in the supporting task REL, 58% F-score, is broadly comparable with levels reported for other relation extraction tasks. For the ID task, the highest-performing system achieved 56% F-score, comparable to the state-of-the-art performance at the established ST'09 task. In the EPI task, the best result was 53% F-score for the full set of extraction targets and 69% F-score for a reduced set of core extraction targets, approaching a level of performance sufficient for user-facing applications. In this study, we extend on previously reported results and perform further analyses of the outputs of the participating systems. We place specific emphasis on aspects of system performance relating to real-world applicability, considering alternate evaluation metrics and performing additional manual analysis of system outputs. We further demonstrate that the strengths of extraction systems can be combined to improve on the performance achieved by any system in isolation. The manually annotated corpora, supporting resources, and evaluation tools for all tasks are available from http://www.bionlp-st.org and the tasks continue as open challenges for all interested parties.
22759457	BioNLP Shared Task--The Bacteria Track.
BMC Bioinformatics 20120626 2012
We present the BioNLP 2011 Shared Task Bacteria Track, the first Information Extraction challenge entirely dedicated to bacteria. It includes three tasks that cover different levels of biological knowledge. The Bacteria Gene Renaming supporting task is aimed at extracting gene renaming and gene name synonymy in PubMed abstracts. The Bacteria Gene Interaction is a gene/protein interaction extraction task from individual sentences. The interactions have been categorized into ten different sub-types, thus giving a detailed account of genetic regulations at the molecular level. Finally, the Bacteria Biotopes task focuses on the localization and environment of bacteria mentioned in textbook articles. We describe the process of creation for the three corpora, including document acquisition and manual annotation, as well as the metrics used to evaluate the participants' submissions. Three teams submitted to the Bacteria Gene Renaming task; the best team achieved an F-score of 87%. For the Bacteria Gene Interaction task, the only participant's score had reached a global F-score of 77%, although the system efficiency varies significantly from one sub-type to another. Three teams submitted to the Bacteria Biotopes task with very different approaches; the best team achieved an F-score of 45%. However, the detailed study of the participating systems efficiency reveals the strengths and weaknesses of each participating system. The three tasks of the Bacteria Track offer participants a chance to address a wide range of issues in Information Extraction, including entity recognition, semantic typing and coreference resolution. We found common trends in the most efficient systems: the systematic use of syntactic dependencies and machine learning. Nevertheless, the originality of the Bacteria Biotopes task encouraged the use of interesting novel methods and techniques, such as term compositionality, scopes wider than the sentence.
22759459	Biomedical event extraction from abstracts and full papers using search-based structured prediction.
BMC Bioinformatics 20120626 2012
Biomedical event extraction has attracted substantial attention as it can assist researchers in understanding the plethora of interactions among genes that are described in publications in molecular biology. While most recent work has focused on abstracts, the BioNLP 2011 shared task evaluated the submitted systems on both abstracts and full papers. In this article, we describe our submission to the shared task which decomposes event extraction into a set of classification tasks that can be learned either independently or jointly using the search-based structured prediction framework. Our intention is to explore how these two learning paradigms compare in the context of the shared task. We report that models learned using search-based structured prediction exceed the accuracy of independently learned classifiers by 8.3 points in F-score, with the gains being more pronounced on the more complex Regulation events (13.23 points). Furthermore, we show how the trade-off between recall and precision can be adjusted in both learning paradigms and that search-based structured prediction achieves better recall at all precision points. Finally, we report on experiments with a simple domain-adaptation method, resulting in the second-best performance achieved by a single system. We demonstrate that joint inference using the search-based structured prediction framework can achieve better performance than independently learned classifiers, thus demonstrating the potential of this learning paradigm for event extraction and other similarly complex information-extraction tasks.
22759462	Event extraction of bacteria biotopes: a knowledge-intensive NLP-based approach.
BMC Bioinformatics 20120626 2012
Bacteria biotopes cover a wide range of diverse habitats including animal and plant hosts, natural, medical and industrial environments. The high volume of publications in the microbiology domain provides a rich source of up-to-date information on bacteria biotopes. This information, as found in scientific articles, is expressed in natural language and is rarely available in a structured format, such as a database. This information is of great importance for fundamental research and microbiology applications (e.g., medicine, agronomy, food, bioenergy). The automatic extraction of this information from texts will provide a great benefit to the field. We present a new method for extracting relationships between bacteria and their locations using the Alvis framework. Recognition of bacteria and their locations was achieved using a pattern-based approach and domain lexical resources. For the detection of environment locations, we propose a new approach that combines lexical information and the syntactic-semantic analysis of corpus terms to overcome the incompleteness of lexical resources. Bacteria location relations extend over sentence borders, and we developed domain-specific rules for dealing with bacteria anaphors. We participated in the BioNLP 2011 Bacteria Biotope (BB) task with the Alvis system. Official evaluation results show that it achieves the best performance of participating systems. New developments since then have increased the F-score by 4.1 points. We have shown that the combination of semantic analysis and domain-adapted resources is both effective and efficient for event information extraction in the bacteria biotope domain. We plan to adapt the method to deal with a larger set of location types and a large-scale scientific article corpus to enable microbiologists to integrate and use the extracted knowledge in combination with experimental data.
22759463	Combining joint models for biomedical event extraction.
BMC Bioinformatics 20120626 2012
We explore techniques for performing model combination between the UMass and Stanford biomedical event extraction systems. Both sub-components address event extraction as a structured prediction problem, and use dual decomposition (UMass) and parsing algorithms (Stanford) to find the best scoring event structure. Our primary focus is on stacking where the predictions from the Stanford system are used as features in the UMass system. For comparison, we look at simpler model combination techniques such as intersection and union which require only the outputs from each system and combine them directly. First, we find that stacking substantially improves performance while intersection and union provide no significant benefits. Second, we investigate the graph properties of event structures and their impact on the combination of our systems. Finally, we trace the origins of events proposed by the stacked model to determine the role each system plays in different components of the output. We learn that, while stacking can propose novel event structures not seen in either base model, these events have extremely low precision. Removing these novel events improves our already state-of-the-art F1 to 56.6% on the test set of Genia (Task 1). Overall, the combined system formed via stacking ("FAUST") performed well in the BioNLP 2011 shared task. The FAUST system obtained 1st place in three out of four tasks: 1st place in Genia Task 1 (56.0% F1) and Task 2 (53.9%), 2nd place in the Epigenetics and Post-translational Modifications track (35.0%), and 1st place in the Infectious Diseases track (55.6%). We present a state-of-the-art event extraction system that relies on the strengths of structured prediction and model combination through stacking. Akin to results on other tasks, stacking outperforms intersection and union and leads to very strong results. The utility of model combination hinges on complementary views of the data, and we show that our sub-systems capture different graph properties of event structures. Finally, by removing low precision novel events, we show that performance from stacking can be further improved.
22759611	A LDA-based approach to promoting ranking diversity for genomics information retrieval.
BMC Genomics 20120611 2012
In the biomedical domain, there are immense data and tremendous increase of genomics and biomedical relevant publications. The wealth of information has led to an increasing amount of interest in and need for applying information retrieval techniques to access the scientific literature in genomics and related biomedical disciplines. In many cases, the desired information of a query asked by biologists is a list of a certain type of entities covering different aspects that are related to the question, such as cells, genes, diseases, proteins, mutations, etc. Hence, it is important of a biomedical IR system to be able to provide relevant and diverse answers to fulfill biologists' information needs. However traditional IR model only concerns with the relevance between retrieved documents and user query, but does not take redundancy between retrieved documents into account. This will lead to high redundancy and low diversity in the retrieval ranked lists. In this paper, we propose an approach which employs a topic generative model called Latent Dirichlet Allocation (LDA) to promoting ranking diversity for biomedical information retrieval. Different from other approaches or models which consider aspects on word level, our approach assumes that aspects should be identified by the topics of retrieved documents. We present LDA model to discover topic distribution of retrieval passages and word distribution of each topic dimension, and then re-rank retrieval results with topic distribution similarity between passages based on N-size slide window. We perform our approach on TREC 2007 Genomics collection and two distinctive IR baseline runs, which can achieve 8% improvement over the highest Aspect MAP reported in TREC 2007 Genomics track. The proposed method is the first study of adopting topic model to genomics information retrieval, and demonstrates its effectiveness in promoting ranking diversity as well as in improving relevance of ranked lists of genomics search. Moreover, we proposes a distance measure to quantify how much a passage can increase topical diversity by considering both topical importance and topical coefficient by LDA, and the distance measure is a modified Euclidean distance.
22759613	A natural language interface plug-in for cooperative query answering in biological databases.
BMC Genomics 20120611 2012
One of the many unique features of biological databases is that the mere existence of a ground data item is not always a precondition for a query response. It may be argued that from a biologist's standpoint, queries are not always best posed using a structured language. By this we mean that approximate and flexible responses to natural language like queries are well suited for this domain. This is partly due to biologists' tendency to seek simpler interfaces and partly due to the fact that questions in biology involve high level concepts that are open to interpretations computed using sophisticated tools. In such highly interpretive environments, rigidly structured databases do not always perform well. In this paper, our goal is to propose a semantic correspondence plug-in to aid natural language query processing over arbitrary biological database schema with an aim to providing cooperative responses to queries tailored to users' interpretations. Natural language interfaces for databases are generally effective when they are tuned to the underlying database schema and its semantics. Therefore, changes in database schema become impossible to support, or a substantial reorganization cost must be absorbed to reflect any change. We leverage developments in natural language parsing, rule languages and ontologies, and data integration technologies to assemble a prototype query processor that is able to transform a natural language query into a semantically equivalent structured query over the database. We allow knowledge rules and their frequent modifications as part of the underlying database schema. The approach we adopt in our plug-in overcomes some of the serious limitations of many contemporary natural language interfaces, including support for schema modifications and independence from underlying database schema. The plug-in introduced in this paper is generic and facilitates connecting user selected natural language interfaces to arbitrary databases using a semantic description of the intended application. We demonstrate the feasibility of our approach with a practical example.
22759614	Hypotheses generation as supervised link discovery with automated class labeling on large-scale biomedical concept networks.
BMC Genomics 20120611 2012
Computational approaches to generate hypotheses from biomedical literature have been studied intensively in recent years. Nevertheless, it still remains a challenge to automatically discover novel, cross-silo biomedical hypotheses from large-scale literature repositories. In order to address this challenge, we first model a biomedical literature repository as a comprehensive network of biomedical concepts and formulate hypotheses generation as a process of link discovery on the concept network. We extract the relevant information from the biomedical literature corpus and generate a concept network and concept-author map on a cluster using Map-Reduce frame-work. We extract a set of heterogeneous features such as random walk based features, neighborhood features and common author features. The potential number of links to consider for the possibility of link discovery is large in our concept network and to address the scalability problem, the features from a concept network are extracted using a cluster with Map-Reduce framework. We further model link discovery as a classification problem carried out on a training data set automatically extracted from two network snapshots taken in two consecutive time duration. A set of heterogeneous features, which cover both topological and semantic features derived from the concept network, have been studied with respect to their impacts on the accuracy of the proposed supervised link discovery process. A case study of hypotheses generation based on the proposed method has been presented in the paper.
22759617	Discriminative application of string similarity methods to chemical and non-chemical names for biomedical abbreviation clustering.
BMC Genomics 20120611 2012
Term clustering, by measuring the string similarities between terms, is known within the natural language processing community to be an effective method for improving the quality of texts and dictionaries. However, we have observed that chemical names are difficult to cluster using string similarity measures. In order to clearly demonstrate this difficulty, we compared the string similarities determined using the edit distance, the Monge-Elkan score, SoftTFIDF, and the bigram Dice coefficient for chemical names with those for non-chemical names. Our experimental results revealed the following: (1) The edit distance had the best performance in the matching of full forms, whereas Cohen et al. reported that SoftTFIDF with the Jaro-Winkler distance would yield the best measure for matching pairs of terms for their experiments. (2) For each of the string similarity measures above, the best threshold for term matching differs for chemical names and for non-chemical names; the difference is especially large for the edit distance. (3) Although the matching results obtained for chemical names using the edit distance, Monge-Elkan scores, or the bigram Dice coefficients are better than the result obtained for non-chemical names, the results were contrary when using SoftTFIDF. (4) A suitable weight for chemical names varies substantially from one for non-chemical names. In particular, a weight vector that has been optimized for non-chemical names is not suitable for chemical names. (5) The matching results using the edit distances improve further by dividing a set of full forms into two subsets, according to whether a full form is a chemical name or not. These results show that our hypothesis is acceptable, and that we can significantly improve the performance of abbreviation-full form clustering by computing chemical names and non-chemical names separately. In conclusion, the discriminative application of string similarity methods to chemical and non-chemical names may be a simple yet effective way to improve the performance of term clustering.
22193754	Automatic medical image annotation and keyword-based image retrieval using relevance feedback.
J Digit Imaging  2012Aug
This paper presents novel multiple keywords annotation for medical images, keyword-based medical image retrieval, and relevance feedback method for image retrieval for enhancing image retrieval performance. For semantic keyword annotation, this study proposes a novel medical image classification method combining local wavelet-based center symmetric-local binary patterns with random forests. For keyword-based image retrieval, our retrieval system use the confidence score that is assigned to each annotated keyword by combining probabilities of random forests with predefined body relation graph. To overcome the limitation of keyword-based image retrieval, we combine our image retrieval system with relevance feedback mechanism based on visual feature and pattern classifier. Compared with other annotation and relevance feedback algorithms, the proposed method shows both improved annotation performance and accurate retrieval results.
22193756	PACS bypass: a semi-automated routing solution to enable filmless operations when PACS fails.
J Digit Imaging  2012Aug
In the filmless imaging department, an integrated imaging and reporting system is only as strong as its weakest link. An outage or downtime of a key segment, such as the Picture Archive Communications System (PACS), is a significant threat to efficient workflow, quality of image interpretation, ordering clinician's review, and ultimately patient care. A multidisciplinary team (including physicists, technologists, radiologists, operations, and IT) developed a backup system to provide business continuity (i.e., quality control, interpretation, reporting, and clinician access) during an extended outage of the main departmental PACS.
22349993	Information from Searching Content with an Ontology-Utilizing Toolkit (iSCOUT).
J Digit Imaging  2012Aug
Radiology reports are permanent legal documents that serve as official interpretation of imaging tests. Manual analysis of textual information contained in these reports requires significant time and effort. This study describes the development and initial evaluation of a toolkit that enables automated identification of relevant information from within these largely unstructured text reports. We developed and made publicly available a natural language processing toolkit, Information from Searching Content with an Ontology-Utilizing Toolkit (iSCOUT). Core functions are included in the following modules: the Data Loader, Header Extractor, Terminology Interface, Reviewer, and Analyzer. The toolkit enables search for specific terms and retrieval of (radiology) reports containing exact term matches as well as similar or synonymous term matches within the text of the report. The Terminology Interface is the main component of the toolkit. It allows query expansion based on synonyms from a controlled terminology (e.g., RadLex or National Cancer Institute Thesaurus [NCIT]). We evaluated iSCOUT document retrieval of radiology reports that contained liver cysts, and compared precision and recall with and without using NCIT synonyms for query expansion. iSCOUT retrieved radiology reports with documented liver cysts with a precision of 0.92 and recall of 0.96, utilizing NCIT. This recall (i.e., utilizing the Terminology Interface) is significantly better than using each of two search terms alone (0.72, p=0.03 for liver cyst and 0.52, p=0.0002 for hepatic cyst). iSCOUT reliably assembled relevant radiology reports for a cohort of patients with liver cysts with significant improvement in document retrieval when utilizing controlled lexicons.
22692771	A software and hardware architecture for a high-availability PACS.
J Digit Imaging  2012Aug
Increasing radiology studies has led to the emergence of new requirements for management medical information, mainly affecting the storage of digital images. Today, it is a necessary interaction between workflow management and legal rules that govern it, to allow an efficient control of medical technology and associated costs. Another important topic that is growing in importance within the healthcare sector is compliance, which includes the retention of studies, information security, and patient privacy. Previously, we conducted a series of extensive analysis and measurements of pre-existing operating conditions. These studies and projects have been described in other papers. The first phase: hardware and software installation and initial tests were completed in March 2006. The storage phase was built step by step until the PACS-INR was totally completed. Two important aspects were considered in the integration of components: (1) the reliability and performance of the system to transfer and display DICOM images, and (2) the availability of data backups for disaster recovery and downtime scenarios. This paper describes the high-availability model for a large-scale PACS to support the storage and retrieve of data using CAS and DAS technologies to provide an open storage platform. This solution offers a simple framework that integrates and automates the information at low cost and minimum risk. Likewise, the model allows an optimized use of the information infrastructure in the clinical environment. The tests of the model include massive data migration, openness, scalability, and standard compatibility to avoid locking data into a proprietary technology.
22498704	Closing the loop for memory prosthesis: detecting the role of hippocampal neural ensembles using nonlinear models.
IEEE Trans Neural Syst Rehabil Eng 20120406 2012Jul
A major factor involved in providing closed loop feedback for control of neural function is to understand how neural ensembles encode online information critical to the final behavioral endpoint. This issue was directly assessed in rats performing a short-term delay memory task in which successful encoding of task information is dependent upon specific spatio-temporal firing patterns recorded from ensembles of CA3 and CA1 hippocampal neurons. Such patterns, extracted by a specially designed nonlinear multi-input multi-output (MIMO) nonlinear mathematical model, were used to predict successful performance online via a closed loop paradigm which regulated trial difficulty (time of retention) as a function of the "strength" of stimulus encoding. The significance of the MIMO model as a neural prosthesis has been demonstrated by substituting trains of electrical stimulation pulses to mimic these same ensemble firing patterns. This feature was used repeatedly to vary "normal" encoding as a means of understanding how neural ensembles can be "tuned" to mimic the inherent process of selecting codes of different strength and functional specificity. The capacity to enhance and tune hippocampal encoding via MIMO model detection and insertion of critical ensemble firing patterns shown here provides the basis for possible extension to other disrupted brain circuitry.
22773159	Knowledge enrichment analysis for human tissue-specific genes uncover new biological insights.
J Integr Bioinform 20120709 2012
The expression and regulation of genes in different tissues are fundamental questions to be answered in biology. Knowledge enrichment analysis for tissue specific (TS) and housekeeping (HK) genes may help identify their roles in biological process or diseases and gain new biological insights. In this paper, we performed the knowledge enrichment analysis for 17,343 genes in 84 human tissues using Gene Set Enrichment Analysis (GSEA) and Hypergeometric Analysis (HA) against three biological ontologies: Gene Ontology (GO), KEGG pathways and Disease Ontology (DO) respectively. The analyses results demonstrated that the functions of most gene groups are consistent with their tissue origins. Meanwhile three interesting new associations for HK genes and the skeletal muscle tissue genes are found. Firstly, Hypergeometric analysis against KEGG database for HK genes disclosed that three disease terms (Parkinson's disease, Huntington's disease, Alzheimer's disease) are intensively enriched. Secondly, Hypergeometric analysis against the KEGG database for Skeletal Muscle tissue genes shows that two cardiac diseases of "Hypertrophic cardiomyopathy (HCM)" and "Arrhythmogenic right ventricular cardiomyopathy (ARVC)" are heavily enriched, which are also considered as no relationship with skeletal functions. Thirdly, "Prostate cancer" is intensively enriched in Hypergeometric analysis against the disease ontology (DO) for the Skeletal Muscle tissue genes, which is a much unexpected phenomenon.
22693047	Sensitivity and predictive value of 15 PubMed search strategies to answer clinical questions rated against full systematic reviews.
J. Med. Internet Res. 20120612 2012
Clinicians perform searches in PubMed daily, but retrieving relevant studies is challenging due to the rapid expansion of medical knowledge. Little is known about the performance of search strategies when they are applied to answer specific clinical questions. To compare the performance of 15 PubMed search strategies in retrieving relevant clinical trials on therapeutic interventions. We used Cochrane systematic reviews to identify relevant trials for 30 clinical questions. Search terms were extracted from the abstract using a predefined procedure based on the population, interventions, comparison, outcomes (PICO) framework and combined into queries. We tested 15 search strategies that varied in their query (PIC or PICO), use of PubMed's Clinical Queries therapeutic filters (broad or narrow), search limits, and PubMed links to related articles. We assessed sensitivity (recall) and positive predictive value (precision) of each strategy on the first 2 PubMed pages (40 articles) and on the complete search output. The performance of the search strategies varied widely according to the clinical question. Unfiltered searches and those using the broad filter of Clinical Queries produced large outputs and retrieved few relevant articles within the first 2 pages, resulting in a median sensitivity of only 10%-25%. In contrast, all searches using the narrow filter performed significantly better, with a median sensitivity of about 50% (all P &lt; .001 compared with unfiltered queries) and positive predictive values of 20%-30% (P &lt; .001 compared with unfiltered queries). This benefit was consistent for most clinical questions. Searches based on related articles retrieved about a third of the relevant studies. The Clinical Queries narrow filter, along with well-formulated queries based on the PICO framework, provided the greatest aid in retrieving relevant clinical trials within the 2 first PubMed pages. These results can help clinicians apply effective strategies to answer their questions at the point of care.
22213521	Use of simulated annealing for the design of multiple repetition time balanced steady-state free precession imaging.
Magn Reson Med 20111228 2012Jul
Balanced steady-state free precession is an ultrafast sequence with high signal-to-noise efficiency, but it also generates a strong fat signal which can mask important features. One method of fat suppression is to modify the balanced steady-state free precession spectrum using multiple repetition times to create a wide stopband over the fat frequency. However, with three or more pulse repetition times, the number of parameters creates a vast search space with many local minima of a cost function. We report on the initial results of using simulated annealing to find optimal sequences for two applications of multiple-pulse repetition time balanced steady-state free precession: positive contrast imaging and fat suppression.
22692258	Building the informatics infrastructure for comparative effectiveness research (CER): a review of the literature.
Med Care  2012Jul
Technological advances in clinical informatics have made large amounts of data accessible and potentially useful for research. As a result, a burgeoning literature addresses efforts to bridge the fields of health services research and biomedical informatics. The Electronic Data Methods Forum review examines peer-reviewed literature at the intersection of comparative effectiveness research and clinical informatics. The authors are specifically interested in characterizing this literature and identifying cross-cutting themes and gaps in the literature. A 3-step systematic literature search was conducted, including a structured search of PubMed, manual reviews of articles from selected publication lists, and manual reviews of research activities based on prospective electronic clinical data. Two thousand four hundred thirty-five citations were identified as potentially relevant. Ultimately, a full-text review was performed for 147 peer-reviewed papers. One hundred thirty-two articles were selected for inclusion in the review. Of these, 88 articles are the focus of the discussion in this paper. Three types of articles were identified, including papers that: (1) provide historical context or frameworks for using clinical informatics for research, (2) describe platforms and projects, and (3) discuss issues, challenges, and applications of natural language processing. In addition, 2 cross-cutting themes emerged: the challenges of conducting research in the absence of standardized ontologies and data collection; and unique data governance concerns related to the transfer, storage, deidentification, and access to electronic clinical data. Finally, the authors identified several current gaps on important topics such as the use of clinical informatics for cohort identification, cloud computing, and single point access to research data.
22693437	Exploring massive, genome scale datasets with the GenometriCorr package.
PLoS Comput. Biol. 20120531 2012May
We have created a statistically grounded tool for determining the correlation of genomewide data with other datasets or known biological features, intended to guide biological exploration of high-dimensional datasets, rather than providing immediate answers. The software enables several biologically motivated approaches to these data and here we describe the rationale and implementation for each approach. Our models and statistics are implemented in an R package that efficiently calculates the spatial correlation between two sets of genomic intervals (data and/or annotated features), for use as a metric of functional interaction. The software handles any type of pointwise or interval data and instead of running analyses with predefined metrics, it computes the significance and direction of several types of spatial association; this is intended to suggest potentially relevant relationships between the datasets. The package, GenometriCorr, can be freely downloaded at http://genometricorr.sourceforge.net/. Installation guidelines and examples are available from the sourceforge repository. The package is pending submission to Bioconductor.
22334528	Liver fibrosis: an intravoxel incoherent motion (IVIM) study.
J Magn Reson Imaging 20120214 2012Jul
To characterize longitudinal changes in molecular water diffusion, blood microcirculation, and their contributions to the apparent diffusion changes using intravoxel incoherent motion (IVIM) analysis in an experimental mouse model of liver fibrosis. Liver fibrosis was induced in male adult C57BL/6N mice (22-25 g; n = 12) by repetitive dosing of carbon tetrachloride (CCl(4) ). The respiratory-gated diffusion-weighted (DW) images were acquired using single-shot spin-echo EPI (SE-EPI) with 8 b-values and single diffusion gradient direction. True diffusion coefficient (D(true) ), blood pseudodiffusion coefficient (D(pseudo) ), and perfusion fraction (P(fraction) ) were measured. Diffusion tensor imaging (DTI) was also performed for comparison. Histology was performed with hematoxylin-eosin and Masson's trichrome staining. A significant decrease in D(true) was found at 2 weeks and 4 weeks following CCl(4) insult, as compared with that before insult. Similarly, D(pseudo) values before injury was significantly higher than those at 2 weeks and 4 weeks after CCl(4) insult. Meanwhile, P(fraction) values showed no significant differences over different timepoints. For DTI, significant decrease in ADC was observed following CCl(4) administration. Fractional anisotropy at 2 weeks after CCl(4) insult was significantly lower than that before insult, and subsequently normalized at 4 weeks after the insult. Liver histology showed collagen deposition, the presence of intracellular fat vacuoles, and cell necrosis/apoptosis in livers with CCl(4) insult. Both molecular water diffusion and blood microcirculation contribute to the alteration in apparent diffusion changes in liver fibrosis. Reduction in D(true) and D(pseudo) values resulted from diffusion and perfusion changes, respectively, during the progression of liver fibrosis. IVIM analysis may serve as valuable and robust tool in detecting and characterizing liver fibrosis at early stages, monitoring its progression in a noninvasive manner.
22695686	What do web-use skill differences imply for online health information searches?
J. Med. Internet Res. 20120613 2012
Online health information is of variable and often low scientific quality. In particular, elderly less-educated populations are said to struggle in accessing quality online information (digital divide). Little is known about (1) how their online behavior differs from that of younger, more-educated, and more-frequent Web users, and (2) how the older population may be supported in accessing good-quality online health information. To specify the digital divide between skilled and less-skilled Web users, we assessed qualitative differences in technical skills, cognitive strategies, and attitudes toward online health information. Based on these findings, we identified educational and technological interventions to help Web users find and access good-quality online health information. We asked 22 native German-speaking adults to search for health information online. The skilled cohort consisted of 10 participants who were younger than 30 years of age, had a higher level of education, and were more experienced using the Web than 12 participants in the less-skilled cohort, who were at least 50 years of age. We observed online health information searches to specify differences in technical skills and analyzed concurrent verbal protocols to identify health information seekers' cognitive strategies and attitudes. Our main findings relate to (1) attitudes: health information seekers in both cohorts doubted the quality of information retrieved online; among poorly skilled seekers, this was mainly because they doubted their skills to navigate vast amounts of information; once a website was accessed, quality concerns disappeared in both cohorts, (2) technical skills: skilled Web users effectively filtered information according to search intentions and data sources; less-skilled users were easily distracted by unrelated information, and (3) cognitive strategies: skilled Web users searched to inform themselves; less-skilled users searched to confirm their health-related opinions such as "vaccinations are harmful." Independent of Web-use skills, most participants stopped a search once they had found the first piece of evidence satisfying search intentions, rather than according to quality criteria. Findings related to Web-use skills differences suggest two classes of interventions to facilitate access to good-quality online health information. Challenges related to findings (1) and (2) should be remedied by improving people's basic Web-use skills. In particular, Web users should be taught how to avoid information overload by generating specific search terms and to avoid low-quality information by requesting results from trusted websites only. Problems related to finding (3) may be remedied by visually labeling search engine results according to quality criteria.
22700311	Using the KEGG database resource.
Curr Protoc Bioinformatics  2012Jun
KEGG (Kyoto Encyclopedia of Genes and Genomes) is a bioinformatics resource for understanding the functions and utilities of cells and organisms from both high-level and genomic perspectives. It is a self-sufficient, integrated resource consisting of genomic, chemical, and network information, with cross-references to numerous outside databases. The genomic and chemical information is a complete set of building blocks (genes and molecules) and the network information includes molecular wiring diagrams (interaction/reaction networks) and hierarchical classifications (relation networks) to represent high-level functions. This unit describes protocols for using KEGG, focusing on molecular network information in KEGG PATHWAY, KEGG BRITE, and KEGG MODULE, perturbed molecular networks in KEGG DISEASE and KEGG DRUG, molecular building block information in KEGG GENES and KEGG LIGAND, and a mechanism for linking genomes to molecular networks in KEGG ORTHOLOGY (KO). All of these many protocols enable the user to take advantage of the full breadth of the functionality provided by KEGG.
22577953	Web tools for predictive toxicology model building.
Expert Opin Drug Metab Toxicol 20120512 2012Jul
The development and use of web tools in chemistry has accumulated more than 15 years of history already. Powered by the advances in the Internet technologies, the current generation of web systems are starting to expand into areas, traditional for desktop applications. The web platforms integrate data storage, cheminformatics and data analysis tools. The ease of use and the collaborative potential of the web is compelling, despite the challenges. The topic of this review is a set of recently published web tools that facilitate predictive toxicology model building. The focus is on software platforms, offering web access to chemical structure-based methods, although some of the frameworks could also provide bioinformatics or hybrid data analysis functionalities. A number of historical and current developments are cited. In order to provide comparable assessment, the following characteristics are considered: support for workflows, descriptor calculations, visualization, modeling algorithms, data management and data sharing capabilities, availability of GUI or programmatic access and implementation details. The success of the Web is largely due to its highly decentralized, yet sufficiently interoperable model for information access. The expected future convergence between cheminformatics and bioinformatics databases provides new challenges toward management and analysis of large data sets. The web tools in predictive toxicology will likely continue to evolve toward the right mix of flexibility, performance, scalability, interoperability, sets of unique features offered, friendly user interfaces, programmatic access for advanced users, platform independence, results reproducibility, curation and crowdsourcing utilities, collaborative sharing and secure access.
22627698	Extracting biological information with computational analysis of Fourier-transform infrared (FTIR) biospectroscopy datasets: current practices to future perspectives.
Analyst 20120525 2012Jul21
Applying Fourier-transform infrared (FTIR) spectroscopy (or related technologies such as Raman spectroscopy) to biological questions (defined as biospectroscopy) is relatively novel. Potential fields of application include cytological, histological and microbial studies. This potentially provides a rapid and non-destructive approach to clinical diagnosis. Its increase in application is primarily a consequence of developing instrumentation along with computational techniques. In the coming decades, biospectroscopy is likely to become a common tool in the screening or diagnostic laboratory, or even in the general practitioner's clinic. Despite many advances in the biological application of FTIR spectroscopy, there remain challenges in sample preparation, instrumentation and data handling. We focus on the latter, where we identify in the reviewed literature, the existence of four main study goals: Pattern Finding; Biomarker Identification; Imaging; and, Diagnosis. These can be grouped into two frameworks: Exploratory; and, Diagnostic. Existing techniques in Quality Control, Pre-processing, Feature Extraction, Clustering, and Classification are critically reviewed. An aspect that is often visited is that of method choice. Based on the state-of-art, we claim that in the near future research should be focused on the challenges of dataset standardization; building information systems; development and validation of data analysis tools; and, technology transfer. A diagnostic case study using a real-world dataset is presented as an illustration. Many of the methods presented in this review are Machine Learning and Statistical techniques that are extendable to other forms of computer-based biomedical analysis, including mass spectrometry and magnetic resonance.
22410333	Comment on "Cheating prevention in visual cryptography".
IEEE Trans Image Process 20120306 2012Jul
Visual cryptography (VC), proposed by Naor and Shamir, has numerous applications, including visual authentication and identification, steganography, and image encryption. In 2006, Horng  showed that cheating is possible in VC, where some participants can deceive the remaining participants by forged transparencies. Since then, designing cheating-prevention visual secret-sharing (CPVSS) schemes has been studied by many researchers. In this paper, we cryptanalyze the Hu-Tzeng CPVSS scheme and show that it is not cheating immune. We also outline an improvement that helps to overcome the problem.
22709736	Inter-observer agreement on a checklist to evaluate scientific publications in the field of animal reproduction.
J Vet Med Educ  2012Summer
This study's objective was to determine respondents' inter-observer agreement on a detailed checklist to evaluate three exemplars (one case report, one randomized controlled study without blinding, and one blinded, randomized controlled study) of the scientific literature in the field of bovine reproduction. Fourteen international scientists in the field of animal reproduction were provided with the three articles, three copies of the checklist, and a supplementary explanation. Overall, 13 responded to more than 90% of the items. Overall repeatability between respondents using Fleiss's κ was 0.35 (fair agreement). Combining the "strongly agree" and "agree" responses and the "strongly disagree" and "disagree" responses increased κ to 0.49 (moderate agreement). Evaluation of information given in the three articles on housing of the animals (35% identical answers) and preconditions or pretreatments (42%) varied widely. Even though the overall repeatability was fair, repeatability concerning the important categories was high (e.g., level of agreement=98%). Our data show that the checklist is a reasonable and practical supporting tool to assess the quality of publications. Therefore, it may be used in teaching and practicing evidence-based veterinary medicine. It can support training in systematic and critical appraisal of information and in clinical decision making.
22712416	SURVIS: a fully-automated aerial baiting system for the distribution of vaccine baits for wildlife.
Berl. Munch. Tierarztl. Wochenschr.  2012 May-Jun
Large-scale oral vaccination of wildlife against rabies using aerial bait distribution has been successfully used to control terrestrial wildlife rabies in Europe and North America. A technical milestone to large-scale oral rabies vaccination campaigns in Europe was the development of fully-automated, computer-supported and cost-efficient technology for aerial distribution of baits like the SURVIS -system. Each bait released is recorded by the control unit through a sensor, with the exact location, time and date of release and subsequently the collected data can be evaluated, e.g. in GIS programmes. Thus, bait delivery systems like SURVIS are an important management tool for flight services and the responsible authorities for the optimization and evaluation of oral vaccination campaigns of wildlife against rabies or the control of other relevant wildlife diseases targeted by oral baits.
22353420	Replacing paper data collection forms with electronic data entry in the field: findings from a study of community-acquired bloodstream infections in Pemba, Zanzibar.
BMC Res Notes 20120221 2012
Entering data on case report forms and subsequently digitizing them in electronic media is the traditional way to maintain a record keeping system in field studies. Direct data entry using an electronic device avoids this two-step process. It is gaining in popularity and has replaced the paper-based data entry system in many studies. We report our experiences with paper- and PDA-based data collection during a fever surveillance study in Pemba Island, Zanzibar, Tanzania. Data were collected on a 14-page case report paper form in the first period of the study. The case report paper forms were then replaced with handheld computers (personal digital assistants or PDAs). The PDAs were used for screening and clinical data collection, including a rapid assessment of patient eligibility, real time errors, and inconsistency checking. A comparison of paper-based data collection with PDA data collection showed that direct data entry via PDA was faster and 25% cheaper. Data was more accurate (7% versus 1% erroneous data) and omission did not occur with electronic data collection. Delayed data turnaround times and late error detections in the paper-based system which made error corrections difficult were avoided using electronic data collection. Electronic data collection offers direct data entry at the initial point of contact. It has numerous advantages and has the potential to replace paper-based data collection in the field. The availability of information and communication technologies for direct data transfer has the potential to improve the conduct of public health research in resource-poor settings.
22714185	Double images encryption method with resistance against the specific attack based on an asymmetric algorithm.
Opt Express  2012May21
A double-image encryption technique that based on an asymmetric algorithm is proposed. In this method, the encryption process is different from the decryption and the encrypting keys are also different from the decrypting keys. In the nonlinear encryption process, the images are encoded into an amplitude cyphertext, and two phase-only masks (POMs) generated based on phase truncation are kept as keys for decryption. By using the classical double random phase encoding (DRPE) system, the primary images can be collected by an intensity detector that located at the output plane. Three random POMs that applied in the asymmetric encryption can be safely applied as public keys. Simulation results are presented to demonstrate the validity and security of the proposed protocol.
22714188	Effective memory reduction of the novel look-up table with one-dimensional sub-principle fringe patterns in computer-generated holograms.
Opt Express  2012May21
We propose a novel approach to massively reduce the memory of the novel look-up table (N-LUT) for computer-generated holograms by employing one-dimensional (1-D) sub-principle fringe patterns (sub-PFPs). Two-dimensional (2-D) PFPs used in the conventional N-LUT method are decomposed into a pair of 1-D sub-PFPs through a trigonometric relation. Then, these 1-D sub-PFPs are pre-calculated and stored in the proposed method, which results in a remarkable reduction of the memory of the N-LUT. Experimental results reveal that the memory capacity of the LUT, N-LUT and proposed methods have been calculated to be 149.01 TB, 2.29 GB and 1.51 MB, respectively for the 3-D object having image points of 500 × 500 × 256, which means the memory of the proposed method could be reduced by 103 × 10(6) fold and 1.55 × 10(3) fold compared to those of the conventional LUT and N-LUT methods, respectively.
22714222	Temporally multiplexed storage of images in a gradient echo memory.
Opt Express  2012May21
We study the storage and retrieval of images in a hot atomic vapor using the gradient echo memory protocol. We demonstrate that this technique allows for the storage of multiple spatial modes. We study both spatial and temporal multiplexing by storing a sequence of two different images in the atomic vapor. The effect of atomic diffusion on the spatial resolution is discussed and characterized experimentally. For short storage time a normalized spatial cross-correlation between a retrieved image and its input of 88 % is reported.
22714485	Generating arbitrary photon-number entangled states for continuous-variable quantum informatics.
Opt Express  2012Jun18
We propose two experimental schemes that can produce an arbitrary photon-number entangled state (PNES) in a finite dimension. This class of entangled states naturally includes non-Gaussian continuous-variable (CV) states that may provide some practical advantages over the Gaussian counterparts (two-mode squeezed states). We particularly compare the entanglement characteristics of the Gaussian and the non-Gaussian states in view of the degree of entanglement and the Einstein-Podolsky-Rosen correlation, and further discuss their applications to the CV teleportation and the nonlocality test. The experimental imperfection due to the on-off photodetectors with nonideal efficiency is also considered in our analysis to show the feasibility of our schemes within existing technologies.
21873473	Implementation of a deidentified federated data network for population-based cohort discovery.
J Am Med Inform Assoc 20110826 2012Jun
The Cross-Institutional Clinical Translational Research project explored a federated query tool and looked at how this tool can facilitate clinical trial cohort discovery by managing access to aggregate patient data located within unaffiliated academic medical centers. The project adapted software from the Informatics for Integrating Biology and the Bedside (i2b2) program to connect three Clinical Translational Research Award sites: University of Washington, Seattle, University of California, Davis, and University of California, San Francisco. The project developed an iterative spiral software development model to support the implementation and coordination of this multisite data resource. By standardizing technical infrastructures, policies, and semantics, the project enabled federated querying of deidentified clinical datasets stored in separate institutional environments and identified barriers to engaging users for measuring utility. The authors discuss the iterative development and evaluation phases of the project and highlight the challenges identified and the lessons learned. The common system architecture and translational processes provide high-level (aggregate) deidentified access to a large patient population (&gt;5 million patients), and represent a novel and extensible resource. Enhancing the network for more focused disease areas will require research-driven partnerships represented across all partner sites.
22140207	Automated discovery of drug treatment patterns for endocrine therapy of breast cancer within an electronic medical record.
J Am Med Inform Assoc 20111201 2012Jun
To develop an algorithm for the discovery of drug treatment patterns for endocrine breast cancer therapy within an electronic medical record and to test the hypothesis that information extracted using it is comparable to the information found by traditional methods. The electronic medical charts of 1507 patients diagnosed with histologically confirmed primary invasive breast cancer. The automatic drug treatment classification tool consisted of components for: (1) extraction of drug treatment-relevant information from clinical narratives using natural language processing (clinical Text Analysis and Knowledge Extraction System); (2) extraction of drug treatment data from an electronic prescribing system; (3) merging information to create a patient treatment timeline; and (4) final classification logic. Agreement between results from the algorithm and from a nurse abstractor is measured for categories: (0) no tamoxifen or aromatase inhibitor (AI) treatment; (1) tamoxifen only; (2) AI only; (3) tamoxifen before AI; (4) AI before tamoxifen; (5) multiple AIs and tamoxifen cycles in no specific order; and (6) no specific treatment dates. Specificity (all categories): 96.14%-100%; sensitivity (categories (0)-(4)): 90.27%-99.83%; sensitivity (categories (5)-(6)): 0-23.53%; positive predictive values: 80%-97.38%; negative predictive values: 96.91%-99.93%. Our approach illustrates a secondary use of the electronic medical record. The main challenge is event temporality. We present an algorithm for automated treatment classification within an electronic medical record to combine information extracted through natural language processing with that extracted from structured databases. The algorithm has high specificity for all categories, high sensitivity for five categories, and low sensitivity for two categories.
22717998	Training evidence-based veterinary medicine by collaborative development of critically appraised topics.
J Vet Med Educ  2012Summer
In current veterinary education, skills such as retrieving, critically appraising, interpreting, and applying the results of published scientific studies are rarely taught. In this study, the authors tested the concept of team-based development of critically appraised topics (CATs) in training students in evidence-based veterinary medicine (EBVM). The 116 participants were in their fifth year and attending the clinical rotation at the Clinic for Animal Reproduction. Students developed 18 CATs of varying quality on topics of their choice. Preparing the CATs in teams stimulated discussion on the topic and the quality of the retrieved papers. Evaluation of the project revealed that more than 90% of the students endorsed training in critical appraisal of information in veterinary education. In addition, more than 90% considered the development of CATs an effective exercise for assessing the quality of scientific literature. A provided literature evaluation form was perceived as a useful tool for systematically summarizing a publication's quality. In conclusion, team-based development of CATs during clinical rotations is highly valuable for training in EBVM. Learning and intrinsic motivation seem to be enhanced by creating a situation similar to veterinary practice because the task is embedded into an authentic clinical problem. This approach to clinical training helps to prepare students to integrate evidence from literature into practice.
22719935	Characterizing interdisciplinarity of researchers and research topics using web search engines.
PLoS ONE 20120613 2012
Researchers' networks have been subject to active modeling and analysis. Earlier literature mostly focused on citation or co-authorship networks reconstructed from annotated scientific publication databases, which have several limitations. Recently, general-purpose web search engines have also been utilized to collect information about social networks. Here we reconstructed, using web search engines, a network representing the relatedness of researchers to their peers as well as to various research topics. Relatedness between researchers and research topics was characterized by visibility boost-increase of a researcher's visibility by focusing on a particular topic. It was observed that researchers who had high visibility boosts by the same research topic tended to be close to each other in their network. We calculated correlations between visibility boosts by research topics and researchers' interdisciplinarity at the individual level (diversity of topics related to the researcher) and at the social level (his/her centrality in the researchers' network). We found that visibility boosts by certain research topics were positively correlated with researchers' individual-level interdisciplinarity despite their negative correlations with the general popularity of researchers. It was also found that visibility boosts by network-related topics had positive correlations with researchers' social-level interdisciplinarity. Research topics' correlations with researchers' individual- and social-level interdisciplinarities were found to be nearly independent from each other. These findings suggest that the notion of "interdisciplinarity" of a researcher should be understood as a multi-dimensional concept that should be evaluated using multiple assessment means.
22616856	Variable reference alignment: an improved peak alignment protocol for NMR spectral data with large intersample variation.
Anal. Chem. 20120529 2012Jun19
In an effort to address the variable correspondence problem across large sample cohorts common in metabolomic/metabonomic studies, we have developed a prealignment protocol that aims to generate spectral segments sharing a common target spectrum. Under the assumption that a single reference spectrum will not correctly represent all spectra of a data set, the goal of this approach is to perform local alignment corrections on spectral regions which share a common "most similar" spectrum. A natural beneficial outcome of this procedure is the automatic definition of spectral segments, a feature that is not common to all alignment methods. This protocol is shown to specifically improve the quality of alignment in (1)H NMR data sets exhibiting large intersample compositional variation (e.g., pH, ionic strength). As a proof-of-principle demonstration, we have utilized two recently developed alignment algorithms specific to NMR data, recursive segment-wise peak alignment and interval correlated shifting, and applied them to two data sets composed of 15 aqueous cell line extract and 20 human urine (1)H NMR profiles. Application of this protocol represents a fundamental shift from current alignment methodologies that seek to correct misalignments utilizing a single representative spectrum, with the added benefit that it can be appended to any alignment algorithm.
22721865	Mining online social network data for biomedical research: a comparison of clinicians' and patients' perceptions about amyotrophic lateral sclerosis treatments.
J. Med. Internet Res. 20120621 2012
While only one drug is known to slow the progress of amyotrophic lateral sclerosis (ALS), numerous drugs can be used to treat its symptoms. However, very few randomized controlled trials have assessed the efficacy, safety, and side effects of these drugs. Due to this lack of randomized controlled trials, consensus among clinicians on how to treat the wide range of ALS symptoms and the efficacy of these treatments is low. Given the lack of clinical trials data, the wide range of reported symptoms, and the low consensus among clinicians on how to treat those symptoms, data on the prevalence and efficacy of treatments from a patient's perspective could help advance the understanding of the symptomatic treatment of ALS. To compare clinicians' and patients' perspectives on the symptomatic treatment of ALS by comparing data from a traditional survey study of clinicians with data from a patient social network. We used a survey of clinicians' perceptions by Forshew and Bromberg as our primary data source and adjusted the data from PatientsLikeMe to allow for comparisons. We first extracted the 14 symptoms and associated top four treatments listed by Forshew and Bromberg. We then searched the PatientsLikeMe database for the same symptom-treatment pairs. The PatientsLikeMe data are structured and thus no preprocessing of the data was required. After we eliminated pairs with a small sample, 15 symptom-treatment pairs remained. All treatments identified as useful were prescription drugs. We found similarities and discrepancies between clinicians' and patients' perceptions of treatment prevalence and efficacy. In 7 of the 15 pairs, the differences between the two groups were above 10%. In 3 pairs the differences were above 20%. Lorazepam to treat anxiety and quinine to treat muscle cramps were among the symptom-treatment pairs with high concordance between clinicians' and patients' perceptions. Conversely, amitriptyline to treat labile emotional effect and oxybutynin to treat urinary urgency displayed low agreement between clinicians and patients. Assessing and comparing the efficacy of the symptomatic treatment of a complex and rare disease such as ALS is not easy and needs to take both clinicians' and patients' perspectives into consideration. Drawing a reliable profile of treatment efficacy requires taking into consideration many interacting aspects (eg, disease stage and severity of symptoms) that were not covered in the present study. Nevertheless, pilot studies such as this one can pave the way for more robust studies by helping researchers anticipate and compensate for limitations in their data sources and study design.
22722688	Securing the data economy: translating privacy and enacting security in the development of DataSHIELD.
Public Health Genomics 20120620 2012
Contemporary bioscience is seeing the emergence of a new data economy: with data as its fundamental unit of exchange. While sharing data within this new 'economy' provides many potential advantages, the sharing of individual data raises important social and ethical concerns. We examine ongoing development of one technology, DataSHIELD, which appears to elide privacy concerns about sharing data by enabling shared analysis while not actually sharing any individual-level data. We combine presentation of the development of DataSHIELD with presentation of an ethnographic study of a workshop to test the technology. DataSHIELD produced an application of the norm of privacy that was practical, flexible and operationalizable in researchers' everyday activities, and one which fulfilled the requirements of ethics committees. We demonstrated that an analysis run via DataSHIELD could precisely replicate results produced by a standard analysis where all data are physically pooled and analyzed together. In developing DataSHIELD, the ethical concept of privacy was transformed into an issue of security. Development of DataSHIELD was based on social practices as well as scientific and ethical motivations. Therefore, the 'success' of DataSHIELD would, likewise, be dependent on more than just the mathematics and the security of the technology.
22722689	IT solutions for privacy protection in biobanking.
Public Health Genomics 20120620 2012
Biobanks containing human biological samples and associated data are key resources for the advancement of medical research. Efficient access to samples and data increases competitiveness in medical research, reduces effort and time for achieving scientific results and promotes scientific progress. In order to address upcoming health challenges, there is increasing need for transnational collaboration. This requires innovative solutions improving interoperability of biobanks in fields such as sample and data management as well as governance including ethical and legal frameworks. In this context, rights and expectations of donors to determine the usage of their biological material and data and to ensure their privacy have to be observed. We discuss the benefits of biobanks, the needs to support medical research and the societal demands and regulations, in particular, securing the rights of donors and present IT solutions that allow both to maintain the security of personal data and to increase the efficiency of access to data in biobanks. Disclosure filters are discussed as a strategy to combine European public expectations concerning informed consent with the requirements of biobank research.
22724293	Re-ranking with context for high-performance biomedical information retrieval.
Int J Data Min Bioinform  2012
In this paper, we present a context-sensitive approach to re-ranking retrieved documents for further improving the effectiveness of high-performance biomedical literature retrieval systems. For each topic, a two-dimensional positive context is learnt from the top N retrieved documents and a group of negative contexts are learnt from the last N' documents in initial retrieval ranked list. The contextual space contains lexical context and conceptual context. The probabilities that retrieved documents are generated within the contextual space are then computed for document re-ranking. Empirical evaluation on the TREC Genomics full-text collection and three high-performance biomedical literature retrieval runs demonstrates that the context-sensitive re-ranking approach yields better retrieval performance.
22730454	CreZOO--the European virtual repository of Cre and other targeted conditional driver strains.
Database (Oxford) 20120621 2012
The CreZOO (http://www.crezoo.org/) is the European virtual repository of Cre and other targeted conditional driver strains. These mice serve as tools for researchers to selectively 'switch off' gene expression in mouse models to examine gene function and disease pathology. CreZOO aims to capture and disseminate extant and new information on these Cre driver strains, such as genetic background and availability information, and details pertaining promoter, allele, inducibility and expression patterns, which are also presented. All transgenic strains carry detailed information according to MGI's official nomenclature, whereas their availability [e.g. live mice, cryopreserved embryos, sperm and embryonic stem (ES) cells] is clearly indicated with links to European and International databases and repositories (EMMA, MGI/IMSR, MMRRC, etc) and laboratories where the particular mouse strain is available together with the respective IDs. Each promoter/gene includes IDs and direct links to MGI, Entrez Gene, Ensembl, OMIM and RGD databases depending on their species origin, whereas allele information is presented with MGI IDs and active hyperlinks to redirect the user to the respective page in a new tab. The tissue/cell (special) and developmental (temporal) specificity expression patterns are clearly presented, whereas handling and genotyping details (in the form of documents or hyperlinks) together with all relevant publications are clearly presented with PMID(s) and direct PubMed links. CreZOO's design offers a user-friendly query interface and provides instant access to the list of conditional driver strains, promoters and inducibility details. Database access is free of charge and there are no registration requirements for data querying. CreZOO is being developed in the context of the CREATE consortium (http://www.creline.org/), a core of major European and international mouse database holders and research groups involved in conditional mutagenesis.  Database URL: http://www.crezoo.org/; alternative URL: http://www.e-mouse.org/
22582806	PROLIX: rapid mining of protein-ligand interactions in large crystal structure databases.
J Chem Inf Model 20120601 2012Jun25
A central problem in structure-based drug design is understanding protein-ligand interactions quantitatively and qualitatively. Several recent studies have highlighted from a qualitative perspective the nature of these interactions and their utility in drug discovery. However, a common limitation is a lack of adequate tools to mine these interactions comprehensively, since exhaustive searches of the protein data bank are time-consuming and difficult to perform. Consequently, fundamental questions remain unanswered: How unique or how common are the protein-ligand interactions observed in a given drug design project when compared to all complexed structures in the protein data bank? Which interaction patterns might explain the affinity of a tool compound toward unwanted targets? To answer these questions and to enable the systematic and comprehensive study of protein-ligand interactions, we introduce PROLIX (Protein Ligand Interaction Explorer), a tool that uses sophisticated fingerprint representations of protein-ligand interaction patterns for rapid data mining in large crystal structure databases. Our implementation strategy pursues a branch-and-bound technique that enables mining against thousands of complexes within a few seconds. Key elements of PROLIX include (i) an intuitive interface that enables users to formulate complex queries easily, (ii) exceptional speed for results retrieval, and (iii) a sophisticated results summarization. Herein we describe the algorithms developed to enable complex queries and fast retrieval of search results, as well as the intuitive aspects of the user interface and summarization viewer.
22228038	Contributions to an animal trait ontology.
J. Anim. Sci. 20120106 2012Jun
Improved understanding of the biology of traits of livestock species necessitates the use and combination of information that is stored in a variety of different sources such as databases and literature. The ability to effectively combine information from different sources, however, depends on a high level of standardization within and between various resources, at least with respect to the used terminology. Ontologies represent a set of concepts that facilitate standardization of terminology within specific domains of interest. The biological mechanisms underlying quantitative traits of farm animal species related to reproduction and host pathogen interactions are complex and not well understood. This knowledge could be improved through the availability of domain-specific ontologies that provide enhanced possibilities for data annotation, data retrieval, data integration, data exchange, data analysis, and ontology-based searches. Here we describe a framework for domain-specific ontologies and the development of 2 first-generation ontologies: Reproductive Trait and Phenotype Ontology (REPO) and Host Pathogen Interactions Ontology . In these first-generation ontologies, we focused on "female fertility in cattle" and "interactions between pigs and Salmonella". Through this, we contribute to the global initiative toward the development of an Animal Trait Ontology for livestock species. To demonstrate its usefulness, we show how REPO can be used to select candidate genes for fertility.
22648672	Chapter 4: effective search strategies for systematic reviews of medical tests.
J Gen Intern Med  2012Jun
This article discusses techniques that are appropriate when developing search strategies for systematic reviews of medical tests. This includes general advice for searching for systematic reviews and issues specific to systematic reviews of medical tests. Diagnostic search filters are currently not sufficiently developed for use when searching for systematic reviews. Instead, authors should construct a highly sensitive search strategy that uses both controlled vocabulary and text words. A comprehensive search should include multiple databases and sources of grey literature. A list of subject-specific databases is included in this article.
21844624	A multimedia retrieval framework based on semi-supervised ranking and relevance feedback.
IEEE Trans Pattern Anal Mach Intell  2012Apr
We present a new framework for multimedia content analysis and retrieval which consists of two independent algorithms. First, we propose a new semi-supervised algorithm called ranking with Local Regression and Global Alignment (LRGA) to learn a robust Laplacian matrix for data ranking. In LRGA, for each data point, a local linear regression model is used to predict the ranking scores of its neighboring points. A unified objective function is then proposed to globally align the local models from all the data points so that an optimal ranking score can be assigned to each data point. Second, we propose a semi-supervised long-term Relevance Feedback (RF) algorithm to refine the multimedia data representation. The proposed long-term RF algorithm utilizes both the multimedia data distribution in multimedia feature space and the history RF information provided by users. A trace ratio optimization problem is then formulated and solved by an efficient algorithm. The algorithms have been applied to several content-based multimedia retrieval applications, including cross-media retrieval, image retrieval, and 3D motion/pose data retrieval. Comprehensive experiments on four data sets have demonstrated its advantages in precision, robustness, scalability, and computational efficiency.
22489867	EnzyBase: a novel database for enzybiotic studies.
BMC Microbiol. 20120411 2012
Enzybiotics are becoming increasingly recognized as potential alternative therapies for drug-resistant bacteria. Although only a few enzybiotics are currently well characterized, much information is still missing or is unavailable for researchers. The construction of an enzybiotics database would therefore increase efficiency and convenience in investigating these bioactive proteins and thus help reduce or delay the recent increase in antibiotic resistance. In the present manuscript, we describe the development of a novel and original database called EnzyBase, which contains 1144 enzybiotics from 216 natural sources. To ensure data quality, we limited the source of information to authoritative public databases and published scientific literature. The interface of EnzyBase is easy to use and allows users to rapidly retrieve data according to their desired search criteria and blast the database for homologous sequences. We also describe examples of database-aided enzybiotics discovery and design. EnzyBase serves as a unique tool for enzybiotic studies. It has several potential applications, e.g. in silico enzybiotic combination as cocktails, and novel enzybiotic design, in response to continuously emerging drug-resistant pathogens. This database is a valuable platform for researchers who are interested in enzybiotic studies. EnzyBase is available online at http://biotechlab.fudan.edu.cn/database/EnzyBase/home.php.
22652650	Defining the pathogenesis of inflammatory and immune diseases through database mining.
Front Biosci (Elite Ed) 20120601 2012
Recent research in human and animal genomes, transcriptomes, proteomes, and antigen-omes has generated a large library of data and has led to the establishment of many experimental data-based searchable databases. Scientists now face new, unprecedented challenges to develop more systemic methods to analyze experimental data and generate new hypotheses. This review will briefly summarize our pioneering efforts in using new database mining methods to answer important questions in inflammatory and immune-related diseases. The new principles and basic methodologies of database mining developed in Dr. Yang's laboratory will be delineated in the following studies: 1) a stimulation-responsive alternative splicing model for generating untolerized autoantigen epitopes; 2) a three-tier model for caspase-1 activation and inflammation privileges of various organs; and 3) a group of anti-inflammatory microRNAs which inhibit proatherogenic gene expression during atherogenesis. With technological advances, database mining has provided important insight into new directions for experimental research.
22655438	Risk and reward in the cloud. Choosing a cloud vendor involves weighing risks versus benefits.
Healthc Inform  2012May
More hospitals are looking to the cloud as a viable way to store clinical, imaging, and financial data. Experts acknowledge its advantages, but caution it's a step that requires careful planning and vetting of potential cloud vendors.
22480327	CDAPubMed: a browser extension to retrieve EHR-based biomedical literature.
BMC Med Inform Decis Mak 20120405 2012
Over the last few decades, the ever-increasing output of scientific publications has led to new challenges to keep up to date with the literature. In the biomedical area, this growth has introduced new requirements for professionals, e.g., physicians, who have to locate the exact papers that they need for their clinical and research work amongst a huge number of publications. Against this backdrop, novel information retrieval methods are even more necessary. While web search engines are widespread in many areas, facilitating access to all kinds of information, additional tools are required to automatically link information retrieved from these engines to specific biomedical applications. In the case of clinical environments, this also means considering aspects such as patient data security and confidentiality or structured contents, e.g., electronic health records (EHRs). In this scenario, we have developed a new tool to facilitate query building to retrieve scientific literature related to EHRs. We have developed CDAPubMed, an open-source web browser extension to integrate EHR features in biomedical literature retrieval approaches. Clinical users can use CDAPubMed to: (i) load patient clinical documents, i.e., EHRs based on the Health Level 7-Clinical Document Architecture Standard (HL7-CDA), (ii) identify relevant terms for scientific literature search in these documents, i.e., Medical Subject Headings (MeSH), automatically driven by the CDAPubMed configuration, which advanced users can optimize to adapt to each specific situation, and (iii) generate and launch literature search queries to a major search engine, i.e., PubMed, to retrieve citations related to the EHR under examination. CDAPubMed is a platform-independent tool designed to facilitate literature searching using keywords contained in specific EHRs. CDAPubMed is visually integrated, as an extension of a widespread web browser, within the standard PubMed interface. It has been tested on a public dataset of HL7-CDA documents, returning significantly fewer citations since queries are focused on characteristics identified within the EHR. For instance, compared with more than 200,000 citations retrieved by breast neoplasm, fewer than ten citations were retrieved when ten patient features were added using CDAPubMed. This is an open source tool that can be freely used for non-profit purposes and integrated with other existing systems.
22353403	Learning semantic and visual similarity for endomicroscopy video retrieval.
IEEE Trans Med Imaging 20120216 2012Jun
Content-based image retrieval (CBIR) is a valuable computer vision technique which is increasingly being applied in the medical community for diagnosis support. However, traditional CBIR systems only deliver visual outputs, i.e., images having a similar appearance to the query, which is not directly interpretable by the physicians. Our objective is to provide a system for endomicroscopy video retrieval which delivers both visual and semantic outputs that are consistent with each other. In a previous study, we developed an adapted bag-of-visual-words method for endomicroscopy retrieval, called "Dense-Sift," that computes a visual signature for each video. In this paper, we present a novel approach to complement visual similarity learning with semantic knowledge extraction, in the field of in vivo endomicroscopy. We first leverage a semantic ground truth based on eight binary concepts, in order to transform these visual signatures into semantic signatures that reflect how much the presence of each semantic concept is expressed by the visual words describing the videos. Using cross-validation, we demonstrate that, in terms of semantic detection, our intuitive Fisher-based method transforming visual-word histograms into semantic estimations outperforms support vector machine (SVM) methods with statistical significance. In a second step, we propose to improve retrieval relevance by learning an adjusted similarity distance from a perceived similarity ground truth. As a result, our distance learning method allows to statistically improve the correlation with the perceived similarity. We also demonstrate that, in terms of perceived similarity, the recall performance of the semantic signatures is close to that of visual signatures and significantly better than those of several state-of-the-art CBIR methods. The semantic signatures are thus able to communicate high-level medical knowledge while being consistent with the low-level visual signatures and much shorter than them. In our resulting retrieval system, we decide to use visual signatures for perceived similarity learning and retrieval, and semantic signatures for the output of an additional information, expressed in the endoscopist own language, which provides a relevant semantic translation of the visual retrieval outputs.
22633815	EORTC Radiation Oncology Group quality assurance platform: establishment of a digital central review facility.
Radiother Oncol 20120523 2012Jun
Quality assurance (QA) in clinical trials is essential to ensure treatment is safely and effectively delivered. As QA requirements have increased in complexity in parallel with evolution of radiation therapy (RT) delivery, a need to facilitate digital data exchange emerged. Our objective is to present the platform developed for the integration and standardization of QART activities across all EORTC trials involving RT. The following essential requirements were identified: secure and easy access without on-site software installation; integration within the existing EORTC clinical remote data capture system; and the ability to both customize the platform to specific studies and adapt to future needs. After retrospective testing within several clinical trials, the platform was introduced in phases to participating sites and QART study reviewers. The resulting QA platform, integrating RT analysis software installed at EORTC Headquarters, permits timely, secure, and fully digital central DICOM-RT based data review. Participating sites submit data through a standard secure upload webpage. Supplemental information is submitted in parallel through web-based forms. An internal quality check by the QART office verifies data consistency, formatting, and anonymization. QART reviewers have remote access through a terminal server. Reviewers evaluate submissions for protocol compliance through an online evaluation matrix. Comments are collected by the coordinating centre and institutions are informed of the results. This web-based central review platform facilitates rapid, extensive, and prospective QART review. This reduces the risk that trial outcomes are compromised through inadequate radiotherapy and facilitates correlation of results with clinical outcomes.
22615351	Rewritable digital data storage in live cells via engineered control of recombination directionality.
Proc. Natl. Acad. Sci. U.S.A. 20120521 2012Jun5
The use of synthetic biological systems in research, healthcare, and manufacturing often requires autonomous history-dependent behavior and therefore some form of engineered biological memory. For example, the study or reprogramming of aging, cancer, or development would benefit from genetically encoded counters capable of recording up to several hundred cell division or differentiation events. Although genetic material itself provides a natural data storage medium, tools that allow researchers to reliably and reversibly write information to DNA in vivo are lacking. Here, we demonstrate a rewriteable recombinase addressable data (RAD) module that reliably stores digital information within a chromosome. RAD modules use serine integrase and excisionase functions adapted from bacteriophage to invert and restore specific DNA sequences. Our core RAD memory element is capable of passive information storage in the absence of heterologous gene expression for over 100 cell divisions and can be switched repeatedly without performance degradation, as is required to support combinatorial data storage. We also demonstrate how programmed stochasticity in RAD system performance arising from bidirectional recombination can be achieved and tuned by varying the synthesis and degradation rates of recombinase proteins. The serine recombinase functions used here do not require cell-specific cofactors and should be useful in extending computing and control methods to the study and engineering of many biological systems.
22672889	Using Internet search engines to obtain medical information: a comparative study.
J. Med. Internet Res. 20120516 2012
The Internet has become one of the most important means to obtain health and medical information. It is often the first step in checking for basic information about a disease and its treatment. The search results are often useful to general users. Various search engines such as Google, Yahoo!, Bing, and Ask.com can play an important role in obtaining medical information for both medical professionals and lay people. However, the usability and effectiveness of various search engines for medical information have not been comprehensively compared and evaluated. To compare major Internet search engines in their usability of obtaining medical and health information. We applied usability testing as a software engineering technique and a standard industry practice to compare the four major search engines (Google, Yahoo!, Bing, and Ask.com) in obtaining health and medical information. For this purpose, we searched the keyword breast cancer in Google, Yahoo!, Bing, and Ask.com and saved the results of the top 200 links from each search engine. We combined nonredundant links from the four search engines and gave them to volunteer users in an alphabetical order. The volunteer users evaluated the websites and scored each website from 0 to 10 (lowest to highest) based on the usefulness of the content relevant to breast cancer. A medical expert identified six well-known websites related to breast cancer in advance as standards. We also used five keywords associated with breast cancer defined in the latest release of Systematized Nomenclature of Medicine-Clinical Terms (SNOMED CT) and analyzed their occurrence in the websites. Each search engine provided rich information related to breast cancer in the search results. All six standard websites were among the top 30 in search results of all four search engines. Google had the best search validity (in terms of whether a website could be opened), followed by Bing, Ask.com, and Yahoo!. The search results highly overlapped between the search engines, and the overlap between any two search engines was about half or more. On the other hand, each search engine emphasized various types of content differently. In terms of user satisfaction analysis, volunteer users scored Bing the highest for its usefulness, followed by Yahoo!, Google, and Ask.com. Google, Yahoo!, Bing, and Ask.com are by and large effective search engines for helping lay users get health and medical information. Nevertheless, the current ranking methods have some pitfalls and there is room for improvement to help users get more accurate and useful information. We suggest that search engine users explore multiple search engines to search different types of health information and medical knowledge for their own needs and get a professional consultation if necessary.
22447642	Hidden effectiveness? Results of hand-searching Italian language journals for occupational health interventions.
Occup Environ Med 20120323 2012Jul
To compare the yield of hand-searching with optimised electronic search strategies in retrieving occupational health (OH) intervention studies published in a language other than English. The authors systematically hand-searched and screened reports of OH intervention studies published in Italian in peer-reviewed scientific journals between 1990 and 2008. The authors evaluated how many of them met the Cochrane Occupational Safety and Health Review Groups (OSHRG) definition of being an OH intervention study and how many potentially relevant studies retrieved by hand-searching would not be found by PubMed alone using the OSHRG's most specific and most sensitive search strings. Hand-searching retrieved 25 articles (reporting 27 studies), including nine not indexed in MEDLINE. Most studies (81%, 22/27) had a before-after design and only one was a randomised trial. The OSHRG's most sensitive search string retrieved all 16 articles published in the Italian language journals that were indexed in MEDLINE, while the most specific search strategy retrieved nine articles (56%, 9/16). The most specific search string showed a lower 'number needed to read' value than the most sensitive one (60 vs 132). These findings suggest that a sensitive electronic search strategy may be able to find most of the OH interventions published in languages other than English that are indexed in MEDLINE. Hand-searching of important national journals not indexed in MEDLINE should be considered when conducting particularly in-depth research.
22680494	Optimizing information flow in small genetic networks. III. A self-interacting gene.
Phys Rev E Stat Nonlin Soft Matter Phys 20120405 2012Apr
Living cells must control the reading out or "expression" of information encoded in their genomes, and this regulation often is mediated by transcription factors--proteins that bind to DNA and either enhance or repress the expression of nearby genes. But the expression of transcription factor proteins is itself regulated, and many transcription factors regulate their own expression in addition to responding to other input signals. Here we analyze the simplest of such self-regulatory circuits, asking how parameters can be chosen to optimize information transmission from inputs to outputs in the steady state. Some nonzero level of self-regulation is almost always optimal, with self-activation dominant when transcription factor concentrations are low and self-repression dominant when concentrations are high. In steady state the optimal self-activation is never strong enough to induce bistability, although there is a limit in which the optimal parameters are very close to the critical point.
22680509	Biomimetic model of the outer plexiform layer by incorporating memristive devices.
Phys Rev E Stat Nonlin Soft Matter Phys 20120426 2012Apr
In this paper we present a biorealistic model for the first part of the early vision of processing by incorporating memristive nanodevices. The architecture of the proposed network is based on the organization and functioning of the outer plexiform layer (OPL) in the vertebrate retina. We demonstrate that memristive devices are indeed a valuable building block for neuromorphic architectures, as their highly nonlinear and adaptive response could be exploited for establishing ultradense networks with dynamics similar to that of their biological counterparts. We particularly show that hexagonal memristive grids can be employed for faithfully emulating the smoothing effect occurring in the OPL to enhance the dynamic range of the system. In addition, we employ a memristor-based thresholding scheme for detecting the edges of grayscale images, while the proposed system is also evaluated for its adaptation and fault tolerance capacity against different light or noise conditions as well as its distinct device yields.
22680512	Quantifying impacts of short-term plasticity on neuronal information transfer.
Phys Rev E Stat Nonlin Soft Matter Phys 20120427 2012Apr
Short-term changes in efficacy have been postulated to enhance the ability of synapses to transmit information between neurons, and within neuronal networks. Even at the level of connections between single neurons, direct confirmation of this simple conjecture has proven elusive. By combining paired-cell recordings, realistic synaptic modeling, and information theory, we provide evidence that short-term plasticity can not only improve, but also reduce information transfer between neurons. We focus on a concrete example in rat neocortex, but our results may generalize to other systems. When information is contained in the timings of individual spikes, we find that facilitation, depression, and recovery affect information transmission in proportion to their impacts upon the probability of neurotransmitter release. When information is instead conveyed by mean spike rate only, the influences of short-term plasticity critically depend on the range of spike frequencies that the target network can distinguish (its effective dynamic range). Our results suggest that to efficiently transmit information, the brain must match synaptic type, coding strategy, and network connectivity during development and behavior.
22680558	Complexity measures, emergence, and multiparticle correlations.
Phys Rev E Stat Nonlin Soft Matter Phys 20120413 2012Apr
We study correlation measures for complex systems. First, we investigate some recently proposed measures based on information geometry. We show that these measures can increase under local transformations as well as under discarding particles, thereby questioning their interpretation as a quantifier for complexity or correlations. We then propose a refined definition of these measures, investigate its properties, and discuss its numerical evaluation. As an example, we study coupled logistic maps and study the behavior of the different measures for that case. Finally, we investigate other local effects during the coarse graining of the complex system.
22685626	Google in a quantum network.
Sci Rep 20120608 2012
We introduce the characterization of a class of quantum PageRank algorithms in a scenario in which some kind of quantum network is realizable out of the current classical internet web, but no quantum computer is yet available. This class represents a quantization of the PageRank protocol currently employed to list web pages according to their importance. We have found an instance of this class of quantum protocols that outperforms its classical counterpart and may break the classical hierarchy of web pages depending on the topology of the web.
22500000	ChemSpot: a hybrid system for chemical named entity recognition.
Bioinformatics 20120412 2012Jun15
The accurate identification of chemicals in text is important for many applications, including computer-assisted reconstruction of metabolic networks or retrieval of information about substances in drug development. But due to the diversity of naming conventions and traditions for such molecules, this task is highly complex and should be supported by computational tools. We present ChemSpot, a named entity recognition (NER) tool for identifying mentions of chemicals in natural language texts, including trivial names, drugs, abbreviations, molecular formulas and International Union of Pure and Applied Chemistry entities. Since the different classes of relevant entities have rather different naming characteristics, ChemSpot uses a hybrid approach combining a Conditional Random Field with a dictionary. It achieves an F(1) measure of 68.1% on the SCAI corpus, outperforming the only other freely available chemical NER tool, OSCAR4, by 10.8 percentage points. ChemSpot is freely available at: http://www.informatik.hu-berlin.de/wbi/resources.
22436670	The National Adult Inpatient Survey conducted in the English National Health Service from 2002 to 2009: how have the data been used and what do we know as a result?
BMC Health Serv Res 20120321 2012
When it was initiated in 2001, England's national patient survey programme was one of the first in the world and has now been widely emulated in other healthcare systems. The aim of the survey programme was to make the National Health Service (NHS) more "patient centred" and more responsive to patient feedback. The national inpatient survey has now been running in England annually since 2002 gathering data from over 600,000 patients. The aim of this study is to investigate how the data have been used and to summarise what has been learned about patients' evaluation of care as a result. Two independent researchers systematically gathered all research that included analyses of the English national adult inpatient survey data. Journals, databases and relevant websites were searched. Publications prior to 2002 were excluded. Articles were also identified following consultation with experts. All documents were then critically appraised by two co-authors both of whom have a background in statistical analysis. We found that the majority of the studies identified were reports produced by organisations contracted to gather the data or co-ordinate the data collection and used mainly descriptive statistics. A few articles used the survey data for evidence based reporting or linked the survey to other healthcare data. The patient's socio-demographic characteristics appeared to influence their evaluation of their care but characteristics of the workforce and the. At a national level, the results of the survey have been remarkably stable over time. Only in those areas where there have been co-ordinated government-led campaigns, targets and incentives, have improvements been shown. The main findings of the review are that while the survey data have been used for different purposes they seem to have incited little academic interest. The national inpatient survey has been a useful resource for many authors and organisations but the full potential inherent in this large, longitudinal publicly available dataset about patients' experiences has not as yet been fully exploited.This review suggests that the presence of survey results alone is not enough to improve patients' experiences and further research is required to understand whether and how the survey can be best used to improve standards of care in the NHS.
22595086	Discovering context-specific relationships from biological literature by using multi-level context terms.
BMC Med Inform Decis Mak 20120430 2012
The Swanson's ABC model is powerful to infer hidden relationships buried in biological literature. However, the model is inadequate to infer relations with context information. In addition, the model generates a very large amount of candidates from biological text, and it is a semi-automatic, labor-intensive technique requiring human expert's manual input. To tackle these problems, we incorporate context terms to infer relations between AB interactions and BC interactions. We propose 3 steps to discover meaningful hidden relationships between drugs and diseases: 1) multi-level (gene, drug, disease, symptom) entity recognition, 2) interaction extraction (drug-gene, gene-disease) from literature, 3) context vector based similarity score calculation. Subsequently, we evaluate our hypothesis with the datasets of the "Alzheimer's disease" related 77,711 PubMed abstracts. As golden standards, PharmGKB and CTD databases are used. Evaluation is conducted in 2 ways: first, comparing precision of the proposed method and the previous method and second, analysing top 10 ranked results to examine whether highly ranked interactions are truly meaningful or not. The results indicate that context-based relation inference achieved better precision than the previous ABC model approach. The literature analysis also shows that interactions inferred by the context-based approach are more meaningful than interactions by the previous ABC model. We propose a novel interaction inference technique that incorporates context term vectors into the ABC model to discover meaningful hidden relationships. By utilizing multi-level context terms, our model shows better performance than the previous ABC model.
22595088	ASCOT: a text mining-based web-service for efficient search and assisted creation of clinical trials.
BMC Med Inform Decis Mak 20120430 2012
Clinical trials are mandatory protocols describing medical research on humans and among the most valuable sources of medical practice evidence. Searching for trials relevant to some query is laborious due to the immense number of existing protocols. Apart from search, writing new trials includes composing detailed eligibility criteria, which might be time-consuming, especially for new researchers. In this paper we present ASCOT, an efficient search application customised for clinical trials. ASCOT uses text mining and data mining methods to enrich clinical trials with metadata, that in turn serve as effective tools to narrow down search. In addition, ASCOT integrates a component for recommending eligibility criteria based on a set of selected protocols.
22595089	Detecting modification of biomedical events using a deep parsing approach.
BMC Med Inform Decis Mak 20120430 2012
This work describes a system for identifying event mentions in bio-molecular research abstracts that are either speculative (e.g. analysis of IkappaBalpha phosphorylation, where it is not specified whether phosphorylation did or did not occur) or negated (e.g. inhibition of IkappaBalpha phosphorylation, where phosphorylation did not occur). The data comes from a standard dataset created for the BioNLP 2009 Shared Task. The system uses a machine-learning approach, where the features used for classification are a combination of shallow features derived from the words of the sentences and more complex features based on the semantic outputs produced by a deep parser. To detect event modification, we use a Maximum Entropy learner with features extracted from the data relative to the trigger words of the events. The shallow features are bag-of-words features based on a small sliding context window of 3-4 tokens on either side of the trigger word. The deep parser features are derived from parses produced by the English Resource Grammar and the RASP parser. The outputs of these parsers are converted into the Minimal Recursion Semantics formalism, and from this, we extract features motivated by linguistics and the data itself. All of these features are combined to create training or test data for the machine learning algorithm. Over the test data, our methods produce approximately a 4% absolute increase in F-score for detection of event modification compared to a baseline based only on the shallow bag-of-words features. Our results indicate that grammar-based techniques can enhance the accuracy of methods for detecting event modification.
22595090	Semantic text mining support for lignocellulose research.
BMC Med Inform Decis Mak 20120430 2012
Biofuels produced from biomass are considered to be promising sustainable alternatives to fossil fuels. The conversion of lignocellulose into fermentable sugars for biofuels production requires the use of enzyme cocktails that can efficiently and economically hydrolyze lignocellulosic biomass. As many fungi naturally break down lignocellulose, the identification and characterization of the enzymes involved is a key challenge in the research and development of biomass-derived products and fuels. One approach to meeting this challenge is to mine the rapidly-expanding repertoire of microbial genomes for enzymes with the appropriate catalytic properties. Semantic technologies, including natural language processing, ontologies, semantic Web services and Web-based collaboration tools, promise to support users in handling complex data, thereby facilitating knowledge-intensive tasks. An ongoing challenge is to select the appropriate technologies and combine them in a coherent system that brings measurable improvements to the users. We present our ongoing development of a semantic infrastructure in support of genomics-based lignocellulose research. Part of this effort is the automated curation of knowledge from information on fungal enzymes that is available in the literature and genome resources. Working closely with fungal biology researchers who manually curate the existing literature, we developed ontological natural language processing pipelines integrated in a Web-based interface to assist them in two main tasks: mining the literature for relevant knowledge, and at the same time providing rich and semantically linked information.
22178292	A user-defined data type for the storage of time series data allowing efficient similarity screening.
Eur J Pharm Sci 20111209 2012Jul16
The volume of the experimentally measured time series data is rapidly growing, while storage solutions offering better data types than simple arrays of numbers or opaque blobs for keeping series data are sorely lacking. A number of indexing methods have been proposed to provide efficient access to time series data, but none has so far been integrated into a tried-and-proven database system. To explore the possibility of such integration, we have developed a data type for time series storage in PostgreSQL, an object-relational database system, and equipped it with an access method based on SAX (Symbolic Aggregate approXimation). This new data type has been successfully tested in a database supporting a large-scale plant gene expression experiment, and it was additionally tested on a very large set of simulated time series data.
22543364	Semi-supervised classification of patient safety event reports.
J Patient Saf  2012Jun
The Veterans Health Administration patient safety reporting system receives more than 100,000 reports annually. The information contained in these reports is primarily in the form of natural language text. Improving the ability to efficiently mine these patient safety reports for information is the objective of a proposed semi-supervised method. A semi-supervised classification method leverages information from both labeled and unlabeled reports to predict categories for the unlabeled reports. Two different scenarios involving a semi-supervised learning process are examined, and both demonstrate good predictive results. The semi-supervised method shows much promise in assisting researchers and analysts toward accurately and more quickly separating reports of varying and often overlapping topics. The method is able to use the "stories" provided in patient safety reports to extend existing patient safety taxonomies beyond their static design.
22564547	Interactive GPU-based maximum intensity projection of large medical data sets using visibility culling based on the initial occluder and the visible block classification.
Comput Med Imaging Graph 20120505 2012Jul
Maximum intensity projection (MIP) is an important visualization method that has been widely used for the diagnosis of enhanced vessels or bones by rotating or zooming MIP images. With the rapid spread of multidetector-row computed tomography (MDCT) scanners, MDCT scans of a patient generate a large data set. However, previous acceleration methods for MIP rendering of such a data set failed to generate MIP images at interactive rates. In this paper, we propose novel culling methods in both object and image space for interactive MIP rendering of large medical data sets. In object space, for the visibility test of a block, we propose the initial occluder resulting from a preceding image to utilize temporal coherence and increase the block culling ratio a lot. In addition, we propose the hole filling method using the mesh generation and rendering to improve the culling performance during the generation of the initial occluder. In image space, we find out that there is a trade-off between the block culling ratio in object space and the culling efficiency in image space. In this paper, we classify the visible blocks into two types by their visibility. And we propose a balanced culling method by applying a different culling algorithm in image space for each type to utilize the trade-off and improve the rendering speed. Experimental results on twenty CT data sets showed that our method achieved 3.85 times speed up in average without any loss of image quality comparing with conventional bricking method. Using our visibility culling method, we achieved interactive GPU-based MIP rendering of large medical data sets.
22615469	The memory of surfaces: epitaxial growth on quasi-crystals.
Philos Trans A Math Phys Eng Sci  2012Jun28
If crystal structures can be viewed as repositories of information, then crystal surfaces offer a pathway by which this information can be used to grow new structures through the process of epitaxy. The information transfer process is one of self-organization, and the kinetic and energetic factors influencing this are complex. They include the relative strengths of the adsorbate-adsorbate and adsorbate-substrate interactions, the flux of incoming species and the temperature of the system. In this brief review, we explore how the interplay of these factors influences the degree to which the epitaxial structures retain the 'memory' of the template, illustrating the discussion with examples from epitaxy on quasi-crystal surfaces.
22506982	The barriers and facilitators to routine outcome measurement by allied health professionals in practice: a systematic review.
BMC Health Serv Res 20120522 2012
Allied Health Professionals today are required, more than ever before, to demonstrate their impact. However, despite at least 20 years of expectation, many services fail to deliver routine outcome measurement in practice. This systematic review investigates what helps and hinders routine outcome measurement of allied health professionals practice. A systematic review protocol was developed comprising: a defined search strategy for PsycINFO, MEDLINE and CINHAL databases and inclusion criteria and systematic procedures for data extraction and quality appraisal. Studies were included if they were published in English and investigated facilitators and/or barriers to routine outcome measurement by allied health professionals. No restrictions were placed on publication type, design, country, or year of publication. Reference lists of included publications were searched to identify additional papers. Descriptive methods were used to synthesise the findings. 960 papers were retrieved; 15 met the inclusion criteria. Professional groups represented were Physiotherapy, Occupational Therapy, and Speech and Language Therapy. The included literature varied in quality and design. Facilitators and barriers to routine outcome measurement exist at individual, managerial and organisational levels. Key factors affecting professionals' use of routine outcome measurement include: professionals' level of knowledge and confidence about using outcome measures, and the degree of organisational and peer-support professionals received with a view to promoting their work in practice. Whilst the importance of routinely measuring outcomes within the allied health professions is well recognised, it has largely failed to be delivered in practice. Factors that influence clinicians' ability and desire to undertake routine outcome measurement are bi-directional: they can act as either facilitators or barriers. Routine outcome measurement may only be deliverable if appropriate action is taken at individual therapist, team, and organisational levels of an organisation.
21819179	Searching hospital discharge records for snow sport injury: no easy run?
Int J Inj Contr Saf Promot 20110805 2012
When using hospital discharge data to shape sports injury prevention policy, it is important to correctly identify cases. The objectives of this study were to examine the ease with which snow-skiing and snowboarding injury cases could be identified from national hospital discharge data and to assess the suitability of the information obtained for shaping policy. Hospital discharges for 2000-2004 were linked to compensated claims and searched sequentially using coded and narrative information. One thousand three hundred seventy-six eligible cases were identified, with 717 classified as snowboarding and 659 as snow-skiing. For the most part, cases could not be identified and distinguished using simple searches of coded data; keyword searches of narratives played a key role in case identification but not in describing the mechanism of injury. Identification and characterisation of snow sport injury from in-patient discharge records is problematic due to inadequacies in the coding systems and/or their implementation. Narrative reporting could be improved.
22452993	The Stanford Data Miner: a novel approach for integrating and exploring heterogeneous immunological data.
J Transl Med 20120328 2012
Systems-level approaches are increasingly common in both murine and human translational studies. These approaches employ multiple high information content assays. As a result, there is a need for tools to integrate heterogeneous types of laboratory and clinical/demographic data, and to allow the exploration of that data by aggregating and/or segregating results based on particular variables (e.g., mean cytokine levels by age and gender). Here we describe the application of standard data warehousing tools to create a novel environment for user-driven upload, integration, and exploration of heterogeneous data. The system presented here currently supports flow cytometry and immunoassays performed in the Stanford Human Immune Monitoring Center, but could be applied more generally. Users upload assay results contained in platform-specific spreadsheets of a defined format, and clinical and demographic data in spreadsheets of flexible format. Users then map sample IDs to connect the assay results with the metadata. An OLAP (on-line analytical processing) data exploration interface allows filtering and display of various dimensions (e.g., Luminex analytes in rows, treatment group in columns, filtered on a particular study). Statistics such as mean, median, and N can be displayed. The views can be expanded or contracted to aggregate or segregate data at various levels. Individual-level data is accessible with a single click. The result is a user-driven system that permits data integration and exploration in a variety of settings. We show how the system can be used to find gender-specific differences in serum cytokine levels, and compare them across experiments and assay types. We have used the tools and techniques of data warehousing, including open-source business intelligence software, to support investigator-driven data integration and mining of diverse immunological data.
22483422	Detecting and resolving inconsistencies between domain experts' different perspectives on (classification) tasks.
Artif Intell Med 20120404 2012Jun
The work reported here focuses on developing novel techniques which enable an expert to detect inconsistencies in 2 (or more) perspectives that the expert might have on the same (classification) task. The high level task which the experts (physicians) had set themselves was to classify, on a 5-point severity scale (A-E), the hourly reports produced by an intensive care unit's patient management system. The INSIGHT system has been developed to support domain experts exploring, and removing inconsistencies in their conceptualization of a task. We report here a study of intensive care physicians reconciling 2 perspectives on their patients. The 2 perspectives provided to INSIGHT were an annotated set of patient records where the expert had selected the appropriate category to describe that snapshot of the patient, and a set of rules which are able to classify the various time points on the same 5-point scale. Inconsistencies between these 2 perspectives are displayed as a confusion matrix; moreover INSIGHT then allows the expert to revise both the annotated datasets (correcting data errors, or changing the assigned categories) and the actual rule-set. Each of the 3 experts achieved a very high degree of consensus (~97%) between his refined knowledge sources (i.e., annotated hourly patient records and the rule-set). We then had the experts produce a common rule-set and then refine their several sets of annotations against it; this again resulted in inter-expert agreements of ~97%. The resulting rule-set can then be used in applications with considerable confidence. This study has shown that under some circumstances, it is possible for domain experts to achieve a high degree of correlation between 2 perspectives of the same task. The experts agreed that the immediate feedback provided by INSIGHT was a significant contribution to this successful outcome.
22630572	Novel approach to utilizing electronic health records for dermatologic research: developing a multi-institutional federated data network for clinical and translational research in psoriasis and psoriatic arthritis.
Dermatol. Online J. 20120515 2012May
The implementation of Electronic Health Records (EHR) in the United States has created new opportunities for research using automated data extraction methods. A large amount of information from the EHR can be utilized for clinical and translational research. To date, a number of institutions have the capability of extracting clinical data from EHR to create local repositories of de-identified data amenable to research queries through the Informatics for Integrated Biology and the Bedside (i2b2) platform. Collaborations among institutions sharing a common i2b2 platform hold exciting opportunities for research in psoriasis and psoriatic arthritis. With the automated extraction of patient-level data from multiple institutions, this novel informatics network has the ability to address high-priority research questions. With commitment to high-quality data through applied algorithms for cohort identification and validation of outcomes, the creation of Psoriasis and Psoriatic Arthritis Integrated Research Data Network (PIONEER) will make a significant contribution to psoriasis and psoriatic arthritis research.
22389073	Herniamed: an internet-based registry for outcome research in hernia surgery.
Hernia 20120303 2012Jun
Despite the high frequency of hernia surgery procedures and continuous improvements, thanks to new hernia meshes and fixation techniques, in Germany, for example, the recurrence rate and rate of chronic inguinal pain after inguinal surgery are more than 10% far too high. Introduction of a hernia register in Denmark led to a significant reduction in the recurrence rate. The aim of a hernia registry as an application-oriented outcome research tool is to monitor and evaluate (concomitant research) how the knowledge gleaned from evidence-based science is implemented in the everyday clinical setting and, ultimately, investigate its effectiveness (outcome research). The new Internet-based English- and German-language registry for the entire spectrum of inpatient and outpatient hernia surgery is designed to improve the quality of patient care and provide valid data on outcome research. Via the Internet, all relevant patient data (comorbidities, previous operations, staging, specific surgical technique, medical devices used, perioperative complications and follow-up data) can be entered into the registry database. The participating hospitals and surgeons can at any time view their own data by means of an evaluation statistics tool. The outcome research project Herniamed focuses on inguinal hernias, umbilical hernias, incisional hernias, epigastric hernias, parastomal hernias and hiatus hernias. The online-based outcome research registry meets the most stringent data protection criteria. With the Internet-based English- and German-language hernia register, a new instrument is now available for outcome research in hernia surgery.
22635174	A multi-thread scheduling method for 3D CT image reconstruction using multi-GPU.
J Xray Sci Technol  2012
As a whole process, we present a concept that the complete reconstruction of CT image should include the computation part on GPUs and the data storage part on hard disks. From this point of view, we propose a Multi-Thread Scheduling (MTS) method to implement the 3D CT image reconstruction such as using FDK algorithm, to trade off the computing and storage time. In this method we use Multi-Threads to control GPUs and a separate thread to accomplish data storage, so that we make the calculation and data storage simultaneously. In addition, we use the 4-channel texture to maintain symmetrical projection data in CUDA framework, which can reduce the calculation time significantly. Numerical experiment shows that the time for the whole process with our method is almost the same as the data storage time.
21243409	Progressive visualization of losslessly compressed DICOM files over the Internet.
J Med Syst 20110118 2012Jun
Presented in this paper is a method for progressive transfer and visualization of losslessly compressed DICOM files. The files from the same study are considered as a volumetric object, which is progressively compressed using a quadtree-based method. The textual information of the DICOM file is compressed using a predicting scheme. Decompression and visualization are implemented with Java applet technology. Users can display and interact with the object even during the data transfer. Tests have shown that an object can be recognized up to 70% faster than with the classical approaches.
22559181	Information intervention in the pharmaceutical sciences.
Med Ref Serv Q  2012
Professional guidelines state that higher-order thinking skills are a desirable outcome of pharmacy education. In this context, courses in pharmaceutics at the University of Southern California are taught in a learner-centered manner that requires use of chemical reference sources and interpretation of physicochemical information for drug molecules. To facilitate these activities, a librarian worked with faculty to design a class on reference sources and primary literature. Students believed the librarian instruction was beneficial. After the intervention, faculty fielded fewer information-related questions and the librarian received more sophisticated questions. The class emphasizes the importance of collaboration between librarians and faculty in achieving these results.
22129225	Estimating meme fitness in adaptive memetic algorithms for combinatorial problems.
Evol Comput 20120130 2012Summer
Among the most promising and active research areas in heuristic optimisation is the field of adaptive memetic algorithms (AMAs). These gain much of their reported robustness by adapting the probability with which each of a set of local improvement operators is applied, according to an estimate of their current value to the search process. This paper addresses the issue of how the current value should be estimated. Assuming the estimate occurs over several applications of a meme, we consider whether the extreme or mean improvements should be used, and whether this aggregation should be global, or local to some part of the solution space. To investigate these issues, we use the well-established COMA framework that coevolves the specification of a population of memes (representing different local search algorithms) alongside a population of candidate solutions to the problem at hand. Two very different memetic algorithms are considered: the first using adaptive operator pursuit to adjust the probabilities of applying a fixed set of memes, and a second which applies genetic operators to dynamically adapt and create memes and their functional definitions. For the latter, especially on combinatorial problems, credit assignment mechanisms based on historical records, or on notions of landscape locality, will have limited application, and it is necessary to estimate the value of a meme via some form of sampling. The results on a set of binary encoded combinatorial problems show that both methods are very effective, and that for some problems it is necessary to use thousands of variables in order to tease apart the differences between different reward schemes. However, for both memetic algorithms, a significant pattern emerges that reward based on mean improvement is better than that based on extreme improvement. This contradicts recent findings from adapting the parameters of operators involved in global evolutionary search. The results also show that local reward schemes outperform global reward schemes in combinatorial spaces, unlike in continuous spaces. An analysis of evolving meme behaviour is used to explain these findings.
22570592	Storage of correlated patterns in standard and bistable Purkinje cell models.
PLoS Comput. Biol. 20120426 2012
The cerebellum has long been considered to undergo supervised learning, with climbing fibers acting as a 'teaching' or 'error' signal. Purkinje cells (PCs), the sole output of the cerebellar cortex, have been considered as analogs of perceptrons storing input/output associations. In support of this hypothesis, a recent study found that the distribution of synaptic weights of a perceptron at maximal capacity is in striking agreement with experimental data in adult rats. However, the calculation was performed using random uncorrelated inputs and outputs. This is a clearly unrealistic assumption since sensory inputs and motor outputs carry a substantial degree of temporal correlations. In this paper, we consider a binary output neuron with a large number of inputs, which is required to store associations between temporally correlated sequences of binary inputs and outputs, modelled as Markov chains. Storage capacity is found to increase with both input and output correlations, and diverges in the limit where both go to unity. We also investigate the capacity of a bistable output unit, since PCs have been shown to be bistable in some experimental conditions. Bistability is shown to enhance storage capacity whenever the output correlation is stronger than the input correlation. Distribution of synaptic weights at maximal capacity is shown to be independent on correlations, and is also unaffected by the presence of bistability.
22526598	3D ultrasound DICOM data of the thyroid gland. First experiences in exporting, archiving, second reading and 3D processing.
Nuklearmedizin 20120416 2012
It has recently become possible to generate and archive three-dimensional ultrasound (3D-US) volume data with the DICOM standard Enhanced Ultrasound Volume Storage (EUVS). The objective of this study was to examine the application of the EUVS standard based on the example of thyroid ultrasound. 32 patients, who were referred for thyroid diagnosis, were given a 3D-US examination of the thyroid gland (GE Voluson E8, convex 3D probe RAB4-8-D). The 3D data sets were exported to EUVS. Necessary additions to DICOM entries and transformation into an established DICOM standard were carried out. The visual assessment and volume measurements were performed by two experts on nuclear medicine using standard software in our hospital. In 24/32 (75%) of the patients, the whole organ was successfully recorded in a single 3D scan; in 8/32 (25%), only part of organ could be covered. In all cases, 3D-US data could be exported and archived. After supplementing the DICOM entry Patient Orientation and transformation into the DICOM PET format, 3D-US data could be displayed in the correct orientation and size at any viewing workstation and any web browser-based PACS viewer. Afterwards, 3D processing such as multiplanar reformation, volumetric measurements and image fusion with data of other cross sectional modalities could be performed. The intraclass correlation of the volume measurements was 0,94 and the interobserver variability was 5.7%. EUVS allows the generation, distribution and archiving of 3D-US data of the thyroid, facilitates a second reading by another physician and creates conditions for advanced 3D processing using routine software.
22543579	Irreversible metamagnetic transition and magnetic memory in small-bandwidth manganite Pr(1-x)Ca(x)MnO3 (x = 0.0-0.5).
J Phys Condens Matter 20120427 2012May30
The present paper reports detailed structural and magnetic characterization of the low-bandwidth manganite Pr(1-x)Ca(x)MnO(3) (with x = 0.0-0.5) (PCMO) polycrystalline samples. With increasing Ca content, reduction of the unit cell volume and improvement in perovskite structure symmetry was observed at room temperature. Magnetic characterization shows the signature of coexisting AFM-FM ordering and spin-glass phase at the low doping range (x = 0.0-0.2) while increased hole doping (x = 0.3-0.5) leads to charge ordering, training effect and an irreversible metamagnetic phenomenon. The large irreversible metamagnetism in the CO phase of PCMO and the corresponding spin memory effect is a direct consequence of hysteretic first-order phase transition arising from the weakening of the CO state under the external magnetic field and trapping of the spins due to a strong pinning potential in the material.
22535905	Scientific information repository assisting reflectance spectrometry in legal medicine.
J Lab Autom 20120424 2012Jun
Reflectance spectrometry is a fast and reliable method for the characterization of human skin if the spectra are analyzed with respect to a physical model describing the optical properties of human skin. For a field study performed at the Institute of Legal Medicine and the Freiburg Materials Research Center of the University of Freiburg, a scientific information repository has been developed, which is a variant of an electronic laboratory notebook and assists in the acquisition, management, and high-throughput analysis of reflectance spectra in heterogeneous research environments. At the core of the repository is a database management system hosting the master data. It is filled with primary data via a graphical user interface (GUI) programmed in Java, which also enables the user to browse the database and access the results of data analysis. The latter is carried out via Matlab, Python, and C programs, which retrieve the primary data from the scientific information repository, perform the analysis, and store the results in the database for further usage.
22403116	Informatics in radiology: use of CouchDB for document-based storage of DICOM objects.
Radiographics 20120308 2012 May-Jun
Picture archiving and communication systems traditionally have depended on schema-based Structured Query Language (SQL) databases for imaging data management. To optimize database size and performance, many such systems store a reduced set of Digital Imaging and Communications in Medicine (DICOM) metadata, discarding informational content that might be needed in the future. As an alternative to traditional database systems, document-based key-value stores recently have gained popularity. These systems store documents containing key-value pairs that facilitate data searches without predefined schemas. Document-based key-value stores are especially suited to archive DICOM objects because DICOM metadata are highly heterogeneous collections of tag-value pairs conveying specific information about imaging modalities, acquisition protocols, and vendor-supported postprocessing options. The authors used an open-source document-based database management system (Apache CouchDB) to create and test two such databases; CouchDB was selected for its overall ease of use, capability for managing attachments, and reliance on HTTP and Representational State Transfer standards for accessing and retrieving data. A large database was created first in which the DICOM metadata from 5880 anonymized magnetic resonance imaging studies (1,949,753 images) were loaded by using a Ruby script. To provide the usual DICOM query functionality, several predefined "views" (standard queries) were created by using JavaScript. For performance comparison, the same queries were executed in both the CouchDB database and a SQL-based DICOM archive. The capabilities of CouchDB for attachment management and database replication were separately assessed in tests of a similar, smaller database. Results showed that CouchDB allowed efficient storage and interrogation of all DICOM objects; with the use of information retrieval algorithms such as map-reduce, all the DICOM metadata stored in the large database were searchable with only a minimal increase in retrieval time over that with the traditional database management system. Results also indicated possible uses for document-based databases in data mining applications such as dose monitoring, quality assurance, and protocol optimization.
22067434	An optimization of allocation of information granularity in the interpretation of data structures: toward granular fuzzy clustering.
IEEE Trans Syst Man Cybern B Cybern 20111103 2012Jun
Clustering forms one of the most visible conceptual and algorithmic framework of developing information granules. In spite of the algorithm being used, the representation of information granules-clusters is predominantly numeric (coming in the form of prototypes, partition matrices, dendrograms, etc.). In this paper, we consider a concept of granular prototypes that generalizes the numeric representation of the clusters and, in this way, helps capture more details about the data structure. By invoking the granulation-degranulation scheme, we design granular prototypes being reflective of the structure of data to a higher extent than the representation that is provided by their numeric counterparts (prototypes). The design is formulated as an optimization problem, which is guided by the coverage criterion, meaning that we maximize the number of data for which their granular realization includes the original data. The granularity of the prototypes themselves is treated as an important design asset; hence, its allocation to the individual prototypes is optimized so that the coverage criterion becomes maximized. With this regard, several schemes of optimal allocation of information granularity are investigated, where interval-valued prototypes are formed around the already produced numeric representatives. Experimental studies are provided in which the design of granular prototypes of interval format is discussed and characterized.
22147305	Practical detection of spammers and content promoters in online video sharing systems.
IEEE Trans Syst Man Cybern B Cybern 20111130 2012Jun
A number of online video sharing systems, out of which YouTube is the most popular, provide features that allow users to post a video as a response to a discussion topic. These features open opportunities for users to introduce polluted content, or simply pollution, into the system. For instance, spammers may post an unrelated video as response to a popular one, aiming at increasing the likelihood of the response being viewed by a larger number of users. Moreover, content promoters may try to gain visibility to a specific video by posting a large number of (potentially unrelated) responses to boost the rank of the responded video, making it appear in the top lists maintained by the system. Content pollution may jeopardize the trust of users on the system, thus compromising its success in promoting social interactions. In spite of that, the available literature is very limited in providing a deep understanding of this problem. In this paper, we address the issue of detecting video spammers and promoters. Towards that end, we first manually build a test collection of real YouTube users, classifying them as spammers, promoters, and legitimate users. Using our test collection, we provide a characterization of content, individual, and social attributes that help distinguish each user class. We then investigate the feasibility of using supervised classification algorithms to automatically detect spammers and promoters, and assess their effectiveness in our test collection. While our classification approach succeeds at separating spammers and promoters from legitimate users, the high cost of manually labeling vast amounts of examples compromises its full potential in realistic scenarios. For this reason, we further propose an active learning approach that automatically chooses a set of examples to label, which is likely to provide the highest amount of information, drastically reducing the amount of required training data while maintaining comparable classification effectiveness.
22249744	Automatic image annotation and retrieval using group sparsity.
IEEE Trans Syst Man Cybern B Cybern 20120110 2012Jun
Automatically assigning relevant text keywords to images is an important problem. Many algorithms have been proposed in the past decade and achieved good performance. Efforts have focused upon model representations of keywords, whereas properties of features have not been well investigated. In most cases, a group of features is preselected, yet important feature properties are not well used to select features. In this paper, we introduce a regularization-based feature selection algorithm to leverage both the sparsity and clustering properties of features, and incorporate it into the image annotation task. Using this group-sparsity-based method, the whole group of features [e.g., red green blue (RGB) or hue, saturation, and value (HSV)] is either selected or removed. Thus, we do not need to extract this group of features when new data comes. A novel approach is also proposed to iteratively obtain similar and dissimilar pairs from both the keyword similarity and the relevance feedback. Thus, keyword similarity is modeled in the annotation framework. We also show that our framework can be employed in image retrieval tasks by selecting different image pairs. Extensive experiments are designed to compare the performance between features, feature combinations, and regularization-based feature selection methods applied on the image annotation task, which gives insight into the properties of features in the image annotation task. The experimental results demonstrate that the group-sparsity-based method is more accurate and stable than others.
22318491	Fast graph-based relaxed clustering for large data sets using minimal enclosing ball.
IEEE Trans Syst Man Cybern B Cybern 20120203 2012Jun
Although graph-based relaxed clustering (GRC) is one of the spectral clustering algorithms with straightforwardness and self-adaptability, it is sensitive to the parameters of the adopted similarity measure and also has high time complexity  O(N(3)) which severely weakens its usefulness for large data sets. In order to overcome these shortcomings, after introducing certain constraints for GRC, an enhanced version of GRC [constrained GRC (CGRC)] is proposed to increase the robustness of GRC to the parameters of the adopted similarity measure, and accordingly, a novel algorithm called fast GRC (FGRC) based on CGRC is developed in this paper by using the core-set-based minimal enclosing ball approximation. A distinctive advantage of FGRC is that its asymptotic time complexity is linear with the data set size  N. At the same time, FGRC also inherits the straightforwardness and self-adaptability from GRC, making the proposed FGRC a fast and effective clustering algorithm for large data sets. The advantages of FGRC are validated by various benchmarking and real data sets.
22334025	Efficient clustering aggregation based on data fragments.
IEEE Trans Syst Man Cybern B Cybern 20120210 2012Jun
Clustering aggregation, known as clustering ensembles, has emerged as a powerful technique for combining different clustering results to obtain a single better clustering. Existing clustering aggregation algorithms are applied directly to data points, in what is referred to as the point-based approach. The algorithms are inefficient if the number of data points is large. We define an efficient approach for clustering aggregation based on data fragments. In this fragment-based approach, a data fragment is any subset of the data that is not split by any of the clustering results. To establish the theoretical bases of the proposed approach, we prove that clustering aggregation can be performed directly on data fragments under two widely used goodness measures for clustering aggregation taken from the literature. Three new clustering aggregation algorithms are described. The experimental results obtained using several public data sets show that the new algorithms have lower computational complexity than three well-known existing point-based clustering aggregation algorithms (Agglomerative, Furthest, and LocalSearch); nevertheless, the new algorithms do not sacrifice the accuracy.
22589136	Computational analysis of RNA-seq.
Methods Mol. Biol.  2012
Using High-Throughput DNA Sequencing (HTS) to examine gene expression is rapidly becoming a -viable choice and is typically referred to as RNA-seq. Often the depth and breadth of coverage of RNA-seq data can exceed what is achievable using microarrays. However, the strengths of RNA-seq are often its greatest weaknesses. Accurately and comprehensively mapping millions of relatively short reads to a reference genome sequence can require not only specialized software, but also more structured and automated procedures to manage, analyze, and visualize the data. Additionally, the computational hardware required to efficiently process and store the data can be a necessary and often-overlooked component of a research plan. We discuss several aspects of the computational analysis of RNA-seq, including file management and data quality control, analysis, and visualization. We provide a framework for a standard nomenclature -system that can facilitate automation and the ability to track data provenance. Finally, we provide a general workflow of the computational analysis of RNA-seq and a downloadable package of scripts to automate the processing.
22587829	Routine development of objectively derived search strategies.
Syst Rev 20120229 2012
Over the past few years, information retrieval has become more and more professionalized, and information specialists are considered full members of a research team conducting systematic reviews. Research groups preparing systematic reviews and clinical practice guidelines have been the driving force in the development of search strategies, but open questions remain regarding the transparency of the development process and the available resources. An empirically guided approach to the development of a search strategy provides a way to increase transparency and efficiency. Our aim in this paper is to describe the empirically guided development process for search strategies as applied by the German Institute for Quality and Efficiency in Health Care (Institut für Qualität und Wirtschaftlichkeit im Gesundheitswesen, or "IQWiG"). This strategy consists of the following steps: generation of a test set, as well as the development, validation and standardized documentation of the search strategy. We illustrate our approach by means of an example, that is, a search for literature on brachytherapy in patients with prostate cancer. For this purpose, a test set was generated, including a total of 38 references from 3 systematic reviews. The development set for the generation of the strategy included 25 references. After application of textual analytic procedures, a strategy was developed that included all references in the development set. To test the search strategy on an independent set of references, the remaining 13 references in the test set (the validation set) were used. The validation set was also completely identified. Our conclusion is that an objectively derived approach similar to that used in search filter development is a feasible way to develop and validate reliable search strategies. Besides creating high-quality strategies, the widespread application of this approach will result in a substantial increase in the transparency of the development process of search strategies.
22588034	Should methodological filters for diagnostic test accuracy studies be used in systematic reviews of psychometric instruments? A case study involving screening for postnatal depression.
Syst Rev 20120209 2012
Challenges exist when searching for diagnostic test accuracy (DTA) studies that include the design of DTA search strategies and selection of appropriate filters. This paper compares the performance of three MEDLINE search strategies for psychometric diagnostic test accuracy (DTA) studies in postnatal depression. A reference set of six relevant studies was derived from a forward citation search via Web of Knowledge. The performance of the 'target condition and index test' method recommended by the Cochrane DTA Group was compared to two alternative strategies which included methodological filters. Outcome measures were total citations retrieved, sensitivity, precision and associated 95% confidence intervals (95%CI). The Cochrane recommended strategy and one of the filtered search strategies were equivalent in performance and both retrieved a total of 105 citations, sensitivity was 100% (95% CI 61%, 100%) and precision was 5.2% (2.6%, 11.9%). The second filtered search retrieved a total of 31 citations, sensitivity was 66.6% (30%, 90%) and precision was 12.9% (5.1%, 28.6%). This search missed the DTA study with most relevance to the DTA review. The Cochrane recommended search strategy, 'target condition and index test', method was pragmatic and sensitive. It was considered the optimum method for retrieval of relevant studies for a psychometric DTA review (in this case for postnatal depression). Potential limitations of using filtered searches during a psychometric mental health DTA review should be considered.
22526859	[Using value of information analysis in decision making about applied research. The case of genetic screening for hemochromatosis in Germany].
Bundesgesundheitsblatt Gesundheitsforschung Gesundheitsschutz  2012May
Public decision makers face demands to invest in applied research in order to accelerate the adoption of new genetic tests. However, such an investment is profitable only if the results gained from further investigations have a significant impact on health care practice. An upper limit for the value of additional information aimed at improving the basis for reimbursement decisions is given by the expected value of perfect information (EVPI). This study illustrates the significance of the concept of EVPI on the basis of a probabilistic cost-effectiveness model of screening for hereditary hemochromatosis among German men. In the present example, population-based screening can barely be recommended at threshold values of 50,000 or 100,000 Euro per life year gained and also the value of additional research which might cause this decision to be overturned is small: At the mentioned threshold values, the EVPI in the German public health care system was ca. 500,000 and 2,200,000 Euro, respectively. An analysis of EVPI by individual parameters or groups of parameters shows that additional research about adherence to preventive phlebotomy could potentially provide the highest benefit. The potential value of further research also depends on methodological assumptions regarding the decision maker's time horizon as well as on scenarios with an impact on the number of affected patients and the cost-effectiveness of screening.
22536972	An ICT infrastructure to integrate clinical and molecular data in oncology research.
BMC Bioinformatics 20120328 2012
The ONCO-i2b2 platform is a bioinformatics tool designed to integrate clinical and research data and support translational research in oncology. It is implemented by the University of Pavia and the IRCCS Fondazione Maugeri hospital (FSM), and grounded on the software developed by the Informatics for Integrating Biology and the Bedside (i2b2) research center. I2b2 has delivered an open source suite based on a data warehouse, which is efficiently interrogated to find sets of interesting patients through a query tool interface. Onco-i2b2 integrates data coming from multiple sources and allows the users to jointly query them. I2b2 data are then stored in a data warehouse, where facts are hierarchically structured as ontologies. Onco-i2b2 gathers data from the FSM pathology unit (PU) database and from the hospital biobank and merges them with the clinical information from the hospital information system. Our main effort was to provide a robust integrated research environment, giving a particular emphasis to the integration process and facing different challenges, consecutively listed: biospecimen samples privacy and anonymization; synchronization of the biobank database with the i2b2 data warehouse through a series of Extract, Transform, Load (ETL) operations; development and integration of a Natural Language Processing (NLP) module, to retrieve coded information, such as SNOMED terms and malignant tumors (TNM) classifications, and clinical tests results from unstructured medical records. Furthermore, we have developed an internal SNOMED ontology rested on the NCBO BioPortal web services. Onco-i2b2 manages data of more than 6,500 patients with breast cancer diagnosis collected between 2001 and 2011 (over 390 of them have at least one biological sample in the cancer biobank), more than 47,000 visits and 96,000 observations over 960 medical concepts. Onco-i2b2 is a concrete example of how integrated Information and Communication Technology architecture can be implemented to support translational research. The next steps of our project will involve the extension of its capabilities by implementing new plug-in devoted to bioinformatics data analysis as well as a temporal query module.
22424986	Search filters can find some but not all knowledge translation articles in MEDLINE: an analytic survey.
J Clin Epidemiol 20120316 2012Jun
Advances from health research are not well applied giving rise to over- and underuse of resources and inferior care. Knowledge translation (KT), actions and processes of getting research findings used in practice, can improve research application. The KT literature is difficult to find because of nonstandardized terminology, rapid evolution of the field, and it is spread across several domains. We created multiple search filters to retrieve KT articles from MEDLINE. Analytic survey using articles from 12 journals tagged as having KT content and also as describing a KT application or containing a KT theory. Of 2,594 articles, 579 were KT articles of which 201 were about KT applications and 152 about KT theory. Search filter sensitivity (retrieval efficiency) maximized at 83%-94% with specificity (no retrieval of irrelevant material) approximately 50%. Filter performances were enhanced with multiple terms, but these filters often had reduced specificity. Performance was higher for KT applications and KT theory articles. These filters can select KT material although many irrelevant articles also will be retrieved. KT search filters were developed and tested, with good sensitivity but suboptimal specificity. Further research must improve their performance.
20974098	Abstracting pain management documentation from the electronic medical record: comparison of three hospitals.
Appl Nurs Res 20100629 2012May
Pain management science results are derived from research conducted using medical record. This article describes methodological issues arising from abstracting pain management documentation (PMD) from the electronic medical record in three hospitals. After approval, PMD data were collected from the patient's history and physical, discharge summary, operative care notes, computerized nursing flow sheets, progress notes, and medication records. Each acute care facility required a different approach to abstract data. Inconsistent documentation in pain management assessments, interventions, and reassessments were identified across hospitals. Inconsistencies pose measurement threats and hinder benchmarking efforts. Work to standardize PMD across propriety computer systems is warranted.
20933295	Mammogram retrieval on similar mass lesions.
Comput Methods Programs Biomed 20101008 2012Jun
Enormous numbers of digital mammograms have been produced in hospitals and breast screening centers. To exploit those valuable resources in aiding diagnoses and research, content-based mammogram retrieval systems are required to effectively access the mammogram databases. This paper presents a content-based mammogram retrieval system, which allows medical professionals to seek mass lesions that are pathologically similar to a given example. In this retrieval system, shape and margin features of mass lesions are extracted to represent the characteristics of mammographic lesions. To compare the similarity between the query example and any lesion within the databases, this study proposes a similarity measure scheme which involves the hierarchical arrangement of mammographic features and a weighting distance measure. This makes similarity measure of the retrieval system consistent with the way radiologists observe mass lesions. This study used the DDSM dataset to evaluate the effectiveness of the extracted shape feature and margin feature, respectively. Experimental results demonstrate that, when Zernike moments are used, round-shape masses are the most discriminative among four types of shape; the circumscribed-margin masses can be effectively discriminated among the four types of margins. Moreover, the result also shows that, when retrieving round-shape and circumscribed margin masses, this retrieval system can achieve the highest precision among all mass lesion types.
22249712	Image editing with spatiograms transfer.
IEEE Trans Image Process 20120109 2012May
Histogram equalization is a well-known method for image contrast enhancement. Nevertheless, as histograms do not include any information on the spatial repartition of colors, their application to local image editing problems remains limited. To cope with this lack of spatial information, spatiograms have been recently proposed for tracking purposes. A spatiogram is an image descriptor that combines a histogram with the mean and the variance of the position of each color. In this paper, we address the problem of local retouching of images by proposing a variational method for spatiogram transfer. More precisely, a reference spatiogram is used to modify the color value of a given region of interest of the processed image. Experiments on shadow removal and inpainting demonstrate the strength of the proposed approach.
22550104	Ethics and the electronic health record in dental school clinics.
J Dent Educ  2012May
Electronic health records (EHRs) are a major development in the practice of dentistry, and dental schools and dental curricula have benefitted from this technology. Patient data entry, storage, retrieval, transmission, and archiving have been streamlined, and the potential for teledentistry and improvement in epidemiological research is beginning to be realized. However, maintaining patient health information in an electronic form has also changed the environment in dental education, setting up potential ethical dilemmas for students and faculty members. The purpose of this article is to explore some of the ethical issues related to EHRs, the advantages and concerns related to the use of computers in the dental operatory, the impact of the EHR on the doctor-patient relationship, the introduction of web-based EHRs, the link between technology and ethics, and potential solutions for the management of ethical concerns related to EHRs in dental schools.
21678039	A PACS archive architecture supported on cloud services.
Int J Comput Assist Radiol Surg 20110616 2012May
Diagnostic imaging procedures have continuously increased over the last decade and this trend may continue in coming years, creating a great impact on storage and retrieval capabilities of current PACS. Moreover, many smaller centers do not have financial resources or requirements that justify the acquisition of a traditional infrastructure. Alternative solutions, such as cloud computing, may help address this emerging need. A tremendous amount of ubiquitous computational power, such as that provided by Google and Amazon, are used every day as a normal commodity. Taking advantage of this new paradigm, an architecture for a Cloud-based PACS archive that provides data privacy, integrity, and availability is proposed. The solution is independent from the cloud provider and the core modules were successfully instantiated in examples of two cloud computing providers. Operational metrics for several medical imaging modalities were tabulated and compared for Google Storage, Amazon S3, and LAN PACS. A PACS-as-a-Service archive that provides storage of medical studies using the Cloud was developed. The results show that the solution is robust and that it is possible to store, query, and retrieve all desired studies in a similar way as in a local PACS approach. Cloud computing is an emerging solution that promises high scalability of infrastructures, software, and applications, according to a "pay-as-you-go" business model. The presented architecture uses the cloud to setup medical data repositories and can have a significant impact on healthcare institutions by reducing IT infrastructures.
21800188	Case-based fracture image retrieval.
Int J Comput Assist Radiol Surg 20110729 2012May
Case-based fracture image retrieval can assist surgeons in decisions regarding new cases by supplying visually similar past cases. This tool may guide fracture fixation and management through comparison of long-term outcomes in similar cases. A fracture image database collected over 10 years at the orthopedic service of the University Hospitals of Geneva was used. This database contains 2,690 fracture cases associated with 43 classes (based on the AO/OTA classification). A case-based retrieval engine was developed and evaluated using retrieval precision as a performance metric. Only cases in the same class as the query case are considered as relevant. The scale-invariant feature transform (SIFT) is used for image analysis. Performance evaluation was computed in terms of mean average precision (MAP) and early precision (P10, P30). Retrieval results produced with the GNU image finding tool (GIFT) were used as a baseline. Two sampling strategies were evaluated. One used a dense 40 × 40 pixel grid sampling, and the second one used the standard SIFT features. Based on dense pixel grid sampling, three unsupervised feature selection strategies were introduced to further improve retrieval performance. With dense pixel grid sampling, the image is divided into 1,600 (40 × 40) square blocks. The goal is to emphasize the salient regions (blocks) and ignore irrelevant regions. Regions are considered as important when a high variance of the visual features is found. The first strategy is to calculate the variance of all descriptors on the global database. The second strategy is to calculate the variance of all descriptors for each case. A third strategy is to perform a thumbnail image clustering in a first step and then to calculate the variance for each cluster. Finally, a fusion between a SIFT-based system and GIFT is performed. A first comparison on the selection of sampling strategies using SIFT features shows that dense sampling using a pixel grid (MAP = 0.18) outperformed the SIFT detector-based sampling approach (MAP = 0.10). In a second step, three unsupervised feature selection strategies were evaluated. A grid parameter search is applied to optimize parameters for feature selection and clustering. Results show that using half of the regions (700 or 800) obtains the best performance for all three strategies. Increasing the number of clusters in clustering can also improve the retrieval performance. The SIFT descriptor variance in each case gave the best indication of saliency for the regions (MAP = 0.23), better than the other two strategies (MAP = 0.20 and 0.21). Combining GIFT (MAP = 0.23) and the best SIFT strategy (MAP = 0.23) produced significantly better results (MAP = 0.27) than each system alone. A case-based fracture retrieval engine was developed and is available for online demonstration. SIFT is used to extract local features, and three feature selection strategies were introduced and evaluated. A baseline using the GIFT system was used to evaluate the salient point-based approaches. Without supervised learning, SIFT-based systems with optimized parameters slightly outperformed the GIFT system. A fusion of the two approaches shows that the information contained in the two approaches is complementary. Supervised learning on the feature space is foreseen as the next step of this study.
21846778	Search terms and a validated brief search filter to retrieve publications on health-related values in Medline: a word frequency analysis study.
J Am Med Inform Assoc 20110816 2012 May-Jun
Healthcare debates and policy developments are increasingly concerned with a broad range of values-related areas. These include not only ethical, moral, religious, and other types of values 'proper', but also beliefs, preferences, experiences, choices, satisfaction, quality of life, etc. Research on such issues may be difficult to retrieve. This study used word frequency analysis to generate a broad pool of search terms and a brief filter to facilitate relevant searches in bibliographic databases. Word frequency analysis for 'values terms' was performed on citations on diabetes, obesity, dementia, and schizophrenia (Medline; 2004-2006; 4440 citations; 1,110,291 words). Concordance® and SPSS 14.0 were used. Text words and MeSH terms of high frequency and precision were compiled into a search filter. It was validated on datasets of citations on dentistry and food hypersensitivity. 144 unique text words and 124 unique MeSH terms of moderate and high frequency (≥ 20) and very high precision (≥ 90%) were identified. Of these, 19 text words and seven MeSH terms were compiled into a 'brief values filter'. In the derivation dataset, it had a sensitivity of 76.8% and precision of 86.8%. In the validation datasets, its sensitivity and precision were, respectively, 70.1% and 63.6% (food hypersensitivity) and 47.1% and 82.6% (dentistry). This study provided a varied pool of search terms and a simple and highly effective tool for retrieving publications on health-related values. Further work is required to facilitate access to such research and enhance its chances of being translated into practice, policy, and service improvements.
21917645	Predicting biomedical document access as a function of past use.
J Am Med Inform Assoc 20110913 2012 May-Jun
To determine whether past access to biomedical documents can predict future document access. The authors used 394 days of query log (August 1, 2009 to August 29, 2010) from PubMed users in the Texas Medical Center, which is the largest medical center in the world. The authors evaluated two document access models based on the work of Anderson and Schooler. The first is based on how frequently a document was accessed. The second is based on both frequency and recency. The model based only on frequency of past access was highly correlated with the empirical data (R²=0.932), whereas the model based on frequency and recency had a much lower correlation (R²=0.668). The frequency-only model accurately predicted whether a document will be accessed based on past use. Modeling accesses as a function of frequency requires storing only the number of accesses and the creation date for the document. This model requires low storage overheads and is computationally efficient, making it scalable to large corpora such as MEDLINE. It is feasible to accurately model the probability of a document being accessed in the future based on past accesses.
21946235	Search filters to identify geriatric medicine in Medline.
J Am Med Inform Assoc 20110923 2012 May-Jun
To create user-friendly search filters with high sensitivity, specificity, and precision to identify articles on geriatric medicine in Medline. A diagnostic test assessment framework was used. A reference set of 2255 articles was created by hand-searching 22 biomedical journals in Medline, and each article was labeled as 'relevant', 'not relevant', or 'possibly relevant' for geriatric medicine. From the relevant articles, search terms were identified to compile different search strategies. The articles retrieved by the various search strategies were compared with articles from the reference set as the index test to create the search filters. Sensitivity, specificity, precision, accuracy, and number-needed-to-read (NNR) were calculated by comparing the results retrieved by the different search strategies with the reference set. The most sensitive search filter had a sensitivity of 94.8%, a specificity of 88.7%, a precision of 73.0%, and an accuracy of 90.2%. It had an NNR of 1.37. The most specific search filter had a specificity of 96.6%, a sensitivity of 69.1%, a precision of 86.6%, and an accuracy of 89.9%. It had an NNR of 1.15. These geriatric search filters simplify searching for relevant literature and therefore contribute to better evidence-based practice. The filters are useful to both the clinician who wants to find a quick answer to a clinical question and the researcher who wants to find as many relevant articles as possible without retrieving too many irrelevant articles.
21984605	PASTE: patient-centered SMS text tagging in a medication management system.
J Am Med Inform Assoc 20111008 2012 May-Jun
To evaluate the performance of a system that extracts medication information and administration-related actions from patient short message service (SMS) messages. Mobile technologies provide a platform for electronic patient-centered medication management. MyMediHealth (MMH) is a medication management system that includes a medication scheduler, a medication administration record, and a reminder engine that sends text messages to cell phones. The object of this work was to extend MMH to allow two-way interaction using mobile phone-based SMS technology. Unprompted text-message communication with patients using natural language could engage patients in their healthcare, but presents unique natural language processing challenges. The authors developed a new functional component of MMH, the Patient-centered Automated SMS Tagging Engine (PASTE). The PASTE web service uses natural language processing methods, custom lexicons, and existing knowledge sources to extract and tag medication information from patient text messages. A pilot evaluation of PASTE was completed using 130 medication messages anonymously submitted by 16 volunteers via a website. System output was compared with manually tagged messages. Verified medication names, medication terms, and action terms reached high F-measures of 91.3%, 94.7%, and 90.4%, respectively. The overall medication name F-measure was 79.8%, and the medication action term F-measure was 90%. Other studies have demonstrated systems that successfully extract medication information from clinical documents using semantic tagging, regular expression-based approaches, or a combination of both approaches. This evaluation demonstrates the feasibility of extracting medication information from patient-generated medication messages.
22491102	The Swedish strategy and method for development of a national healthcare information architecture.
Stud Health Technol Inform  2012
"We need a precise framework of regulations in order to maintain appropriate and structured health care documentation that ensures that the information maintains a sufficient level of quality to be used in treatment, in research and by the actual patient. The users shall be aided by clearly and uniformly defined terms and concepts, and there should be an information structure that clarifies what to document and how to make the information more useful. Most of all, we need to standardize the information, not just the technical systems." (eHälsa - nytta och näring, Riksdag report 2011/12:RFR5, p. 37). In 2010, the Swedish Government adopted the National e-Health - the national strategy for accessible and secure information in healthcare. The strategy is a revision and extension of the previous strategy from 2006, which was used as input for the most recent efforts to develop a national information structure utilizing business-oriented generic models. A national decision on healthcare informatics standards was made by the Swedish County Councils, which decided to follow and use EN/ISO 13606 as a standard for the development of a universally applicable information structure, including archetypes and templates. The overall aim of the Swedish strategy for development of National Healthcare Information Architecture is to achieve high level semantic interoperability for clinical content and clinical contexts. High level semantic interoperability requires consistently structured clinical data and other types of data with coherent traceability to be mapped to reference clinical models. Archetypes that are formal definitions of the clinical and demographic concepts and some administrative data were developed. Each archetype describes the information structure and content of overarching core clinical concepts. Information that is defined in archetypes should be used for different purposes. Generic clinical process model was made concrete and analyzed. For each decision-making step in the process where information is processed, the amount and type of information and its structure were defined in terms of reference templates. Reference templates manage clinical, administrative and demographic types of information in a specific clinical context. Based on a survey of clinical processes at the reference level, the identification of specific clinical processes such as diabetes and congestive heart failure in adults were made. Process-specific templates were defined by using reference templates and populated with information that was relevant to each health problem in a specific clinical context. Throughout this process, medical data for knowledge management were collected for each health problem. Parallel with the efforts to define archetypes and templates, terminology binding work is on-going. Different strategies are used depending on the terminology binding level.
22491103	Lessons from the English National Programme for IT about structure, process and utility.
Stud Health Technol Inform  2012
Sharing of health data though the effective deployment of information systems should allow safer and more efficient health systems. However, to date many large IT system deployments in health care have had major short comings. This paper critically appraises the UK National Programme for IT and suggests where there are important lessons of for other large scale eHealth projects. Our method combined the classic evaluation methods of Donnabedian with Pawson's realistic review to analyze the impact of the program at health service, locality or major provider, and client-service impact levels. Financial incentives promoted uptake and use of IT systems at all levels. Health service level interventions that were capable of incorporation into clinical workflow were used. These included: a national unique identifier, creation of national registries and electronic transfer of data, records, and results. At the regional and major provider level we identified how vendors offer very different electronic patient record (EPR) systems which influence what is recorded and health care delivery. Using the EPR at the point of care takes longer, but this investment of time creates a more usable record and facilitates quality. National IT systems need to be clinically orientated, patient accessible, and underpinned by a secure, standardized back office system that enables messaging and information sharing between authenticated users. Learning the lessons from the UK and other large system deployments might enable other countries to leap to the forefront of health care computing.
22491116	Information system for the implementation of individual rehabilitation programs for persons with disabilities in Nizhny Novgorod region.
Stud Health Technol Inform  2012
In December 2011, the first phase of the project aimed at developing an information system for the implementation of individual rehabilitation programs for persons with disabilities was finished in Nizhny Novgorod region of Russia. It included the installation of 40 workstations in the Ministry for Social Policy and 8 institutions of Nizhny Novgorod region. Accumulated data were moved to a new information system based on a distributed database. In 2012, the rest of the regional rehabilitation institutions are to join this information system. A transition to a centralized database is planned.
22491117	Consistent data recording across a health system and web-enablement allow service quality comparisons: online data for commissioning dermatology services.
Stud Health Technol Inform  2012
A new distributed model of health care management is being introduced in England. Family practitioners have new responsibilities for the management of health care budgets and commissioning of services. There are national datasets available about health care providers and the geographical areas they serve. These data could be better used to assist the family practitioner turned health service commissioners. Unfortunately these data are not in a form that is readily usable by these fledgling family commissioning groups. We therefore Web enabled all the national hospital dermatology treatment data in England combining it with locality data to provide a smart commissioning tool for local communities. We used open-source software including the Ruby on Rails Web framework and MySQL. The system has a Web front-end, which uses hypertext markup language cascading style sheets (HTML/CSS) and JavaScript to deliver and present data provided by the database. A combination of advanced caching and schema structures allows for faster data retrieval on every execution. The system provides an intuitive environment for data analysis and processing across a large health system dataset. Web-enablement has enabled data about in patients, day cases and outpatients to be readily grouped, viewed, and linked to other data. The combination of web-enablement, consistent data collection from all providers; readily available locality data; and a registration based primary system enables the creation of data, which can be used to commission dermatology services in small areas. Standardized datasets collected across large health enterprises when web enabled can readily benchmark local services and inform commissioning decisions.
22491119	Cloud computing technology applied in healthcare for developing large scale flexible solutions.
Stud Health Technol Inform  2012
An extremely important area in which there is also vital information needed in different locations is the healthcare domain. In the areas of healthcare there is an important exchange of information since there are many departments where a patient can be sent for investigation. In this regard cloud computing is a technology that could really help supporting flexibility, seamless care and financial cuts.
22491124	A user-friendly tool for medical-related patent retrieval.
Stud Health Technol Inform  2012
Health-related information retrieval is complicated by the variety of nomenclatures available to name entities, since different communities of users will use different ways to name a same entity. We present in this report the development and evaluation of a user-friendly interactive Web application aiming at facilitating health-related patent search. Our tool, called TWINC, relies on a search engine tuned during several patent retrieval competitions, enhanced with intelligent interaction modules, such as chemical query, normalization and expansion. While the functionality of related article search showed promising performances, the ad hoc search results in fairly contrasted results. Nonetheless, TWINC performed well during the PatOlympics competition and was appreciated by intellectual property experts. This result should be balanced by the limited evaluation sample. We can also assume that it can be customized to be applied in corporate search environments to process domain and company-specific vocabularies, including non-English literature and patents reports.
22495919	[Use of medical treatment data outside of the patient supply: best way pseudonymisation].
Dtsch. Med. Wochenschr. 20120411 2012Apr
The use of data pending during the patient supply to research, quality assurance                     as also to education is desirable. However, this use is not allowed in every                     German federal land without approval of the patient. Just in the case of                     retrospective research an approval of the patient is often not to be received.                     For the use of medical routine data for the research the pseudonymisation is                     good therefore. Pseudonymisation is a procedure by which all person-related data                     within a data record is replaced by one artificial identifier. Therefore                     pseudonymisation facilitates the linking of medical data and the data                     identifying the patient only under certain, before defined and controllable                     conditions. Through that medical data can be passed on to third party without                     this third party being able to identify the person who the medical data belong                     to. Under consideration of the present possibilities of the information                     technology as also the available technical preparations for the use of the                     pseudonymisation and the advantages being inherent in the pseudonymisation the                     pseudonymisation represents the method of choice during the use of data of the                     patient supply to the research, quality assurance as well as education.
22496632	Automatic filtering and substantiation of drug safety signals.
PLoS Comput. Biol. 20120405 2012
Drug safety issues pose serious health threats to the population and constitute a major cause of mortality worldwide. Due to the prominent implications to both public health and the pharmaceutical industry, it is of great importance to unravel the molecular mechanisms by which an adverse drug reaction can be potentially elicited. These mechanisms can be investigated by placing the pharmaco-epidemiologically detected adverse drug reaction in an information-rich context and by exploiting all currently available biomedical knowledge to substantiate it. We present a computational framework for the biological annotation of potential adverse drug reactions. First, the proposed framework investigates previous evidences on the drug-event association in the context of biomedical literature (signal filtering). Then, it seeks to provide a biological explanation (signal substantiation) by exploring mechanistic connections that might explain why a drug produces a specific adverse reaction. The mechanistic connections include the activity of the drug, related compounds and drug metabolites on protein targets, the association of protein targets to clinical events, and the annotation of proteins (both protein targets and proteins associated with clinical events) to biological pathways. Hence, the workflows for signal filtering and substantiation integrate modules for literature and database mining, in silico drug-target profiling, and analyses based on gene-disease networks and biological pathways. Application examples of these workflows carried out on selected cases of drug safety signals are discussed. The methodology and workflows presented offer a novel approach to explore the molecular mechanisms underlying adverse drug reactions.
22155961	Interlayer bit allocation for scalable video coding.
IEEE Trans Image Process 20111202 2012May
In this paper, we present a theoretical analysis of the distortion in multilayer coding structures. Specifically, we analyze the prediction structure used to achieve temporal, spatial, and quality scalability of scalable video coding (SVC) and show that the average peak signal-to-noise ratio (PSNR) of SVC is a weighted combination of the bit rates assigned to all the streams. Our analysis utilizes the end user's preference for certain resolutions. We also propose a rate-distortion (R-D) optimization algorithm and compare its performance with that of a state-of-the-art scalable bit allocation algorithm. The reported experiment results demonstrate that the R-D algorithm significantly outperforms the compared approach in terms of the average PSNR.
22249710	Scale-invariant features for 3-D mesh models.
IEEE Trans Image Process 20120109 2012May
In this paper, we present a framework for detecting interest points in 3-D meshes and computing their corresponding descriptors. For that, we propose an intrinsic scale detection scheme per interest point and utilize it to derive two scale-invariant local features for mesh models. First, we present the scale-invariant spin image local descriptor that is a scale-invariant formulation of the spin image descriptor. Second, we adapt the scale-invariant feature transform feature to mesh data by representing the vicinity of each interest point as a depth map and estimating its dominant angle using the principal component analysis to achieve rotation invariance. The proposed features were experimentally shown to be robust to scale changes and partial mesh matching, and they were compared favorably with other local mesh features on the SHREC'10 and SHREC'11 testbeds. We applied the proposed local features to mesh retrieval using the bag-of-features approach and achieved state-of-the-art retrieval accuracy. Last, we applied the proposed local features to register models to scanned depth scenes and achieved high registration accuracy.
22262681	A unified feature and instance selection framework using optimum experimental design.
IEEE Trans Image Process 20120112 2012May
The goal of feature selection is to identify the most informative features for compact representation, whereas the goal of active learning is to select the most informative instances for prediction. Previous studies separately address these two problems, despite of the fact that selecting features and instances are dual operations over a data matrix. In this paper, we consider the novel problem of simultaneously selecting the most informative features and instances and develop a solution from the perspective of optimum experimental design. That is, by using the selected features as the new representation and the selected instances as training data, the variance of the parameter estimate of a learning function can be minimized. Specifically, we propose a novel approach, which is called Unified criterion for Feature and Instance selection (UFI), to simultaneously identify the most informative features and instances that minimize the trace of the parameter covariance matrix. A greedy algorithm is introduced to efficiently solve the optimization problem. Experimental results on two benchmark data sets demonstrate the effectiveness of our proposed method.
22287242	Sparse representations for range data restoration.
IEEE Trans Image Process 20120127 2012May
In this paper, the problem of denoising and occlusion restoration of 3-D range data based on dictionary learning and sparse representation methods is explored. We apply these techniques after converting the noisy 3-D surface into one or more images. We present experimental results on the proposed approaches.
22311861	A fast O(N) multiresolution polygonal approximation algorithm for GPS trajectory simplification.
IEEE Trans Image Process 20120131 2012May
Recent advances in geopositioning mobile phones have made it possible for users to collect a large number of GPS trajectories by recording their location information. However, these mobile phones with built-in GPS devices usually record far more data than needed, which brings about both heavy data storage and a computationally expensive burden in the rendering process for a Web browser. To address this practical problem, we present a fast polygonal approximation algorithm in 2-D space for the GPS trajectory simplification under the so-called integral square synchronous distance error criterion in a linear time complexity. The underlying algorithm is designed and implemented using a bottom-up multiresolution method, where the input of polygonal approximation in the coarser resolution is the polygonal curve achieved in the finer resolution. For each resolution (map scale), priority-queue structure is exploited in graph construction to construct the initialized approximated curve. Once the polygonal curve is initialized, two fine-tune algorithms are employed in order to achieve the desirable quality level. Experimental results validated that the proposed algorithm is fast and achieves a better approximation result than the existing competitive methods.
22334005	Efficient rate-distortion optimal packetization of embedded bitstreams into independent source packets.
IEEE Trans Image Process 20120210 2012May
This paper addresses the rate-distortion (R-D) optimal packetization (RDOP) of embedded bitstreams into independent source packets, in order to limit error propagation in transmission of images over packet noisy channels. The input embedded stream is assumed to be an interleaving of K independently decodable basic streams. To form independent source packets, the set of basic streams is partitioned into N groups. The streams within each group are then interleaved to generate a source packet. Error/erasure protection may be further applied along/across source packets, to produce the channel packets to be transmitted. The RDOP problem previously formulated by Wu et at. has the goal of finding the partitioning that minimizes the distortion when all source packets are decoded. We extend the problem formulation such that to also include the minimization of the expected distortion for general transmission scenarios that may apply uneven erasure/ error protection. Further, we show that the dynamic programming (DP) algorithm of Wu et al. can be extended to solve the general RDOP problem. The main contribution of this paper is a fast divide-and-conquer algorithm (D&amp;C) to find the globally optimal solution, under the assumption that all basic streams have convex R-D curves. Instrumental in obtaining the fast solution is our result which proves that the problem can be formulated as a series of matrix search problems in totally monotone matrices. The proposed D&amp;C reduces the running time from O(K(2)LN) where L is the size of each packet achieved by the DP solution to O(NKL log K). Experiments on SPIHT coded images demonstrate that the speedup is significant in practice.
22334006	Segmentation of stochastic images with a stochastic random walker method.
IEEE Trans Image Process 20120210 2012May
We present an extension of the random walker segmentation to images with uncertain gray values. Such gray-value uncertainty may result from noise or other imaging artifacts or more general from measurement errors in the image acquisition process. The purpose is to quantify the influence of the gray-value uncertainty onto the result when using random walker segmentation. In random walker segmentation, a weighted graph is built from the image, where the edge weights depend on the image gradient between the pixels. For given seed regions, the probability is evaluated for a random walk on this graph starting at a pixel to end in one of the seed regions. Here, we extend this method to images with uncertain gray values. To this end, we consider the pixel values to be random variables (RVs), thus introducing the notion of stochastic images. We end up with stochastic weights for the graph in random walker segmentation and a stochastic partial differential equation (PDE) that has to be solved. We discretize the RVs and the stochastic PDE by the method of generalized polynomial chaos, combining the recent developments in numerical methods for the discretization of stochastic PDEs and an interactive segmentation algorithm. The resulting algorithm allows for the detection of regions where the segmentation result is highly influenced by the uncertain pixel values. Thus, it gives a reliability estimate for the resulting segmentation, and it furthermore allows determining the probability density function of the segmented object volume.
22514130	Local tetra patterns: a new feature descriptor for content-based image retrieval.
IEEE Trans Image Process  2012May
In this paper, we propose a novel image indexing and retrieval algorithm using local tetra patterns (LTrPs) for content-based image retrieval (CBIR). The standard local binary pattern (LBP) and local ternary pattern (LTP) encode the relationship between the referenced pixel and its surrounding neighbors by computing gray-level difference. The proposed method encodes the relationship between the referenced pixel and its neighbors, based on the directions that are calculated using the first-order derivatives in vertical and horizontal directions. In addition, we propose a generic strategy to compute  nth-order LTrP using  (n - 1)th-order horizontal and vertical derivatives for efficient CBIR and analyze the effectiveness of our proposed algorithm by combining it with the Gabor transform. The performance of the proposed method is compared with the LBP, the local derivative patterns, and the LTP based on the results obtained using benchmark image databases viz., Corel 1000 database (DB1), Brodatz texture database (DB2), and MIT VisTex database (DB3). Performance analysis shows that the proposed method improves the retrieval result from 70.34%/44.9% to 75.9%/48.7% in terms of average precision/average recall on database DB1, and from 79.97% to 85.30% and 82.23% to 90.02% in terms of average retrieval rate on databases DB2 and DB3, respectively, as compared with the standard LBP.
22514507	Rethinking information delivery: using a natural language processing application for point-of-care data discovery.
J Med Libr Assoc  2012Apr
This paper examines the use of Semantic MEDLINE, a natural language processing application enhanced with a statistical algorithm known as Combo, as a potential decision support tool for clinicians. Semantic MEDLINE summarizes text in PubMed citations, transforming it into compact declarations that are filtered according to a user's information need that can be displayed in a graphic interface. Integration of the Combo algorithm enables Semantic MEDLINE to deliver information salient to many diverse needs. The authors selected three disease topics and crafted PubMed search queries to retrieve citations addressing the prevention of these diseases. They then processed the citations with Semantic MEDLINE, with the Combo algorithm enhancement. To evaluate the results, they constructed a reference standard for each disease topic consisting of preventive interventions recommended by a commercial decision support tool. Semantic MEDLINE with Combo produced an average recall of 79% in primary and secondary analyses, an average precision of 45%, and a final average F-score of 0.57. This new approach to point-of-care information delivery holds promise as a decision support tool for clinicians. Health sciences libraries could implement such technologies to deliver tailored information to their users.
22514508	Comparing patient characteristics, type of intervention, control, and outcome (PICO) queries with unguided searching: a randomized controlled crossover trial.
J Med Libr Assoc  2012Apr
Translating a question into a query using patient characteristics, type of intervention, control, and outcome (PICO) should help answer therapeutic questions in PubMed searches. The authors performed a randomized crossover trial to determine whether the PICO format was useful for quick searches of PubMed. Twenty-two residents and specialists working at the Radboud University Nijmegen Medical Centre were trained in formulating PICO queries and then presented with a randomized set of questions derived from Cochrane reviews. They were asked to use the best query possible in a five-minute search, using standard and PICO queries. Recall and precision were calculated for both standard and PICO queries. Twenty-two physicians created 434 queries using both techniques. Average precision was 4.02% for standard queries and 3.44% for PICO queries (difference nonsignificant, t(21) = -0.56, P = 0.58). Average recall was 12.27% for standard queries and 13.62% for PICO queries (difference nonsignificant, t(21) = -0.76, P = 0.46). PICO queries do not result in better recall or precision in time-limited searches. Standard queries containing enough detail are sufficient for quick searches.
22514969	Benefits of cloud computing for PACS and archiving.
Radiol Manage  2012 Mar-Apr
The goal of cloud-based services is to provide easy, scalable access to computing resources and IT services. The healthcare industry requires a private cloud that adheres to government mandates designed to ensure privacy and security of patient data while enabling access by authorized users. Cloud-based computing in the imaging market has evolved from a service that provided cost effective disaster recovery for archived data to fully featured PACS and vendor neutral archiving services that can address the needs of healthcare providers of all sizes. Healthcare providers worldwide are now using the cloud to distribute images to remote radiologists while supporting advanced reading tools, deliver radiology reports and imaging studies to referring physicians, and provide redundant data storage. Vendor managed cloud services eliminate large capital investments in equipment and maintenance, as well as staffing for the data center--creating a reduction in total cost of ownership for the healthcare provider.
21924446	[Literature review of the subject of a research project].
Radiologia 20110914 2012 Mar-Apr
It can be very complicated to obtain relevant information through searching the medical literature if you do not know how it is organized and indexed or if you do not know how to use the specialized databases. For a successful review of the literature, you need to know what you are looking for and the key words for an effective search of the specialized databases and libraries and especially of the internet. It is essential to critically evaluate the information selected. Finally, using a reference manager can facilitate the gathering, organization, systematization, and integration of the bibliographic references in the documents generated in the study. This article aims to provide guidelines for efficient searching for information and for accurate, critical use of the literature. It makes recommendations about strategies for managing references to help to ensure the success of a research project.
22523575	TumorHoPe: a database of tumor homing peptides.
PLoS ONE 20120416 2012
Cancer is responsible for millions of immature deaths every year and is an economical burden on developing countries. One of the major challenges in the present era is to design drugs that can specifically target tumor cells not normal cells. In this context, tumor homing peptides have drawn much attention. These peptides are playing a vital role in delivering drugs in tumor tissues with high specificity. In order to provide service to scientific community, we have developed a database of tumor homing peptides called TumorHoPe. TumorHoPe is a manually curated database of experimentally validated tumor homing peptides that specifically recognize tumor cells and tumor associated microenvironment, i.e., angiogenesis. These peptides were collected and compiled from published papers, patents and databases. Current release of TumorHoPe contains 744 peptides. Each entry provides comprehensive information of a peptide that includes its sequence, target tumor, target cell, techniques of identification, peptide receptor, etc. In addition, we have derived various types of information from these peptide sequences that include secondary/tertiary structure, amino acid composition, and physicochemical properties of peptides. Peptides in this database have been found to target different types of tumors that include breast, lung, prostate, melanoma, colon, etc. These peptides have some common motifs including RGD (Arg-Gly-Asp) and NGR (Asn-Gly-Arg) motifs, which specifically recognize tumor angiogenic markers. TumorHoPe has been integrated with many web-based tools like simple/complex search, database browsing and peptide mapping. These tools allow a user to search tumor homing peptides based on their amino acid composition, charge, polarity, hydrophobicity, etc. TumorHoPe is a unique database of its kind, which provides comprehensive information about experimentally validated tumor homing peptides and their target cells. This database will be very useful in designing peptide-based drugs and drug-delivery system. It is freely available at http://crdd.osdd.net/raghava/tumorhope/.
22188722	A classifier ensemble approach for the missing feature problem.
Artif Intell Med 20111220 2012May
Many classification problems must deal with data that contains missing values. In such cases data imputation is critical. This paper evaluates the performance of several statistical and machine learning imputation methods, including our novel multiple imputation ensemble approach, using different datasets. Several state-of-the-art approaches are compared using different datasets. Some state-of-the-art classifiers (including support vector machines and input decimated ensembles) are tested with several imputation methods. The novel approach proposed in this work is a multiple imputation method based on random subspace, where each missing value is calculated considering a different cluster of the data. We have used a fuzzy clustering approach for the clustering algorithm. Our experiments have shown that the proposed multiple imputation approach based on clustering and a random subspace classifier outperforms several other state-of-the-art approaches. Using the Wilcoxon signed-rank test (reject the null hypothesis, level of significance 0.05) we have shown that the proposed best approach is outperformed by the classifier trained using the original data (i.e., without missing values) only when &gt;20% of the data are missed. Moreover, we have shown that coupling an imputation method with our cluster based imputation we outperform the base method (level of significance ∼0.05). Starting from the assumptions that the feature set must be partially redundant and that the redundancy is distributed randomly over the feature set, we have proposed a method that works quite well even when a large percentage of the features is missing (≥30%). Our best approach is available (MATLAB code) at bias.csr.unibo.it/nanni/MI.rar.
20703642	Performance evaluation of a web-based system to exchange Electronic Health Records using Queueing model (M/M/1).
J Med Syst 20100715 2012Apr
Response time measurement of a web-based system is essential to evaluate its performance. This paper shows a comparison of the response times of a Web-based system for Ophthalmologic Electronic Health Records (EHRs), TeleOftalWeb. It makes use of different database models like Oracle 10 g, dbXML 2.0, Xindice 1.2, and eXist 1.1.1. The system's modelling, which uses Tandem Queue networks, will allow us to estimate the service times of the different components of the system (CPU, network and databases). In order to calculate those times, associated to the different databases, benchmarking techniques are used. The final objective of the comparison is to choose the database system resulting in the lowest response time to TeleOftalWeb and to compare the obtained results using a new benchmarking.
20703702	A novel strategy for load balancing of distributed medical applications.
J Med Syst 20100608 2012Apr
Current trends in medicine, specifically in the electronic handling of medical applications, ranging from digital imaging, paperless hospital administration and electronic medical records, telemedicine, to computer-aided diagnosis, creates a burden on the network. Distributed Service Architectures, such as Intelligent Network (IN), Telecommunication Information Networking Architecture (TINA) and Open Service Access (OSA), are able to meet this new challenge. Distribution enables computational tasks to be spread among multiple processors; hence, performance is an important issue. This paper proposes a novel approach in load balancing, the Random Sender Initiated Algorithm, for distribution of tasks among several nodes sharing the same computational object (CO) instances in Distributed Service Architectures. Simulations illustrate that the proposed algorithm produces better network performance than the benchmark load balancing algorithms-the Random Node Selection Algorithm and the Shortest Queue Algorithm, especially under medium and heavily loaded conditions.
22453367	Multiple-hologram recording with one-beam encoding.
Opt Express  2012Mar26
This research proposes a method that uses a single object beam to record multiple images in a photorefractive crystal medium without having to use any reference wave. The object beam in this study is modulated using a lenticular lens array sheet to produce a set of sub-object beams. These beams are then angularly separated on the recording plane and their scattered waves overlapped in an iron-doped photorefractive LiNbO₃ crystal. This single-exposure, multiple-holographic-recording method is simple and proven successful via the experiments that recorded four holograms in a 30 x 1 mm³ LiNbO₃:Fe crystal with single exposure.
22424304	Presenting evidence-based health information for people with multiple sclerosis: the IN-DEEP project protocol.
BMC Med Inform Decis Mak 20120316 2012
Increasingly, evidence-based health information, in particular evidence from systematic reviews, is being made available to lay audiences, in addition to health professionals. Research efforts have focused on different formats for the lay presentation of health information. However, there is a paucity of data on how patients integrate evidence-based health information with other factors such as their preferences for information and experiences with information-seeking. The aim of this project is to explore how people with multiple sclerosis (MS) integrate health information with their needs, experiences, preferences and values and how these factors can be incorporated into an online resource of evidence-based health information provision for people with MS and their families. This project is an Australian-Italian collaboration between researchers, MS societies and people with MS. Using a four-stage mixed methods design, a model will be developed for presenting evidence-based health information on the Internet for people with MS and their families. This evidence-based health information will draw upon systematic reviews of MS interventions from The Cochrane Library. Each stage of the project will build on the last. After conducting focus groups with people with MS and their family members (Stage 1), we will develop a model for summarising and presenting Cochrane MS reviews that is integrated with supporting information to aid understanding and decision making. This will be reviewed and finalised with people with MS, family members, health professionals and MS Society staff (Stage 2), before being uploaded to the Internet and evaluated (Stages 3 and 4). This project aims to produce accessible and meaningful evidence-based health information about MS for use in the varied decision making and management situations people encounter in everyday life. It is expected that the findings will be relevant to broader efforts to provide evidence-based health information for patients and the general public. The international collaboration also permits exploration of cultural differences that could inform international practice.
22458505	Decision tree models for data mining in hit discovery.
Expert Opin Drug Discov 20120229 2012Apr
Decision tree induction (DTI) is a powerful means of modeling data without much prior preparation. Models are readable by humans, robust and easily applied in real-world applications, features that are mutually exclusive in other commonly used machine learning paradigms. While DTI is widely used in disciplines ranging from economics to medicine, they are an intriguing option in pharmaceutical research, especially when dealing with large data stores. This review covers the automated technologies available for creating decision trees and other rules efficiently, even from large datasets such as chemical libraries. The authors discuss the need for properly documented and validated models. Lastly, the authors cover several case studies in hit discovery, drug metabolism and toxicology, and drug surveillance, and compare them with other established techniques. DTI is a competitive and easy-to-use tool in basic research as well as in hit and drug discovery. Its strengths lie in its ability to handle all sorts of different data formats, the visual nature of the models, and the small computational effort needed for implementation in real-world systems. Limitations include lack of robustness and over-fitted models for certain types of data. As with any modeling technique, proper validation and quality measures are of utmost importance.
22462987	Using time-delayed mutual information to discover and interpret temporal correlation structure in complex populations.
Chaos  2012Mar
This paper addresses how to calculate and interpret the time-delayed mutual information (TDMI) for a complex, diversely and sparsely measured, possibly non-stationary population of time-series of unknown composition and origin. The primary vehicle used for this analysis is a comparison between the time-delayed mutual information averaged over the population and the time-delayed mutual information of an aggregated population (here, aggregation implies the population is conjoined before any statistical estimates are implemented). Through the use of information theoretic tools, a sequence of practically implementable calculations are detailed that allow for the average and aggregate time-delayed mutual information to be interpreted. Moreover, these calculations can also be used to understand the degree of homo or heterogeneity present in the population. To demonstrate that the proposed methods can be used in nearly any situation, the methods are applied and demonstrated on the time series of glucose measurements from two different subpopulations of individuals from the Columbia University Medical Center electronic health record repository, revealing a picture of the composition of the population as well as physiological features.
22463244	Sampling rate of spatial stochastic processes with independent components in modeling random search paths.
Phys Rev E Stat Nonlin Soft Matter Phys 20120213 2012Feb
Continuous-time modeling of random searches is designed to be robust to the sampling rate while the spatial model is required to be of rotation-invariant type, which is often computationally prohibitive. Such computational difficulty may be circumvented by employing a model with independent components. We demonstrate that its disadvantages in statistical properties are blurred under lower frequency. We propose a quantitative criterion for choice of the sampling rate at which a spatial model with independent components resembles a rotation-invariant model. Our findings have the potential to assist the observer to employ simpler models in the continuous-time framework to avoid expensive computation required for statistical inference.
22463266	Effect on information transfer of synaptic pruning driven by spike-timing-dependent plasticity.
Phys Rev E Stat Nonlin Soft Matter Phys 20120213 2012Feb
Spike-timing-dependent plasticity (STDP) is an important driving force of self-organization in neural systems. With properly chosen input signals, STDP can yield a synaptic pruning process, whose functional role needs to be further investigated. We explore this issue from an information theoretic standpoint. Temporally correlated stimuli are introduced to neurons of an input layer. Then synapses on the dendrite, and thus the receptive field, of an output neuron are refined by STDP. The mutual information between input and output spike trains is calculated with the context tree method. The results show that synapse removal can enhance information transfer, i.e., that "less can be more" under certain constraints that stress the balance between potentiation and depression dictated by the parameters of the STDP rule, as well as the temporal scale of the input correlation.
22470145	Searching Online Mendelian Inheritance in Man (OMIM) for information on genetic loci involved in human disease.
Curr Protoc Hum Genet  2012Apr
Online Mendelian Inheritance in Man (OMIM) is a comprehensive compendium of information on human genes and genetic disorders, with a particular emphasis on the interplay between observed phenotypes and underlying genotypes. This unit focuses on the basic methodology for formulating OMIM searches and illustrates the types of information that can be retrieved from OMIM, including descriptions of clinical manifestations resulting from genetic abnormalities. This unit also provides information on additional relevant medical and molecular biology databases. A basic knowledge of OMIM should be part of the armamentarium of physicians and scientists with an interest in research on the clinical aspects of genetic disorders.
22364307	Should you search the Internet for information about your acute symptom?
Telemed J E Health 20120224 2012Apr
To determine if symptom-related Web sites give sufficient information for users to seek urgent care when warranted. We reviewed 120 Web sites (15 sites for each of eight acute symptoms). Symptom-related sites were identified with Google, Yahoo!®, and Bing™ searches and focused on potentially hazardous symptoms such as chest pain, shortness of breath, abdominal pain, and syncope. We reviewed each symptom-related site for the presence of critical symptom indicators (key symptom characteristics and associated factors) that triage the user to urgent care. Of the 120 sites reviewed, 41 (33%) contained no critical symptom indicators. No site contained a complete set of critical symptom indicators. Overall, out of the 1,020 total critical symptoms searched for in the sites, we only found 329 (32%). When present, critical symptom indicators were found on the top half of the first page of the site in only 34%. Specific recommendations for further care were absent in 42% of the cases where critical symptom indicators were identified. Symptom-related sites ranked highly by major search engines lack much of the information needed to make a decision about whether a symptom needs urgent attention. When present, this information is usually not located where users can rapidly access it and often lacks prescriptive guidance for users to seek care. Until more sites contain at least minimal triage advice, relying on an Internet search to help determine the urgency of a symptom could be risky.
22417955	How do I find a point-of-care answer to my clinical question?
CJEM  2012Jan
Emergency physicians often need point-of-care access to current, valid information to guide patient management. Most emergency physicians do not work in a hospital with a computerized decision support system that prompts and provides them with information to answer their clinical questions. Searching for answers to clinical questions online, especially those related to diagnosis and treatment, can be challenging, in part because determining the validity and clinical applicability of the results of individual studies is beyond the time constraints of most emergency physicians. This article describes currently available point-of-care sources of evidence-based information to answer clinical questions and provides the access information for each.
22418312	Detuned surface plasmon resonance scattering of gold nanorods for continuous wave multilayered optical recording and readout.
Opt Express  2012Feb27
In a multilayered structure of absorptive optical recording media, continuous-wave laser operation is highly disadvantageous due to heavy beam extinction. For a gold nanorod based recording medium, the narrow surface plasmon resonance (SPR) profile of gold nanorods enables the variation of extinction through mulilayers by a simple detuning of the readout wavelength from the SPR peak. The level of signal extinction through the layers can then be greatly reduced, resulting more efficient readout at deeper layers. The scattering signal strength may be decreased at the detuned wavelength, but balancing these two factors results an optimal scattering peak wavelength that is specific to each layer. In this paper, we propose to use detuned SPR scattering from gold nanorods as a new mechanism for continuous-wave readout scheme on gold nanorod based multilayered optical storage. Using this detuned scattering method, readout using continuous-wave laser is demonstrated on a 16 layer optical recording medium doped with heavily distributed, randomly oriented gold nanorods. Compared to SPR on-resonant readout, this method reduced the required readout power more than one order of magnitude, with only 60 nm detuning from SPR peak. The proposed method will be highly beneficial to multilayered optical storage applications as well as applications using a continuous medium doped heavily with plasmonic nanoparticles.
22418382	Data detection algorithms for multiplexed quantum dot encoding.
Opt Express  2012Feb27
A group of quantum dots can be designed to have a unique spectral emission by varying the size of the quantum dots (wavelength) and number of quantum dots (intensity). This technique has been previously proposed for biological tags and object identification. The potential of this system lies in the ability to have a large number of distinguishable wavelengths and intensity levels. This paper presents a communications system model for MxQDs including the interference between neighbouring QD colours and detector noise. An analytical model of the signal-to-noise ratio of a Charge-Coupled Device (CCD) spectrometer is presented and confirmed with experimental results. We then apply a communications system perspective and propose data detection algorithms that increase the readability of the quantum dots tags. It is demonstrated that multiplexed quantum dot barcodes can be read with 99.7% accuracy using the proposed data detection algorithms in a system with 6 colours and 6 intensity values resulting in 46,655 unique spectral codes.
22418387	Storage and retrieval of ghost images in hot atomic vapor.
Opt Express  2012Feb27
Ghost imaging is an imaging technique in which the image of an object is revealed only in the correlation measurement between two beams of light, whereas the individual measurements contain no imaging information. Here, we experimentally demonstrate storage and retrieval of ghost images in hot atomic rubidium vapor. Since ghost imaging requires (quantum or classical) multimode spatial correlation between two beams of light, our experiment shows that the spatially multimode correlation, a second-order correlation property of light, can indeed be preserved during the storage-retrieval process. Our work, thus, opens up new possibilities for quantum and classical two-photon imaging, all-optical image processing, and quantum communication.
22418483	Enhanced non-volatile and updatable holography using a polymer composite system.
Opt Express  2012Mar12
Updatable holography is considered as the ultimate technique for true 3D information recording and display. However, there is no practical solution to preserve the required features of both non-volatility and reversibility which conflict with each other when the reading has the same wavelength as the recording. We demonstrate a non-volatile and updatable holographic approach by exploiting new features of molecular transformations in a polymer recording system. In addition, by using a new composite recording film containing photo-reconfigurable liquid-crystal (LC) polymer, the holographic recording is enhanced due to the collective reorientation of LC molecules around the reconfigured polymer chains.
22336431	Automatic identification and normalization of dosage forms in drug monographs.
BMC Med Inform Decis Mak 20120215 2012
Each day, millions of health consumers seek drug-related information on the Web. Despite some efforts in linking related resources, drug information is largely scattered in a wide variety of websites of different quality and credibility. As a step toward providing users with integrated access to multiple trustworthy drug resources, we aim to develop a method capable of identifying drug's dosage form information in addition to drug name recognition. We developed rules and patterns for identifying dosage forms from different sections of full-text drug monographs, and subsequently normalized them to standardized RxNorm dosage forms. Our method represents a significant improvement compared with a baseline lookup approach, achieving overall macro-averaged Precision of 80%, Recall of 98%, and F-Measure of 85%. We successfully developed an automatic approach for drug dosage form identification, which is critical for building links between different drug-related resources.
22155335	BOAT: automatic alignment of biomedical ontologies using term informativeness and candidate selection.
J Biomed Inform 20111202 2012Apr
The biomedical sciences is one of the few domains where ontologies are widely being developed to facilitate information retrieval and knowledge sharing, but there still remains the problem that applications using different ontologies cannot share knowledge without explicit references between overlapping concepts. Ontology alignment is the task of identifying such equivalence relations between concepts across ontologies. Its application to the biomedical domain should address two open issues: (1) determining the equivalence of concept-pairs which have overlapping terms in their names, and (2) the high run-time required to align large ontologies which are typical in the biomedical domain. To address them, we present a novel approach, named the Biomedical Ontologies Alignment Technique (BOAT), which is state-of-the-art in terms of F-measure, precision and speed. A key feature of BOAT is that it considers the informativeness of each component word in the concept labels, which has significant impact on biomedical ontologies, resulting in a 12.2% increase in F-measure. Another important feature of BOAT is that it selects for comparison only concept pairs that show high likelihoods of equivalence, based on the similarity of their annotations. BOAT's F-measure of 0.88 for the alignment of the mouse and human anatomy ontologies is on par with that of another state-of-the-art matcher, AgreementMaker, while taking a shorter time.
22376010	Performance evaluation of Unified Medical Language System®'s synonyms expansion to query PubMed.
BMC Med Inform Decis Mak 20120229 2012
PubMed is the main access to medical literature on the Internet. In order to enhance the performance of its information retrieval tools, primarily non-indexed citations, the authors propose a method: expanding users' queries using Unified Medical Language System' (UMLS) synonyms i.e. all the terms gathered under one unique Concept Unique Identifier. This method was evaluated using queries constructed to emphasize the differences between this new method and the current PubMed automatic term mapping. Four experts assessed citation relevance. Using UMLS, we were able to retrieve new citations in 45.5% of queries, which implies a small increase in recall. The new strategy led to a heterogeneous 23.7% mean increase in non-indexed citation retrieved. Of these, 82% have been published less than 4 months earlier. The overall mean precision was 48.4% but differed according to the evaluators, ranging from 36.7% to 88.1% (Inter rater agreement was poor: kappa = 0.34). This study highlights the need for specific search tools for each type of user and use-cases. The proposed strategy may be useful to retrieve recent scientific advancement.
22328458	Benefits and risks of structuring and/or coding the presenting patient history in the electronic health record: systematic review.
BMJ Qual Saf 20120210 2012Apr
Patient histories in electronic health records currently exist mainly in free text format thereby limiting the possibility that decision support technology may contribute to the accuracy and timeliness of clinical diagnoses. Structuring and/or coding make patient histories potentially computable. A systematic review was undertaken of the benefits and risks of structuring and/or coding patient history by searching nine international databases for published and unpublished studies over the period 1990-2010. The focus was on the current patient history, defined as information reported by a patient or the patient's caregiver about the patient's present health situation and health status. Findings were synthesised through a theoretically based textural analysis. Of the 9207 potentially eligible papers identified, 10 studies satisfied the eligibility criteria. There was evidence of a modest number of benefits associated with structuring the current patient history, including obtaining more complete clinical histories, improved accuracy of patient self-documented histories, and better associated decision-making by professionals. However, no studies demonstrated any resulting improvements in patient care or outcomes. When more detailed records were obtained through the use of a structured format no attempt was made to confirm if this additional information was clinically useful. No studies investigated possible risks associated with structuring the patient history. No studies examined coding of the patient history. There is an insufficient evidence base for sound policy making on the benefits and risks of structuring and/or coding patient history. The authors suggest this field of enquiry warrants further investigation given the interest in use of decision support technology to aid diagnoses.
21965212	Camera constraint-free view-based 3-D object retrieval.
IEEE Trans Image Process 20110929 2012Apr
Recently, extensive research efforts have been dedicated to view-based methods for 3-D object retrieval due to the highly discriminative property of multiviews for 3-D object representation. However, most of state-of-the-art approaches highly depend on their own camera array settings for capturing views of 3-D objects. In order to move toward a general framework for 3-D object retrieval without the limitation of camera array restriction, a camera constraint-free view-based (CCFV) 3-D object retrieval algorithm is proposed in this paper. In this framework, each object is represented by a free set of views, which means that these views can be captured from any direction without camera constraint. For each query object, we first cluster all query views to generate the view clusters, which are then used to build the query models. For a more accurate 3-D object comparison, a positive matching model and a negative matching model are individually trained using positive and negative matched samples, respectively. The CCFV model is generated on the basis of the query Gaussian models by combining the positive matching model and the negative matching model. The CCFV removes the constraint of static camera array settings for view capturing and can be applied to any view-based 3-D object database. We conduct experiments on the National Taiwan University 3-D model database and the ETH 3-D object database. Experimental results show that the proposed scheme can achieve better performance than state-of-the-art methods.
22020689	Iterative truncated arithmetic mean filter and its properties.
IEEE Trans Image Process 20111019 2012Apr
The arithmetic mean and the order statistical median are two fundamental operations in signal and image processing. They have their own merits and limitations in noise attenuation and image structure preservation. This paper proposes an iterative algorithm that truncates the extreme values of samples in the filter window to a dynamic threshold. The resulting nonlinear filter shows some merits of both the fundamental operations. Some dynamic truncation thresholds are proposed that guarantee the filter output, starting from the mean, to approach the median of the input samples. As a by-product, this paper unveils some statistics of a finite data set as the upper bounds of the deviation of the median from the mean. Some stopping criteria are suggested to facilitate edge preservation and noise attenuation for both the long- and short-tailed distributions. Although the proposed iterative truncated mean (ITM) algorithm is not aimed at the median, it offers a way to estimate the median by simple arithmetic computing. Some properties of the ITM filters are analyzed and experimentally verified on synthetic data and real images.
22084049	Robust image deblurring with an inaccurate blur kernel.
IEEE Trans Image Process 20111108 2012Apr
Most existing nonblind image deblurring methods assume that the blur kernel is free of error. However, it is often unavoidable in practice that the input blur kernel is erroneous to some extent. Sometimes, the error could be severe, e.g., for images degraded by nonuniform motion blurring. When an inaccurate blur kernel is used as the input, significant distortions will appear in the image recovered by existing methods. In this paper, we present a novel convex minimization model that explicitly takes account of error in the blur kernel. The resulting minimization problem can be efficiently solved by the so-called accelerated proximal gradient method. In addition, a new boundary extension scheme is incorporated in the proposed model to further improve the results. The experiments on both synthesized and real images showed the efficiency and robustness of our algorithm to both the image noise and the model error in the blur kernel.
22128005	Face identification using large feature sets.
IEEE Trans Image Process 20111122 2012Apr
With the goal of matching unknown faces against a gallery of known people, the face identification task has been studied for several decades. There are very accurate techniques to perform face identification in controlled environments, particularly when large numbers of samples are available for each face. However, face identification under uncontrolled environments or with a lack of training data is still an unsolved problem. We employ a large and rich set of feature descriptors (with more than 70,000 descriptors) for face identification using partial least squares to perform multichannel feature weighting. Then, we extend the method to a tree-based discriminative structure to reduce the time required to evaluate probe samples. The method is evaluated on Facial Recognition Technology (FERET) and Face Recognition Grand Challenge (FRGC) data sets. Experiments show that our identification method outperforms current state-of-the-art results, particularly for identifying faces acquired across varying conditions.
22155954	Semisupervised biased maximum margin analysis for interactive image retrieval.
IEEE Trans Image Process 20111202 2012Apr
With many potential practical applications, content-based image retrieval (CBIR) has attracted substantial attention during the past few years. A variety of relevance feedback (RF) schemes have been developed as a powerful tool to bridge the semantic gap between low-level visual features and high-level semantic concepts, and thus to improve the performance of CBIR systems. Among various RF approaches, support-vector-machine (SVM)-based RF is one of the most popular techniques in CBIR. Despite the success, directly using SVM as an RF scheme has two main drawbacks. First, it treats the positive and negative feedbacks equally, which is not appropriate since the two groups of training feedbacks have distinct properties. Second, most of the SVM-based RF techniques do not take into account the unlabeled samples, although they are very helpful in constructing a good classifier. To explore solutions to overcome these two drawbacks, in this paper, we propose a biased maximum margin analysis (BMMA) and a semisupervised BMMA (SemiBMMA) for integrating the distinct properties of feedbacks and utilizing the information of unlabeled samples for SVM-based RF schemes. The BMMA differentiates positive feedbacks from negative ones based on local analysis, whereas the SemiBMMA can effectively integrate information of unlabeled samples by introducing a Laplacian regularizer to the BMMA. We formally formulate this problem into a general subspace learning task and then propose an automatic approach of determining the dimensionality of the embedded subspace for RF. Extensive experiments on a large real-world image database demonstrate that the proposed scheme combined with the SVM RF can significantly improve the performance of CBIR systems.
22155956	Design and optimization of color lookup tables on a simplex topology.
IEEE Trans Image Process 20111201 2012Apr
An important computational problem in color imaging is the design of color transforms that map color between devices or from a device-dependent space (e.g., RGB/CMYK) to a device-independent space (e.g., CIELAB) and vice versa. Real-time processing constraints entail that such nonlinear color transforms be implemented using multidimensional lookup tables (LUTs). Furthermore, relatively sparse LUTs (with efficient interpolation) are employed in practice because of storage and memory constraints. This paper presents a principled design methodology rooted in constrained convex optimization to design color LUTs on a simplex topology. The use of   n simplexes, i.e., simplexes in  n dimensions, as opposed to traditional lattices, recently has been of great interest in color LUT design for simplex topologies that allow both more analytically tractable formulations and greater efficiency in the LUT. In this framework of  n-simplex interpolation, our central contribution is to develop an elegant iterative algorithm that jointly optimizes the placement of nodes of the color LUT and the output values at those nodes to minimize interpolation error in an expected sense. This is in contrast to existing work, which exclusively designs either node locations or the output values. We also develop new analytical results for the problem of node location optimization, which reduces to constrained optimization of a large but sparse interpolation matrix in our framework. We evaluate our  n -simplex color LUTs against the state-of-the-art lattice (e.g., International Color Consortium profiles) and simplex-based techniques for approximating two representative multidimensional color transforms that characterize a CMYK xerographic printer and an RGB scanner, respectively. The results show that color LUTs designed on simplexes offer very significant benefits over traditional lattice-based alternatives in improving color transform accuracy even with a much smaller number of nodes.
22167628	Automatic single-image-based rain streaks removal via image decomposition.
IEEE Trans Image Process 20111209 2012Apr
Rain removal from a video is a challenging problem and has been recently investigated extensively. Nevertheless, the problem of rain removal from a single image was rarely studied in the literature, where no temporal information among successive images can be exploited, making the problem very challenging. In this paper, we propose a single-image-based rain removal framework via properly formulating rain removal as an image decomposition problem based on morphological component analysis. Instead of directly applying a conventional image decomposition technique, the proposed method first decomposes an image into the low- and high-frequency (HF) parts using a bilateral filter. The HF part is then decomposed into a "rain component" and a "nonrain component" by performing dictionary learning and sparse coding. As a result, the rain component can be successfully removed from the image while preserving most original image details. Experimental results demonstrate the efficacy of the proposed algorithm.
22180509	Removing label ambiguity in learning-based visual saliency estimation.
IEEE Trans Image Process 20111213 2012Apr
Visual saliency is a useful clue to depict visually important image/video contents in many multimedia applications. In visual saliency estimation, a feasible solution is to learn a "feature-saliency" mapping model from the user data obtained by manually labeling activities or eye-tracking devices. However, label ambiguities may also arise due to the inaccurate and inadequate user data. To process the noisy training data, we propose a multi-instance learning to rank approach for visual saliency estimation. In our approach, the correlations between various image patches are incorporated into an ordinal regression framework. By iteratively refining a ranking model and relabeling the image patches with respect to their mutual correlations, the label ambiguities can be effectively removed from the training data. Consequently, visual saliency can be effectively estimated by the ranking model, which can pop out real targets and suppress real distractors. Extensive experiments on two public image data sets show that our approach outperforms 11 state-of-the-art methods remarkably in visual saliency estimation.
22194244	Fast wavelet-based image characterization for highly adaptive image retrieval.
IEEE Trans Image Process 20111221 2012Apr
Adaptive wavelet-based image characterizations have been proposed in previous works for content-based image retrieval (CBIR) applications. In these applications, the same wavelet basis was used to characterize each query image: This wavelet basis was tuned to maximize the retrieval performance in a training data set. We take it one step further in this paper: A different wavelet basis is used to characterize each query image. A regression function, which is tuned to maximize the retrieval performance in the training data set, is used to estimate the best wavelet filter, i.e., in terms of expected retrieval performance, for each query image. A simple image characterization, which is based on the standardized moments of the wavelet coefficient distributions, is presented. An algorithm is proposed to compute this image characterization almost instantly for every possible separable or nonseparable wavelet filter. Therefore, using a different wavelet basis for each query image does not considerably increase computation times. On the other hand, significant retrieval performance increases were obtained in a medical image data set, a texture data set, a face recognition data set, and an object picture data set. This additional flexibility in wavelet adaptation paves the way to relevance feedback on image characterization itself and not simply on the way image characterizations are combined.
22194245	Semantic-gap-oriented active learning for multilabel image annotation.
IEEE Trans Image Process 20111221 2012Apr
User interaction is an effective way to handle the semantic gap problem in image annotation. To minimize user effort in the interactions, many active learning methods were proposed. These methods treat the semantic concepts individually or correlatively. However, they still neglect the key motivation of user feedback: to tackle the semantic gap. The size of the semantic gap of each concept is an important factor that affects the performance of user feedback. User should pay more efforts to the concepts with large semantic gaps, and vice versa. In this paper, we propose a semantic-gap-oriented active learning method, which incorporates the semantic gap measure into the information-minimization-based sample selection strategy. The basic learning model used in the active learning framework is an extended multilabel version of the sparse-graph-based semisupervised learning method that incorporates the semantic correlation. Extensive experiments conducted on two benchmark image data sets demonstrated the importance of bringing the semantic gap measure into the active learning process.
22438720	Performance study of the application of Artificial Neural Networks to the completion and prediction of data retrieved by underwater sensors.
Sensors (Basel) 20120202 2012
This paper presents a proposal for an Artificial Neural Network (ANN)-based architecture for completion and prediction of data retrieved by underwater sensors. Due to the specific conditions under which these sensors operate, it is not uncommon for them to fail, and maintenance operations are difficult and costly. Therefore, completion and prediction of the missing data can greatly improve the quality of the underwater datasets. A performance study using real data is presented to validate the approach, concluding that the proposed architecture is able to provide very low errors. The numbers show as well that the solution is especially suitable for cases where large portions of data are missing, while in situations where the missing values are isolated the improvement over other simple interpolation methods is limited.
22438740	AURP: an AUV-aided underwater routing protocol for underwater acoustic sensor networks.
Sensors (Basel) 20120209 2012
Deploying a multi-hop underwater acoustic sensor network (UASN) in a large area brings about new challenges in reliable data transmissions and survivability of network due to the limited underwater communication range/bandwidth and the limited energy of underwater sensor nodes. In order to address those challenges and achieve the objectives of maximization of data delivery ratio and minimization of energy consumption of underwater sensor nodes, this paper proposes a new underwater routing scheme, namely AURP (AUV-aided underwater routing protocol), which uses not only heterogeneous acoustic communication channels but also controlled mobility of multiple autonomous underwater vehicles (AUVs). In AURP, the total data transmissions are minimized by using AUVs as relay nodes, which collect sensed data from gateway nodes and then forward to the sink. Moreover, controlled mobility of AUVs makes it possible to apply a short-range high data rate underwater channel for transmissions of a large amount of data. To the best to our knowledge, this work is the first attempt to employ multiple AUVs as relay nodes in a multi-hop UASN to improve the network performance in terms of data delivery ratio and energy consumption. Simulations, which are incorporated with a realistic underwater acoustic communication channel model, are carried out to evaluate the performance of the proposed scheme, and the results indicate that a high delivery ratio and low energy consumption can be achieved.
22438742	Parametric dense stereovision implementation on a system-on chip (SoC).
Sensors (Basel) 20120210 2012
This paper proposes a novel hardware implementation of a dense recovery of stereovision 3D measurements. Traditionally 3D stereo systems have imposed the maximum number of stereo correspondences, introducing a large restriction on artificial vision algorithms. The proposed system-on-chip (SoC) provides great performance and efficiency, with a scalable architecture available for many different situations, addressing real time processing of stereo image flow. Using double buffering techniques properly combined with pipelined processing, the use of reconfigurable hardware achieves a parametrisable SoC which gives the designer the opportunity to decide its right dimension and features. The proposed architecture does not need any external memory because the processing is done as image flow arrives. Our SoC provides 3D data directly without the storage of whole stereo images. Our goal is to obtain high processing speed while maintaining the accuracy of 3D data using minimum resources. Configurable parameters may be controlled by later/parallel stages of the vision algorithm executed on an embedded processor. Considering hardware FPGA clock of 100 MHz, image flows up to 50 frames per second (fps) of dense stereo maps of more than 30,000 depth points could be obtained considering 2 Mpix images, with a minimum initial latency. The implementation of computer vision algorithms on reconfigurable hardware, explicitly low level processing, opens up the prospect of its use in autonomous systems, and they can act as a coprocessor to reconstruct 3D images with high density information in real time.
22357345	Systematic criteria for type and screen based on procedure's probability of erythrocyte transfusion.
Anesthesiology  2012Apr
At many hospitals, the type and screen decision is guided by the hospital's maximum surgical blood order schedule, a document that includes for each scheduled (elective) surgical procedure a recommendation of whether a preoperative type and screen be performed. There is substantial heterogeneity in the scientific literature for how that decision should be made. Anesthesia information management system data were retrieved from the 160,207 scheduled noncardiac cases in adults of 1,253 procedures at a hospital. Neither assuming a Poisson distribution of mean erythrocyte units transfused, nor grouping rare procedures into larger groups based on their anesthesia Current Procedural Terminology code, was reliable. In contrast, procedures could be defined to have minimal estimated blood loss (less than 50 ml) based on low incidence of transfusion and low incidence of the hemoglobin being checked preoperatively. Among these procedures, when the lower 95% confidence limit for erythrocyte transfusion was less than 5%, type and screen was shown to be unnecessary. The method was useful based on including multiple differences from the hospital's maximum surgical blood order schedule and clinicians' test ordering (greater than or equal to 29% fewer type and screen). Results were the same with a Bayesian random effects model. We validated a method to determine procedures on the maximum surgical blood order schedule for which type and screen was not indicated using the estimated blood losses and incidences of transfusion.
22304988	BaFe12O19 single-particle-chain nanofibers: preparation, characterization, formation principle, and magnetization reversal mechanism.
ACS Nano 20120213 2012Mar27
BaFe(12)O(19) single-particle-chain nanofibers have been successfully prepared by an electrospinning method and calcination process, and their morphology, chemistry, and crystal structure have been characterized at the nanoscale. It is found that individual BaFe(12)O(19) nanofibers consist of single nanoparticles which are found to stack along the nanofiber axis. The chemical analysis shows that the atomic ratio of Ba/Fe is 1:12, suggesting a BaFe(12)O(19) composition. The crystal structure of the BaFe(12)O(19) single-particle-chain nanofibers is proved to be M-type hexagonal. The single crystallites on each BaFe(12)O(19) single-particle-chain nanofibers have random orientations. A formation mechanism is proposed based on thermogravimetry/differential thermal analysis (TG-DTA), X-ray diffraction (XRD), and transmission electron microscopy (TEM) at six temperatures, 250, 400, 500, 600, 650, and 800 °C. The magnetic measurement of the BaFe(12)O(19) single-particle-chain nanofibers reveals that the coercivity reaches a maximum of 5943 Oe and the saturated magnetization is 71.5 emu/g at room temperature. Theoretical analysis at the micromagnetism level is adapted to describe the magnetic behavior of the BaFe(12)O(19) single-particle-chain nanofibers.
22451539	Automatic selective removal of embedded patient information from image content of DICOM files.
AJR Am J Roentgenol  2012Apr
Patient data may appear as burned-in text on the image content of DICOM image files, which is commonly seen in ultrasound images and some secondarily scanned images. The purpose of this article is to discuss the removal of such information, which can be cumbersome and demands human intervention. This article presents a computerized scheme that automatically removes patient data from the image content by finding burned-in texts that match information in the DICOM header, thus cleaning the entire DICOM image file while preserving other useful labels.
22258753	Content-based image-retrieval system in chest computed tomography for a solitary pulmonary nodule: method and preliminary experiments.
Int J Comput Assist Radiol Surg 20120119 2012Mar
The aim of this study was to develop a new diagnostic support system using content-based image-retrieval technology. In this article, we describe the mechanism and preliminary evaluation of this system for use with CT images of solitary pulmonary nodules. With the approval of the institutional review board of Shizuoka Cancer Center, we built a database that included CT images of 461 solitary pulmonary nodules. With this database, we developed a system that automatically extracts the pulmonary nodule when the nodule area is clicked, retrieves previous cases based on an image analysis of the extracted lesion, and generates reports of the pulmonary nodule semi-automatically. We compared the percentage of correct diagnoses with and without the system using 30 solitary pulmonary nodules, which were not included in the database, with one radiologist and two residents. As a per-user evaluation, the number of clicks required to extract the nodule region and the extracted regions was compared, and presented candidate cases were evaluated. As an evaluation of the retrieval results, the presented candidate cases were evaluated by comparing the number of diagnostic matches (benign/malignant) between the queries and four presented cases. Additionally, to evaluate the validity of the retrieval technology, the radiologist selected the most similar cases presented by the system and evaluated the visual similarity on a five-point scale. With this system, the percentage of correct diagnoses for the radiologist improved from 80 to 93%. For the two residents, the diagnostic accuracy improved from 66.7 to 80% and from 76.7 to 90%, respectively. The evaluation of the number of clicks required indicated that for 19 cases with the radiologist and 12 and 11 cases with the two residents, respectively, only one click was required to extract the region. When the extracted regions were compared between the radiologist and the residents, 22 and 19 cases had a Dice's Coefficient of 0.85 or higher, respectively. For the radiologist, the number of cases that matched the diagnosis (benign/malignant) averaged 3.7 ± 0.5 among 23 malignant cases and 1.7 ± 1.4 among 7 benign cases, while for the residents, these values were 3.6 ± 0.5 and 1.1 ± 0.9, and 3.4 ± 0.8 and 1.1 ± 1.3, respectively. With regard to visual evaluations by the radiologist, there were 15 similar cases and 11 somewhat similar cases. These results suggest that, despite some differences in the search results among the users, this system has been confirmed that it can improve the accuracy of diagnosis as it displays similar cases at high probability. In addition, with the use of this system, past cases and their reports can be effectively referred to. Therefore, this diagnostic-assistant system has the potential to improve the efficiency of the CT image-reading workflow.
22233160	A digital toolkit to implement and manage a multisite study.
J Nurs Educ 20120113 2012Mar
Calls for multisite studies are increasing in nursing education. However, the challenge of implementing consistent protocols and maintaining rigorous standards across sites can be daunting. One purpose of a recent multisite, collaborative, simulation study was to evaluate a digital toolkit's effectiveness for managing a multisite study. We describe the digital toolkit composed of Web-based technologies used to manage a study involving five sites including one United Kingdom site. The digital toolkit included a wiki, a project Web site to coordinate the protocols and study materials, software to organize study materials, and a secure location for sharing data. Most of these are familiar tools; however, combined as a toolkit, they became a useful management system. Web-based communication strategies and coordinated technical support served as key adjuncts to foster collaboration. This article also offers practical implications and recommendations for using a digital toolkit in other multisite studies.
22390523	Public health surveillance and meaningful use regulations: a crisis of opportunity.
Am J Public Health 20120119 2012Mar
The Health Information Technology for Economic and Clinical Health Act is intended to enhance reimbursement of health care providers for meaningful use of electronic health records systems. This presents both opportunities and challenges for public health departments.  To earn incentive payments, clinical providers must exchange specified types of data with the public health system, such as immunization and syndromic surveillance data and notifiable disease reporting. However, a crisis looms because public health's information technology systems largely lack the capabilities to accept the types of data proposed for exchange.  Cloud computing may be a solution for public health information systems. Through shared computing resources, public health departments could reap the benefits of electronic reporting within federal funding constraints.
21803786	Tutorial videos of bioinformatics resources: online distribution trial in Japan named TogoTV.
Brief. Bioinformatics 20110729 2012Mar
In recent years, biological web resources such as databases and tools have become more complex because of the enormous amounts of data generated in the field of life sciences. Traditional methods of distributing tutorials include publishing textbooks and posting web documents, but these static contents cannot adequately describe recent dynamic web services. Due to improvements in computer technology, it is now possible to create dynamic content such as video with minimal effort and low cost on most modern computers. The ease of creating and distributing video tutorials instead of static content improves accessibility for researchers, annotators and curators. This article focuses on online video repositories for educational and tutorial videos provided by resource developers and users. It also describes a project in Japan named TogoTV (http://togotv.dbcls.jp/en/) and discusses the production and distribution of high-quality tutorial videos, which would be useful to viewer, with examples. This article intends to stimulate and encourage researchers who develop and use databases and tools to distribute how-to videos as a tool to enhance product usability.
22075810	Towards a repository for standardized medical image and signal case data annotated with ground truth.
J Digit Imaging  2012Apr
Validation of medical signal and image processing systems requires quality-assured, representative and generally acknowledged databases accompanied by appropriate reference (ground truth) and clinical metadata, which are composed laboriously for each project and are not shared with the scientific community. In our vision, such data will be stored centrally in an open repository. We propose an architecture for a standardized case data and ground truth information repository supporting the evaluation and analysis of computer-aided diagnosis based on (a) the Reference Model for an Open Archival Information System (OAIS) provided by the NASA Consultative Committee for Space Data Systems (ISO 14721:2003), (b) the Dublin Core Metadata Initiative (DCMI) Element Set (ISO 15836:2009), (c) the Open Archive Initiative (OAI) Protocol for Metadata Harvesting, and (d) the Image Retrieval in Medical Applications (IRMA) framework. In our implementation, a portal bunches all of the functionalities that are needed for data submission and retrieval. The complete life cycle of the data (define, create, store, sustain, share, use, and improve) is managed. Sophisticated search tools make it easier to use the datasets, which may be merged from different providers. An integrated history record guarantees reproducibility. A standardized creation report is generated with a permanent digital object identifier. This creation report must be referenced by all of the data users. Peer-reviewed e-publishing of these reports will create a reputation for the data contributors and will form de-facto standards regarding image and signal datasets. Good practice guidelines for validation methodology complement the concept of the case repository. This procedure will increase the comparability of evaluation studies for medical signal and image processing methods and applications.
22080292	A flexible database architecture for mining DICOM objects: the DICOM data warehouse.
J Digit Imaging  2012Apr
Digital Imaging and Communications in Medicine (DICOM) has brought a very high level of standardization to medical images, allowing interoperability in many cases. However, there are still challenges facing the informaticist attempting to data mine DICOM objects. Images (and other objects) from different vintage equipment will encompass different levels of the standard, and there are also proprietary "shadow" tags to be aware of. The database architecture described herein "flattens" such differences by compiling a knowledge base of specific DICOM implementations and mapping variable data elements to a common lexicon for subsequent queries. The project is open sourced, built on open infrastructure, and is available at GitHub.
22396936	Development of a methodology for structured reporting of information in echocardiography.
Med Ultrason  2012Mar
In order to conduct research relying on ultrasound images, it is necessary to access a large number of relevant cases represented by images and their interpretation. DICOM standard defines the structured reporting information object. Templates are tree-like structures which offer structural guidance in report construction. Laying the foundations of a structured reporting methodology in echocardiography, through the generation of a consistent set of DICOM templates. We developed an information system with the ability of managing echocardiographic images and structured reports. In order to perform a complete description of the cardiac structures, we used 1900 coded concepts organized into 344 contexts by their semantic meaning in a variety of cardiac diseases. We developed 30 templates, with up to 10 nesting levels. The list of templates has a pyramid-like architecture. Two templates are used for reporting every measurement and description: "EchoMeasurement" and "EchoDescription". Intermediate level templates specify how to report the features of echoDoppler findings: "Spectral Curve", "Color Jet", "Intracardiac mass". Templates for every cardiovascular structure include the previous ones. "Echocardiography Procedure Report" includes all other templates. The templates were tested in reporting echo features of 100 patients by analyzing 500 DICOM images. The benefits of these templates has been proven during the testing process, through the quality of the echocardiography report, the ability to argue and to link every diagnostic feature to a defining image and by opening up opportunities for education, research. In the future, our template-based reporting methodology might be extended to other imaging modalities.
22401330	Interface design and human factors considerations for model-based tight glycemic control in critical care.
J Diabetes Sci Technol 20120101 2012Jan
Tight glycemic control (TGC) has shown benefits but has been difficult to implement. Model-based methods and computerized protocols offer the opportunity to improve TGC quality and compliance. This research presents an interface design to maximize compliance, minimize real and perceived clinical effort, and minimize error based on simple human factors and end user input. The graphical user interface (GUI) design is presented by construction based on a series of simple, short design criteria based on fundamental human factors engineering and includes the use of user feedback and focus groups comprising nursing staff at Christchurch Hospital. The overall design maximizes ease of use and minimizes (unnecessary) interaction and use. It is coupled to a protocol that allows nurse staff to select measurement intervals and thus self-manage workload. The overall GUI design is presented and requires only one data entry point per intervention cycle. The design and main interface are heavily focused on the nurse end users who are the predominant users, while additional detailed and longitudinal data, which are of interest to doctors guiding overall patient care, are available via tabs. This dichotomy of needs and interests based on the end user's immediate focus and goals shows how interfaces must adapt to offer different information to multiple types of users. The interface is designed to minimize real and perceived clinical effort, and ongoing pilot trials have reported high levels of acceptance. The overall design principles, approach, and testing methods are based on fundamental human factors principles designed to reduce user effort and error and are readily generalizable.
22401331	Data entry errors and design for model-based tight glycemic control in critical care.
J Diabetes Sci Technol 20120101 2012Jan
Tight glycemic control (TGC) has shown benefits but has been difficult to achieve consistently. Model-based methods and computerized protocols offer the opportunity to improve TGC quality but require human data entry, particularly of blood glucose (BG) values, which can be significantly prone to error. This study presents the design and optimization of data entry methods to minimize error for a computerized and model-based TGC method prior to pilot clinical trials. To minimize data entry error, two tests were carried out to optimize a method with errors less than the 5%-plus reported in other studies. Four initial methods were tested on 40 subjects in random order, and the best two were tested more rigorously on 34 subjects. The tests measured entry speed and accuracy. Errors were reported as corrected and uncorrected errors, with the sum comprising a total error rate. The first set of tests used randomly selected values, while the second set used the same values for all subjects to allow comparisons across users and direct assessment of the magnitude of errors. These research tests were approved by the University of Canterbury Ethics Committee. The final data entry method tested reduced errors to less than 1-2%, a 60-80% reduction from reported values. The magnitude of errors was clinically significant and was typically by 10.0 mmol/liter or an order of magnitude but only for extreme values of BG &lt; 2.0 mmol/liter or BG &gt; 15.0-20.0 mmol/liter, both of which could be easily corrected with automated checking of extreme values for safety. The data entry method selected significantly reduced data entry errors in the limited design tests presented, and is in use on a clinical pilot TGC study. The overall approach and testing methods are easily performed and generalizable to other applications and protocols.
22404001	[Research and implementation of the TLS network transport security technology based on DICOM standard].
Sheng Wu Yi Xue Gong Cheng Xue Za Zhi  2012Feb
With the development of medical information, Picture Archiving and Communications System (PACS), Hospital Information System/Radiology Information System(HIS/RIS) and other medical information management system become popular and developed, and interoperability between these systems becomes more frequent. So, these enclosed systems will be open and regionalized by means of network, and this is inevitable. If the trend becomes true, the security of information transmission may be the first problem to be solved. Based on the need for network security, we investigated the Digital Imaging and Communications in Medicine (DICOM) Standard and Transport Layer Security (TLS) Protocol, and implemented the TLS transmission of the DICOM medical information with OpenSSL toolkit and DCMTK toolkit.
21803548	Building a reference multimedia database for interstitial lung diseases.
Comput Med Imaging Graph 20110730 2012Apr
This paper describes the methodology used to create a multimedia collection of cases with interstitial lung diseases (ILDs) at the University Hospitals of Geneva. The dataset contains high-resolution computed tomography (HRCT) image series with three-dimensional annotated regions of pathological lung tissue along with clinical parameters from patients with pathologically proven diagnoses of ILDs. The motivations for this work is to palliate the lack of publicly available collections of ILD cases to serve as a basis for the development and evaluation of image-based computerized diagnostic aid. After 38 months of data collection, the library contains 128 patients affected with one of the 13 histological diagnoses of ILDs, 108 image series with more than 41l of annotated lung tissue patterns as well as a comprehensive set of 99 clinical parameters related to ILDs. The database is available for research on request and after signature of a license agreement.
22098158	Relevance-driven information search in "pseudodiagnostic" reasoning.
Q J Exp Psychol (Hove) 20111118 2012
When faced with two competing hypotheses, people sometimes prefer to look at multiple sources of information in support of one hypothesis rather than to establish the diagnostic value of a single piece of information for the two hypotheses. This is termed pseudodiagnostic reasoning and has often been understood to reflect, among other things, poor information search strategies. Past research suggests that diagnostic reasoning may be more easily fostered when participants seek data to help in the selection of one of two competing courses of action as opposed to situations where they seek data to help infer which of two competing hypotheses is true. In the experiment reported here, we provide the first empirical evidence demonstrating that manipulating the relevance of the feature for which participants initially receive information determines whether they will make a nominally diagnostic or pseudodiagnostic selection. The discussion of these findings focuses on implications for the ability to engage in diagnostic hypothesis testing.
22335285	Expert searching in health librarianship: a literature review to identify international issues and Australian concerns.
Health Info Libr J  2012Mar
The traditional role of health librarians as expert searchers is under challenge. The purpose of this review is to establish health librarians' views, practices and educational processes on expert searching. The search strategy was developed in LISTA and then customised for ten other databases: ALISA, PubMed, Embase, Scopus, Web of Science, CINAHL, ERIC, PsycINFO, Cochrane Library and Google Scholar. The search terms were (expert search* OR expert retriev* OR mediated search* OR information retriev*) AND librar*. The searches, completed in December 2010 and repeated in May 2011, were limited to English language publications from 2000 to 2011 (unless seminal works). Expert searching remains a key role for health librarians, especially for those supporting systematic reviews or employed as clinical librarians answering clinical questions. Although clients tend to be satisfied with searches carried out for them, improvements are required to effectively position the profession. Evidence-based guidelines, adherence to transparent standards, review of entry-level education requirements and a commitment to accredited, rigorous, ongoing professional development will ensure best practice.
22335287	Sensitivity and precision of adverse effects search filters in MEDLINE and EMBASE: a case study of fractures with thiazolidinediones.
Health Info Libr J 20111219 2012Mar
Search filters have been developed in MEDLINE and EMBASE to help overcome the challenges of searching electronic databases for information on adverse effects. However, little evaluation of their effectiveness has been carried out. To measure the sensitivity and precision of available adverse effects search filters in MEDLINE and EMBASE. A case study systematic review of fracture related adverse effects associated with the use of thiazolidinediones was used. Twelve MEDLINE search strategies and three EMBASE search strategies were tested. Nineteen relevant references from MEDLINE and 24 from EMBASE were included in the review. Four search filters in MEDLINE achieved high sensitivity (95 or 100%) with an improved level of precision from searches without any adverse effects filter. High precision in MEDLINE could also be achieved (up to 53%) using search filters that rely on Medical Subject Headings. No search filter in EMBASE achieved high precision (all were under 5%) and the highest sensitivity in EMBASE was 83%. Adverse effects search filters appear to be effective in MEDLINE for achieving either high sensitivity or high precision. Search filters in EMBASE, however, do not appear as effective, particularly in improving precision.
22335288	What type of leader am I?: a training needs analysis of health library and information managers.
Health Info Libr J 20111103 2012Mar
Leadership is a necessary facet of professional practice for health library and information managers (HLIMs). Several training needs analyses (TNA) in the health library and information services field have been conducted in recent years, all identifying a need for professional development in leadership skills. However, these previous TNAs have not focused on specific elements of leadership skills required by health library and information managers. The National Library for Health (NLH) commissioned the School of Health and Related Research (ScHARR) at the University of Sheffield to conduct a TNA where HLIMs assess their current leadership skills and identify any future development needs in this area. The results would inform a programme of influencing skills workshops. HLIMs in the UK were invited to complete a self-assessment online questionnaire. The questionnaire utilised items from Manning and Robertson's Influencing Skills Style Profile (ISSP). This allowed the results to be characterised by influencing 'style'. HLIMs considered themselves to have strengths in the leadership areas of influencing, negotiating, managing change and delivering presentations to decision-makers. They identified significant development needs in communicating with stakeholders, conflict resolution, using body language and being assertive. Most HLIMs demonstrated two collaborative styles identified by the ISSP, namely strategic collaborator and opportunistic collaborator. In difficult times, HLIMs may need to adapt to more of an 'opportunistic-battler' influencing style. It is important that HLIMs not only assess their own leadership skills but also that they take opportunities to employ 360(°) feedback, comprising assessment from subordinates, peers and supervisors.
22335291	Calling all students!!!
Health Info Libr J  2012Mar
'Dissertations into Practice' is a new regular feature in the Health Information and Libraries Journal, which aims to encourage students to write for publication. The idea is that students will write an extended abstract of their health-related dissertations, outlining the methods used and commenting on the implications for practice. Co-written with their dissertation or workplace supervisor, this feature will provide a safe environment for students to see their writing in print, possibly for the first time, while ensuring that invaluable research reaches a wider audience than might otherwise be the case.
22335292	International trends in health science librarianship: part 1 - the English speaking world.
Health Info Libr J  2012Mar
This is the second in a series of articles exploring international trends in health science librarianship in the first decade of the 21st century. The invited authors were asked to reflect on developments in their country - viz. Australia, Canada, New Zealand and the United States. Future issues will track trends in Northern Europe, the Nordic countries, Southern Europe and Latin America. JM.
22335293	Evaluating educational interventions for information literacy.
Health Info Libr J  2012Mar
This article considers how information literacy training initiatives delivered by health library services are evaluated. It presents three validated assessment and evaluation models, and using examples from practice, discusses how these can be used to establish the impact of information literacy training and to improve current evaluation practices. HS.
22337430	Registries and evidence-based medicine in craniofacial and plastic surgery.
J Craniofac Surg  2012Jan
Evidence-based medicine is a vital process for maintaining and improving quality and value in health care. However, evidence-based practice is most limited by the availability of research and outcomes data. Although randomized controlled trials (RCTs) have been identified by numerous research organizations as the criterion standard for research methodology (eg, "level I evidence"), the execution of well-designed RCTs has proved either challenging or impossible in many surgical fields and with rare disease conditions. In particular, craniofacial and plastic surgery has been noted to be lacking in both the number and quality of RCTs. Many reasons are discussed for this dearth of research including inadequate sample size and challenges in randomization, blinding, and clinical equipoise. Yet, data for outcomes assessment are highly valued by surgeons and by consumers and payers. Therefore, alternative and more practical means for research and data collection must be sought. Observational studies of clinical practice are particularly useful for outcomes assessment despite relegation to a lower tier of evidence (eg, "level II evidence"). Functional databases with well-defined processes for data collection, called medical data registries, are an essential informatics tool to collect and store outcomes data and produce high-quality observational, practice-based research studies. A properly designed and implemented registry can provide surgeons with an abundance of data to perform research and quality improvement projects. In fact, registries may be superior in many ways to RCTs for craniofacial and plastic surgeons both pragmatically and functionally. In this commentary, we discuss the production of such registries in the framework of evidence-based practice and the relevant studies in craniofacial surgery.
22318615	The prevention and treatment of missing data in clinical trials: an FDA perspective on the importance of dealing with it.
Clin. Pharmacol. Ther. 20120208 2012Mar
At the request of the Food and Drug Administration (FDA) and with its funding, the Panel on the Handling of Missing Data in Clinical Trials was created by the National Research Council's Committee on National Statistics. This panel recently published a report(1) with recommendations that will be of use not only to the FDA but also to the entire clinical trial community so that the latter can take measures to improve the conduct and analysis of clinical trials.
22194230	Proposing a standardized protocol for raw biosignal transmission.
IEEE Trans Biomed Eng 20111221 2012Mar
In this paper, we propose a standardized interface called TiA (TOBI interface A) to transmit raw biosignals, supporting multirate and block-oriented transmission of different kinds of signals from various acquisition devices (e.g., EEG, electrooculogram, near-infrared spectroscopy signals, etc.) at the same time. To facilitate a distinction between those kinds of signals, so-called signal types are introduced. TiA is a single-server, multiple-client system, whereby clients can connect to the server at runtime. Information transfer between client and server is divided into control and data connections. The control connections use transmission control protocol (TCP) and transmit extensible-markup-language (XML)-encoded meta information. The data transmission utilizes a user datagram protocol (UDP) or TCP with a binary data stream. A standardized handshaking procedure for the connection setup and a standardized binary data packet has been defined. Thus, a standardized layer, abstracting used hardware devices and facilitating distributed raw data transmission in a standardized way, has been evolved. A cross-platform library, implemented in C ++, is available for download.
20703715	Design and implementation of web-based discharge summary note based on service-oriented architecture.
J Med Syst 20100406 2012Feb
Discharge summary note is one of the essential clinical data in medical records, and it concisely capsules a patient's status during hospitalization. In the article, we adopt web-based architecture in developing a new discharge summary system for the Healthcare Information System of National Taiwan University Hospital, to improve the traditional client/sever architecture. The article elaborates the design approaches and implementation illustrations in detail, including patients' summary query and searching, model and phrase quoted, summary check list, major editing blocks as well as other functionalities. The system has been on-line and achieves successfully since October 2009.
20703749	A system for building clinical research applications using semantic web-based approach.
J Med Syst 20100224 2012Feb
In this paper we present a system using Semantic Web by which applications can be effectively constructed for clinical research purposes. We are aware of the immense difficulties and variations involved in clinical research applications. With a purpose of mitigating some of these difficulties in the process of developing clinical research applications we are presenting an approach for building information systems based on Semantic Web. We have developed a working prototype using C-Map tools leveraging the underlying principles of Abstract Software Design Framework to convert domain knowledge into machine-actable information.
20703751	Multi-center, multi-topic heart sound databases and their applications.
J Med Syst 20100223 2012Feb
This paper describes a large resource of multi-center and multi-topic heart sound databases, which were based on the measured data from more than 9,000 heart sound samples (saved in WAV file format). According to different research topics, these samples were respectively stored in different folders (corresponding to different research topics and distributed over various cooperative research centers), most of which as subfolds were stored in a pooled folder in the principal center. According to different research topics, the measured data from these samples were used to create different databases. Relevant data for a specific topic can be pooled in a large database for further analysis. This resource is shared by members of related centers for their own specific topic. The applications of this resource include evaluation of cardiac safety of pregnant women, evaluation of cardiac reserve for children, athletes, addicts, astronauts, and general populations, as well as studies on a bedside method for evaluating cardiac energy, reversal of S1-S2 ratio, etc.
22352137	A hazy outlook for cloud computing.
Healthc Inform  2012Jan
Because of competing priorities as well as cost, security, and implementation concerns, cloud-based storage development has gotten off to a slow start in healthcare. CIOs, CTOs, and other healthcare IT leaders are adopting a variety of strategies in this area, based on their organizations' needs, resources, and priorities.
22358156	Optical image hiding with silhouette removal based on the optical interference principle.
Appl Opt  2012Feb20
The earlier proposed interference-based encryption method with two phase-only masks (POMs), which actually is a special case of our method, is quite simple and does not need iterative encoding. However, it has been found recently that the encryption method has security problems and cannot be directly applied to image encryption due to the inherent silhouette problem. Several methods based on chaotic encryption algorithms have been proposed to remove the problem by postprocessing of the POMs, which increased the computation time or led to digital inverse computation in decryption. Here we propose a new method for image encryption based on optical interference and analytical algorithm that can be directly used for image encryption. The information of the target image is hidden into three POMs, and the silhouette problem that exists in the method with two POMs can be resolved during the generation procedure of POMs based on the interference principle. Simulation results are presented to verify the validity of the proposed approach.
22234348	Kidney transplantation search filters for PubMed, Ovid Medline, and Embase.
Transplantation  2012Mar15
Clinicians commonly search bibliographic databases such as Medline to find sound evidence to guide patient care. Unfortunately, this can be a frustrating experience because database searches often miss relevant articles. We addressed this problem for transplant professionals by developing kidney transplantation search filters for use in Medline through PubMed and Ovid Technologies, and Embase. We began by reading the full-text versions of 22,992 articles from 39 journals published across 5 years. These articles were labeled relevant to kidney transplantation or not forming our "gold standard." We then developed close to five million kidney transplantation filters using different terms and their combinations. Afterward, these filters were applied to development and validation subsets of the articles to determine their accuracy and reliability in identifying articles with kidney transplantation content. The final kidney transplantation filters used multiple terms in combination. The best performing filters achieved 97.5% sensitivity (95% confidence interval, 96.4%-98.5%), and 98.0% specificity (95% confidence interval, 97.8%-98.3%). Similar high performance was achieved for filters developed for Ovid Medline and Embase. Proof-of-concept searches confirmed more relevant articles are retrieved using these filters. These kidney transplantation filters can now be used in Medline and Embase databases to improve clinician searching.
21406451	'What's happening?' A content analysis of concussion-related traffic on Twitter.
Br J Sports Med 20110315 2012Mar
Twitter is a rapidly growing social networking site (SNS) with approximately 124 million users worldwide. Twitter allows users to post brief messages ('tweets') online, on a range of everyday topics including those dealing with health and wellbeing. Currently, little is known about how tweets are used to convey information relating to specific injuries, such as concussion, that commonly occur in youth sports. The purpose of this study was to analyse the online content of concussion-related tweets on the SNS Twitter, to determine the concept and context of mild traumatic brain injury as it relates to an online population. A prospective observational study using content analysis. Twitter traffic was investigated over a 7-day period in July 2010, using eight concussion-related search terms. From the 3488 tweets identified, 1000 were randomly selected and independently analysed using a customised coding scheme to determine major content themes. The most frequent theme was 'news' (33%) followed by 'sharing personal information/situation' (27%) and 'inferred management' (13%). Demographic data were available for 60% of the sample, with the majority of tweets (82%) originating from the USA, followed by Asia (5%) and the UK (4.5%). This study highlights the capacity of Twitter to serve as a powerful broadcast medium for sports concussion information and education.
22145975	Analysis of commercial and public bioactivity databases.
J Chem Inf Model 20111206 2012Feb27
Activity data for small molecules are invaluable in chemoinformatics. Various bioactivity databases exist containing detailed information of target proteins and quantitative binding data for small molecules extracted from journals and patents. In the current work, we have merged several public and commercial bioactivity databases into one bioactivity metabase. The molecular presentation, target information, and activity data of the vendor databases were standardized. The main motivation of the work was to create a single relational database which allows fast and simple data retrieval by in-house scientists. Second, we wanted to know the amount of overlap between databases by commercial and public vendors to see whether the former contain data complementing the latter. Third, we quantified the degree of inconsistency between data sources by comparing data points derived from the same scientific article cited by more than one vendor. We found that each data source contains unique data which is due to different scientific articles cited by the vendors. When comparing data derived from the same article we found that inconsistencies between the vendors are common. In conclusion, using databases of different vendors is still useful since the data overlap is not complete. It should be noted that this can be partially explained by the inconsistencies and errors in the source data.
22373911	OMERO: flexible, model-driven data management for experimental biology.
Nat. Methods 20120228 2012Mar
Data-intensive research depends on tools that manage multidimensional, heterogeneous datasets. We built OME Remote Objects (OMERO), a software platform that enables access to and use of a wide range of biological data. OMERO uses a server-based middleware application to provide a unified interface for images, matrices and tables. OMERO's design and flexibility have enabled its use for light-microscopy, high-content-screening, electron-microscopy and even non-image-genotype data. OMERO is open-source software, available at http://openmicroscopy.org/.
21958017	Towards a taxonomy of Bacteria and Archaea based on interactive and cumulative data repositories.
Environ. Microbiol. 20110929 2012Feb
Taxonomy in the second decade of the 21st century is benefiting from technological advances in molecular microbiology, especially those related to genomics. Gene and genome databases are significantly increasing due to intense research activities in the field of molecular ecology and genomics. Taxa, and especially species, are tailored by means of the recognition of a phylogenetic, genomic and phenotypic coherence that reveal their uniqueness in the classification schema. Phylogenetic coherence is mainly revealed by means of 16S rRNA gene analyses for which curated databases such as EzTaxon and LTP provide a valuable tool for tree reconstruction to taxonomy users. On the other hand, in silico full or partial genomic sequence comparisons are called on to substitute cumbersome techniques such as DNA-DNA hybridization (DDH) to genomically circumscribe species. DDH similarity values around 70% would be equivalent to ANI values of 96%. Finally, finding an exclusive phenotypic property for the taxa to be classified is of paramount relevance to producing an operative and predictive classification system. The current methods used for taxonomic classification require significant laboratory experimentation, and generally will not produce interactive databases. The new high-throughput metabolomic technologies, such as ICR-FT and MALDI-TOF mass spectrometry methods, open the door to the construction of metabolic databases for taxonomic purposes. It is to be foreseen that, in the future, taxonomists will benefit significantly from public databases speeding up the classification process. However, serious effort will be needed to harmonize them and to prevent inaccurate material.
22056694	Boosting performance of gene mention tagging system by hybrid methods.
J Biomed Inform 20111028 2012Feb
NER (Named Entity Recognition) in biomedical literature is presently one of the internationally concerned NLP (Natural Language Processing) research questions. In order to get higher performance, a hybrid experimental framework is presented for the gene mention tagging task. Six classifiers are firstly constructed by four toolkits (CRF++, YamCha, Maximum Entropy (ME) and MALLET) with different training methods and features sets, and then combined with three different hybrid methods respectively: simple set operation method, voting method and two layer stacking method. Experiments carried out on the corpus of BioCreative II GM task show that the three hybrid methods get the F-measure of 87.40%, 87.31% and 87.70% separately without any post-processing, which are all higher than those of any single ones. Our best hybrid method (two layer stacking method) achieves an F-measure of 88.42% after post-processing, which outperforms most of the state-of-the-art systems. We also discuss the influence on the performance of the ensemble system by the number, performance and divergence of single classifiers in each hybrid method, and give the corresponding analysis why our hybrid models can improve the performance.
22138863	Solid-state memories based on ferroelectric tunnel junctions.
Nat Nanotechnol 20111204 2012Feb
Ferroic-order parameters are useful as state variables in non-volatile information storage media because they show a hysteretic dependence on their electric or magnetic field. Coupling ferroics with quantum-mechanical tunnelling allows a simple and fast readout of the stored information through the influence of ferroic orders on the tunnel current. For example, data in magnetic random-access memories are stored in the relative alignment of two ferromagnetic electrodes separated by a non-magnetic tunnel barrier, and data readout is accomplished by a tunnel current measurement. However, such devices based on tunnel magnetoresistance typically exhibit OFF/ON ratios of less than 4, and require high powers for write operations (&gt;1 × 10(6) A cm(-2)). Here, we report non-volatile memories with OFF/ON ratios as high as 100 and write powers as low as ∼1 × 10(4) A cm(-2) at room temperature by storing data in the electric polarization direction of a ferroelectric tunnel barrier. The junctions show large, stable, reproducible and reliable tunnel electroresistance, with resistance switching occurring at the coercive voltage of ferroelectric switching. These ferroelectric devices emerge as an alternative to other resistive memories, and have the advantage of not being based on voltage-induced migration of matter at the nanoscale, but on a purely electronic mechanism.
22057091	Accurate on-line ν-support vector learning.
Neural Netw 20111020 2012Mar
The ν-Support Vector Machine (ν-SVM) for classification proposed by Schölkopf et al. has the advantage of using a parameter ν on controlling the number of support vectors and margin errors. However, comparing to standard C-Support Vector Machine (C-SVM), its formulation is more complicated, up until now there are no effective methods on solving accurate on-line learning for it. In this paper, we propose a new effective accurate on-line algorithm which is designed based on a modified formulation of the original ν-SVM. The accurate on-line algorithm includes two special steps: the first one is relaxed adiabatic incremental adjustments; the second one is strict restoration adjustments. The experiments on several benchmark datasets demonstrate that using these two steps the accurate on-line algorithm can avoid the infeasible updating path as far as possible, and successfully converge to the optimal solution. It achieves the fast convergence especially on the Gaussian kernel and is faster than the batch algorithm.
22030094	A framework for analysis of brain cine MR sequences.
Comput Med Imaging Graph 20111026 2012Mar
In this paper, we propose a framework to automate the assessment of the movements of a third cerebral ventricle in a cine MR sequence. Indeed, the goal of this assessment is to build an atlas of the movements of the healthy ventricles in the context of the hydrocephalus pathology. This approach is composed of two phases: a contour extraction, using fractional integration and a registration method, based on dynamic evolutionary optimization. The first phase of the framework is based on the fractional integration thresholding, that allows delineating the contours of the area of interest. In order to track over time each point of the primitive and achieve the assessment of the deformation, a matching method, based on a new dynamic optimization algorithm, called Dynamic Covariance Matrix Adaptation Evolution Strategy (D-CMAES), is used. The obtained results for quantification have been clinically validated by an expert and compared to those presented in the literature.
22128010	An infobutton for Web 2.0 clinical discussions: the knowledge linkage framework.
IEEE Trans Inf Technol Biomed 20111122 2012Jan
This paper aims to develop an infobutton to automatically retrieve published papers corresponding to a topic-specific online clinical discussion. The knowledge linkages infobutton is designed to supplement online clinical conversations with pertinent medical literature from Pubmed. The project involves three distinct steps: 1) Clinical messages around a specific problem are grouped together into a thread. 2) These threads are processed using Metamap to link the conversations to keywords from the MeSH lexicon. 3) These keywords are used in a novel search strategy to retrieve a set of papers from Pubmed, which are then returned to the user. A pilot study using the messages from 2007 and 2008, was conducted to compare the knowledge linkage search strategy to a vector space model and extended Boolean model. The knowledge linkage model proved to be significantly better in terms of precision ( p = 0.013 and 0.003, respectively) and recall ( p = 0.351 and 0.013). Pertinent papers were returned to over 55% of the threads. This approach has demonstrated how clinicians can supplement their peer communications with evidence based research. Future work should focus on how to improve the threading and keyword-mapping strategies.
22157061	Mobile medical visual information retrieval.
IEEE Trans Inf Technol Biomed 20111206 2012Jan
In this paper, we propose mobile access to peer-reviewed medical information based on textual search and content-based visual image retrieval. Web-based interfaces designed for limited screen space were developed to query via web services a medical information retrieval engine optimizing the amount of data to be transferred in wireless form. Visual and textual retrieval engines with state-of-the-art performance were integrated. Results obtained show a good usability of the software. Future use in clinical environments has the potential of increasing quality of patient care through bedside access to the medical literature in context.
21727204	Reconciliation of the cloud computing model with US federal electronic health record regulations.
J Am Med Inform Assoc 20110704 2012 Mar-Apr
Cloud computing refers to subscription-based, fee-for-service utilization of computer hardware and software over the Internet. The model is gaining acceptance for business information technology (IT) applications because it allows capacity and functionality to increase on the fly without major investment in infrastructure, personnel or licensing fees. Large IT investments can be converted to a series of smaller operating expenses. Cloud architectures could potentially be superior to traditional electronic health record (EHR) designs in terms of economy, efficiency and utility. A central issue for EHR developers in the US is that these systems are constrained by federal regulatory legislation and oversight. These laws focus on security and privacy, which are well-recognized challenges for cloud computing systems in general. EHRs built with the cloud computing model can achieve acceptable privacy and security through business associate contracts with cloud providers that specify compliance requirements, performance metrics and liability sharing.
21900701	Exploiting domain information for Word Sense Disambiguation of medical documents.
J Am Med Inform Assoc 20110907 2012 Mar-Apr
Current techniques for knowledge-based Word Sense Disambiguation (WSD) of ambiguous biomedical terms rely on relations in the Unified Medical Language System Metathesaurus but do not take into account the domain of the target documents. The authors' goal is to improve these methods by using information about the topic of the document in which the ambiguous term appears. The authors proposed and implemented several methods to extract lists of key terms associated with Medical Subject Heading terms. These key terms are used to represent the document topic in a knowledge-based WSD system. They are applied both alone and in combination with local context. A standard measure of accuracy was calculated over the set of target words in the widely used National Library of Medicine WSD dataset. The authors report a significant improvement when combining those key terms with local context, showing that domain information improves the results of a WSD system based on the Unified Medical Language System Metathesaurus alone. The best results were obtained using key terms obtained by relevance feedback and weighted by inverse document frequency.
22081224	iDASH: integrating data for analysis, anonymization, and sharing.
J Am Med Inform Assoc 20111110 2012 Mar-Apr
iDASH (integrating data for analysis, anonymization, and sharing) is the newest National Center for Biomedical Computing funded by the NIH. It focuses on algorithms and tools for sharing data in a privacy-preserving manner. Foundational privacy technology research performed within iDASH is coupled with innovative engineering for collaborative tool development and data-sharing capabilities in a private Health Insurance Portability and Accountability Act (HIPAA)-certified cloud. Driving Biological Projects, which span different biological levels (from molecules to individuals to populations) and focus on various health conditions, help guide research and development within this Center. Furthermore, training and dissemination efforts connect the Center with its stakeholders and educate data owners and data consumers on how to share and use clinical and biological data. Through these various mechanisms, iDASH implements its goal of providing biomedical and behavioral researchers with access to data, software, and a high-performance computing environment, thus enabling them to generate and test new hypotheses.
22319176	Importance of multi-modal approaches to effectively identify cataract cases from electronic health records.
J Am Med Inform Assoc  2012 Mar-Apr
There is increasing interest in using electronic health records (EHRs) to identify subjects for genomic association studies, due in part to the availability of large amounts of clinical data and the expected cost efficiencies of subject identification. We describe the construction and validation of an EHR-based algorithm to identify subjects with age-related cataracts. We used a multi-modal strategy consisting of structured database querying, natural language processing on free-text documents, and optical character recognition on scanned clinical images to identify cataract subjects and related cataract attributes. Extensive validation on 3657 subjects compared the multi-modal results to manual chart review. The algorithm was also implemented at participating electronic MEdical Records and GEnomics (eMERGE) institutions. An EHR-based cataract phenotyping algorithm was successfully developed and validated, resulting in positive predictive values (PPVs) &gt;95%. The multi-modal approach increased the identification of cataract subject attributes by a factor of three compared to single-mode approaches while maintaining high PPV. Components of the cataract algorithm were successfully deployed at three other institutions with similar accuracy. A multi-modal strategy incorporating optical character recognition and natural language processing may increase the number of cases identified while maintaining similar PPVs. Such algorithms, however, require that the needed information be embedded within clinical documents. We have demonstrated that algorithms to identify and characterize cataracts can be developed utilizing data collected via the EHR. These algorithms provide a high level of accuracy even when implemented across multiple EHRs and institutional boundaries.
21727053	The Israel DNA database--the establishment of a rapid, semi-automated analysis system.
Forensic Sci Int Genet 20110702 2012Mar
The Israel Police DNA database, also known as IPDIS (Israel Police DNA Index System), has been operating since February 2007. During that time more than 135,000 reference samples have been uploaded and more than 2000 hits reported. We have developed an effective semi-automated system that includes two automated punchers, three liquid handler robots and four genetic analyzers. An inhouse LIMS program enables full tracking of every sample through the entire process of registration, pre-PCR handling, analysis of profiles, uploading to the database, hit reports and ultimately storage. The LIMS is also responsible for the future tracking of samples and their profiles to be expunged from the database according to the Israeli DNA legislation. The database is administered by an in-house developed software program, where reference and evidentiary profiles are uploaded, stored, searched and matched. The DNA database has proven to be an effective investigative tool which has gained the confidence of the Israeli public and on which the Israel National Police force has grown to rely.
22322467	[Cardiology online: impact and pitfalls of Internet medical news].
G Ital Cardiol (Rome)  2012Jan
Twenty years ago, the main sources for physicians seeking information on new procedures, drugs, or devices were meetings and medical journals. The dawn of the Internet radically transformed how news and information is delivered and absorbed, beginning with the launch of online journals back in the mid-1990s. A decade and a half later, physicians can learn about new innovations the moment they are made public, and they can get that news from their phones and tablets, their Twitter or Facebook accounts, or via their favorite blog or medical news web site. Along with the clear advantages of accessing new medical information any time of day comes the need for physicians to be aware of the pitfalls of online medical content and to have a heightened sense of responsibility when it comes to integrating information gleaned online into their medical practices.
22320777	Retrieval boosted computer-aided diagnosis of clustered microcalcifications for breast cancer.
Med Phys  2012Feb
The authors propose an image-retrieval based approach for case-adaptive classifier design in computer-aided diagnosis (CADx). The conventional approach in CADx is to first train a pattern-classifier based on a set of existing training samples and then apply this classifier to subsequent new cases. The purpose of this work is to improve the classification accuracy of a CADx classifier by making use of a set of known cases retrieved from a reference library that are similar to the case under consideration. In the proposed approach, the authors will first apply image-retrieval to obtain a set of lesion images from a library of known cases that have similar image features to a case being diagnosed (i.e., query). These retrieved cases are then used to optimize a pattern-classifier toward boosting its classification accuracy on the query case. The basic idea is to put more emphasis on those cases that are similar to the query. The proposed approach is demonstrated first using a linear classifier and then extended to a nonlinear classifier induced by kernel principal component analysis. The proposed retrieval-driven approach was tested on a library of mammogram images from 1006 cases (646 benign and 360 malignant) obtained from multiple institutions and was demonstrated to yield significant improvement in classification performance. Measured by the area under the receiver operating characteristic curve (AUC), the case-adaptive approach could boost the classification performance of a linear classifier from AUC = 0.7415 to AUC = 0.7807; similar improvement was also obtained for a nonlinear classifier, with AUC boosted from 0.7527 to 0.7838. Use of additional cases from a reference library that have similar image features can improve the classification accuracy of a CADx classifier on a query case. It can even outperform retraining the classifier with all the cases from the entire reference library. This implies that cases with similar image features are more relevant in defining the local decision boundary of the CADx classifier around the query.
22320813	Quantitative accuracy of MAP reconstruction for dynamic PET imaging in small animals.
Med Phys  2012Feb
Iterative reconstruction algorithms are becoming more commonly employed in positron emission tomography (PET) imaging; however, the quantitative accuracy of the reconstructed images still requires validation for various levels of contrast and counting statistics. The authors present an evaluation of the quantitative accuracy of the 3D maximum a posteriori (3D-MAP) image reconstruction algorithm for dynamic PET imaging with comparisons to two of the most widely used reconstruction algorithms: the 2D filtered-backprojection (2D-FBP) and 2D-ordered subsets expectation maximization (2D-OSEM) on the Siemens microPET scanners. The study was performed for various levels of count density encountered in typical dynamic scanning as well as the imaging of cardiac activity concentration in small animal studies on the Focus 120. Specially designed phantoms were used for evaluation of the spatial resolution, image quality, and quantitative accuracy. A normal mouse was employed to evaluate the accuracy of the blood time activity concentration extracted from left ventricle regions of interest (ROIs) within the images as compared to the actual blood activity concentration measured from arterial blood sampling. For MAP reconstructions, the spatial resolution and contrast have been found to reach a stable value after 20 iterations independent of the β values (i.e., hyper parameter which controls the weight of the penalty term) and count density within the frame. The spatial resolution obtained with 3D-MAP reaches values of ∼1.0 mm with a β of 0.01 while the 2D-FBP has value of 1.8 mm and 2D-OSEM has a value of 1.6 mm. It has been observed that the lower the hyper parameter β used in MAP, more iterations are needed to reach the stable noise level (i.e., image roughness). The spatial resolution is improved by using a lower β value at the expense of higher image noise. However, with similar noise level the spatial resolution achieved by 3D-MAP was observed to be better than that by 2D-FBP or 2D-OSEM. Using an image quality phantom containing hot spheres, the estimated activity concentration in the largest sphere has the expected concentration relative to the background area for all the MAP images. The obtained recovery coefficients have been also shown to be almost independent of the count density. 2D-FBP and 2D-OSEM do not perform as well, yielding recovery coefficients lower than those observed with 3D-MAP (approximately 33% lower for the smallest sphere). However, a small positive bias was observed in MAP reconstructed images for frames of very low count density. This bias is present in the uniform area for count density of less than 0.05 × 10(6) counts/ml. For the dynamic mouse study, it was observed that 3D-MAP (even gated at diastole) cannot predict accurately the blood activity concentration due to residual spill-over activity from the myocardium into the left ventricle (approximately 15%). However, 3D-MAP predicts blood activity concentration closer to blood sampling than 2D-FBP. The authors observed that 3D-MAP produces more accurate activity concentration estimates than 2D-FBP or 2D-OSEM at all practical levels of statistics and contrasts due to improved spatial resolution leading to lesser partial volume effect.
22084254	Gene set analysis in the cloud.
Bioinformatics 20111113 2012Jan15
Cloud computing offers low cost and highly flexible opportunities in bioinformatics. Its potential has already been demonstrated in high-throughput sequence data analysis. Pathway-based or gene set analysis of expression data has received relatively less attention. We developed a gene set analysis algorithm for biomarker identification in the cloud. The resulting tool, YunBe, is ready to use on Amazon Web Services. Moreover, here we compare its performance to those obtained with desktop and computing cluster solutions. YunBe is open-source and freely accessible within the Amazon Elastic MapReduce service at s3n://lrcv-crp-sante/app/yunbe.jar. Source code and user's guidelines can be downloaded from http://tinyurl.com/yunbedownload.
22106335	Data-driven information retrieval in heterogeneous collections of transcriptomics data links SIM2s to malignant pleural mesothelioma.
Bioinformatics 20111120 2012Jan15
Genome-wide measurement of transcript levels is an ubiquitous tool in biomedical research. As experimental data continues to be deposited in public databases, it is becoming important to develop search engines that enable the retrieval of relevant studies given a query study. While retrieval systems based on meta-data already exist, data-driven approaches that retrieve studies based on similarities in the expression data itself have a greater potential of uncovering novel biological insights. We propose an information retrieval method based on differential expression. Our method deals with arbitrary experimental designs and performs competitively with alternative approaches, while making the search results interpretable in terms of differential expression patterns. We show that our model yields meaningful connections between biological conditions from different studies. Finally, we validate a previously unknown connection between malignant pleural mesothelioma and SIM2s suggested by our method, via real-time polymerase chain reaction in an independent set of mesothelioma samples. Supplementary data and source code are available from http://www.ebi.ac.uk/fg/research/rex.
22110245	PGDSpider: an automated data conversion tool for connecting population genetics and genomics programs.
Bioinformatics 20111121 2012Jan15
The analysis of genetic data often requires a combination of several approaches using different and sometimes incompatible programs. In order to facilitate data exchange and file conversions between population genetics programs, we introduce PGDSpider, a Java program that can read 27 different file formats and export data into 29, partially overlapping, other file formats. The PGDSpider package includes both an intuitive graphical user interface and a command-line version allowing its integration in complex data analysis pipelines. PGDSpider is freely available under the BSD 3-Clause license on http://cmpg.unibe.ch/software/PGDSpider/.
21494902	Integration of imaging signs into RadLex.
J Digit Imaging  2012Feb
Imaging signs form an important part of the language of radiology, but are not represented in established lexicons. We sought to incorporate imaging signs into RSNA's RadLex® ontology of radiology terms. Names of imaging signs and their definitions were culled from books, journal articles, dictionaries, and biomedical web sites. Imaging signs were added into RadLex as subclasses of the term "imaging sign," which was defined in RadLex as a subclass of "imaging observation." A total of 743 unique imaging signs were added to RadLex with their 392 synonyms to yield a total of 1,135 new terms. All included definitions and related RadLex terms, including imaging modality, anatomy, and disorder, when appropriate. The information will allow RadLex users to identify imaging signs by modality (e.g., ultrasound signs) and to find all signs related to specific pathophysiology. The addition of imaging signs to RadLex augments its use to index the radiology literature, create and interpret clinical radiology reports, and retrieve relevant cases and images.
21547518	A comprehensive descriptor of shape: method and application to content-based retrieval of similar appearing lesions in medical images.
J Digit Imaging  2012Feb
We have developed a method to quantify the shape of liver lesions in CT images and to evaluate its performance for retrieval of images with similarly-shaped lesions. We employed a machine learning method to combine several shape descriptors and defined similarity measures for a pair of shapes as a weighted combination of distances calculated based on each feature. We created a dataset of 144 simulated shapes and established several reference standards for similarity and computed the optimal weights so that the retrieval result agrees best with the reference standard. Then we evaluated our method on a clinical database consisting of 79 portal-venous-phase CT liver images, where we derived a reference standard of similarity from radiologists' visual evaluation. Normalized Discounted Cumulative Gain (NDCG) was calculated to compare this ordering with the expected ordering based on the reference standard. For the simulated lesions, the mean NDCG values ranged from 91% to 100%, indicating that our methods for combining features were very accurate in representing true similarity. For the clinical images, the mean NDCG values were still around 90%, suggesting a strong correlation between the computed similarity and the independent similarity reference derived the radiologists.
21562929	A parallel method to improve medical image transmission.
J Digit Imaging  2012Feb
The staggering number of images acquired by modern modalities requires new approaches for medical data transmission. There have been several attempts to improve data transmission time between medical imaging systems. These attempts were mostly based on compression. Although the compression methods can help in many cases, they are sometimes ineffectual in high-speed networks. This paper introduces parallelism to provide an effective method of medical data transmission over both local area network (LAN) and wide area network (WAN). It is based on the Digital Imaging and Communications in Medicine (DICOM) protocol and uses parallel TCP connections in storage services within the protocol. Using the proposed interface in our method, current medical imaging applications can take advantage of parallelism without any modification. Experimental results show a speedup of about 1.3 to 1.5 for CT images and relatively high speedup of about 2.2 to 3.5 times for magnetic resonance (MR) images over LAN. The transmission time is improved drastically over WAN. The speedup is about 16.1 for CT images and about 5.6 to 11.5 for MR images.
21748413	Accurate determination of imaging modality using an ensemble of text- and image-based classifiers.
J Digit Imaging  2012Feb
Imaging modality can aid retrieval of medical images for clinical practice, research, and education. We evaluated whether an ensemble classifier could outperform its constituent individual classifiers in determining the modality of figures from radiology journals. Seventeen automated classifiers analyzed 77,495 images from two radiology journals. Each classifier assigned one of eight imaging modalities--computed tomography, graphic, magnetic resonance imaging, nuclear medicine, positron emission tomography, photograph, ultrasound, or radiograph-to each image based on visual and/or textual information. Three physicians determined the modality of 5,000 randomly selected images as a reference standard. A "Simple Vote" ensemble classifier assigned each image to the modality that received the greatest number of individual classifiers' votes. A "Weighted Vote" classifier weighted each individual classifier's vote based on performance over a training set. For each image, this classifier's output was the imaging modality that received the greatest weighted vote score. We measured precision, recall, and F score (the harmonic mean of precision and recall) for each classifier. Individual classifiers' F scores ranged from 0.184 to 0.892. The simple vote and weighted vote classifiers correctly assigned 4,565 images (F score, 0.913; 95% confidence interval, 0.905-0.921) and 4,672 images (F score, 0.934; 95% confidence interval, 0.927-0.941), respectively. The weighted vote classifier performed significantly better than all individual classifiers. An ensemble classifier correctly determined the imaging modality of 93% of figures in our sample. The imaging modality of figures published in radiology journals can be determined with high accuracy, which will improve systems for image retrieval.
22038514	Automated detection of critical results in radiology reports.
J Digit Imaging  2012Feb
The goal of this study was to develop and validate text-mining algorithms to automatically identify radiology reports containing critical results including tension or increasing/new large pneumothorax, acute pulmonary embolism, acute cholecystitis, acute appendicitis, ectopic pregnancy, scrotal torsion, unexplained free intraperitoneal air, new or increasing intracranial hemorrhage, and malpositioned tubes and lines. The algorithms were developed using rule-based approaches and designed to search for common words and phrases in radiology reports that indicate critical results. Certain text-mining features were utilized such as wildcards, stemming, negation detection, proximity matching, and expanded searches with applicable synonyms. To further improve accuracy, the algorithms utilized modality and exam-specific queries, searched under the "Impression" field of the radiology report, and excluded reports with a low level of diagnostic certainty. Algorithm accuracy was determined using precision, recall, and F-measure using human review as the reference standard. The overall accuracy (F-measure) of the algorithms ranged from 81% to 100%, with a mean precision and recall of 96% and 91%, respectively. These algorithms can be applied to radiology report databases for quality assurance and accreditation, integrated with existing dashboards for display and monitoring, and ported to other institutions for their own use.
22272155	Utilizing grounded theory to explore the information-seeking behavior of senior nursing students.
J Med Libr Assoc  2012Jan
The ability to find and retrieve information efficiently is an important skill for undergraduate nursing students. Yet a number of studies reveal that nursing students are not confident in their library searching skills and encounter barriers to retrieving relevant information for assignments. This grounded theory study examined strategies used by students to locate information for class assignments and identified barriers to their success. Purposive sampling was used to recruit eleven students, who were asked to record their searching processes while completing a class assignment, and semi-structured, open-ended, audiotaped interviews took place to discuss the students' journals and solicit additional data. Methods of information seeking, strategies used to find information, and barriers to searching were identified. Students' main concern was frustration caused by the challenge of choosing appropriate words or phrases to query databases. The central theme that united all categories and explained most of the variation among the data was "discovering vocabulary." Teaching strategies to identify possible words and phrases to use when querying information sources should be emphasized more in the information literacy training of undergraduate nursing students.
22272157	Investigating biomedical research literature in the blogosphere: a case study of diabetes and glycated hemoglobin (HbA1c).
J Med Libr Assoc  2012Jan
The research investigated the relationship between biomedical literature and blogosphere discussions about diabetes in order to explore the role of Web 2.0 technologies in disseminating health information. Are blogs that cite biomedical literature perceived as more trustworthy in the blogosphere, as measured by their popularity and interconnections with other blogs? Web mining, social network analysis, and content analysis were used to analyze a large sample of blogs to determine how often biomedical literature is referenced in blogs on diabetes and how these blogs interconnect with others in the health blogosphere. Approximately 10% of the 3,005 blogs analyzed cite at least 1 article from the dataset of 2,246 articles. The most influential blogs, as measured by in-links, are written by diabetes patients and tend not to cite biomedical literature. In general, blogs that do not cite biomedical literature tend not to link to blogs that do. There is a large communication gap between health professional and personal diabetes blogs. Personal blogs do not tend to link to blogs by health professionals. Diabetes patients may be turning to the blogosphere for reasons other than authoritative information. They may be seeking emotional support and exchange of personal stories.
22222089	ABrowse--a customizable next-generation genome browser framework.
BMC Bioinformatics 20120105 2012
With the rapid growth of genome sequencing projects, genome browser is becoming indispensable, not only as a visualization system but also as an interactive platform to support open data access and collaborative work. Thus a customizable genome browser framework with rich functions and flexible configuration is needed to facilitate various genome research projects. Based on next-generation web technologies, we have developed a general-purpose genome browser framework ABrowse which provides interactive browsing experience, open data access and collaborative work support. By supporting Google-map-like smooth navigation, ABrowse offers end users highly interactive browsing experience. To facilitate further data analysis, multiple data access approaches are supported for external platforms to retrieve data from ABrowse. To promote collaborative work, an online user-space is provided for end users to create, store and share comments, annotations and landmarks. For data providers, ABrowse is highly customizable and configurable. The framework provides a set of utilities to import annotation data conveniently. To build ABrowse on existing annotation databases, data providers could specify SQL statements according to database schema. And customized pages for detailed information display of annotation entries could be easily plugged in. For developers, new drawing strategies could be integrated into ABrowse for new types of annotation data. In addition, standard web service is provided for data retrieval remotely, providing underlying machine-oriented programming interface for open data access. ABrowse framework is valuable for end users, data providers and developers by providing rich user functions and flexible customization approaches. The source code is published under GNU Lesser General Public License v3.0 and is accessible at http://www.abrowse.org/. To demonstrate all the features of ABrowse, a live demo for Arabidopsis thaliana genome has been built at http://arabidopsis.cbi.edu.cn/.
22274365	Chipscale, single-shot gated ultrafast optical recorder.
Opt Express  2012Jan2
We introduce a novel, chipscale device capable of single-shot ultrafast recording with picosecond-scale resolution over hundreds of picoseconds of record length. The device consists of two vertically-stacked III-V planar waveguides forming a Mach-Zehnder interferometer, and makes use of a transient, optically-induced phase difference to sample a temporal waveform injected into the waveguides. The pump beam is incident on the chip from above in the form of a diagonally-oriented stripe focused by a cylindrical lens. Due to time-of-flight, this diagonal orientation enables the sampling window to be shifted linearly in time as a function of position across the lateral axis of the waveguides. This time-to-space mapping allows an ordinary camera to record the ultrafast waveform with high fidelity. We investigate the theoretical limits of this technique, present a simulation of device operation, and report a proof-of-concept experiment in GaAs, demonstrating picosecond-scale resolution over 140 ps of record length.
22037056	A step-by-step guide to systematically identify all relevant animal studies.
Lab. Anim. 20111028 2012Jan
Before starting a new animal experiment, thorough analysis of previously performed experiments is essential from a scientific as well as from an ethical point of view. The method that is most suitable to carry out such a thorough analysis of the literature is a systematic review (SR). An essential first step in an SR is to search and find all potentially relevant studies. It is important to include all available evidence in an SR to minimize bias and reduce hampered interpretation of experimental outcomes. Despite the recent development of search filters to find animal studies in PubMed and EMBASE, searching for all available animal studies remains a challenge. Available guidelines from the clinical field cannot be copied directly to the situation within animal research, and although there are plenty of books and courses on searching the literature, there is no compact guide available to search and find relevant animal studies. Therefore, in order to facilitate a structured, thorough and transparent search for animal studies (in both preclinical and fundamental science), an easy-to-use, step-by-step guide was prepared and optimized using feedback from scientists in the field of animal experimentation. The step-by-step guide will assist scientists in performing a comprehensive literature search and, consequently, improve the scientific quality of the resulting review and prevent unnecessary animal use in the future.
22221313	Self-organizing ontology of biochemically relevant small molecules.
BMC Bioinformatics 20120106 2012
The advent of high-throughput experimentation in biochemistry has led to the generation of vast amounts of chemical data, necessitating the development of novel analysis, characterization, and cataloguing techniques and tools. Recently, a movement to publically release such data has advanced biochemical structure-activity relationship research, while providing new challenges, the biggest being the curation, annotation, and classification of this information to facilitate useful biochemical pattern analysis. Unfortunately, the human resources currently employed by the organizations supporting these efforts (e.g. ChEBI) are expanding linearly, while new useful scientific information is being released in a seemingly exponential fashion. Compounding this, currently existing chemical classification and annotation systems are not amenable to automated classification, formal and transparent chemical class definition axiomatization, facile class redefinition, or novel class integration, thus further limiting chemical ontology growth by necessitating human involvement in curation. Clearly, there is a need for the automation of this process, especially for novel chemical entities of biological interest. To address this, we present a formal framework based on Semantic Web technologies for the automatic design of chemical ontology which can be used for automated classification of novel entities. We demonstrate the automatic self-assembly of a structure-based chemical ontology based on 60 MeSH and 40 ChEBI chemical classes. This ontology is then used to classify 200 compounds with an accuracy of 92.7%. We extend these structure-based classes with molecular feature information and demonstrate the utility of our framework for classification of functionally relevant chemicals. Finally, we discuss an iterative approach that we envision for future biochemical ontology development. We conclude that the proposed methodology can ease the burden of chemical data annotators and dramatically increase their productivity. We anticipate that the use of formal logic in our proposed framework will make chemical classification criteria more transparent to humans and machines alike and will thus facilitate predictive and integrative bioactivity model development.
20623189	Building organizational knowledge and value: informed decision making in Kansas children's community-based mental health services.
Community Ment Health J 20100710 2012Feb
Knowledge is managers' principal asset and knowledge building is managers' primary work. This qualitative study explores knowledge building by directors of children's community-based mental health services in Kansas. Of the state's 27 directors, 25 completed a survey about knowledge building, in their preference of online or telephone format. Fourteen participants took part either in preliminary interviews for study development, or in follow-up interviews for further detail and member checking. Study findings indicate that with requisite resources, directors inform their decision making with streams of information, which they manage and generate to build organizational knowledge and value for local practice effectiveness.
22130590	An infrastructure for ontology-based information systems in biomedicine: RICORDO case study.
Bioinformatics 20111129 2012Feb1
The article presents an infrastructure for supporting the semantic interoperability of biomedical resources based on the management (storing and inference-based querying) of their ontology-based annotations. This infrastructure consists of: (i) a repository to store and query ontology-based annotations; (ii) a knowledge base server with an inference engine to support the storage of and reasoning over ontologies used in the annotation of resources; (iii) a set of applications and services allowing interaction with the integrated repository and knowledge base. The infrastructure is being prototyped and developed and evaluated by the RICORDO project in support of the knowledge management of biomedical resources, including physiology and pharmacology models and associated clinical data. The RICORDO toolkit and its source code are freely available from http://ricordo.eu/relevant-resources. sarala@ebi.ac.uk.
22066602	Host biomarkers of clinical relevance in tuberculosis: review of gene and protein expression studies.
Biomarkers 20111108 2012Feb
Identification of clinically relevant biomarkers is required for better diagnosis, prevention and treatment of tuberculosis. In this review, potential host biomarkers in blood or blood cells in tuberculosis were identified by a systematic approach. A total of 55 articles were selected from PubMed and Google Scholar that analyzed gene and or protein expression in humans in active and or latent TB. Articles were scored according to certain criteria and categorized as strong or weak studies. Biomarkers reported by more than one article or by a single strong article were identified as potential biomarkers. Six most promising markers (IP-10, IL-6, IL-10, IL-4, FOXP3 and IL-12) were identified based on their presence in both mycobacterial antigen-stimulated and -unstimulated samples. With this review we hope to provide a reliable guideline for biomarker studies in tuberculosis.
22289091	A study in usability: redesigning a health sciences library's mobile site.
Med Ref Serv Q  2012
A mobile site redesign was conducted at a medium-sized academic health sciences library with the goal of creating a site that meets the mobile information needs of its users. The redesign phases included (1) needs assessment, (2) usability testing, and (3) site design. The survey results showed that Apple devices were the most prevalent; the most desirable activities performed on a mobile site were searching for articles, accessing full-text articles and e-books, searching databases, and searching the catalog. These activities guided the development of the usability testing tasks and the redesign. All phases were completed within six months, and the total project cost was $50 for incentive purchases.
22289092	Rethinking mobile delivery: using Quick Response codes to access information at the point of need.
Med Ref Serv Q  2012
This article covers the use of Quick Response (QR) codes to provide instant mobile access to information, digital collections, educational offerings, library website, subject guides, text messages, videos, and library personnel. The array of uses and the value of using QR codes to push customized information to patrons are explained. A case is developed for using QR codes for mobile delivery of customized information to patrons. Applications in use at the Libraries of the University of Utah will be reviewed to provide readers with ideas for use in their library.
22289093	Rethinking library service to distance education students: analyzing the embedded librarian model.
Med Ref Serv Q  2012
Since fall 2009, reference librarians at The George Washington University's Himmelfarb Health Sciences Library have been embedded in online classes through Blackboard within the School of Nursing and School of Medicine and Health Sciences. The authors sought to determine the types of questions asked of the librarian, with the goal of informing future interactions with distance education classes to help develop a standard "protocol" for working with this population of students. Eighty-two questions were categorized and qualitatively analyzed. The findings have prompted librarians to explore tools such as Elluminate Live!, a tool that allows librarians to provide synchronous instruction within the Blackboard environment.
22289094	Library and informatics skills competencies statements from major health professional associations.
Med Ref Serv Q  2012
Every major health profession now provides competency statements for preparing new members for their respective professions. These competency statements normally include expectations for training health professions students in library/informatics skills. For purposes of this article, searches were conducted using various sources to produce a comprehensive 32-page Compendium &lt;https://repository.unm.edu/handle/1928/15363&gt; that inventories library/informatics-related competency statements. This compendium should aid readers in integrating their library/informatics skills training into various health professions education curricula.
22289095	Comparison of select reference management tools.
Med Ref Serv Q  2012
Bibliographic management tools have been widely used by researchers to store, organize, and manage their references for research papers, theses, dissertations, journal articles, and other publications. There are a number of reference management tools available. In order for users to decide which tool is best for their needs, it is important to know each tool's strengths and weaknesses. This article compares four reference management tools, one of which is licensed by University of Medicine and Dentistry of New Jersey libraries and the other three are open source and freely available. They were chosen based on their functionality, ease of use, availability to library users, and popularity. These four tools are EndNote/EndNote Web, Zotero, Connotea, and Mendeley Desktop/Mendeley Web. Each tool is analyzed in terms of the following features: accessing, collecting, organizing, collaborating, and citing/formatting. A comparison table is included to summarize the key features of these tools.
22289098	Cloud computing basics for librarians.
Med Ref Serv Q  2012
"Cloud computing" is the name for the recent trend of moving software and computing resources to an online, shared-service model. This article briefly defines cloud computing, discusses different models, explores the advantages and disadvantages, and describes some of the ways cloud computing can be used in libraries. Examples of cloud services are included at the end of the article.
22291958	Intensive case finding and isoniazid preventative therapy in HIV infected individuals in Africa: economic model and value of information analysis.
PLoS ONE 20120123 2012
Tuberculosis (TB) accounts of much of the morbidity and mortality associated with HIV. We evaluate the cost-effectiveness of different strategies to actively screen for TB disease in HIV positive individuals, where isoniazid preventative therapy (IPT) is given to those screening negative, and use value of information analysis (VOI) to identify future research priorities. We built an individual sampling model to investigate the costs (2010 US Dollars) and consequences of screening for TB, and providing TB treatment or IPT in adults testing HIV positive in Sub-Saharan Africa. A systematic review and meta-analysis was conducted to assess performance of the nine different TB screening strategies evaluated. Probabilistic sensitivity analysis was conducted to incorporate decision uncertainty, and expected value of perfect information for the entire model and for groups of parameters was calculated. Screening all HIV infected individuals with sputum microscopy was the least costly strategy, with other strategies not cost-effective at WHO recommended thresholds. Screening those with TB symptoms with sputum microscopy and CXR would be cost-effective at a threshold ICER of $7,800 per quality-adjusted life year (QALY), but associated with significant uncertainty. VOI analysis suggests further information would be of value. Resource-constrained countries in sub-Saharan Africa wishing to scale up TB preventative services in their HIV infected populations should consider expanding laboratory facilities to enable increased screening for TB with sputum microscopy, whilst improved estimates of the TB prevalence in the population to be screened are needed, as it may influence the optimal strategy.
20828857	Novel recovery mechanism for the restoration of image contents in teleconsultation sessions.
Comput Methods Programs Biomed 20100915 2012Jan
In teleconsultation sessions, a critical dependency exists between the image contents and the type and sequential order of the image processing commands used by the various participants. Accordingly, for re-entrant/late users, a significant challenge exists in restoring the image contents of the teleconsultation session in such a way that all the participants maintain a consistent view of the medical images. In this paper, this problem is resolved using a novel recovery mechanism comprising two major components, namely an enhanced content-recording scheme designated as three-level indexing hierarchy (TIH) and a prioritized recovery policy. TIH maintains a record of all the commands which affect the appearance of each of medical images such that when a restoration process is required, these image-affect commands can be rapidly identified and transmitted to the user. As a result, a significant reduction can be gained in both the command identification/transmission time and the image restoration time compared to traditional recovery schemes, which restore the contents by re-executing all of the commands invoked during the course of the session. The prioritized recovery policy further reduces the time required for re-entrant/late users to catch up with the on-going session by utilizing the cross-linkage design within the TIH architecture to restore the foreground image (i.e. the image under current discussion) before the background images are restored (i.e. the remaining images in the session). To resolve the problem which arises when a background image is selected as the new foreground image before the restoration process is completed, the prioritized recovery policy maintains a set of resuming pointers for each re-entrant/late user to facilitate the process of suspending the current restoration process and switching to the restoration of the new foreground image. The evaluation results confirm that the TIH architecture and prioritized recovery policy yield a significant reduction in the recovery-latency delay compared to that required by traditional message-logging restoration systems.
22214757	Understanding why evidence from randomised clinical trials may not be retrieved from Medline: comparison of indexed and non-indexed records.
BMJ 20120103 2012
To explore why reports that seem to describe randomised controlled trials are sometimes not indexed ("tagged") with RCT (randomised controlled trial) [pt] (publication type) in Medline. Design Cross sectional study. The Cochrane Collaboration and US National Library of Medicine worked together to identify and retag records of randomised controlled trials with RCT [pt], 1994 to 2006. Data source Published reports entered into Medline in 2005. Type of trial information presented (for example, main results, design, and methods), trial design, and other Medline indexing terms applied. 572/591 (97%) untagged records and 578/594 (97%) tagged records contained information from randomised controlled trials. Type of trial information and design differed between untagged and tagged reports. Fewer than half (234/572, 41%, 95% confidence interval 37% to 45%) of untagged reports but most tagged reports (526/578, 91%, 89% to 93%) described the main results of the trial. Untagged reports were more likely than tagged reports to contain information on design and methods, baseline characteristics, long term follow-up, and secondary analyses. Untagged reports of main results were more likely than tagged reports to be from trials using a crossover design (36% v 10%, difference 25%, 95% confidence interval 19% to 32%). The Medical Subject Heading "Randomized Controlled Trials" was the most common clinical trial term applied to untagged reports, although more than half of untagged reports had no indexing related to trials. Based on the results for 2005, at least 3000 records describing randomised controlled trials but not indexed using RCT [pt] may have been entered into Medline between 2006 and 2011. Researchers and healthcare decision makers relying on using RCT [pt] may be missing important evidence in their searches, particularly for design and methods, baseline characteristics, long term follow-up, and secondary data analyses.
21055892	Evaluation of mosaic pattern areas in HRCT with Min-IP reconstructions in patients with pulmonary hypertension: could this evaluation replace lung perfusion scintigraphy?
Eur J Radiol 20101104 2012Jan
The aim of this study is to evaluate a possible correlation between areas of lung attenuation, found in minimum intensity projection (Min-IP) reconstruction images performed with high resolution computed tomography without contrast medium (HRCT), and areas of lung perfusion alteration, found in lung perfusion scintigraphy (LPS). Two independent radiologists, unaware of LPS results, evaluated retrospectively a group of 113 patients affected by pulmonary hypertension (HP) of different aetiology. These have been examined in a period of two years in our centre both by spiral computed tomography (CT) with and without contrast-medium and by LPS. The final diagnosis was determined on clinical data, right heart catheterisation and contrast enhanced CT in angiographic phase (CTPA). We reconstructed the Min-IP images of lung parenchyma in all the cases both in HRCT without contrast-medium, and in contrast enhanced CT in angiographic phase (CTPA) in axial, sagittal and coronal planes. The obtained images were qualitatively graded into three categories of pulmonary attenuation: homogeneous, inhomogeneous with non-segmental patchy defects, inhomogeneous with segmental defects. The same criteria of classification were used also for LPS images. In the group of patients with chronic thromboembolic pulmonary hypertension (CTEPH) we also compared the number of areas of lung attenuation found in Min-IP images in HRCT without contrast-medium, and their exact localization, with not perfused areas in LPS. Gold standard for the diagnosis of pulmonary embolism was spiral contrast enhanced CT in angiographic phase (CTPA). In all cases we found exact correspondence between the Min-IP images in HRCT with and without contras agent. The attenuation pattern seen on Min-IP images was concordant with those of LPS in 96 out of 113 patients (85%). In the remaining 17 cases (15%) it was discordant: in 12 cases inhomogeneous in Min-IP images (7 with non-segmental patchy defects, 5 with segmental defects) and homogeneous in LPS, in 5 cases inhomogeneous (1 with non-segmental patchy defects, 4 with segmental defects) in LPS images and homogeneous in Min-IP. In a general view, Min-IP reconstruction without contrast-medium showed a sensitivity of 100% and specificity of 96.1%, positive predictive value (PPV) of 92.3% and negative predictive value (NPV) of 100%, to recognize a pattern of lung attenuation inhomogeneous with segmental defects correspondent to a chronic thromboembolic condition, no false negative cases and three false positive cases; on the other hand LPS, on its own, showed a sensitivity of 91.67% and specificity of 93.51%, positive predictive value (PPV) of 86.84% and negative predictive value (NPV) of 96%, 3 false negative cases and 5 false positive cases. Min-IP obtained in HRCT without contrast-medium and in CTPA were equivalent. Min-IP images generally showed a higher sensitivity and specificity than LPS in the evaluation of lung perfusion regarding patients with pulmonary hypertension caused by different etiology, particularly in CTEPH patients. These results can be completed with the evaluation of HRCT and CTPA basal scans, providing more informations than ventilation/perfusion lung scintigraphy. HRCT images integrated by Min-IP reconstruction can represent the first step in the diagnostic algorithm of patients affected by dyspnoea and pulmonary hypertension of unknown causes, reserving the use of contrast-medium only in selected patients and reducing the patients' X-ray-exposition.
21334152	Non- invasive in vivo analysis of a murine aortic graft using high resolution ultrasound microimaging.
Eur J Radiol 20110218 2012Feb
As yet, murine aortic grafts have merely been monitored histopathologically. The aim of our study was to examine how these grafts can be monitored in vivo and non-invasively by using high-resolution ultrasound microimaging to evaluate function and morphology. A further aim was to prove if this in vivo monitoring can be correlated to immunohistological data that indicates graft integrity. Murine infrarenal aortic isografts were orthotopically transplanted into 14 female mice (C57BL/6-Background) whereas a group of sham-operated animals (n = 10) served as controls. To assess the graft morphology and hemodynamics, we examined the mice over a post-operative period of 8 weeks with a sophisticated ultrasound system (Vevo 770, Visual Sonics). The non-invasive graft monitoring was feasible in all transplanted mice. We could demonstrate a regular post-transplant graft function and morphology, such as anterior/posterior wall displacement and wall thickness. Mild alterations of anterior wall motion dynamics could only be observed at the site of distal graft anastomosis (8 weeks after grafting (transplant vs. sham mice: 0.02 mm ± 0.01 vs. 0.03 mm ± 0.01, p&lt;0.05). However, the integrity of the entire graft wall could be confirmed by histopathological evaluation of the grafts. With regard to graft patency, function and morphology, high resolution ultrasound microimaging has proven to be a valuable tool for longitudinal, non-invasive, in vivo graft monitoring in this murine aortic transplantation model. Consequently, this experimental animal model provides an excellent basis for molecular and pharmacological studies using genetically engineered mice.
22132841	Optical encoding by plasmon-based patterning: hard and inorganic materials become photosensitive.
Nano Lett. 20111205 2012Jan11
The photosensitivity of nanocomposite AlN films with embedded silver nanospheres is reported. It stems from localized surface plasmon resonances (LSPR) whose modulation is photoinduced by laser annealing that induces a combined effect of metallic nanoparticle enlargement and dielectric matrix recrystallization; the photoindunced changes of the refractive index of the matrix result in strong spectral shift of LSPR. We demonstrate the utilization of this process for spectrally selective optical encoding into hard, durable, and chemically inert films.
22141918	A functional hybrid memristor crossbar-array/CMOS system for data storage and neuromorphic applications.
Nano Lett. 20111209 2012Jan11
Crossbar arrays based on two-terminal resistive switches have been proposed as a leading candidate for future memory and logic applications. Here we demonstrate a high-density, fully operational hybrid crossbar/CMOS system composed of a transistor- and diode-less memristor crossbar array vertically integrated on top of a CMOS chip by taking advantage of the intrinsic nonlinear characteristics of the memristor element. The hybrid crossbar/CMOS system can reliably store complex binary and multilevel 1600 pixel bitmap images using a new programming scheme.
22235242	Turning text into research networks: information retrieval and computational ontologies in the creation of scientific databases.
PLoS ONE 20120103 2012
Web-based, free-text documents on science and technology have been increasing growing on the web. However, most of these documents are not immediately processable by computers slowing down the acquisition of useful information. Computational ontologies might represent a possible solution by enabling semantically machine readable data sets. But, the process of ontology creation, instantiation and maintenance is still based on manual methodologies and thus time and cost intensive. We focused on a large corpus containing information on researchers, research fields, and institutions. We based our strategy on traditional entity recognition, social computing and correlation. We devised a semi automatic approach for the recognition, correlation and extraction of named entities and relations from textual documents which are then used to create, instantiate, and maintain an ontology. We present a prototype demonstrating the applicability of the proposed strategy, along with a case study describing how direct and indirect relations can be extracted from academic and professional activities registered in a database of curriculum vitae in free-text format. We present evidence that this system can identify entities to assist in the process of knowledge extraction and representation to support ontology maintenance. We also demonstrate the extraction of relationships among ontology classes and their instances. We have demonstrated that our system can be used for the conversion of research information in free text format into database with a semantic structure. Future studies should test this system using the growing number of free-text information available at the institutional and national levels.
21960719	mz5: space- and time-efficient storage of mass spectrometry data sets.
Mol. Cell Proteomics 20110929 2012Jan
Across a host of MS-driven-omics fields, researchers witness the acquisition of ever increasing amounts of high throughput MS data and face the need for their compact yet efficiently accessible storage. Addressing the need for an open data exchange format, the Proteomics Standards Initiative and the Seattle Proteome Center at the Institute for Systems Biology independently developed the mzData and mzXML formats, respectively. In a subsequent joint effort, they defined an ontology and associated controlled vocabulary that specifies the contents of MS data files, implemented as the newer mzML format. All three formats are based on XML and are thus not particularly efficient in either storage space requirements or read/write speed. This contribution introduces mz5, a complete reimplementation of the mzML ontology that is based on the efficient, industrial strength storage backend HDF5. Compared with the current mzML standard, this strategy yields an average file size reduction to ∼54% and increases linear read and write speeds ∼3-4-fold. The format is implemented as part of the ProteoWizard project and is available under a permissive Apache license. Additional information and download links are available from http://software.steenlab.org/mz5.
22238640	Darwin Core: an evolving community-developed biodiversity data standard.
PLoS ONE 20120106 2012
Biodiversity data derive from myriad sources stored in various formats on many distinct hardware and software platforms. An essential step towards understanding global patterns of biodiversity is to provide a standardized view of these heterogeneous data sources to improve interoperability. Fundamental to this advance are definitions of common terms. This paper describes the evolution and development of Darwin Core, a data standard for publishing and integrating biodiversity information. We focus on the categories of terms that define the standard, differences between simple and relational Darwin Core, how the standard has been implemented, and the community processes that are essential for maintenance and growth of the standard. We present case-study extensions of the Darwin Core into new research communities, including metagenomics and genetic resources. We close by showing how Darwin Core records are integrated to create new knowledge products documenting species distributions and changes due to environmental perturbations.
22032934	A new method to measure the polymerization shrinkage kinetics of composites using a particle tracking method with computer vision.
Dent Mater 20111026 2012Feb
The aim of this study was to develop a new method to measure the polymerization shrinkage of light cured composites and to evaluate the overall utility and significance of the technique. An optical instrument to measure the linear polymerization shrinkage of composites without directly contacting the specimen was developed using a particle tracking method with computer vision. The measurement system consisted of a CCD color video camera, a lens, an image storage device, and image processing and analysis software. The shrinkage kinetics of a commercial silorane-based composite (P90) and two conventional methacrylate-based composites (Z250 and a flowable Z350) were investigated and compared with the data measured using the "bonded disc method". The linear shrinkage of the composites was 0.33-1.41%. The shrinkage value was lowest for the silorane-based (P90) composite and highest for the flowable Z350 composite. The estimated volume shrinkages of the materials were comparable to the axial shrinkages measured with the bonded disc method. The new instrument was able to measure the true linear shrinkage of composites without sensitivity to the specimen geometry and the viscosity of the material. Therefore, this instrument can be used to characterize the shrinkage kinetics for a wide range of commercial and experimental visible-light-cure materials in relation to the composition and chemistry.
21539630	Addiction research centres and the nurturing of creativity. Monitoring the European drug situation: the ongoing challenge for the European Monitoring Centre for Drugs and Drug Addiction (EMCDDA).
Addiction 20110503 2012Feb
The European Monitoring Centre for Drugs and Drug Addiction (EMCDDA) is the designated hub for drug-related information in the European Union. The organization's role is to provide the European Union (EU) and its Member States with a factual overview of European drug problems and a common information framework to support the drugs debate. In order to achieve its mission, the EMCDDA coordinates and relies on a network of 30 national monitoring centres, the Reitox National Focal Points. The Centre publishes on a wide range of drug-related topics, across epidemiology, interventions, laws and policies. Every November, the EMCDDA publishes its Annual Report, providing a yearly update on the European drug situation, translated into 23 EU languages. In line with its founding regulation, the EMCDDA has a role acting as an interface between the worlds of science and policy. While not a research centre in the formal sense, the results the Centre generates serve as catalysts for new research questions and help to identify priorities. Current challenges facing the agency include continuing to increase scientific standards while maintaining a strong institutional role, as well as supporting European efforts to identify, share and codify best practice in the drugs field.
21798750	Recovery and reanalysis of archived airborne gamma spectrometry data from the 1991 Dounreay survey.
Appl Radiat Isot 20110703 2012Jan
Archived Airborne Gamma Spectrometry (AGS) data from the 1991 NIREX characterisations of Caithness have been recovered. The separate gamma spectrometry and positional data streams for approximately 120,000 measurements have been combined into a single data stream using the European Radiometrics and Spectrometry (ERS) data format. An analysis using working calibration coefficients and spectral stripping procedure has verified that the original survey recorded high quality data. The converted data stream is in a format more accessible to future research use, including evaluation of environmental change in the Caithness region.
22012916	Studies on magnetism and bioelectromagnetics for 45 years: from magnetic analog memory to human brain stimulation and imaging.
Bioelectromagnetics 20111019 2012Jan
Forty-five years of studies on magnetism and bioelectromagnetics, in our laboratory, are presented. This article is prepared for the d'Arsonval Award Lecture. After a short introduction of our early work on magnetic analog memory, we review and discuss the following topics: (1) Magnetic nerve stimulation and localized transcranial magnetic stimulation (TMS) of the human brain by figure-eight coils; (2) Measurements of weak magnetic fields generated from the brain by superconducting quantum interference device (SQUID) systems, called magnetoencephalography (MEG), and its application in functional brain studies; (3) New methods of magnetic resonance imaging (MRI) for the imaging of impedance of the brain, called impedance MRI, and the imaging of neuronal current activities in the brain, called current MRI; (4) Cancer therapy and other medical treatments by pulsed magnetic fields; (5) Effects of static magnetic fields and magnetic control of cell orientation and cell growth; and (6) Effects of radio frequency magnetic fields and control of iron ion release and uptake from and into ferritins, iron cage proteins. These bioelectromagnetic studies have opened new horizons in magnetism and medicine, in particular for brain research and treatment of ailments such as depression, Parkinson's, and Alzheimer's diseases.
21875672	Optimization of functional brain ROIs via maximization of consistency of structural connectivity profiles.
Neuroimage 20110819 2012Jan16
Segregation and integration are two general principles of the brain's functional architecture. Therefore, brain network analysis is of significant importance in understanding brain function. Critical to brain network construction and analysis is the identification of reliable, reproducible, and accurate network nodes, or Regions of Interest (ROIs). Task-based fMRI has been widely considered as a reliable approach to identify functionally meaningful ROIs in the brain. However, recent studies have shown that factors such as spatial smoothing could considerably shift the locations of detected activation peaks. As a result, structural and functional connectivity patterns can be significantly altered. Here, we propose a novel framework by which to optimize ROI sizes and locations, ensuring that differences between the structural connectivity profiles among a group of subjects is minimized. This framework is based on functional ROIs derived from task-based fMRI and diffusion tensor imaging (DTI) data. Accordingly, we present a new approach to describe and measure the fiber bundle similarity quantitatively within and across subjects which will facilitate the optimization procedure. Experimental results demonstrated that this framework improved the localizations of fMRI-derived ROIs. Through our optimization procedure, structural and functional connectivities were more consistent across different individuals. Overall, the ability to accurately localize network ROIs could facilitate many applications in brain imaging that rely on the accurate identification of ROIs.
22161324	Heterogeneous biological network visualization system: case study in context of medical image data.
Adv. Exp. Med. Biol.  2012
We have developed a system called megNet for integrating and visualizing heterogeneous biological data in order to enable modeling biological phenomena using a systems approach. Herein we describe megNet, including a recently developed user interface for visualizing biological networks in three dimensions and a web user interface for taking input parameters from the user, and an in-house text mining system that utilizes an existing knowledge base. We demonstrate the software with a case study in which we integrate lipidomics data acquired in-house with interaction data from external databases, and then find novel interactions that could possibly explain our previous associations between biological data and medical images. The flexibility of megNet assures that the tool can be applied in diverse applications, from target discovery in medical applications to metabolic engineering in industrial biotechnology.
21851274	Geometry-invariant texture retrieval using a dual-output pulse-coupled neural network.
Neural Comput 20110818 2012Jan
This letter proposes a novel dual-output pulse coupled neural network model (DPCNN). The new model is applied to obtain a more stable texture description in the face of the geometric transformation. Time series, which are computed from output binary images of DPCNN, are employed as translation-, rotation-, scale-, and distortion-invariant texture features. In the experiments, DPCNN has been well tested by using Brodatz's album and the VisTex database. Several existing models are compared with the proposed DPCNN model. The experimental results, based on different testing data sets for images with different translations, orientations, scales, and affine transformations, show that our proposed model outperforms existing models in geometry-invariant texture retrieval. Furthermore, the robustness of DPCNN to noisy data is examined in the experiments.
21906906	Development of the Australasian vascular surgical audit.
J. Vasc. Surg. 20110909 2012Jan
The purpose of this study was to describe the development of the Australasian Vascular Audit that was created to unify audit activities under the umbrella of the Australian and New Zealand Society for Vascular Surgery as a Web-based application. Constitutional change in late 2008 deemed participation in this audit compulsory for Society members. The Web-based application was developed and tested during 2009. Data for all open vascular surgery and for all endovascular procedures are collected at two points in the admission episode: at the time of operation and at discharge, and entered into the application. Data are analyzed to produce risk-adjusted outcomes. An algorithm has been developed to deal with outliers according to natural justice and to comply with the requirements of regulatory bodies. The Audit is protected by legislated privilege and is officially endorsed and indemnified by the Royal Australasian College of Surgeons. Confidentiality of surgeons and patients alike is ensured by a legally protected coding system and computer encryption system. Validation is by a verification process of 5% of members per year who are randomly selected. The application is completely funded by the Society. Data entry commenced on January 1, 2010. Over 40,000 vascular procedures were entered in the first year. The Audit application allows instantaneous on-line access to individual data and to deidentified group data and specific reports. It also allows real-time instantaneous production of log books for vascular trainees. The Audit has already gained recognition in the Australasian public arena during its first year of operation as an important benchmark of correct professional surgical behavior. Compliance has been extremely high in public hospitals but less so in private hospitals such that only 60% of members received a certificate of complete participation at the end of its first year of operation. An Internet-based compulsory audit of complete surgical practice is possible to create and be maintained by a society of surgeons with a membership of just over 200. The 60% compliance rate for complete data entry has created an immediate constitutional challenge for the Society. Future challenges are to improve total participation to an acceptable level and to ensure accurate data entry via a robust validation system.
21609882	Image authentication using distributed source coding.
IEEE Trans Image Process 20110523 2012Jan
We present a novel approach using distributed source coding for image authentication. The key idea is to provide a Slepian-Wolf encoded quantized image projection as authentication data. This version can be correctly decoded with the help of an authentic image as side information. Distributed source coding provides the desired robustness against legitimate variations while detecting illegitimate modification. The decoder incorporating expectation maximization algorithms can authenticate images which have undergone contrast, brightness, and affine warping adjustments. Our authentication system also offers tampering localization by using the sum-product algorithm.
21690014	Nonrigid brain MR image registration using uniform spherical region descriptor.
IEEE Trans Image Process 20110616 2012Jan
There are two main issues that make nonrigid image registration a challenging task. First, voxel intensity similarity may not be necessarily equivalent to anatomical similarity in the image correspondence searching process. Second, during the imaging process, some interferences such as unexpected rotations of input volumes and monotonic gray-level bias fields can adversely affect the registration quality. In this paper, a new feature-based nonrigid image registration method is proposed. The proposed method is based on a new type of image feature, namely, uniform spherical region descriptor (USRD), as signatures for each voxel. The USRD is rotation and monotonic gray-level transformation invariant and can be efficiently calculated. The registration process is therefore formulated as a feature matching problem. The USRD feature is integrated with the Markov random field labeling framework in which energy function is defined for registration. The energy function is then optimized by the  α-expansion algorithm. The proposed method has been compared with five state-of-the-art registration approaches on both the simulated and real 3-D databases obtained from the BrainWeb and Internet Brain Segmentation Repository, respectively. Experimental results demonstrate that the proposed method can achieve high registration accuracy and reliable robustness behavior.
21693418	Sparse demixing of hyperspectral images.
IEEE Trans Image Process 20110620 2012Jan
In the LMM for hyperspectral images, all the image spectra lie on a high-dimensional simplex with corners called endmembers. Given a set of endmembers, the standard calculation of fractional abundances with constrained least squares typically identifies the spectra as combinations of most, if not all, endmembers. We assume instead that pixels are combinations of only a few endmembers, yielding abundance vectors that are sparse. We introduce sparse demixing (SD), which is a method that is similar to orthogonal matching pursuit, for calculating these sparse abundances. We demonstrate that SD outperforms an existing  L(1)  demixing algorithm, which we prove to depend adversely on the angles between endmembers. We combine SD with dictionary learning methods to calculate automatically endmembers for a provided set of spectra. Applying it to an airborne visible/infrared imaging spectrometer image of Cuprite, NV, yields endmembers that compare favorably with signatures from the USGS spectral library.
21693424	A secret-sharing-based method for authentication of grayscale document images via the use of the PNG image with a data repair capability.
IEEE Trans Image Process 20110620 2012Jan
A new blind authentication method based on the secret sharing technique with a data repair capability for grayscale document images via the use of the Portable Network Graphics (PNG) image is proposed. An authentication signal is generated for each block of a grayscale document image, which, together with the binarized block content, is transformed into several shares using the Shamir secret sharing scheme. The involved parameters are carefully chosen so that as many shares as possible are generated and embedded into an alpha channel plane. The alpha channel plane is then combined with the original grayscale image to form a PNG image. During the embedding process, the computed share values are mapped into a range of alpha channel values near their maximum value of 255 to yield a transparent stego-image with a disguise effect. In the process of image authentication, an image block is marked as tampered if the authentication signal computed from the current block content does not match that extracted from the shares embedded in the alpha channel plane. Data repairing is then applied to each tampered block by a reverse Shamir scheme after collecting two shares from unmarked blocks. Measures for protecting the security of the data hidden in the alpha channel are also proposed. Good experimental results prove the effectiveness of the proposed method for real applications.
21712160	An algorithm for the contextual adaption of SURF octave selection with good matching performance: best octaves.
IEEE Trans Image Process 20110627 2012Jan
Speeded-Up Robust Features is a feature extraction algorithm designed for real-time execution, although this is rarely achievable on low-power hardware such as that in mobile robots. One way to reduce the computation is to discard some of the scale-space octaves, and previous research has simply discarded the higher octaves. This paper shows that this approach is not always the most sensible and presents an algorithm for choosing which octaves to discard based on the properties of the imagery. Results obtained with this best octaves algorithm show that it is able to achieve a significant reduction in computation without compromising matching performance.
21775261	Low distortion transform for reversible watermarking.
IEEE Trans Image Process 20110718 2012Jan
This paper proposes a low-distortion transform for prediction-error expansion reversible watermarking. The transform is derived by taking a simple linear predictor and by embedding the expanded prediction error not only into the current pixel but also into its prediction context. The embedding ensures the minimization of the square error introduced by the watermarking. The proposed transform introduces less distortion than the classical prediction-error expansion for complex predictors such as the median edge detector or the gradient-adjusted predictor. Reversible watermarking algorithms based on the proposed transform are analyzed. Experimental results are provided.
21975452	Neural coding of continuous speech in auditory cortex during monaural and dichotic listening.
J. Neurophysiol. 20111005 2012Jan
The cortical representation of the acoustic features of continuous speech is the foundation of speech perception. In this study, noninvasive magnetoencephalography (MEG) recordings are obtained from human subjects actively listening to spoken narratives, in both simple and cocktail party-like auditory scenes. By modeling how acoustic features of speech are encoded in ongoing MEG activity as a spectrotemporal response function, we demonstrate that the slow temporal modulations of speech in a broad spectral region are represented bilaterally in auditory cortex by a phase-locked temporal code. For speech presented monaurally to either ear, this phase-locked response is always more faithful in the right hemisphere, but with a shorter latency in the hemisphere contralateral to the stimulated ear. When different spoken narratives are presented to each ear simultaneously (dichotic listening), the resulting cortical neural activity precisely encodes the acoustic features of both of the spoken narratives, but slightly weakened and delayed compared with the monaural response. Critically, the early sensory response to the attended speech is considerably stronger than that to the unattended speech, demonstrating top-down attentional gain control. This attentional gain is substantial even during the subjects' very first exposure to the speech mixture and therefore largely independent of knowledge of the speech content. Together, these findings characterize how the spectrotemporal features of speech are encoded in human auditory cortex and establish a single-trial-based paradigm to study the neural basis underlying the cocktail party phenomenon.
21789500	NeuroNames: an ontology for the BrainInfo portal to neuroscience on the web.
Neuroinformatics  2012Jan
BrainInfo (                            http://braininfo.org                                                     ) is a growing portal to neuroscientific information on the Web. It is indexed by NeuroNames, an ontology designed to compensate for ambiguities in neuroanatomical nomenclature. The 20-year old ontology continues to evolve toward the ideal of recognizing all names of neuroanatomical entities and accommodating all structural concepts about which neuroscientists communicate, including multiple concepts of entities for which neuroanatomists have yet to determine the best or 'true' conceptualization. To make the definitions of structural concepts unambiguous and terminologically consistent we created a 'default vocabulary' of unique structure names selected from existing terminology. We selected standard names by criteria designed to maximize practicality for use in verbal communication as well as computerized knowledge management. The ontology of NeuroNames accommodates synonyms and homonyms of the standard terms in many languages. It defines complex structures as models composed of primary structures, which are defined in unambiguous operational terms. NeuroNames currently relates more than 16,000 names in eight languages to some 2,500 neuroanatomical concepts. The ontology is maintained in a relational database with three core tables: Names, Concepts and Models. BrainInfo uses NeuroNames to index information by structure, to interpret users' queries and to clarify terminology on remote web pages. NeuroNames is a resource vocabulary of the NLM's Unified Medical Language System (UMLS, 2011) and the basis for the brain regions component of NIFSTD (NeuroLex, 2011). The current version has been downloaded to hundreds of laboratories for indexing data and linking to BrainInfo, which attracts some 400 visitors/day, downloading 2,000 pages/day.
22369103	Preimplantation development regulatory pathway construction through a text-mining approach.
BMC Genomics 20111222 2011Dec22
The integration of sequencing and gene interaction data and subsequent generation of pathways and networks contained in databases such as KEGG Pathway is essential for the comprehension of complex biological processes. We noticed the absence of a chart or pathway describing the well-studied preimplantation development stages; furthermore, not all genes involved in the process have entries in KEGG Orthology, important information for knowledge application with relation to other organisms. In this work we sought to develop the regulatory pathway for the preimplantation development stage using text-mining tools such as Medline Ranker and PESCADOR to reveal biointeractions among the genes involved in this process. The genes present in the resulting pathway were also used as seeds for software developed by our group called SeedServer to create clusters of homologous genes. These homologues allowed the determination of the last common ancestor for each gene and revealed that the preimplantation development pathway consists of a conserved ancient core of genes with the addition of modern elements. The generation of regulatory pathways through text-mining tools allows the integration of data generated by several studies for a more complete visualization of complex biological processes. Using the genes in this pathway as "seeds" for the generation of clusters of homologues, the pathway can be visualized for other organisms. The clustering of homologous genes together with determination of the ancestry leads to a better understanding of the evolution of such process.
22369633	A singular value decomposition approach for improved taxonomic classification of biological sequences.
BMC Genomics 20111222 2011Dec22
Singular value decomposition (SVD) is a powerful technique for information retrieval; it helps uncover relationships between elements that are not prima facie related. SVD was initially developed to reduce the time needed for information retrieval and analysis of very large data sets in the complex internet environment. Since information retrieval from large-scale genome and proteome data sets has a similar level of complexity, SVD-based methods could also facilitate data analysis in this research area. We found that SVD applied to amino acid sequences demonstrates relationships and provides a basis for producing clusters and cladograms, demonstrating evolutionary relatedness of species that correlates well with Linnaean taxonomy. The choice of a reasonable number of singular values is crucial for SVD-based studies. We found that fewer singular values are needed to produce biologically significant clusters when SVD is employed. Subsequently, we developed a method to determine the lowest number of singular values and fewest clusters needed to guarantee biological significance; this system was developed and validated by comparison with Linnaean taxonomic classification. By using SVD, we can reduce uncertainty concerning the appropriate rank value necessary to perform accurate information retrieval analyses. In tests, clusters that we developed with SVD perfectly matched what was expected based on Linnaean taxonomy.
22369688	A rigorous approach to facilitate and guarantee the correctness of the genetic testing management in human genome information systems.
BMC Genomics 20111222 2011Dec22
Recent medical and biological technology advances have stimulated the development of new testing systems that have been providing huge, varied amounts of molecular and clinical data. Growing data volumes pose significant challenges for information processing systems in research centers. Additionally, the routines of genomics laboratory are typically characterized by high parallelism in testing and constant procedure changes. This paper describes a formal approach to address this challenge through the implementation of a genetic testing management system applied to human genome laboratory. We introduced the Human Genome Research Center Information System (CEGH) in Brazil, a system that is able to support constant changes in human genome testing and can provide patients updated results based on the most recent and validated genetic knowledge. Our approach uses a common repository for process planning to ensure reusability, specification, instantiation, monitoring, and execution of processes, which are defined using a relational database and rigorous control flow specifications based on process algebra (ACP). The main difference between our approach and related works is that we were able to join two important aspects: 1) process scalability achieved through relational database implementation, and 2) correctness of processes using process algebra. Furthermore, the software allows end users to define genetic testing without requiring any knowledge about business process notation or process algebra. This paper presents the CEGH information system that is a Laboratory Information Management System (LIMS) based on a formal framework to support genetic testing management for Mendelian disorder studies. We have proved the feasibility and showed usability benefits of a rigorous approach that is able to specify, validate, and perform genetic testing using easy end user interfaces.
22369201	Liverome: a curated database of liver cancer-related gene signatures with self-contained context information.
BMC Genomics 20111130 2011Nov30
Hepatocellular carcinoma (HCC) is the fifth most common cancer worldwide. A number of molecular profiling studies have investigated the changes in gene and protein expression that are associated with various clinicopathological characteristics of HCC and generated a wealth of scattered information, usually in the form of gene signature tables. A database of the published HCC gene signatures would be useful to liver cancer researchers seeking to retrieve existing differential expression information on a candidate gene and to make comparisons between signatures for prioritization of common genes. A challenge in constructing such database is that a direct import of the signatures as appeared in articles would lead to a loss or ambiguity of their context information that is essential for a correct biological interpretation of a gene's expression change. This challenge arises because designation of compared sample groups is most often abbreviated, ad hoc, or even missing from published signature tables. Without manual curation, the context information becomes lost, leading to uninformative database contents. Although several databases of gene signatures are available, none of them contains informative form of signatures nor shows comprehensive coverage on liver cancer. Thus we constructed Liverome, a curated database of liver cancer-related gene signatures with self-contained context information. Liverome's data coverage is more than three times larger than any other signature database, consisting of 143 signatures taken from 98 HCC studies, mostly microarray and proteome, and involving 6,927 genes. The signatures were post-processed into an informative and uniform representation and annotated with an itemized summary so that all context information is unambiguously self-contained within the database. The signatures were further informatively named and meaningfully organized according to ten functional categories for guided browsing. Its web interface enables a straightforward retrieval of known differential expression information on a query gene and a comparison of signatures to prioritize common genes. The utility of Liverome-collected data is shown by case studies in which useful biological insights on HCC are produced. Liverome database provides a comprehensive collection of well-curated HCC gene signatures and straightforward interfaces for gene search and signature comparison as well. Liverome is available at http://liverome.kobic.re.kr.
22369384	Effective gene collection from the metatranscriptome of marine microorganisms.
BMC Genomics 20111130 2011Nov30
Metagenomic studies, accelerated by the evolution of sequencing technologies and the rapid development of genomic analysis methods, can reveal genetic diversity and biodiversity in various samples including those of uncultured or unknown species. This approach, however, cannot be used to identify active functional genes under actual environmental conditions. Metatranscriptomics, which is similar in approach to metagenomics except that it utilizes RNA samples, is a powerful tool for the transcriptomic study of environmental samples. Unlike metagenomic studies, metatranscriptomic studies have not been popular to date due to problems with reliability, repeatability, redundancy and cost performance. Here, we propose a normalized metatranscriptomic method that is suitable for the collection of genes from samples as a platform for comparative transcriptomics. We constructed two libraries, one non-normalized and the other normalized library, from samples of marine microorganisms taken during daylight hours from Hiroshima bay in Japan. We sequenced 0.6M reads for each sample on a Roche GS FLX, and obtained 0.2M genes after quality control and assembly. A comparison of the two libraries showed that the number of unique genes was larger in the normalized library than in the non-normalized library. Functional analysis of genes revealed that a small number of gene groups, ribosomal RNA genes and chloroplast genes, were dominant in both libraries. Taxonomic distribution analysis of the libraries suggests that Stramenopiles form a major taxon that includes diatoms. The normalization technique thus increases unique genes, functional categories of genes, and taxonomic richness. Normalization of the marine metatranscriptome could be useful in increasing the number of genes collected, and in reducing redundancies among highly expressed genes. Gene collection through the normalization method was effective in providing a foundation for comparative transcriptomic analysis.
22369587	Identification of nucleotide patterns enriched in secreted RNAs as putative cis-acting elements targeting them to exosome nano-vesicles.
BMC Genomics 20111130 2011Nov30
Exosomes are nanoscale membrane vesicles released by most cells. They are postulated to be involved in cell-cell communication and genetic reprogramming of their target cells. In addition to proteins and lipids, they release RNA molecules many of which are not present in the donor cells implying a highly selective mode of their packaging into these vesicles. Sequence motifs targeting RNA to the vesicles are currently unknown. Ab initio approach was applied for computational identification of potential RNA secretory motifs in the primary sequences of exosome-enriched RNAs (eRNAs). Exhaustive motif analysis for the first time revealed unique sequence features of eRNAs. We discovered multiple linear motifs specifically enriched in secreted RNAs. Their potential function as cis-acting elements targeting RNAs to exosomes is proposed. The motifs co-localized in the same transcripts suggesting combinatorial organization of these secretory signals. We investigated associations of the discovered motifs with other RNA parameters. Secreted RNAs were found to have almost twice shorter half-life times on average, in comparison with cytoplasmic RNAs, and the occurrence of some eRNA-specific motifs significantly correlated with this eRNA feature. Also, we found that eRNAs are highly enriched in long noncoding RNAs. Secreted RNAs share specific sequence motifs that may potentially function as cis-acting elements targeting RNAs to exosomes. Discovery of these motifs will be useful for our understanding the roles of eRNAs in cell-cell communication and genetic reprogramming of the target cells. It will also facilitate nano-scale vesicle engineering and selective targeting of RNAs of interest to these vesicles for gene therapy purposes.
22369658	DetoxiProt: an integrated database for detoxification proteins.
BMC Genomics 20111130 2011Nov30
Detoxification proteins are a class of proteins for degradation and/or elimination of endogenous and exogenous toxins or medicines, as well as reactive oxygen species (ROS) produced by these materials. Most of these proteins are generated as a response to the stimulation of toxins or medicines. They are essential for the clearance of harmful substances and for maintenance of physiological balance in organisms. Thus, it is important to collect and integrate information on detoxification proteins. To store, retrieve and analyze the information related to their features and functions, we developed the DetoxiProt, a comprehensive database for annotation of these proteins. This database provides detailed introductions about different classes of the detoxification proteins. Extensive annotations of these proteins, including sequences, structures, features, inducers, inhibitors, substrates, chromosomal location, functional domains as well as physiological-biochemical properties were generated. Furthermore, pre-computed BLAST results, multiple sequence alignments and evolutionary trees for detoxification proteins are also provided for evolutionary study of conserved function and pathways. The current version of DetoxiProt contains 5956 protein entries distributed in 628 organisms. An easy to use web interface was designed, so that annotations about each detoxification protein can be retrieved by browsing with a specific method or by searching with different criteria. DetoxiProt provides an effective and efficient way of accessing the detoxification protein sequences and other high-quality information. This database would be a valuable source for toxicologists, pharmacologists and medicinal chemists. DetoxiProt database is freely available at http://lifecenter.sgst.cn/detoxiprot/.
22369691	Comparative analysis and assessment of M. tuberculosis H37Rv protein-protein interaction datasets.
BMC Genomics 20111130 2011Nov30
M. tuberculosis is a formidable bacterial pathogen. There is thus an increasing demand on understanding the function and relationship of proteins in various strains of M. tuberculosis. Protein-protein interactions (PPIs) data are crucial for this kind of knowledge. However, the quality of the main available M. tuberculosis PPI datasets is unclear. This hampers the effectiveness of research works that rely on these PPI datasets. Here, we analyze the two main available M. tuberculosis H37Rv PPI datasets. The first dataset is the high-throughput B2H PPI dataset from Wang et al's recent paper in Journal of Proteome Research. The second dataset is from STRING database, version 8.3, comprising entirely of H37Rv PPIs predicted using various methods. We find that these two datasets have a surprisingly low level of agreement. We postulate the following causes for this low level of agreement: (i) the H37Rv B2H PPI dataset is of low quality; (ii) the H37Rv STRING PPI dataset is of low quality; and/or (iii) the H37Rv STRING PPIs are predictions of other forms of functional associations rather than direct physical interactions. To test the quality of these two datasets, we evaluate them based on correlated gene expression profiles, coherent informative GO term annotations, and conservation in other organisms. We observe a significantly greater portion of PPIs in the H37Rv STRING PPI dataset (with score ≥ 770) having correlated gene expression profiles and coherent informative GO term annotations in both interaction partners than that in the H37Rv B2H PPI dataset. Predicted H37Rv interologs derived from non-M. tuberculosis experimental PPIs are much more similar to the H37Rv STRING functional associations dataset (with score ≥ 770) than the H37Rv B2H PPI dataset. H37Rv predicted physical interologs from IntAct also show extremely low similarity with the H37Rv B2H PPI dataset; and this similarity level is much lower than that between the S. aureus MRSA252 predicted physical interologs from IntAct and S. aureus MRSA252 pull-down PPIs. Comparative analysis with several representative two-hybrid PPI datasets in other species further confirms that the H37Rv B2H PPI dataset is of low quality. Next, to test the possibility that the H37Rv STRING PPIs are not purely direct physical interactions, we compare M. tuberculosis H37Rv protein pairs that catalyze adjacent steps in enzymatic reactions to B2H PPIs and predicted PPIs in STRING, which shows it has much lower similarities with the B2H PPIs than with STRING PPIs. This result strongly suggests that the H37Rv STRING PPIs more likely correspond to indirect relationships between protein pairs than to B2H PPIs. For more precise support, we turn to S. cerevisiae for its comprehensively studied interactome. We compare S. cerevisiae predicted PPIs in STRING to three independent protein relationship datasets which respectively comprise PPIs reported in Y2H assays, protein pairs reported to be in the same protein complexes, and protein pairs that catalyze successive reaction steps in enzymatic reactions. Our analysis reveals that S. cerevisiae predicted STRING PPIs have much higher similarity to the latter two types of protein pairs than to two-hybrid PPIs. As H37Rv STRING PPIs are predicted using similar methods as S. cerevisiae predicted STRING PPIs, this suggests that these H37Rv STRING PPIs are more likely to correspond to the latter two types of protein pairs rather than to two-hybrid PPIs as well. The H37Rv B2H PPI dataset has low quality. It should not be used as the gold standard to assess the quality of other (possibly predicted) H37Rv PPI datasets. The H37Rv STRING PPI dataset also has low quality; nevertheless, a subset consisting of STRING PPIs with score ≥770 has satisfactory quality. However, these STRING "PPIs" should be interpreted as functional associations, which include a substantial portion of indirect protein interactions, rather than direct physical interactions. These two factors cause the strikingly low similarity between these two main H37Rv PPI datasets. The results and conclusions from this comparative analysis provide valuable guidance in using these M. tuberculosis H37Rv PPI datasets in subsequent studies for a wide range of purposes.
22126369	A unified framework for managing provenance information in translational research.
BMC Bioinformatics 20111129 2011
A critical aspect of the NIH Translational Research roadmap, which seeks to accelerate the delivery of "bench-side" discoveries to patient's "bedside," is the management of the provenance metadata that keeps track of the origin and history of data resources as they traverse the path from the bench to the bedside and back. A comprehensive provenance framework is essential for researchers to verify the quality of data, reproduce scientific results published in peer-reviewed literature, validate scientific process, and associate trust value with data and results. Traditional approaches to provenance management have focused on only partial sections of the translational research life cycle and they do not incorporate "domain semantics", which is essential to support domain-specific querying and analysis by scientists. We identify a common set of challenges in managing provenance information across the pre-publication and post-publication phases of data in the translational research lifecycle. We define the semantic provenance framework (SPF), underpinned by the Provenir upper-level provenance ontology, to address these challenges in the four stages of provenance metadata:(a) Provenance collection - during data generation(b) Provenance representation - to support interoperability, reasoning, and incorporate domain semantics(c) Provenance storage and propagation - to allow efficient storage and seamless propagation of provenance as the data is transferred across applications(d) Provenance query - to support queries with increasing complexity over large data size and also support knowledge discovery applicationsWe apply the SPF to two exemplar translational research projects, namely the Semantic Problem Solving Environment for Trypanosoma cruzi (T.cruzi SPSE) and the Biomedical Knowledge Repository (BKR) project, to demonstrate its effectiveness. The SPF provides a unified framework to effectively manage provenance of translational research data during pre and post-publication phases. This framework is underpinned by an upper-level provenance ontology called Provenir that is extended to create domain-specific provenance ontologies to facilitate provenance interoperability, seamless propagation of provenance, automated querying, and analysis.
22023790	Moving beyond genome sequencing into personalized genomic medicine: biological and computing challenges.
Genome Biol. 20111024 2011
A report of the second annual Beyond the Genome conference held on the 19-22 September 2011 at The Universities at Shady Grove, Rockville, Maryland, USA, where increases in computing that may help make personal genomics a reality were a major focus.
22634953	[A technological device for optimizing the time taken for blind people to learn Braille].
Rev Salud Publica (Bogota)  2011Oct
This project was aimed at designing and putting an electronic prototype into practice for improving the initial time taken by visually handicapped people for learning Braille, especially children. This project was mainly based on a prototype digital electronic device which identifies and translates material written by a user in Braille by a voice synthesis system, producing artificial words to determine whether a handicapped person's writing in Braille has been correct. A global system for mobile communications (GSM) module was also incorporated into the device which allowed it to send text messages, thereby involving innovation in the field of articles for aiding visually handicapped people. This project's main result was an easily accessed and understandable prototype device which improved visually handicapped people's initial learning of Braille. The time taken for visually handicapped people to learn Braille became significantly reduced whilst their interest increased, as did their concentration time regarding such learning.
22688221	Defining datasets and creating data dictionaries for quality improvement and research in chronic disease using routinely collected data: an ontology-driven approach.
Inform Prim Care  2011
The burden of chronic disease is increasing, and research and quality improvement will be less effective if case finding strategies are suboptimal. To describe an ontology-driven approach to case finding in chronic disease and how this approach can be used to create a data dictionary and make the codes used in case finding transparent. A five-step process: (1) identifying a reference coding system or terminology; (2) using an ontology-driven approach to identify cases; (3) developing metadata that can be used to identify the extracted data; (4) mapping the extracted data to the reference terminology; and (5) creating the data dictionary. Hypertension is presented as an exemplar. A patient with hypertension can be represented by a range of codes including diagnostic, history and administrative. Metadata can link the coding system and data extraction queries to the correct data mapping and translation tool, which then maps it to the equivalent code in the reference terminology. The code extracted, the term, its domain and subdomain, and the name of the data extraction query can then be automatically grouped and published online as a readily searchable data dictionary. An exemplar online is: www.clininf.eu/qickd-data-dictionary.html Adopting an ontology-driven approach to case finding could improve the quality of disease registers and of research based on routine data. It would offer considerable advantages over using limited datasets to define cases. This approach should be considered by those involved in research and quality improvement projects which utilise routine data.
22688222	Construction and validation of a scoring system for the selection of high-quality data in a Spanish population primary care database (SIDIAP).
Inform Prim Care  2011
Computerised databases of primary care clinical records are widely used for epidemiological research. In Catalonia, the Information System for the Development of Research in Primary Care (SIDIAP) aims to promote the development of research based on high-quality validated data from primary care electronic medical records. The purpose of this study is to create and validate a scoring system (Registry Quality Score, RQS) that will enable all primary care practices (PCPs) to be selected as providers of researchusable data based on the completeness of their registers. Diseases that were likely to be representative of common diagnoses seen in primary care were selected for RQS calculations. The observed/expected cases ratio was calculated for each disease. Once we had obtained an estimated value for this ratio for each of the selected conditions we added up the ratios calculated for each condition to obtain a final RQS. Rate comparisons between observed and published prevalences of diseases not included in the RQS calculations (atrial fibrillation, diabetes, obesity, schizophrenia, stroke, urinary incontinence and Crohn's disease) were used to set the RQS cutoff which will enable researchers to select PCPs with research-usable data. Apart from Crohn's disease, all prevalences were the same as those published from the RQS fourth quintile (60th percentile) onwards. This RQS cut-off provided a total population of 1 936 443 (39.6% of the total SIDIAP population). SIDIAP is highly representative of the population of Catalonia in terms of geographical, age and sex distributions. We report the usefulness of rate comparison as a valid method to establish research-usable data within primary care electronic medical records.
22718666	Assessment of electronic health record usability with undergraduate nursing students.
Int J Nurs Educ Scholarsh 20110927 2011
Health information technology (HIT), and specifically electronic health records (EHR), are recognized as fundamental tools for collecting, storing, retrieving, and monitoring patient care and information. However, few schools of nursing have incorporated theoretical or practical aspects of HIT competencies within the educational curriculum. The purpose of this study was to conduct a usability assessment to explore undergraduate nursing student electronic health record documentation knowledge and skill, using a patient case scenario to inform the development of an informatics-based undergraduate nursing curriculum. Three themes were identified: "Being a Novice User/Practitioner," "Confidentiality and Security," and "Repetition and Practice." Integration of the EHR into nursing curriculum will allow students an EHR apprenticeship with the potential to enhance understanding and skill of nursing processes, documentation, and critical thinking. Findings will also guide teaching and learning strategies that will respond to rising expectations for competency with health information technology.
22784567	SigCS base: an integrated genetic information resource for human cerebral stroke.
BMC Syst Biol 20111214 2011Dec14
To understand how stroke risk factors mechanistically contribute to stroke, the genetic components regulating each risk factor need to be integrated and evaluated with respect to biological function and through pathway-based algorithms. This resource will provide information to researchers studying the molecular and genetic causes of stroke in terms of genomic variants, genes, and pathways. Reported genetic variants, gene structure, phenotypes, and literature information regarding stroke were collected and extracted from publicly available databases describing variants, genome, proteome, functional annotation, and disease subtypes. Stroke related candidate pathways and etiologic genes that participate significantly in risk were analyzed in terms of canonical pathways in public biological pathway databases. These efforts resulted in a relational database of genetic signals of cerebral stroke, SigCS base, which implements an effective web retrieval system. The current version of SigCS base documents 1943 non-redundant genes with 11472 genetic variants and 165 non-redundant pathways. The web retrieval system of SigCS base consists of two principal search flows, including: 1) a gene-based variant search using gene table browsing or a keyword search, and, 2) a pathway-based variant search using pathway table browsing. SigCS base is freely accessible at http://sysbio.kribb.re.kr/sigcs. SigCS base is an effective tool that can assist researchers in the identification of the genetic factors associated with stroke by utilizing existing literature information, selecting candidate genes and variants for experimental studies, and examining the pathways that contribute to the pathophysiological mechanisms of stroke.
23155634	[Spanish pediatric research in MEDLINE].
Cir Pediatr  2011Oct
The objective of this work was to elaborate and apply search filters to retrieve the Spanish scientific output (SO) in pediatrics. A bibliographic search was carried out in MEDLINE. The search was based on the construction and application of a Spanish geographic filter, elaborated according to a previous model, and a thematic one to retrieve records in the pediatric area. The following was determined: frequency of the records, place of publication, language, journal, number of authors and year of publication; to observe the dynamics of the SO, a bibliographic study was carried out over a period of ten years. By applying the geographic filter, 277,949 records were retrieved; 620 were retrieved in the pediatric Spanish area. Of these, 262 (42.26%) were published between 2000-2009. Most of records were signed by one author. In Spain, the records were published in 42 journals and abroad in 79 journals. A search filter able to retrieve the pediatric Spanish SO was created, that supposed 1.22% of the total records in the pediatric field encompassed by MEDLINE. Approximately 80% of these articles were published in Spain.
23270096	Embedding QR codes in tumor board presentations, enhancing educational content for oncology information management.
J Registry Manag  2011Winter
Quick Response (QR) Codes are standard in supply management and seen with increasing frequency in advertisements. They are now present regularly in healthcare informatics and education. These 2-dimensional square bar codes, originally designed by the Toyota car company, are free of license and have a published international standard. The codes can be generated by free online software and the resulting images incorporated into presentations. The images can be scanned by "smart" phones and tablets using either the iOS or Android platforms, which link the device with the information represented by the QR code (uniform resource locator or URL, online video, text, v-calendar entries, short message service [SMS] and formatted text). Once linked to the device, the information can be viewed at any time after the original presentation, saved in the device or to a Web-based "cloud" repository, printed, or shared with others via email or Bluetooth file transfer. This paper describes how we use QR codes in our tumor board presentations, discusses the benefits, the different QR codes from Web links and how QR codes facilitate the distribution of educational content.
23443698	Stand-alone front-end system for high- frequency, high-frame-rate coded excitation ultrasonic imaging.
IEEE Trans Ultrason Ferroelectr Freq Control  2011Dec
A stand-alone front-end system for high-frequency coded excitation imaging was implemented to achieve a wider dynamic range. The system included an arbitrary waveform amplifier, an arbitrary waveform generator, an analog receiver, a motor position interpreter, a motor controller and power supplies. The digitized arbitrary waveforms at a sampling rate of 150 MHz could be programmed and converted to an analog signal. The pulse was subsequently amplified to excite an ultrasound transducer, and the maximum output voltage level achieved was 120 V(pp). The bandwidth of the arbitrary waveform amplifier was from 1 to 70 MHz. The noise figure of the preamplifier was less than 7.7 dB and the bandwidth was 95 MHz. Phantoms and biological tissues were imaged at a frame rate as high as 68 frames per second (fps) to evaluate the performance of the system. During the measurement, 40-MHz lithium niobate (LiNbO(3)) single-element lightweight (&lt;;0.28 g) transducers were utilized. The wire target measure- ment showed that the -6-dB axial resolution of a chirp-coded excitation was 50 μm and lateral resolution was 120 μm. The echo signal-to-noise ratios were found to be 54 and 65 dB for the short burst and coded excitation, respectively. The contrast resolution in a sphere phantom study was estimated to be 24 dB for the chirp-coded excitation and 15 dB for the short burst modes. In an in vivo study, zebrafish and mouse hearts were imaged. Boundaries of the zebrafish heart in the image could be differentiated because of the low-noise operation of the implemented system. In mouse heart images, valves and chambers could be readily visualized with the coded excitation.
21158703	Developing the evidence base for cancer chemoprevention: use of meta-analysis.
Curr Drug Targets  2011Dec
Meta-analysis is a quantitative approach for systematically combining the results of previous studies in order to arrive at conclusions about the body of research. It answers a specific research question, includes an explicit methodology section, employs strategies to minimize bias, yields objective findings and enables evidence-based decisions. In this review, we examine meta-analysis taking examples from the field of cancer chemoprevention, an innovative area of cancer research that focuses on the prevention of cancer through pharmacological, biologic, and nutritional interventions. In particular, we consider the practical steps involved in the conduct of a meta-analysis, illustrate the statistical techniques for the calculation of summary estimates, present the available methodology for detecting and minimizing bias and, finally, we discuss unresolved issues and future applications.
22212228	Finding translational science publications in MEDLINE/PubMed with translational science filters.
Clin Transl Sci 20111107 2011Dec
Translational Science Search (TSS; http://tscience.nlm.nih.gov) is a web application for finding MEDLINE/PubMed journal articles that are regarded by their authors as novel, promising, or may have potential clinical application. A set of "translational" filters and related terms was created by reviewing journal articles published in clinical and translational science (TS) journals. Through E-Utilities, a user's query and TS filters are submitted to PubMed, and then, the retrieved PubMed citations are matched with a database of MeSH terms (for disease conditions) and RxNorm (for interventions) to locate the search term, translational filters found, and associated interventions in the title and abstract. An algorithm ranks the interventions and conditions, and then highlights them in the results page for quick reading and evaluation. Using previously searched terms and standard formulas, the precision and recall of TSS were 0.99 and 0.47, compared to 0.58 and 1.0 for PubMed Entrez, respectively.
22223056	Creation and implementation of a prospective pediatric clinical outcomes registry.
J Registry Manag  2011Autumn
To build a pediatric clinical outcomes registry (COR) using a contemporary information system designed to support research and outcome studies and to improve patient care and quality of life. In response to physician needs, this process was implemented: 1) database needs assessment survey, 2) evaluate existing systems and vendors, 3) pilot test a COR tool, and 4) build a COR. The COR was designed to include patients with the following conditions: scoliosis, neonatal surgery, urologic surgery, cleft palate, pain management, otitis media, and voice and airway problems. Agency for Healthcare Research and Quality methodology was followed to create the infrastructure and registry. The database needs assessment survey was completed by 99 individuals and most respondents wanted to collect more standardized data than currently available in existing systems. Satisfaction with the existing systems was rated low. The COR was created and a pilot test was successful. The COR was implemented and has been functioning for more than 2 years. By identifying physicians needs, evaluating existing technology and incorporating a multidisciplinary team, the COR was created and implemented to maintain clinical data on a variety of patient diagnoses and outcomes using a single technology platform that enhances potential research collaborations and minimizes redundant data entry and data collection, such as quality of life assessments for the patients.
22248952	Use of information and communication technology among dental students and registrars at the faculty of dental sciences, University of Lagos.
Niger J Clin Pract  2011 Oct-Dec
The aim of this study was to investigate the use of information technology amongst dental students, dental nursing students and resident doctors in training at the faculty of dental Surgery University of Lagos. A structured questionnaire was distributed to 58 clinical dental students in 4 th and 5 th years of training in the 2010/2011 academic year, 36 dental nursing students and 63 resident doctors undergoing specialist training. All participants have access to the computers, 2.5% within the University and 31% at home and internet cafes and about 50% have the basic skills required. A significant difference was observed between the resident doctors and clinical dental students (P = 0.003), between resident doctors and dental nursing students (P = 0.0001) when the use of computer for study was compared. Over 95% of participants have access to internet and about 50% of them use the internet for their studies. A significant difference (P = 0.005) was observed between clinical dental students and dental nursing students that use the internet and word processing. The resident doctors used the computers for multimedia and MedLine search tools more than clinical dental students (P = 0.004) and dental nursing students (0.0006). The findings of the study show that dental students and resident doctors in training have the requisite knowledge to operate the computer for use in their study and personal activities.
22254447	CHRONIOUS: a wearable platform for monitoring and management of patients with chronic disease.
Conf Proc IEEE Eng Med Biol Soc  2011
The CHRONIOUS system has been developed based on an open architecture design that consists of a set of subsystems which interact in order to provide all the needed services to the chronic disease patients. An advanced multi-parametric expert system is being implemented that fuses information effectively from various sources using intelligent techniques. Data are collected by sensors of a body network controlling vital signals while additional tools record dietary habits and plans, drug intake, environmental and biochemical parameters and activity data. The CHRONIOUS platform provides guidelines and standards for the future generations of "chronic disease management systems" and facilitates sophisticated monitoring tools. In addition, an ontological information retrieval system is being delivered satisfying the necessities for up-to-date clinical information of Chronic Obstructive pulmonary disease (COPD) and Chronic Kidney Disease (CKD). Moreover, support tools are being embedded in the system, such as the Mental Tools for the monitoring of patient mental health status. The integrated platform provides real-time patient monitoring and supervision, both indoors and outdoors and represents a generic platform for the management of various chronic diseases.
22254529	DigiScope--unobtrusive collection and annotating of auscultations in real hospital environments.
Conf Proc IEEE Eng Med Biol Soc  2011
Digital stethoscopes are medical devices that can collect, store and sometimes transmit acoustic auscultation signals in a digital format. These can then be replayed, sent to a colleague for a second opinion, studied in detail after an auscultation, used for training or, as we envision it, can be used as a cheap powerful tool for screening cardiac pathologies. In this work, we present the design, development and deployment of a prototype for collecting and annotating auscultation signals within real hospital environments. Our main objective is not only pave the way for future unobtrusive systems for cardiac pathology screening, but more immediately we aim to create a repository of annotated auscultation signals for biomedical signal processing and machine learning research. The presented prototype revolves around a digital stethoscope that can stream the collected audio signal to a nearby tablet PC. Interaction with this system is based on two models: a data collection model adequate for the uncontrolled hospital environments of both emergency room and primary care, and a data annotation model for offline metadata input. A specific data model was created for the repository. The prototype has been deployed and is currently being tested in two Hospitals, one in Portugal and one in Brazil.
22254609	An improved scheme of IPI-based entity identifier generation for securing body sensor networks.
Conf Proc IEEE Eng Med Biol Soc  2011
Securing body sensor network (BSN) in an efficient manner is very important for preserving the privacy of medical data. Protecting data confidentiality, integrity and to authenticate the communicating nodes are basic requirements to secure BSN. The existing method to generate entity identifier (EI) from inter-pulse intervals (IPIs) of heartbeats has its advantages in authenticating and identifying nodes, which however was found in this study that such generated EIs are not so resistant to attacks because of potential error patterns. This paper presents an improved scheme of IPI-based EI generation to eliminate the error patterns. The performance of randomness and node identification, i.e. false acceptance rate and false rejection rate, is experimentally evaluated. The results indicate that compared with the existing one, the new scheme is effective to eliminate the error patterns and thus more tolerant to attacks, while there is no compromise on the randomness level and identification performance.
22254612	Mapping of multiple parameter m-health scenarios to mobile WiMAX QoS variables.
Conf Proc IEEE Eng Med Biol Soc  2011
Multiparameter m-health scenarios with bandwidth demanding requirements will be one of key applications in future 4 G mobile communication systems. These applications will potentially require specific spectrum allocations with higher quality of service requirements. Furthermore, one of the key 4 G technologies targeting m-health will be medical applications based on WiMAX systems. Hence, it is timely to evaluate such multiple parametric m-health scenarios over mobile WiMAX networks. In this paper, we address the preliminary performance analysis of mobile WiMAX network for multiparametric telemedical scenarios. In particular, we map the medical QoS to typical WiMAX QoS parameters to optimise the performance of these parameters in typical m-health scenario. Preliminary performance analyses of the proposed multiparametric scenarios are evaluated to provide essential information for future medical QoS requirements and constraints in these telemedical network environments.
22254660	Distilling clinically interpretable information from data collected on next-generation wearable sensors.
Conf Proc IEEE Eng Med Biol Soc  2011
Medical electronic systems are generating ever larger data sets from a variety of sensors and devices. Such systems are also being packaged in wearable designs for easy and broad use. The large volume of data and the constraints of low-power, extended-duration, and wireless monitoring impose the need for on-chip processing to distill clinically relevant information from the raw data. The higher-level information, rather than the raw data, is what needs to be transmitted. We present one example of information processing for continuous, high-sampling-rate data collected from wearable and portable devices. A wearable cardiac and motion monitor designed by colleagues at MIT simultaneously records electrocardiogram (ECG) and 3-axis acceleration to onboard memory, in an ambulatory setting. The acceleration data is used to generate a continuous estimate of physical activity. Additionally, we use a Portapres continuous blood pressure monitor to concurrently record the arterial blood pressure (ABP) waveform. To help reduce noise, which is an increased challenge in ambulatory monitoring, we use both the ECG and ABP waveforms to generate a robust measure of heart rate from noisy data. We also generate an overall signal abnormality index to aid in the interpretation of the results. Two important cardiovascular quantities, namely cardiac output (CO) and total peripheral resistance (TPR), are then derived from this data over a sequence of physical activities. CO and TPR can be estimated (to within a scale factor) from heart rate, pulse pressure and mean arterial blood pressure, which in turn are directly obtained from the ECG and ABP signals. Data was collected on 10 healthy subjects. The derived quantities vary in a manner that is consistent with known physiology. Further work remains to correlate these values with the cardiac health state.
22254701	An embedded reconfigurable architecture for patient-specific multi-paramater medical monitoring.
Conf Proc IEEE Eng Med Biol Soc  2011
A robust medical monitoring device should be able to provide intelligent diagnosis based on accurate analysis of physiological parameters in real-time. At the same time, such device must be able to adapt to the characteristics of a specific patient and desired diagnostic needs, and continue to operate even in presence of unexpected artifacts and accidental errors. A reconfigurable architecture is proposed for real-time assessment of individual's health status based on development of a patient-specific health index and online analysis and fusion of multi-parameter physiological signals. This is achieved by static configuration of processing elements and communication blocks in the architecture based on the patient's diagnostic needs. The proposed architecture is prototyped as a single integrated device on an FPGA platform and is evaluated using multi-parameter data from intensive care units (ICUs). Three representative test cases of concurrently analyzing Blood Pressure, Heart Rate, and Electrocardiogram (ECG) data from MIMIC database are presented. The results show the effectiveness of the proposed technique in eliminating false alarms caused by patient movements, monitor noise, or imperfections in the detection schemes.
22254768	Onboard tagging for smart medical devices.
Conf Proc IEEE Eng Med Biol Soc  2011
Most medical devices are 'dumb:' their role is to acquire, display, and forward data. They make few if any operational decisions based on those data. Onboard tagging is a means whereby a device can embed information about itself, its data, and the sensibility of those data into its data stream. This diagnostic add-on offers a move toward 'smart' devices that will have the ability to affect changes in operational modes based on onboard contextual decision making, such as decisions to avoid needless wireless transmission of corrupt data. This paper presents a description of three types of onboard tags that relate to device hardware (type I tag), signal statistics (type II tag), and signal viability for the intended application (type III tag). A custom wireless pulse oximeter is presented as a use case to show how type II and III tags that convey photoplethysmogram (PPG) statistics and usability specifiers can be calculated and embedded into the data stream without degrading performance.
22254771	Radio Frequency Identification (RFID) in medical environment: Gaussian Derivative Frequency Modulation (GDFM) as a novel modulation technique with minimal interference properties.
Conf Proc IEEE Eng Med Biol Soc  2011
Radio Frequency Identification (RFID) systems in healthcare facilitate the possibility of contact-free identification and tracking of patients, medical equipment and medication. Thereby, patient safety will be improved and costs as well as medication errors will be reduced considerably. However, the application of RFID and other wireless communication systems has the potential to cause harmful electromagnetic disturbances on sensitive medical devices. This risk mainly depends on the transmission power and the method of data communication. In this contribution we point out the reasons for such incidents and give proposals to overcome these problems. Therefore a novel modulation and transmission technique called Gaussian Derivative Frequency Modulation (GDFM) is developed. Moreover, we carry out measurements to show the inteference properties of different modulation schemes in comparison to our GDFM.
22254774	Improving the reliability of wireless body area networks.
Conf Proc IEEE Eng Med Biol Soc  2011
In this paper we propose a highly reliable wireless body area network (WBAN) that provides increased throughput and avoids single points of failure. Such networks improve upon current WBANs by taking advantage of a new technology, Cooperative Network Coding (CNC). Using CNC in wireless body area network to support real-time applications is an attractive solution to combat packet loss, reduce latency due to retransmissions, avoid single points of failure, and improve the probability of successful recovery of the information at the destination. In this paper, we have extended Cooperative Network Coding, from its original configuration (one-to-one) to many-to-many as in multiple-input-multiple-output (MIMO) systems. Cooperative Network Coding results in increased throughput and network reliability because of the cooperation of the nodes in transmitting coded combination packets across spatially distinct paths to the information sinks.
22254776	Battery friendly packet transmission scheme for body sensor networks.
Conf Proc IEEE Eng Med Biol Soc  2011
For body sensor networks (BSN) nodes, extending battery lifetime is one of the key problems. In this paper, we address the problems of designing battery-friendly packet transmission policies in order to maximize the lifetime of batteries for BSN nodes under certain delay and deadline constraints. We present local optimization scheme for slack time and evaluate it with respect to battery performance. The algorithm first simplifies the analytical battery model under the premise of ensuring battery model precision, and then distributes the available slack time between two adjacent tasks. The scheme was simulated for BSN nodes, and the results demonstrate a 79% reduction in the total charge consumption of six tasks in a BSN node along with the deadline constraint of 40 mins.
22254777	Power allocation strategies to minimize energy consumption in wireless body area networks.
Conf Proc IEEE Eng Med Biol Soc  2011
The wide scale deployment of wireless body area networks (WBANs) hinges on designing energy efficient communication protocols to support the reliable communication as well as to prolong the network lifetime. Cooperative communications, a relatively new idea in wireless communications, offers the benefits of multi-antenna systems, thereby improving the link reliability and boosting energy efficiency. In this short paper, the advantages of resorting to cooperative communications for WBANs in terms of minimized energy consumption are investigated. Adopting an energy model that encompasses energy consumptions in the transmitter and receiver circuits, and transmitting energy per bit, it is seen that cooperative transmission can improve energy efficiency of the wireless network. In particular, the problem of optimal power allocation is studied with the constraint of targeted outage probability. Two strategies of power allocation are considered: power allocation with and without posture state information. Using analysis and simulation-based results, two key points are demonstrated: (i) allocating power to the on-body sensors making use of the posture information can reduce the total energy consumption of the WBAN; and (ii) when the channel condition is good, it is better to recruit less relays for cooperation to enhance energy efficiency.
22254818	A semantically-aided approach for online annotation and retrieval of medical images.
Conf Proc IEEE Eng Med Biol Soc  2011
The need for annotating the continuously increasing volume of medical image data is recognized from medical experts for a variety of purposes, regardless if this is medical practice, research or education. The rich information content latent in medical images can be made explicit and formal with the use of well-defined ontologies. Evolution of the Semantic Web now offers a unique opportunity of a web-based, service-oriented approach. Remote access to FMA and ICD-10 reference ontologies provides the ontological annotation framework. The proposed system utilizes this infrastructure to provide a customizable and robust annotation procedure. It also provides an intelligent search mechanism indicating the advantages of semantic over keyword search. The common representation layer discussed facilitates interoperability between institutions and systems, while semantic content enables inference and knowledge integration.
22255054	Restorative encoding memory integrative neural device: "REMIND".
Conf Proc IEEE Eng Med Biol Soc  2011
Construction and application of a neural prosthesis device that enhances existing and replaces lost memory capacity in humans is the focus of research described here in rodents. A unique approach for the analysis and application of neural population firing has been developed to decipher the pattern in which information is successfully encoded by the hippocampus where mnemonic accuracy is critical. A nonlinear dynamic multi-input multi-output (MIMO) model is utilized to extract memory relevant firing patterns in CA3 and CA1 and to predict online what the consequences of the encoded firing patterns reflect for subsequent information retrieval for successful performance of delayed-nonmatch-to-sample (DNMS) memory task in rodents. The MIMO model has been tested successfully in a number of different contexts, each of which produced improved performance by a) utilizing online predicted codes to regulate task difficulty, b) employing electrical stimulation of CA1 output areas in the same pattern as successful cell firing, c) employing electrical stimulation to recover cell firing compromised by pharmacological agents and d) transferring and improving performance in naïve animals using the same stimulation patterns that are effective in fully trained animals. The results in rodents formed the basis for extension of the MIMO model to nonhuman primates in the same type of memory task that is now being tested in the last step prior to its application in humans.
22255490	A salient information processing system for bionic eye with application to obstacle avoidance.
Conf Proc IEEE Eng Med Biol Soc  2011
In this paper we present a visual processing system for bionic eye with a focus on obstacle avoidance. Bionic eye aims at restoring the sense of vision to people living with blindness and low vision. However, current hardware implant technology limits the image resolution of the electrical stimulation device to be very low (e.g., 100 electrode arrays, which is approx. 12 × 9 pixels). Therefore, we need a visual processing unit that extracts salient information in an unknown environment for assisting patients in daily tasks such as obstacle avoidance. We implemented a fully portable system that includes a camera for capturing videos, a laptop for processing information using a state-of-the-art saliency detection algorithm, and a head-mounted display to visualize results. The experimental environment consists of a number of objects, such as shoes, boxes, and foot stands, on a textured ground plane. Our results show that the system efficiently processes the images, effectively identifies the obstacles, and eventually provides useful information for obstacle avoidance.
22255520	ContextProvider: Context awareness for medical monitoring applications.
Conf Proc IEEE Eng Med Biol Soc  2011
Smartphones are sensor-rich and Internet-enabled. With their on-board sensors, web services, social media, and external biosensors, smartphones can provide contextual information about the device, user, and environment, thereby enabling the creation of rich, biologically driven applications. We introduce ContextProvider, a framework that offers a unified, query-able interface to contextual data on the device. Unlike other context-based frameworks, ContextProvider offers interactive user feedback, self-adaptive sensor polling, and minimal reliance on third-party infrastructure. ContextProvider also allows for rapid development of new context and bio-aware applications. Evaluation of ContextProvider shows the incorporation of an additional monitoring sensor into the framework with fewer than 100 lines of Java code. With adaptive sensor monitoring, power consumption per sensor can be reduced down to 1% overhead. Finally, through the use of context, accuracy of data interpretation can be improved by up to 80%.
22255535	A web based tool for storing and visualising data generated within a smart home.
Conf Proc IEEE Eng Med Biol Soc  2011
There is a growing need to re-assess the current approaches available to researchers for storing and managing heterogeneous data generated within a smart home environment. In our current work we have developed the homeML Application; a web based tool to support researchers engaged in the area of smart home research as they perform experiments. Within this paper the homeML Application is presented which includes the fundamental components of the homeML Repository and the homeML Toolkit. Results from a usability study conducted by 10 computer science researchers are presented; the initial results of which have been positive.
22255612	Wirelessly powered stimulator and recorder for neuronal interfaces.
Conf Proc IEEE Eng Med Biol Soc  2011
Functional Electrical Stimulation (FES) is widely adopted in neuro-engineering to partially alleviate diseased functions in the brain, retina and cochlea. We present a 32-channel wirelessly powered constant current stimulator and low power recording amplifier for FES based applications. The biphasic stimulator utilizes innovative techniques for matched positive/ negative currents and thus improves charge balance. Electrode discharging scheme is added for stimulation artifact suppression. An improved low power amplifier is incorporated for evoked response measurements. Electrical performance is characterized using simulated electrode-electrolyte impedance. Closed-loop stimulation and recording experiments have been performed. Stimulation current magnitudes of 2 μA-200 μA and up to 400 Hz rate have been realized. Theory and limitation of discharging scheme is explored while suppressing artifacts down to 3 ms. Alternate anodic-first and cathodic-first stimulation pulses are adopted for enhanced charge balancing. The low power amplifier exhibits gain of 1200 and bandwidth 350 Hz-1.02 KHz. A multiplexer/ demultiplexer is used to share the front-end among 32 electrodes. The inductively coupled wireless energy harvester works at 125 KHz-135 KHz that can remotely deliver 1.4 mW at 1cm distance to an equivalent of 10K load. The system can accommodate multielectrodes with impedance up to 100 K Ω. The entire hybrid analog-digital system consumes 360 μW quiescent power. Miniaturization makes it suitable for in-vivo applications.
22255626	BioSignalML--a meta-model for biosignals.
Conf Proc IEEE Eng Med Biol Soc  2011
The multitude of biosignal file formats used in research has hampered the easy exchange of biosignals and their use with physiological modelling software. We describe an abstract data model that accommodates the diversity of formats, along with a software implementation which links biosignal data into the Semantic Web, using existing data formats. Initial application of our work is to sleep study research.
22255640	Interactive 3D reconstruction of the spine from radiographs using a statistical shape model and second-order cone programming.
Conf Proc IEEE Eng Med Biol Soc  2011
Three-dimensional models of the spine are commonly used to diagnose, to treat, and to study spinal deformities. Creating these models is however time-consuming and, therefore, expensive. We propose in this paper a reconstruction method that finds the most likely 3D reconstruction given a maximal error bound on a limited set of landmark locations supplied by the user. This problem can be solved using second-order cone programming, leading to a globally convergent method that is considerably faster than currently available methods. A user can, with our current implementation, interactively modify the landmark locations and receive instantaneous feedback on the effect of those changes on the 3D reconstruction instead of blindly selecting landmarks. The proposed method was validated on a set of 53 patients who had adolescent idiopathic scoliosis using real and synthetic tests. Test results showed that the proposed method is considerably faster than currents methods (about forty times faster), is extremely flexible, and offers comparable accuracy.
22255671	Approaches for the efficient extraction and processing of biopotentials in implantable neural interfacing microsystems.
Conf Proc IEEE Eng Med Biol Soc  2011
The accelerating pace of research in neurosciences and rehabilitation engineering has created a considerable demand for implantable microsystems capable of interfacing with large groups of neurons. Such microsystems must provide multiple recording channels incorporating low-noise amplifiers, filters, data converters, neural signal processing circuitry, power management units and low-power transmitters to extract and wirelessly transfer the relevant neural data outside the body for computing and storage. This paper is reviewing several electronic recording strategies to address the challenge of operating large numbers of channels to gather the neural information from several neurons within very low-power constraints.
22255673	An overview of the recent wideband transcutaneous wireless communication techniques.
Conf Proc IEEE Eng Med Biol Soc  2011
Neuroprosthetic devices such as cochlear and retinal implants need to deliver a large volume of data from external sensors into the body, while invasive brain-computer interfaces need to deliver sizeable amounts of data from the central nervous system to target devices outside of the body. Nonetheless, the skin should remain intact. This paper reviews some of the latest techniques to establish wideband wireless communication links across the skin.
22255684	Multiscale information for network characterization in epilepsy.
Conf Proc IEEE Eng Med Biol Soc  2011
We have developed a multiscale approach for the estimation of neuronal network coordination in the epileptic brain, from continuous (long-term) non-invasive electroencephalograms (EEG). The proposed approach specifically assesses the effect of large-scale network behavior on local network coordination, at individual dominant frequencies (modes) of the EEG spectrum. For this purpose a set of conditional information parameters is proposed to explicitly quantify the effect of global network correlation in the brain on pairwise (local) mutual information, via conditioning. These parameters are shown to be modulated in a frequency-specific manner at baseline, as well as during seizure evolution.
22255695	Machine learning and pattern classification in identification of indigenous retinal pathology.
Conf Proc IEEE Eng Med Biol Soc  2011
Diabetic retinopathy (DR) is a complication of diabetes, which if untreated leads to blindness. DR early diagnosis and treatment improve outcomes. Automated assessment of single lesions associated with DR has been investigated for sometime. To improve on classification, especially across different ethnic groups, we present an approach using points-of-interest and visual dictionary that contains important features required to identify retinal pathology. Variation in images of the human retina with respect to differences in pigmentation and presence of diverse lesions can be analyzed without the necessity of preprocessing and utilizing different training sets to account for ethnic differences for instance.
22255733	Characterization of entropy measures against data loss: application to EEG records.
Conf Proc IEEE Eng Med Biol Soc  2011
This study is aimed at characterizing three signal entropy measures, Approximate Entropy (ApEn), Sample Entropy (SampEn) and Multiscale Entropy (MSE) over real EEG signals when a number of samples are randomly lost due to, for example, wireless data transmission. The experimental EEG database comprises two main signal groups: control EEGs and epileptic EEGs. Results show that both SampEn and ApEn enable a clear distinction between control and epileptic signals, but SampEn shows a more robust performance over a wide range of sample loss ratios. MSE exhibits a poor behavior for ratios over a 40% of sample loss. The EEG non-stationary and random trends are kept even when a great number of samples are discarded. This behavior is similar for all the records within the same group.
22273948	Non-linear regularized phase retrieval for unidirectional X-ray differential phase contrast radiography.
Opt Express  2011Dec5
Phase retrieval from unidirectional radiographic differential phase contrast images requires integration of noisy data. A method is presented, which aims to suppress stripe artifacts arising from direct image integration. It is purely algorithmic and therefore, compared to alternative approaches, neither additional alignment nor an increased scan time is required. We report on the theory of this method and present results using numerical as well as experimental data. The method shows significant improvements on the phase retrieval accuracy and enhances contrast in the phase image. Due to its general applicability, the proposed method provides a valuable tool for various 2D imaging applications using differential data.
22274244	Faster-than-Nyquist and beyond: how to improve spectral efficiency by accepting interference.
Opt Express  2011Dec19
We investigate the application of time and frequency packing techniques, an extension of the classical faster-than-Nyquist signaling, to long-haul optical links. These techniques provide a significant spectral efficiency increase and represent a viable alternative to overcome the theoretical and technological issues related to the use of high-order modulation formats. Adopting these techniques, we successfully demonstrate through simulations the transmission of 1 Tbps over 200 GHz bandwidth in a realistic (nonlinear) long-haul optical link.
22292393	[Application of regular expression in extracting key information from Chinese medicine literatures about re-evaluation of post-marketing surveillance].
Zhongguo Zhong Yao Za Zhi  2011Oct
Computerizing extracting information from Chinese medicine literature seems more convenient than hand searching, which could simplify searching process and improve the accuracy. However, many computerized auto-extracting methods are increasingly used, regular expression is so special that could be efficient for extracting useful information in research. This article focused on regular expression applying in extracting information from Chinese medicine literature. Two practical examples were reported in this article about regular expression to extract "case number (non-terminology)" and "efficacy rate (subgroups for related information identification)", which explored how to extract information in Chinese medicine literature by means of some special research method.
22295689	[Development of a lung cancer image database and visualization toolkit].
Sheng Wu Yi Xue Gong Cheng Xue Za Zhi  2011Dec
Lung cancer is the most common tumor and one of the malignant tumors with the lowest livability after diagnosis, as is known so far. Large-scale image database is the foundation of developing computer-aided diagnosis methods, education and training in lung cancer diagnosis to improve medical diagnostic efficiency and to reduce the doctors' burden. In this study, aiming at improving the low data storage efficiency and solving the lacking of tool for data visualization and data retrieval existing in the use of traditional Lung Image Database Consortium (LIDC) from the lung cancer database, we developed a new lung cancer image database platform including an improved data model, a data integration tool, an image and annotation visualization tool and a data retrieving component. Firstly, the data format in LIDC was analyzed and an improved information model was provided to manage and manipulate large amount data stored in it. Next, some tools such as data integration component, DICOM, image and annotation visualization tool, and data query were designed and implemented. The study demonstrated that the lung cancer image database platform had the capacity of data collection, visualization, and query, and could promote diagnose lung cancer research.
22165947	Mining the Gene Wiki for functional genomic knowledge.
BMC Genomics 20111213 2011
Ontology-based gene annotations are important tools for organizing and analyzing genome-scale biological data. Collecting these annotations is a valuable but costly endeavor. The Gene Wiki makes use of Wikipedia as a low-cost, mass-collaborative platform for assembling text-based gene annotations. The Gene Wiki is comprised of more than 10,000 review articles, each describing one human gene. The goal of this study is to define and assess a computational strategy for translating the text of Gene Wiki articles into ontology-based gene annotations. We specifically explore the generation of structured annotations using the Gene Ontology and the Human Disease Ontology. Our system produced 2,983 candidate gene annotations using the Disease Ontology and 11,022 candidate annotations using the Gene Ontology from the text of the Gene Wiki. Based on manual evaluations and comparisons to reference annotation sets, we estimate a precision of 90-93% for the Disease Ontology annotations and 48-64% for the Gene Ontology annotations. We further demonstrate that this data set can systematically improve the results from gene set enrichment analyses. The Gene Wiki is a rapidly growing corpus of text focused on human gene function. Here, we demonstrate that the Gene Wiki can be a powerful resource for generating ontology-based gene annotations. These annotations can be used immediately to improve workflows for building curated gene annotation databases and knowledge-based statistical analyses.
22163811	Design and development of a run-time monitor for multi-core architectures in cloud computing.
Sensors (Basel) 20110325 2011
Cloud computing is a new information technology trend that moves computing and data away from desktops and portable PCs into large data centers. The basic principle of cloud computing is to deliver applications as services over the Internet as well as infrastructure. A cloud is a type of parallel and distributed system consisting of a collection of inter-connected and virtualized computers that are dynamically provisioned and presented as one or more unified computing resources. The large-scale distributed applications on a cloud require adaptive service-based software, which has the capability of monitoring system status changes, analyzing the monitored information, and adapting its service configuration while considering tradeoffs among multiple QoS features simultaneously. In this paper, we design and develop a Run-Time Monitor (RTM) which is a system software to monitor the application behavior at run-time, analyze the collected information, and optimize cloud computing resources for multi-core architectures. RTM monitors application software through library instrumentation as well as underlying hardware through a performance counter optimizing its computing configuration based on the analyzed data.
22163854	A secure cluster-based multipath routing protocol for WMSNs.
Sensors (Basel) 20110415 2011
The new characteristics of Wireless Multimedia Sensor Network (WMSN) and its design issues brought by handling different traffic classes of multimedia content (video streams, audio, and still images) as well as scalar data over the network, make the proposed routing protocols for typical WSNs not directly applicable for WMSNs. Handling real-time multimedia data requires both energy efficiency and QoS assurance in order to ensure efficient utility of different capabilities of sensor resources and correct delivery of collected information. In this paper, we propose a Secure Cluster-based Multipath Routing protocol for WMSNs, SCMR, to satisfy the requirements of delivering different data types and support high data rate multimedia traffic. SCMR exploits the hierarchical structure of powerful cluster heads and the optimized multiple paths to support timeliness and reliable high data rate multimedia communication with minimum energy dissipation. Also, we present a light-weight distributed security mechanism of key management in order to secure the communication between sensor nodes and protect the network against different types of attacks. Performance evaluation from simulation results demonstrates a significant performance improvement comparing with existing protocols (which do not even provide any kind of security feature) in terms of average end-to-end delay, network throughput, packet delivery ratio, and energy consumption.
22115278	Unintended consequences of existential quantifications in biomedical ontologies.
BMC Bioinformatics 20111124 2011
The Open Biomedical Ontologies (OBO) Foundry is a collection of freely available ontologically structured controlled vocabularies in the biomedical domain. Most of them are disseminated via both the OBO Flatfile Format and the semantic web format Web Ontology Language (OWL), which draws upon formal logic. Based on the interpretations underlying OWL description logics (OWL-DL) semantics, we scrutinize the OWL-DL releases of OBO ontologies to assess whether their logical axioms correspond to the meaning intended by their authors. We analyzed ontologies and ontology cross products available via the OBO Foundry site http://www.obofoundry.org for existential restrictions (someValuesFrom), from which we examined a random sample of 2,836 clauses.According to a rating done by four experts, 23% of all existential restrictions in OBO Foundry candidate ontologies are suspicious (Cohens' κ = 0.78). We found a smaller proportion of existential restrictions in OBO Foundry cross products are suspicious, but in this case an accurate quantitative judgment is not possible due to a low inter-rater agreement (κ = 0.07). We identified several typical modeling problems, for which satisfactory ontology design patterns based on OWL-DL were proposed. We further describe several usability issues with OBO ontologies, including the lack of ontological commitment for several common terms, and the proliferation of domain-specific relations. The current OWL releases of OBO Foundry (and Foundry candidate) ontologies contain numerous assertions which do not properly describe the underlying biological reality, or are ambiguous and difficult to interpret. The solution is a better anchoring in upper ontologies and a restriction to relatively few, well defined relation types with given domain and range constraints.
22346581	On-demand information retrieval in sensor networks with localised query and energy-balanced data collection.
Sensors (Basel) 20101230 2011
On-demand information retrieval enables users to query and collect up-to-date sensing information from sensor nodes. Since high energy efficiency is required in a sensor network, it is desirable to disseminate query messages with small traffic overhead and to collect sensing data with low energy consumption. However, on-demand query messages are generally forwarded to sensor nodes in network-wide broadcasts, which create large traffic overhead. In addition, since on-demand information retrieval may introduce intermittent and spatial data collections, the construction and maintenance of conventional aggregation structures such as clusters and chains will be at high cost. In this paper, we propose an on-demand information retrieval approach that exploits the name resolution of data queries according to the attribute and location of each sensor node. The proposed approach localises each query dissemination and enable localised data collection with maximised aggregation. To illustrate the effectiveness of the proposed approach, an analytical model that describes the criteria of sink proxy selection is provided. The evaluation results reveal that the proposed scheme significantly reduces energy consumption and improves the balance of energy consumption among sensor nodes by alleviating heavy traffic near the sink.
22149888	hERGCentral: a large database to store, retrieve, and analyze compound-human Ether-à-go-go related gene channel interactions to facilitate cardiotoxicity assessment in drug development.
Assay Drug Dev Technol  2011Dec
The unintended and often promiscous inhibition of the cardiac human Ether-à-go-go related gene (hERG) potassium channel is a common cause for either delay or removal of therapeutic compounds from development and withdrawal of marketed drugs. The clinical manifestion is prolongation of the duration between QRS complex and T-wave measured by surface electrocardiogram (ECG)-hence Long QT Syndrome. There are several useful online resources documenting hERG inhibition by known drugs and bioactives. However, their utilities remain somewhat limited because they are biased toward well-studied compounds and their number of data points tends to be much smaller than many commercial compound libraries. The hERGCentral ( www.hergcentral.org ) is mainly based on experimental data obtained from a primary screen by electrophysiology against more than 300,000 structurally diverse compounds. The system is aimed to display and combine three resources: primary electrophysiological data, literature, as well as online reports and chemical library collections. Currently, hERGCentral has annotated datasets of more than 300,000 compounds including structures and chemophysiological properties of compounds, raw traces, and biophysical properties. The system enables a variety of query formats, including searches for hERG effects according to either chemical structure or properties, and alternatively according to the specific biophysical properties of current changes caused by a compound. Therefore, the hERGCentral, as a unique and evolving resource, will facilitate investigation of chemically induced hERG inhibition and therefore drug development.
21631416	Master data management: getting your house in order.
Comb. Chem. High Throughput Screen.  2011Nov
The availability of high-throughput techniques combined with more exploratory and confirmatory studies in small-molecule science (e.g., probe- and drug-discovery) creates a significant need for structured approaches to data management. The probe- and drug-discovery scientific processes start and end with lower-throughput experiments, connected often by high-throughput cheminformatics, screening, and small-molecule profiling experiments. A rigorous and disciplined approach to data management ensures that data can be used to ask complex questions of assay results, and allows many questions to be answered computationally, without the need for significant manual effort. A structured approach to recording scientific experimental design and observations involves using a consistently maintained set of 'master data' or 'metadata'. Master data include sets of tightly controlled terminology used to describe an experiment, including both materials and methods. Master data can be used at the level of an individual laboratory or with a scope as extensive as a whole community of scientists. Consistent use of master data increases experimental power by allowing data analysis to connect all parts of the discovery life cycle, across experiments performed by different researchers and from different laboratories, thus decreasing the opportunity cost for making novel connections between results. Despite the promise of this increased experimental power, challenges remain in implementation and consistent use of master data management (MDM) techniques in the laboratory. In this paper, we discuss how specific MDM techniques can enhance the quality and utility of scientific data at a project, laboratory, and institutional level. We present a model for storage and exploitation of master data, practical applications of these techniques in the research context of small-molecule science, and specific benefits of MDM to small-molecule screening aimed at probe- and drug-discovery.
22159987	Cloudy confidentiality: clinical and legal implications of cloud computing in health care.
J. Am. Acad. Psychiatry Law  2011
The Internet has grown into a world of its own, and its ethereal space now offers capabilities that could aid physicians in their duties in numerous ways. In recent years software functions have moved from the individual's local hardware to a central server that operates from a remote location. This centralization is called cloud computing. Privacy laws that speak to the protection of patient confidentiality are complex and often difficult to understand in the context of an ever-growing cloud-based technology. This article is a review of the legal background of protected health records, as well as cloud technology and physician applications. An attempt is made to integrate both concepts and examine Health Insurance Portability and Accountability Act (HIPAA) compliance for each of the examples discussed. The legal regulations that may inform care and standards of practice are reviewed, and the difficulties that arise in assessment and monitoring of the current situation are analyzed. For forensic psychiatrists who may be asked to provide expert opinions regarding malpractice situations pertaining to confidentiality standards, it is important to become acquainted with the new digital language from which these questions may arise.
21537849	Federated querying architecture with clinical &amp; translational health IT application.
J Med Syst 20110503 2011Oct
We present a software architecture that federates data from multiple heterogeneous health informatics data sources owned by multiple organizations. The architecture builds upon state-of-the-art open-source Java and XML frameworks in innovative ways. It consists of (a) federated query engine, which manages federated queries and result set aggregation via a patient identification service; and (b) data source facades, which translate the physical data models into a common model on-the-fly and handle large result set streaming. System modules are connected via reusable Apache Camel integration routes and deployed to an OSGi enterprise service bus. We present an application of our architecture that allows users to construct queries via the i2b2 web front-end, and federates patient data from the University of Utah Enterprise Data Warehouse and the Utah Population database. Our system can be easily adopted, extended and integrated with existing SOA Healthcare and HL7 frameworks such as i2b2 and caGrid.
20703758	Experiences sharing of implementing Template-based Electronic Medical Record System (TEMRS) in a Hong Kong medical organization.
J Med Syst 20100202 2011Dec
This paper aims to investigate the efficacy and feasibility of Template-based Electronic Medical Record System (TEMRS) and factors for its successful implementation. A TEMRS was designed and implemented in one core clinic of a Hong Kong professional multi-disciplinary medical services provider with four core clinics located in different parts of Hong Kong. Eight doctors participated in the study. Surveys and interviews were conducted to acquire the users' feedback and satisfaction level. The design, development, and the factors related to the success of the implementation of TEMRS were analyzed. In the study period, 3,032 cases were collected. The most encountered diagnosis were upper respiratory tract infection (50.59%), gastroenteritis (10.19%), dermatitis (5.87%), dyspepsia (5.28%) and rhinitis (4.82%). The system gained an overall satisfaction by the users and the most satisfied areas were rapid retrieving the necessary information of patient (75%) and fasten the diagnostic selection (75%). TEMRS is an enabling system which can reduce the user resistance in new technology with its flexibility. The consideration of cost, security, human, technical, data migration and standardization issues are essential in the implementation of the TEMRS and further research should be conducted to expand the TEMRS's implementation in health care system.
20703760	Multiple 3D medical data watermarking for healthcare data management.
J Med Syst 20100303 2011Dec
The rapid development of healthcare information management for 3D digital medical libraries, 3D PACS, and 3D medical diagnosis has addressed the security issues pertaining to medical IT technology. This paper presents multiple watermarking schemes for a healthcare information management system for 3D medical image data for the protection, authentication, indexing, and hiding of diagnosis information. The proposed scheme, which is based on POCS watermarking, embeds a robust watermark for a doctor's digital signature and an information retrieval indexing key to the distribution of vertex curvedness; the scheme also embeds a fragile watermark for diagnosis information and an authentication reference message to the vertex distance difference. The multiple embedding process creates three convex sets for robustness, fragileness, and invisibility and projects the 3D medical image data onto these three convex sets alternately and iteratively. Experimental results confirmed that the proposed scheme has the robustness and fragileness to handle various 3D geometric and mesh modifiers simultaneously.
20703772	Choosing the most efficient database for a web-based system to store and exchange ophthalmologic health records.
J Med Syst 20100112 2011Dec
Response times are a critically important parameter when implementing any telematics application. Hence, it is important to evaluate those times to check the performance of the system. Different database will get different response times. This paper presents a response time comparative analysis of the Web system of Electronic Health Record (EHRs), TeleOftalWeb, with the four databases used: Oracle 10 g, dbXML 2.0, Xindice 1.2, and eXist 1.1.1. Final goal of the comparison is choosing the database providing lower response times in TeleOftalWeb. Results obtained using the four databases proposed give the native XML database eXist an edge which, added to other features such as being a free software and easy to set up, makes us opting for it. TeleOftalWeb is being used by 20 specialists from the Institute of Applied Ophthalmobiology (Instituto de Oftalmobiología Aplicada, IOBA) of the University of Valladolid, Spain. At this time, there are more than 1000 EHRs and over 2000 fundus photographs of diabetic patients stored in the system.
22149828	Predicting the fidelity of JPEG2000 compressed CT images using DICOM header information.
Med Phys  2011Dec
To propose multiple logistic regression (MLR) and artificial neural network (ANN) models constructed using digital imaging and communications in medicine (DICOM) header information in predicting the fidelity of Joint Photographic Experts Group (JPEG) 2000 compressed abdomen computed tomography (CT) images. Our institutional review board approved this study and waived informed patient consent. Using a JPEG2000 algorithm, 360 abdomen CT images were compressed reversibly (n = 48, as negative control) or irreversibly (n = 312) to one of different compression ratios (CRs) ranging from 4:1 to 10:1. Five radiologists independently determined whether the original and compressed images were distinguishable or indistinguishable. The 312 irreversibly compressed images were divided randomly into training (n = 156) and testing (n = 156) sets. The MLR and ANN models were constructed regarding the DICOM header information as independent variables and the pooled radiologists' responses as dependent variable. As independent variables, we selected the CR (DICOM tag number: 0028, 2112), effective tube current-time product (0018, 9332), section thickness (0018, 0050), and field of view (0018, 0090) among the DICOM tags. Using the training set, an optimal subset of independent variables was determined by backward stepwise selection in a four-fold cross-validation scheme. The MLR and ANN models were constructed with the determined independent variables using the training set. The models were then evaluated on the testing set by using receiver-operating-characteristic (ROC) analysis regarding the radiologists' pooled responses as the reference standard and by measuring Spearman rank correlation between the model prediction and the number of radiologists who rated the two images as distinguishable. The CR and section thickness were determined as the optimal independent variables. The areas under the ROC curve for the MLR and ANN predictions were 0.91 (95% CI; 0.86, 0.95) and 0.92 (0.87, 0.96), respectively. The correlation coefficients of the MLR and ANN predictions with the number of radiologists who responded as distinguishable were 0.76 (0.69, 0.82, p &lt; 0.001) and 0.78 (0.71, 0.83, p &lt; 0.001), respectively. The MLR and ANN models constructed using the DICOM header information offer promise in predicting the fidelity of JPEG2000 compressed abdomen CT images.
22149850	Technical Note: modification of the standard gain correction algorithm to compensate for the number of used reference flat frames in detector performance studies.
Med Phys  2011Dec
The x-ray performance evaluation of digital x-ray detectors is based on the calculation of the modulation transfer function (MTF), the noise power spectrum (NPS), and the resultant detective quantum efficiency (DQE). The flat images used for the extraction of the NPS should not contain any fixed pattern noise (FPN) to avoid contamination from nonstochastic processes. The "gold standard" method used for the reduction of the FPN (i.e., the different gain between pixels) in linear x-ray detectors is based on normalization with an average reference flat-field. However, the noise in the corrected image depends on the number of flat frames used for the average flat image. The aim of this study is to modify the standard gain correction algorithm to make it independent on the used reference flat frames. Many publications suggest the use of 10-16 reference flat frames, while other studies use higher numbers (e.g., 48 frames) to reduce the propagated noise from the average flat image. This study quantifies experimentally the effect of the number of used reference flat frames on the NPS and DQE values and appropriately modifies the gain correction algorithm to compensate for this effect. It is shown that using the suggested gain correction algorithm a minimum number of reference flat frames (i.e., down to one frame) can be used to eliminate the FPN from the raw flat image. This saves computer memory and time during the x-ray performance evaluation. The authors show that the method presented in the study (a) leads to the maximum DQE value that one would have by using the conventional method and very large number of frames and (b) has been compared to an independent gain correction method based on the subtraction of flat-field images, leading to identical DQE values. They believe this provides robust validation of the proposed method.
22057060	Incremental learning from stream data.
IEEE Trans Neural Netw 20111031 2011Dec
Recent years have witnessed an incredibly increasing interest in the topic of incremental learning. Unlike conventional machine learning situations, data flow targeted by incremental learning becomes available continuously over time. Accordingly, it is desirable to be able to abandon the traditional assumption of the availability of representative training data during the training period to develop decision boundaries. Under scenarios of continuous data flow, the challenge is how to transform the vast amount of stream raw data into information and knowledge representation, and accumulate experience over time to support future decision-making process. In this paper, we propose a general adaptive incremental learning framework named ADAIN that is capable of learning from continuous raw data, accumulating experience over time, and using such knowledge to improve future learning and prediction performance. Detailed system level architecture and design strategies are presented in this paper. Simulation results over several real-world data sets are used to validate the effectiveness of this method.
22168546	Looking back in time: conducting a cohort study of the long-term effects of treatment of adolescent tall girls with synthetic hormones.
BMC Public Health 20111125 2011
Public health research is an endeavour that often involves multiple relationships, far-reaching collaborations, divergent expectations and various outcomes. Using the Tall Girls Study as a case study, this paper will present and discuss a number of methodological, ethical and legal challenges that have implications for other public health research. The Tall Girls Study was the first study to examine the long-term health and psychosocial effects of oestrogen treatment for tall stature. In undertaking this study the research team overcame many hurdles: in maintaining collaboration with treating clinicians and with the women they had treated as girls - groups with opposing points of view and different expectations; using private practice medical records to trace women who had been patients up to forty years earlier; and exploring potential legal issues arising from the collection of data related to treatment. While faced with complex challenges, the Tall Girls Study demonstrated that forward planning, ongoing dialogue between all stakeholders, transparency of processes, and the strict adherence to group-developed protocols were keys to maintaining rigour while undertaking pragmatic research. Public health research often occurs within political and social contexts that need to be considered in the planning and conduct of studies. The quality and acceptability of research findings is enhanced when stakeholders are engaged in all aspects of the research process.
21856687	A Dimensional Bus model for integrating clinical and research data.
J Am Med Inform Assoc 20110819 2011Dec
Many clinical research data integration platforms rely on the Entity-Attribute-Value model because of its flexibility, even though it presents problems in query formulation and execution time. The authors sought more balance in these traits. Borrowing concepts from Entity-Attribute-Value and from enterprise data warehousing, the authors designed an alternative called the Dimensional Bus model and used it to integrate electronic medical record, sponsored study, and biorepository data. Each type of observational collection has its own table, and the structure of these tables varies to suit the source data. The observational tables are linked to the Bus, which holds provenance information and links to various classificatory dimensions that amplify the meaning of the data or facilitate its query and exposure management. The authors implemented a Bus-based clinical research data repository with a query system that flexibly manages data access and confidentiality, facilitates catalog search, and readily formulates and compiles complex queries. The design provides a workable way to manage and query mixed schemas in a data warehouse.
21984588	Strategies for maintaining patient privacy in i2b2.
J Am Med Inform Assoc 20111007 2011Dec
The re-use of patient data from electronic healthcare record systems can provide tremendous benefits for clinical research, but measures to protect patient privacy while utilizing these records have many challenges. Some of these challenges arise from a misperception that the problem should be solved technically when actually the problem needs a holistic solution. The authors' experience with informatics for integrating biology and the bedside (i2b2) use cases indicates that the privacy of the patient should be considered on three fronts: technical de-identification of the data, trust in the researcher and the research, and the security of the underlying technical platforms. The security structure of i2b2 is implemented based on consideration of all three fronts. It has been supported with several use cases across the USA, resulting in five privacy categories of users that serve to protect the data while supporting the use cases. The i2b2 architecture is designed to provide consistency and faithfully implement these user privacy categories. These privacy categories help reflect the policy of both the Health Insurance Portability and Accountability Act and the provisions of the National Research Act of 1974, as embodied by current institutional review boards. By implementing a holistic approach to patient privacy solutions, i2b2 is able to help close the gap between principle and practice.
21984589	The TOKEn project: knowledge synthesis for in silico science.
J Am Med Inform Assoc 20111007 2011Dec
The conduct of investigational studies that involve large-scale data sets presents significant challenges related to the discovery and testing of novel hypotheses capable of supporting in silico discovery science. The use of what are known as Conceptual Knowledge Discovery in Databases (CKDD) methods provides a potential means of scaling hypothesis discovery and testing approaches for large data sets. Such methods enable the high-throughput generation and evaluation of knowledge-anchored relationships between complexes of variables found in targeted data sets. The authors have conducted a multipart model formulation and validation process, focusing on the development of a methodological and technical approach to using CKDD to support hypothesis discovery for in silico science. The model the authors have developed is known as the Translational Ontology-anchored Knowledge Discovery Engine (TOKEn). This model utilizes a specific CKDD approach known as Constructive Induction to identify and prioritize potential hypotheses related to the meaningful semantic relationships between variables found in large-scale and heterogeneous biomedical data sets. The authors have verified and validated TOKEn in the context of a translational research data repository maintained by the NCI-funded Chronic Lymphocytic Leukemia Research Consortium. Such studies have shown that TOKEn is: (1) computationally tractable; and (2) able to generate valid and potentially useful hypotheses concerning relationships between phenotypic and biomolecular variables in that data collection. The TOKEn model represents a potentially useful and systematic approach to knowledge synthesis for in silico discovery science in the context of large-scale and multidimensional research data sets.
22037890	Direct2Experts: a pilot national network to demonstrate interoperability among research-networking platforms.
J Am Med Inform Assoc 20111028 2011Dec
Research-networking tools use data-mining and social networking to enable expertise discovery, matchmaking and collaboration, which are important facets of team science and translational research. Several commercial and academic platforms have been built, and many institutions have deployed these products to help their investigators find local collaborators. Recent studies, though, have shown the growing importance of multiuniversity teams in science. Unfortunately, the lack of a standard data-exchange model and resistance of universities to share information about their faculty have presented barriers to forming an institutionally supported national network. This case report describes an initiative, which, in only 6 months, achieved interoperability among seven major research-networking products at 28 universities by taking an approach that focused on addressing institutional concerns and encouraging their participation. With this necessary groundwork in place, the second phase of this effort can begin, which will expand the network's functionality and focus on the end users.
21420508	Comparison of automated and human assignment of MeSH terms on publicly-available molecular datasets.
J Biomed Inform 20110321 2011Dec
Publicly available molecular datasets can be used for independent verification or investigative repurposing, but depends on the presence, consistency and quality of descriptive annotations. Annotation and indexing of molecular datasets using well-defined controlled vocabularies or ontologies enables accurate and systematic data discovery, yet the majority of molecular datasets available through public data repositories lack such annotations. A number of automated annotation methods have been developed; however few systematic evaluations of the quality of annotations supplied by application of these methods have been performed using annotations from standing public data repositories. Here, we compared manually-assigned Medical Subject Heading (MeSH) annotations associated with experiments by data submitters in the PRoteomics IDEntification (PRIDE) proteomics data repository to automated MeSH annotations derived through the National Center for Biomedical Ontology Annotator and National Library of Medicine MetaMap programs. These programs were applied to free-text annotations for experiments in PRIDE. As many submitted datasets were referenced in publications, we used the manually curated MeSH annotations of those linked publications in MEDLINE as "gold standard". Annotator and MetaMap exhibited recall performance 3-fold greater than that of the manual annotations. We connected PRIDE experiments in a network topology according to shared MeSH annotations and found 373 distinct clusters, many of which were found to be biologically coherent by network analysis. The results of this study suggest that both Annotator and MetaMap are capable of annotating public molecular datasets with a quality comparable, and often exceeding, that of the actual data submitters, highlighting a continuous need to improve and apply automated methods to molecular datasets in public data repositories to maximize their value and utility.
21821150	Research-IQ: development and evaluation of an ontology-anchored integrative query tool.
J Biomed Inform 20110729 2011Dec
Investigators in the translational research and systems medicine domains require highly usable, efficient and integrative tools and methods that allow for the navigation of and reasoning over emerging large-scale data sets. Such resources must cover a spectrum of granularity from bio-molecules to population phenotypes. Given such information needs, we report upon the initial design and evaluation of an ontology-anchored integrative query tool, Research-IQ, which employs a combination of conceptual knowledge engineering and information retrieval techniques to enable the intuitive and rapid construction of queries, in terms of semi-structured textual propositions, that can subsequently be applied to integrative data sets. Our initial results, based upon both quantitative and qualitative evaluations of the efficacy and usability of Research-IQ, demonstrate its potential to increase clinical and translational research throughput.
21888989	Integrating clinical research with the Healthcare Enterprise: from the RE-USE project to the EHR4CR platform.
J Biomed Inform 20110825 2011Dec
There are different approaches for repurposing clinical data collected in the Electronic Healthcare Record (EHR) for use in clinical research. Semantic integration of "siloed" applications across domain boundaries is the raison d'être of the standards-based profiles developed by the Integrating the Healthcare Enterprise (IHE) initiative - an initiative by healthcare professionals and industry promoting the coordinated use of established standards such as DICOM and HL7 to address specific clinical needs in support of optimal patient care. In particular, the combination of two IHE profiles - the integration profile "Retrieve Form for Data Capture" (RFD), and the IHE content profile "Clinical Research Document" (CRD) - offers a straightforward approach to repurposing EHR data by enabling the pre-population of the case report forms (eCRF) used for clinical research data capture by Clinical Data Management Systems (CDMS) with previously collected EHR data. Implement an alternative solution of the RFD-CRD integration profile centered around two approaches: (i) Use of the EHR as the single-source data-entry and persistence point in order to ensure that all the clinical data for a given patient could be found in a single source irrespective of the data collection context, i.e. patient care or clinical research; and (ii) Maximize the automatic pre-population process through the use of a semantic interoperability services that identify duplicate or semantically-equivalent eCRF/EHR data elements as they were collected in the EHR context. The RE-USE architecture and associated profiles are focused on defining a set of scalable, standards-based, IHE-compliant profiles that can enable single-source data collection/entry and cross-system data reuse through semantic integration. Specifically, data reuse is realized through the semantic mapping of data collection fields in electronic Case Report Forms (eCRFs) to data elements previously defined as part of patient care-centric templates in the EHR context. The approach was evaluated in the context of a multi-center clinical trial conducted in a large, multi-disciplinary hospital with an installed EHR. Data elements of seven eCRFs used in a multi-center clinical trial were mapped to data elements of patient care-centric templates in use in the EHR at the George Pompidou hospital. 13.4% of the data elements of the eCRFs were found to be represented in EHR templates and were therefore candidate for pre-population. During the execution phase of the clinical study, the semantic mapping architecture enabled data persisted in the EHR context as part of clinical care to be used to pre-populate eCRFS for use without secondary data entry. To ensure that the pre-populated data is viable for use in the clinical research context, all pre-populated eCRF data needs to be first approved by a trial investigator prior to being persisted in a research data store within a CDMS. Single-source data entry in the clinical care context for use in the clinical research context - a process enabled through the use of the EHR as single point of data entry, can - if demonstrated to be a viable strategy - not only significantly reduce data collection efforts while simultaneously increasing data collection accuracy secondary to elimination of transcription or double-entry errors between the two contexts but also ensure that all the clinical data for a given patient, irrespective of the data collection context, are available in the EHR for decision support and treatment planning. The RE-USE approach used mapping algorithms to identify semantic coherence between clinical care and clinical research data elements and pre-populate eCRFs. The RE-USE project utilized SNOMED International v.3.5 as its "pivot reference terminology" to support EHR-to-eCRF mapping, a decision that likely enhanced the "recall" of the mapping algorithms. The RE-USE results demonstrate the difficult challenges involved in semantic integration between the clinical care and clinical research contexts.
22189176	Finnish physicians' experiences with computer-supported patient information exchange and communication in clinical work.
Int J Electron Healthc  2011
Several researchers share the concern of healthcare information systems failing to support communication and collaboration in clinical practices. The objective of this paper is to investigate the current state of computer-supported patient information exchange and associated communication between clinicians. We report findings from a national survey on Finnish physicians? experiences with their currently used clinical information systems with regard to patient information documentation, retrieval, management and exchange-related tasks. The questionnaire study with 3929 physicians indicated the main concern being cross-organisational patient information delivery. In addition, physicians argued computer usage increasingly steals time and attention from caring activities and even disturbs physician?nurse collaboration. Problems in information management were particularly emphasised among those physicians working in hospitals and wards. The survey findings indicated that collaborative applications and mobile or wireless solutions have not been widely adapted in Finnish healthcare and suggested an urgent need for adopting appropriate information and communication technology applications to support information exchange and communication between physicians, and physicians and nurses.
22195070	Naïve Electronic Health Record phenotype identification for Rheumatoid arthritis.
AMIA Annu Symp Proc 20111022 2011
Electronic Health Records (EHRs) provide a real-world patient cohort for clinical and genomic research. Phenotype identification using informatics algorithms has been shown to replicate known genetic associations found in clinical trials and observational cohorts. However, development of accurate phenotype identification methods can be challenging, requiring significant time and effort. We applied Support Vector Machines (SVMs) to both naïve (i.e., non-curated) and expert-defined collections of EHR features to identify Rheumatoid Arthritis cases using billing codes, medication exposures, and natural language processing-derived concepts. SVMs trained on naïve and expert-defined data outperformed an existing deterministic algorithm; the best performing naïve system had precision of 0.94 and recall of 0.87, compared to precision of 0.75 and recall of 0.51 for the deterministic algorithm. We show that with an expert defined feature set as few as 50-100 training samples are required. This study demonstrates that SVMs operating on non-curated sets of attributes can accurately identify cases from an EHR.
22195074	A multi-site content analysis of social history information in clinical notes.
AMIA Annu Symp Proc 20111022 2011
Within Electronic Health Records (EHRs), the social history section contains information relevant to social, behavioral, and environmental determinants of health. While social history is playing an increasingly important role in patient care, biomedical research, and public health, little analysis has been done to describe content in the EHR or the adequacy of existing standards for representing this information. In this study, social history sections from 260 clinical notes containing 989 sentences and 1,439 statements were analyzed from three sources. In total, 35 statement types were identified along with categories of information within statements for each type. For the 8 most common types, HL7 CDA and openEHR were found to provide different representations capable of capturing the breadth and granularity of information to some extent. The results of this study provide valuable insights for guiding efforts in the enhanced collection, standardization, and use of social history information in the EHR.
22195082	Drug repositioning using disease associated biological processes and network analysis of drug targets.
AMIA Annu Symp Proc 20111022 2011
The analysis of disease using protein-protein interaction networks and network pharmacology has enabled better understanding of disease etiology and drug action. New insights into disease etiology and a better understanding of biological subsystems have opened up the possibility of finding new uses for existing drugs besides their original medical indication. We present an approach which makes use of the biological processes associated with diseases along with their known drugs and drug targets to predict Biological Process-Drug relationships. Network analysis is used to further refine these associations to eventually predict new Disease-Drug relationships. The approach is validated by the observation that, out of 2078 predicted disease-drug relationships, 401 (18.1%) have been used in a clinical trial.
22195083	A generative model based approach to retrieving ischemic stroke images.
AMIA Annu Symp Proc 20111022 2011
This paper proposes a generative model approach to automatically annotate medical images to improve the efficiency and effectiveness of image retrieval systems for teaching, research, and diagnosis. The generative model captures the probabilistic relationships among relevant classification tags, tentative lesion patterns, and selected input features. Operating on the imperfect segmentation results of input images, the probabilistic framework can effectively handle the inherent uncertainties in the images and insufficient information in the training data. Preliminary assessment in the ischemic stroke subtype classification shows that the proposed system is capable of generating the relevant tags for ischemic stroke brain images. The main benefit of this approach is its scalability; the method can be applied in large image databases as it requires only minimal manual labeling of the training data and does not demand high-precision segmentation of the images.
22195093	Practical challenges in the secondary use of real-world data: the notifiable condition detector.
AMIA Annu Symp Proc 20111022 2011
The interoperability specifications for electronic laboratory reporting specify the use of HL7, LOINC, SNOMED CT and UCUM. We explored the degree to which health care transactions comply with these standards by evaluating laboratory data captured in a health information exchange to support automated detection of public health notifiable diseases. We studied the NCD's ability to detect and report Lead, Influenza and MRSA. We found that due to incomplete LOINC mapping, alternate approaches such as keyword searches within local test names and codes could identify additional potentially reportable messages. We also found that non-adherence to HL7 messaging standards and inconsistently recorded laboratory results require the use of complex systems with complementary NLP techniques to accurately report notifiable conditions. We conclude that the incomplete adoption of and adherence to specified standards poses challenges to deploying processes that utilize real-world data for secondary purposes.
22195101	NeuroLOG: sharing neuroimaging data using an ontology-based federated approach.
AMIA Annu Symp Proc 20111022 2011
This paper describes the design of the NeuroLOG middleware data management layer, which provides a platform to share heterogeneous and distributed neuroimaging data using a federated approach. The semantics of shared information is captured through a multi-layer application ontology and a derived Federated Schema used to align the heterogeneous database schemata from different legacy repositories. The system also provides a facility to translate the relational data into a semantic representation that can be queried using a semantic search engine thus enabling the exploitation of knowledge embedded in the ontology. This work shows the relevance of the distributed approach for neurosciences data management. Although more complex than a centralized approach, it is also more realistic when considering the federation of large data sets, and open strong perspectives to implement multi-centric neurosciences studies.
22195112	Evaluation of semantic-based information retrieval methods in the autism phenotype domain.
AMIA Annu Symp Proc 20111022 2011
Biomedical ontologies are increasingly being used to improve information retrieval methods. In this paper, we present a novel information retrieval approach that exploits knowledge specified by the Semantic Web ontology and rule languages OWL and SWRL. We evaluate our approach using an autism ontology that has 156 SWRL rules defining 145 autism phenotypes. Our approach uses a vector space model to correlate how well these phenotypes relate to the publications used to define them. We compare a vector space phenotype representation using class hierarchies with one that extends this method to incorporate additional semantics encoded in SWRL rules. From a PubMed-extracted corpus of 75 articles, we show that average rank of a related paper using the class hierarchy method is 4.6 whereas the average rank using the extended rule-based method is 3.3. Our results indicate that incorporating rule-based definitions in information retrieval methods can improve search for relevant publications.
22195123	Use of topic modeling for recommending relevant education material to diabetic patients.
AMIA Annu Symp Proc 20111022 2011
The need for and challenges of educating and informing patients are well known and these are even greater for patients with low levels of literacy. Furthermore, as the population ages and with the increase in prevalence of chronic diseases where patient self-management is essential to holding disease in abeyance, patient education becomes increasingly important. With the advent of electronic medical records, there is an opportunity for automated tools to assist in addressing these challenges. In this paper we report on one approach to recommending relevant educational articles to patients. We attempt to infer the patient's information needs from his/her electronic medical records and use topic modeling to identify and match topics. A manual evaluation of the articles recommended by the proposed method showed that these articles are significantly more relevant (p &lt; 0.01) to the patient's disease state than articles selected at random from within the same disease domain.
22195124	The Knowledge Program: an innovative, comprehensive electronic data capture system and warehouse.
AMIA Annu Symp Proc 20111022 2011
Data contained in the electronic health record (EHR) present a tremendous opportunity to improve quality-of-care and enhance research capabilities. However, the EHR is not structured to provide data for such purposes: most clinical information is entered as free text and content varies substantially between providers. Discrete information on patients' functional status is typically not collected. Data extraction tools are often unavailable. We have developed the Knowledge Program (KP), a comprehensive initiative to improve the collection of discrete clinical information into the EHR and the retrievability of data for use in research, quality, and patient care. A distinct feature of the KP is the systematic collection of patient-reported outcomes, which is captured discretely, allowing more refined analyses of care outcomes. The KP capitalizes on features of the Epic EHR and utilizes an external IT infrastructure distinct from Epic for enhanced functionality. Here, we describe the development and implementation of the KP.
22195132	SEACOIN--an investigative tool for biomedical informatics researchers.
AMIA Annu Symp Proc 20111022 2011
Peer-reviewed scientific literature is a prime source for accessing knowledge in the biomedical field. Its rapid growth and diverse domain coverage require systematic efforts in developing interactive tools for efficiently searching and summarizing current advances for acquiring knowledge and referencing, and for furthering scientific discovery. Although information retrieval systems exist, the conventional tools and systems remain difficult for biomedical investigators to use. There remain gaps even in the state-of-the-art systems as little attention has been devoted to understanding the needs of biomedical researchers. Our work attempts to bridge the gap between the needs of biomedical users and systems design efforts. We first study the needs of users and then design a simple visual analytic application tool, SEACOIN. A key motivation stems from biomedical researchers' request for a "simple interface" that is suitable for novice users in information technology. The system minimizes information overload, and allows users to search easily even in time-constrained situations. Users can manipulate the depth of information according to the purpose of usage. SEACOIN enables interactive exploration and filtering of search results via "metamorphose topological visualization" and "tag cloud," visualization tools that are commonly used in social network sites. We illustrate SEACOIN's usage through applications on PubMed publications on heart disease, cancer, Alzheimer's disease, diabetes, and asthma.
22195150	Query log analysis of an electronic health record search engine.
AMIA Annu Symp Proc 20111022 2011
We analyzed a longitudinal collection of query logs of a full-text search engine designed to facilitate information retrieval in electronic health records (EHR). The collection, 202,905 queries and 35,928 user sessions recorded over a course of 4 years, represents the information-seeking behavior of 533 medical professionals, including frontline practitioners, coding personnel, patient safety officers, and biomedical researchers for patient data stored in EHR systems. In this paper, we present descriptive statistics of the queries, a categorization of information needs manifested through the queries, as well as temporal patterns of the users' information-seeking behavior. The results suggest that information needs in medical domain are substantially more sophisticated than those that general-purpose web search engines need to accommodate. Therefore, we envision there exists a significant challenge, along with significant opportunities, to provide intelligent query recommendations to facilitate information retrieval in EHR.
22195154	An investigation into the feasibility of spoken clinical question answering.
AMIA Annu Symp Proc 20111022 2011
Spoken question answering for clinical decision support is a potentially revolutionary technology for improving the efficiency and quality of health care delivery. This application involves many technologies currently being researched, including automatic speech recognition (ASR), information retrieval (IR), and summarization, all in the biomedical domain. In certain domains, the problem of spoken document retrieval has been declared solved because of the robustness of IR to ASR errors. This study investigates the extent to which spoken medical question answering benefits from that same robustness. We used the best results from previous speech recognition experiments as inputs to a clinical question answering system, and had physicians perform blind evaluations of results generated both by ASR transcripts of questions and gold standard transcripts of the same questions. Our results suggest that the medical domain differs enough from the open domain to require additional work in automatic speech recognition adapted for the biomedical domain.
22195158	Temporal evolution of biomedical research grant collaborations across multiple scales--a CTSA baseline study.
AMIA Annu Symp Proc 20111022 2011
The evolution of biomedical research grant collaborations (BRGC) across time (2006, 2009) and hierarchically related scales (Staff, Department) at the University of Arkansas for Medical Sciences (UAMS) is investigated using network abstractions. This baseline study is a part of the Clinical Translational Science Award (CTSA) efforts in promoting team science and exploring network science approaches for CTSA evaluation. The BRGC data were retrieved from the internally developed grants management system (Automated Research Information Administrator, ARIA). Our analysis revealed the BRGC networks to be disconnected with mutually exclusive research clusters. However, a dominant weakly-connected cluster with positively skewed degree centrality and betweenness distribution was observed across scales and time. Variation in the centrality measures, clustering coefficient, and the impact of perturbing the most-influential nodes as a function of time and scale is investigated. The results presented provide novel insights into the complex nature of BRGC networks that may persist across similar settings.
22195170	Using RxNorm and NDF-RT to classify medication data extracted from electronic health records: experiences from the Rochester Epidemiology Project.
AMIA Annu Symp Proc 20111022 2011
RxNorm and NDF-RT published by the National Library of Medicine (NLM) and Veterans Affairs (VA), respectively, are two publicly available federal medication terminologies. In this study, we evaluate the applicability of RxNorm and National Drug File-Reference Terminology (NDF-RT) for extraction and classification of medication data retrieved using structured querying and natural language processing techniques from electronic health records at two different medical centers within the Rochester Epidemiology Project (REP). Specifically, we explore how mappings between RxNorm concept codes and NDF-RT drug classes can be leveraged for hierarchical organization and grouping of REP medication data, identify gaps and coverage issues, and analyze the recently released NLM's NDF-RT Web service API. Our study concludes that RxNorm and NDF-RT can be applied together for classification of medication extracted from multiple EHR systems, although several issues and challenges remain to be addressed. We further conclude that the Web service APIs developed by the NLM provide useful functionalities for such activities.
22195180	MiDas: automatic extraction of a common domain of discourse in sleep medicine for multi-center data integration.
AMIA Annu Symp Proc 20111022 2011
Clinical studies often use data dictionaries with controlled sets of terms to facilitate data collection, limited interoperability and sharing at a local site. Multi-center retrospective clinical studies require that these data dictionaries, originating from individual participating centers, be harmonized in preparation for the integration of the corresponding clinical research data. Domain ontologies are often used to facilitate multi-center data integration by modeling terms from data dictionaries in a logic-based language, but interoperability among domain ontologies (using automated techniques) is an unresolved issue. Although many upper-level reference ontologies have been proposed to address this challenge, our experience in integrating multi-center sleep medicine data highlights the need for an upper level ontology that models a common set of terms at multiple-levels of abstraction, which is not covered by the existing upper-level ontologies. We introduce a methodology underpinned by a Minimal Domain of Discourse (MiDas) algorithm to automatically extract a minimal common domain of discourse (upper-domain ontology) from an existing domain ontology. Using the Multi-Modality, Multi-Resource Environment for Physiological and Clinical Research (Physio-MIMI) multi-center project in sleep medicine as a use case, we demonstrate the use of MiDas in extracting a minimal domain of discourse for sleep medicine, from Physio-MIMI's Sleep Domain Ontology (SDO). We then extend the resulting domain of discourse with terms from the data dictionary of the Sleep Heart and Health Study (SHHS) to validate MiDas. To illustrate the wider applicability of MiDas, we automatically extract the respective domains of discourse from 6 sample domain ontologies from the National Center for Biomedical Ontologies (NCBO) and the OBO Foundry.
22195185	Qualitative analysis of workflow modifications used to generate the reference standard for the 2010 i2b2/VA challenge.
AMIA Annu Symp Proc 20111022 2011
The Department of Veterans Affairs (VA) and the Informatics for Integrating Biology and the Bedside (i2b2) team partnered to generate the reference standard for the 2010 i2b2/VA challenge task on concept extraction, assertion classification, and relation classification. The purpose of this paper is to report an in-depth qualitative analysis of the experience and perceptions of human annotators for these tasks. Transcripts of semi-structured interviews were analyzed using qualitative methods to identify key constructs and themes related to these annotation tasks. Interventions were embedded with these tasks using pre-annotation of clinical concepts and a modified annotation workflow. From the human perspective, annotation tasks involve an inherent conflict between bias, accuracy, and efficiency. This analysis deepens understanding of the biases, complexities and impact of variations in the annotation process that may affect annotation task reliability and reference standard validity that are generalizable for other similar large-scale clinical corpus annotation projects.
22195200	An OWL meta-ontology for representing the Clinical Element Model.
AMIA Annu Symp Proc 20111022 2011
The Clinical Element Model (CEM) is a strategy designed to represent logical models for clinical data elements to ensure unambiguous data representation, interpretation, and exchange within and across heterogeneous sources and applications. The current representations of CEMs have limitations on expressing semantics and formal definitions of the structure and the semantics. Here we introduce our initial efforts on representing the CEM in OWL, so that the enrichment with OWL semantics and further semantic processing can be achieved in CEM. The focus of this paper is the CEM meta-ontology where the basic structures, the properties and their relationships, and the constraints are defined. These OWL representation specifications have been reviewed by CEM experts to ensure they capture the intended meaning of the model faithfully.
22195215	Search filter precision can be improved by NOTing out irrelevant content.
AMIA Annu Symp Proc 20111022 2011
Most methodologic search filters developed for use in large electronic databases such as MEDLINE have low precision. One method that has been proposed but not tested for improving precision is NOTing out irrelevant content. To determine if search filter precision can be improved by NOTing out the text words and index terms assigned to those articles that are retrieved but are off-target. Analytic survey. NOTing out unique terms in off-target articles and testing search filter performance in the Clinical Hedges Database. Sensitivity, specificity, precision and number needed to read (NNR). For all purpose categories (diagnosis, prognosis and etiology) except treatment and for all databases (MEDLINE, EMBASE, CINAHL and PsycINFO), constructing search filters that NOTed out irrelevant content resulted in substantive improvements in NNR (over four-fold for some purpose categories and databases). Search filter precision can be improved by NOTing out irrelevant content.
22195216	Graph-based methods for discovery browsing with semantic predications.
AMIA Annu Symp Proc 20111022 2011
We present an extension to literature-based discovery that goes beyond making discoveries to a principled way of navigating through selected aspects of some biomedical domain. The method is a type of "discovery browsing" that guides the user through the research literature on a specified phenomenon. Poorly understood relationships may be explored through novel points of view, and potentially interesting relationships need not be known ahead of time. In a process of "cooperative reciprocity" the user iteratively focuses system output, thus controlling the large number of relationships often generated in literature-based discovery systems. The underlying technology exploits SemRep semantic predications represented as a graph of interconnected nodes (predication arguments) and edges (predicates). The system suggests paths in this graph, which represent chains of relationships. The methodology is illustrated with depressive disorder and focuses on the interaction of inflammation, circadian phenomena, and the neurotransmitter norepinephrine. Insight provided may contribute to enhanced understanding of the pathophysiology, treatment, and prevention of this disorder.
22195220	Semantic characteristics of NLP-extracted concepts in clinical notes vs. biomedical literature.
AMIA Annu Symp Proc 20111022 2011
Natural language processing (NLP) has become crucial in unlocking information stored in free text, from both clinical notes and biomedical literature. Clinical notes convey clinical information related to individual patient health care, while biomedical literature communicates scientific findings. This work focuses on semantic characterization of texts at an enterprise scale, comparing and contrasting the two domains and their NLP approaches. We analyzed the empirical distributional characteristics of NLP-discovered named entities in Mayo Clinic clinical notes from 2001-2010, and in the 2011 MetaMapped Medline Baseline. We give qualitative and quantitative measures of domain similarity and point to the feasibility of transferring resources and techniques. An important by-product for this study is the development of a weighted ontology for each domain, which gives distributional semantic information that may be used to improve NLP applications.
22195225	Automatic identification of critical follow-up recommendation sentences in radiology reports.
AMIA Annu Symp Proc 20111022 2011
Communication of follow-up recommendations when abnormalities are identified on imaging studies is prone to error. When recommendations are not systematically identified and promptly communicated to referrers, poor patient outcomes can result. Using information technology can improve communication and improve patient safety. In this paper, we describe a text processing approach that uses natural language processing (NLP) and supervised text classification methods to automatically identify critical recommendation sentences in radiology reports. To increase the classification performance we enhanced the simple unigram token representation approach with lexical, semantic, knowledge-base, and structural features. We tested different combinations of those features with the Maximum Entropy (MaxEnt) classification algorithm. Classifiers were trained and tested with a gold standard corpus annotated by a domain expert. We applied 5-fold cross validation and our best performing classifier achieved 95.60% precision, 79.82% recall, 87.0% F-score, and 99.59% classification accuracy in identifying the critical recommendation sentences in radiology reports.
22195226	Leveraging rich annotations to improve learning of medical concepts from clinical free text.
AMIA Annu Symp Proc 20111022 2011
Information extraction from clinical free text is one of the key elements in medical informatics research. In this paper we propose a general framework to improve learning-based information extraction systems with the help of rich annotations (i.e., annotators provide the medical assertion as well as evidences that support the assertion). A special graphical interface was developed to facilitate the annotation process, and we show how to implement this framework with a state-of-the-art context-based question answering system. Empirical studies demonstrate that with about 10% longer annotation time, we can significantly improve the accuracy of the system. An approach to provide supporting evidence for test documents is also briefly discussed with promising preliminary results.
22195227	Evaluating measures of redundancy in clinical texts.
AMIA Annu Symp Proc 20111022 2011
Although information redundancy has been reported as an important problem for clinicians when using electronic health records and clinical reports, measuring redundancy in clinical text has not been extensively investigated. We evaluated several automated techniques to quantify the redundancy in clinical documents using an expert-derived reference standard consisting of outpatient clinical documents. The technique that resulted in the best correlation (82%) with human ratings consisted a modified dynamic programming alignment algorithm over a sliding window augmented with a) lexical normalization and b) stopword removal. When this method was applied to the overall outpatient record, we found that overall information redundancy in clinical notes increased over time and that mean document redundancy scores for individual patient documents appear to have cyclical patterns corresponding to clinical events. These results show that outpatient documents have large amounts of redundant information and that development of effective redundancy measures warrants additional investigation.
22195230	Using Medical Text Extraction, Reasoning and Mapping System (MTERMS) to process medication information in outpatient clinical notes.
AMIA Annu Symp Proc 20111022 2011
Clinical information is often coded using different terminologies, and therefore is not interoperable. Our goal is to develop a general natural language processing (NLP) system, called Medical Text Extraction, Reasoning and Mapping System (MTERMS), which encodes clinical text using different terminologies and simultaneously establishes dynamic mappings between them. MTERMS applies a modular, pipeline approach flowing from a preprocessor, semantic tagger, terminology mapper, context analyzer, and parser to structure inputted clinical notes. Evaluators manually reviewed 30 free-text and 10 structured outpatient clinical notes compared to MTERMS output. MTERMS achieved an overall F-measure of 90.6 and 94.0 for free-text and structured notes respectively for medication and temporal information. The local medication terminology had 83.0% coverage compared to RxNorm's 98.0% coverage for free-text notes. 61.6% of mappings between the terminologies are exact match. Capture of duration was significantly improved (91.7% vs. 52.5%) from systems in the third i2b2 challenge.
21985429	Enriching a biomedical event corpus with meta-knowledge annotation.
BMC Bioinformatics 20111010 2011
Biomedical papers contain rich information about entities, facts and events of biological relevance. To discover these automatically, we use text mining techniques, which rely on annotated corpora for training. In order to extract protein-protein interactions, genotype-phenotype/gene-disease associations, etc., we rely on event corpora that are annotated with classified, structured representations of important facts and findings contained within text. These provide an important resource for the training of domain-specific information extraction (IE) systems, to facilitate semantic-based searching of documents. Correct interpretation of these events is not possible without additional information, e.g., does an event describe a fact, a hypothesis, an experimental result or an analysis of results? How confident is the author about the validity of her analyses? These and other types of information, which we collectively term meta-knowledge, can be derived from the context of the event. We have designed an annotation scheme for meta-knowledge enrichment of biomedical event corpora. The scheme is multi-dimensional, in that each event is annotated for 5 different aspects of meta-knowledge that can be derived from the textual context of the event. Textual clues used to determine the values are also annotated. The scheme is intended to be general enough to allow integration with different types of bio-event annotation, whilst being detailed enough to capture important subtleties in the nature of the meta-knowledge expressed in the text. We report here on both the main features of the annotation scheme, as well as its application to the GENIA event corpus (1000 abstracts with 36,858 events). High levels of inter-annotator agreement have been achieved, falling in the range of 0.84-0.93 Kappa. By augmenting event annotations with meta-knowledge, more sophisticated IE systems can be trained, which allow interpretative information to be specified as part of the search criteria. This can assist in a number of important tasks, e.g., finding new experimental knowledge to facilitate database curation, enabling textual inference to detect entailments and contradictions, etc. To our knowledge, our scheme is unique within the field with regards to the diversity of meta-knowledge aspects annotated for each event.
22109441	Robust holographic storage system design.
Opt Express  2011Nov21
Demand is increasing daily for large data storage systems that are useful for applications in spacecraft, space satellites, and space robots, which are all exposed to radiation-rich space environment. As candidates for use in space embedded systems, holographic storage systems are promising because they can easily provided the demanded large-storage capability. Particularly, holographic storage systems, which have no rotation mechanism, are demanded because they are virtually maintenance-free. Although a holographic memory itself is an extremely robust device even in a space radiation environment, its associated lasers and drive circuit devices are vulnerable. Such vulnerabilities sometimes engendered severe problems that prevent reading of all contents of the holographic memory, which is a turn-off failure mode of a laser array. This paper therefore presents a proposal for a recovery method for the turn-off failure mode of a laser array on a holographic storage system, and describes results of an experimental demonstration.
22109460	Feedforward carrier recovery via pilot-aided transmission for single-carrier systems with arbitrary M-QAM constellations.
Opt Express  2011Nov21
We exploit pilot-aided (PA) transmission enabled by single-sideband (SSB) subcarrier modulation of both quadrature signals in the DSP domain to achieve fully feedforward carrier recovery (FFCR) in single-carrier (SC) coherent systems with arbitrary M-QAM constellations. A thorough mathematical description of the proposed PA-FFCR is presented, its linewidth tolerance is assessed by simulations and compared to other FFCR schemes in literature. Also, implementation and complexity issues of PA-FFCR are presented and briefly compared with other CR schemes. Simulation results show that PA-FFCR performs close to the best known CR technique in the literature with less computation complexity. Quantitatively, for 1 dB optical-signal-to-noise-ratio (OSNR) penalty at BER = 3.8 × 10(-3), PA-FFCR tolerates linewidth-symbol-duration products (Δf.Ts) of 1.5 × 10(-4) (4-QAM), 4 × 10(-5) (16-QAM) and 1 × 10(-5) (64-QAM). Finally, we propose the use of maximum likelihood (ML) phase estimation next to pilot phase compensation. This significantly improves tolerable Δf.Ts values to 7.5 × 10(-4) (4-QAM), 1.8 × 10(-4) (16-QAM) and 3.5 × 10(-5) (64-QAM). It turns out that PA-FFCR with ML always performs better or at least the same compared to other CR techniques known in literature with lower complexity in addition to the fact that pilot information can be as well exploited for tasks other than CR e.g., fiber nonlinearity compensation, with no extra complexity.
21727199	Identifying continuous quality improvement publications: what makes an improvement intervention 'CQI'?
BMJ Qual Saf 20110704 2011Dec
The term continuous quality improvement (CQI) is often used to refer to a method for improving care, but no consensus statement exists on the definition of CQI. Evidence reviews are critical for advancing science, and depend on reliable definitions for article selection. As a preliminary step towards improving CQI evidence reviews, this study aimed to use expert panel methods to identify key CQI definitional features and develop and test a screening instrument for reliably identifying articles with the key features. We used a previously published method to identify 106 articles meeting the general definition of a quality improvement intervention (QII) from 9427 electronically identified articles from PubMed. Two raters then applied a six-item CQI screen to the 106 articles. Per cent agreement ranged from 55.7% to 75.5% for the six items, and reviewer-adjusted intra-class correlation ranged from 0.43 to 0.62. 'Feedback of systematically collected data' was the most common feature (64%), followed by being at least 'somewhat' adapted to local conditions (61%), feedback at meetings involving participant leaders (46%), using an iterative development process (40%), being at least 'somewhat' data driven (34%), and using a recognised change method (28%). All six features were present in 14.2% of QII articles. We conclude that CQI features can be extracted from QII articles with reasonable reliability, but only a small proportion of QII articles include all features. Further consensus development is needed to support meaningful use of the term CQI for scientific communication.
21997286	A channel differential EZW coding scheme for EEG data compression.
IEEE Trans Inf Technol Biomed 20111013 2011Nov
In this paper, a method is proposed to compress multichannel electroencephalographic (EEG) signals in a scalable fashion. Correlation between EEG channels is exploited through clustering using a k-means method. Representative channels for each of the clusters are encoded individually while other channels are encoded differentially, i.e., with respect to their respective cluster representatives. The compression is performed using the embedded zero-tree wavelet encoding adapted to 1-D signals. Simulations show that the scalable features of the scheme lead to a flexible quality/rate tradeoff, without requiring detailed EEG signal modeling.
21735248	ORchestra: an online reference database of OR/MS literature in health care.
Health Care Manag Sci 20110707 2011Dec
We introduce the categorized reference database ORchestra, which is available online at http://www.utwente.nl/choir/orchestra/.
22118455	Computer vision in cell biology.
Cell  2011Nov23
Computer vision refers to the theory and implementation of artificial systems that extract information from images to understand their content. Although computers are widely used by cell biologists for visualization and measurement, interpretation of image content, i.e., the selection of events worth observing and the definition of what they mean in terms of cellular mechanisms, is mostly left to human intuition. This Essay attempts to outline roles computer vision may play and should play in image-based studies of cellular life.
21993595	How to use implantable loop recorders in clinical trials and hybrid therapy.
J Interv Card Electrophysiol 20111013 2011Dec
Epidemiological studies show that atrial fibrillation (AF) is associated with a doubling of mortality, even after adjustment for confounders. AF can be asymptomatic, but this does not decrease the thromboembolic risk of the patient. Office ECGs, occasional 24-h Holter recordings and long-term ECG event recording might not be sensitive and accurate enough in patients with AF, especially in those with paroxysmal episodes. In one study, 7 days of continuous monitoring with event recorders detected paroxysmal AF in 20 of 65 patients with a previous negative 24-h Holter recording. Over the last decade, enormous improvements have been made in the technology of implantable devices, which can now store significant information regarding heart rhythm. The first subcutaneous implantable monitor (Reveal XT, Medtronic) was validated for continuous AF monitoring by the XPECT study. The dedicated AF detection algorithm uses irregularity and incoherence of R-R intervals to identify and classify patterns in ventricular conduction. Its sensitivity in identifying patients with AF is &gt;96%. Numerous clinical data from continuous monitoring of AF have recently been published. The first applications of this technology have been in the field of surgical and catheter AF ablation. With regard to cryptogenic stroke, an international randomized trial is ongoing to compare standard care with standard care plus the implantable cardiac monitor for AF detection in patients discharged with the diagnosis of cryptogenic stroke: the Crystal AF trial. Continuous AF monitoring provides an optimal picture of daily AF burden, both symptomatic and asymptomatic. Implantable cardiac monitors have high sensitivity, enable better assessment of therapy success and may guide further AF therapy.
22001067	The implementation of electronic health records: a case study of bush computing the Ngaanyatjarra lands.
Int J Med Inform 20111015 2011Dec
The adoption of Information and Communication Technologies (ICT) in the health sector has often lagged behind adoption in other sectors as there are a number of systemic inhibitors that make the adoption of ICT far more complex. This paper aims to explore and investigate the drivers, facilitators and barriers to electronic health records adoption in the health sector and provide some guidance on how to improve the prospect of successful adoption. A case study of the successful development of electronic health records in remote Western Australia is used to identify and highlight the drivers, facilitators and barriers to electronic records adoption. A content analysis was undertaken on the in-depth interviews with participants in the region to identify the key issues and challenges and how they were overcome. The geographically isolated nature of the region had a number of benefits since it meant the electronic health records project was a manageable size and cost, the focus could be on developing a workable system rather than a 'perfect solution', decisions could be made autonomously without considering integration issues with other regions, the transient nature of the patients was a key driver, the patient centred approach elevated the importance of overcoming problems, and the system champions were determined to make the system a workable success. Complex systemic problems often derail ICT projects. The case study of an isolated region provides a number of lessons and insights to improve electronic records adoption projects. The limited resources and limited choices faced by the region lead to the development of a number of key approaches that revolved around aiming for a workable system rather than a high-end flawless solution. This ethos pervaded the electronic health records project and was underpinned by a patient centred approach and a strong desire to improve the service given to patients.
21856441	Coreference resolution: a review of general methodologies and applications in the clinical domain.
J Biomed Inform 20110812 2011Dec
Coreference resolution is the task of determining linguistic expressions that refer to the same real-world entity in natural language. Research on coreference resolution in the general English domain dates back to 1960s and 1970s. However, research on coreference resolution in the clinical free text has not seen major development. The recent US government initiatives that promote the use of electronic health records (EHRs) provide opportunities to mine patient notes as more and more health care institutions adopt EHR. Our goal was to review recent advances in general purpose coreference resolution to lay the foundation for methodologies in the clinical domain, facilitated by the availability of a shared lexical resource of gold standard coreference annotations, the Ontology Development and Information Extraction (ODIE) corpus.
21856442	Toward automated consumer question answering: automatically separating consumer questions from professional questions in the healthcare domain.
J Biomed Inform 20110812 2011Dec
Both healthcare professionals and healthcare consumers have information needs that can be met through the use of computers, specifically via medical question answering systems. However, the information needs of both groups are different in terms of literacy levels and technical expertise, and an effective question answering system must be able to account for these differences if it is to formulate the most relevant responses for users from each group. In this paper, we propose that a first step toward answering the queries of different users is automatically classifying questions according to whether they were asked by healthcare professionals or consumers. We obtained two sets of consumer questions (~10,000 questions in total) from Yahoo answers. The professional questions consist of two question collections: 4654 point-of-care questions (denoted as PointCare) obtained from interviews of a group of family doctors following patient visits and 5378 questions from physician practices through professional online services (denoted as OnlinePractice). With more than 20,000 questions combined, we developed supervised machine-learning models for automatic classification between consumer questions and professional questions. To evaluate the robustness of our models, we tested the model that was trained on the Consumer-PointCare dataset on the Consumer-OnlinePractice dataset. We evaluated both linguistic features and statistical features and examined how the characteristics in two different types of professional questions (PointCare vs. OnlinePractice) may affect the classification performance. We explored information gain for feature reduction and the back-off linguistic category features. The 10-fold cross-validation results showed the best F1-measure of 0.936 and 0.946 on Consumer-PointCare and Consumer-OnlinePractice respectively, and the best F1-measure of 0.891 when testing the Consumer-PointCare model on the Consumer-OnlinePractice dataset. Healthcare consumer questions posted at Yahoo online communities can be reliably classified from professional questions posted by point-of-care clinicians and online physicians. The supervised machine-learning models are robust for this task. Our study will significantly benefit further development in automated consumer question answering.
21864715	Semantic integration of information about orthologs and diseases: the OGO system.
J Biomed Inform 20110816 2011Dec
Semantic Web technologies like RDF and OWL are currently applied in life sciences to improve knowledge management by integrating disparate information. Many of the systems that perform such task, however, only offer a SPARQL query interface, which is difficult to use for life scientists. We present the OGO system, which consists of a knowledge base that integrates information of orthologous sequences and genetic diseases, providing an easy to use ontology-constrain driven query interface. Such interface allows the users to define SPARQL queries through a graphical process, therefore not requiring SPARQL expertise.
21786125	Combined respiratory and cardiac triggering improves blood pool contrast-enhanced pediatric cardiovascular MRI.
Pediatr Radiol 20110724 2011Dec
Contrast-enhanced cardiac MRA suffers from cardiac motion artifacts and often requires a breath-hold. This work develops and evaluates a blood pool contrast-enhanced combined respiratory- and ECG-triggered MRA method. An SPGR sequence was modified to enable combined cardiac and respiratory triggering on a 1.5-T scanner. Twenty-three consecutive children referred for pediatric heart disease receiving gadofosveset were recruited in HIPAA-compliant fashion with IRB approval and informed consent. Children underwent standard non-triggered contrast-enhanced MRA with or without suspended respiration. Additionally, a free-breathing-triggered MRA was acquired. Triggered and non-triggered studies were presented in blinded random order independently to two radiologists twice. Anatomical structure delineation was graded for each triggered and non-triggered acquisition and the visual quality on triggered MRA was compared directly to that on non-triggered MRA. Triggered images received higher scores from each radiologist for all anatomical structures on each of the two reading sessions (Wilcoxon rank sum test, P &lt; 0.05). In direct comparison, triggered images were preferred over non-triggered images for delineating cardiac structures, with most comparisons reaching statistical significance (binomial test, P &lt; 0.05). Combined cardiac and respiratory triggering, enabled by a blood pool contrast agent, improves delineation of most anatomical structures in pediatric cardiovascular MRA.
22030761	Involving clinical librarians at the point of care: results of a controlled intervention.
Acad Med  2011Dec
To measure the effect of including a clinical librarian in the health care team on medical residents and clinical clerks. In 2009, medical residents and clinical clerks were preassigned to one of two patient care teams (intervention and control). Each team had a month-long rotation on the general medicine teaching unit. The clinical librarian joined the intervention team for morning intake, clinical rounding, or an afternoon patient list review, providing immediate literature searches, formal group instruction, informal bedside teaching, and/or individual mentoring for use of preappraised resources and evidence-based medicine search techniques. Both intervention and control teams completed pre and post surveys comparing their confidence levels and awareness of resources as well as their self-reported use of evidence for making patient care decisions. The nonintervention team was surveyed as the control group. The clinical librarian intervention had a significant positive effect on medical trainees' self-reported ability to independently locate and evaluate evidence resources to support patient care decisions. Notably, 30 of 34 (88%) reported having changed a treatment plan based on skills taught by the clinical librarian, and 27 of 34 (79%) changed a treatment plan based on the librarian's mediated search support. Clinical librarians on the care team led to positive effects on self-reported provider attitudes, provider information retrieval tendencies, and, notably, clinical decision making. Future research should evaluate economic effects of widespread implementation of on-site clinical librarians.
20498506	An application of multivariate statistical analysis for Query-Driven Visualization.
IEEE Trans Vis Comput Graph  2011Mar
Driven by the ability to generate ever-larger, increasingly complex data, there is an urgent need in the scientific community for scalable analysis methods that can rapidly identify salient trends in scientific data. Query-Driven Visualization (QDV) strategies are among the small subset of techniques that can address both large and highly complex data sets. This paper extends the utility of QDV strategies with a statistics-based framework that integrates nonparametric distribution estimation techniques with a new segmentation strategy to visually identify statistically significant trends and features within the solution space of a query. In this framework, query distribution estimates help users to interactively explore their query's solution and visually identify the regions where the combined behavior of constrained variables is most important, statistically, to their inquiry. Our new segmentation strategy extends the distribution estimation analysis by visually conveying the individual importance of each variable to these regions of high statistical significance. We demonstrate the analysis benefits these two strategies provide and show how they maybe used to facilitate the refinement of constraints over variables expressed in a user's query. We apply our method to data sets from two different scientific domains to demonstrate its broad applicability.
20421680	Tugging graphs faster: efficiently modifying path-preserving hierarchies for browsing paths.
IEEE Trans Vis Comput Graph  2011Mar
Many graph visualization systems use graph hierarchies to organize a large input graph into logical components. These approaches detect features globally in the data and place these features inside levels of a hierarchy. However, this feature detection is a global process and does not consider nodes of the graph near a feature of interest. TugGraph is a system for exploring paths and proximity around nodes and subgraphs in a graph. The approach modifies a pre-existing hierarchy in order to see how a node or subgraph of interest extends out into the larger graph. It is guaranteed to create path-preserving hierarchies, so that the abstraction shown is meaningful with respect to the underlying structure of the graph. The system works well on graphs of hundreds of thousands of nodes and millions of edges. TugGraph is able to present views of this proximal information in the context of the entire graph in seconds, and does not require a layout of the full graph as input.
22130465	Do family physicians retrieve synopses of clinical research previously read as email alerts?
J. Med. Internet Res. 20111130 2011
A synopsis of new clinical research highlights important aspects of one study in a brief structured format. When delivered as email alerts, synopses enable clinicians to become aware of new developments relevant for practice. Once read, a synopsis can become a known item of clinical information. In time-pressured situations, remembering a known item may facilitate information retrieval by the clinician. However, exactly how synopses first delivered as email alerts influence retrieval at some later time is not known. We examined searches for clinical information in which a synopsis previously read as an email alert was retrieved (defined as a dyad). Our study objectives were to (1) examine whether family physicians retrieved synopses they previously read as email alerts and then to (2) explore whether family physicians purposefully retrieved these synopses. We conducted a mixed-methods study in which a qualitative multiple case study explored the retrieval of email alerts within a prospective longitudinal cohort of practicing family physicians. Reading of research-based synopses was tracked in two contexts: (1) push, meaning to read on email and (2) pull, meaning to read after retrieval from one electronic knowledge resource. Dyads, defined as synopses first read as email alerts and subsequently retrieved in a search of a knowledge resource, were prospectively identified. Participants were interviewed about all of their dyads. Outcomes were the total number of dyads and their type. Over a period of 341 days, 194 unique synopses delivered to 41 participants resulted in 4937 synopsis readings. In all, 1205 synopses were retrieved over an average of 320 days. Of the 1205 retrieved synopses, 21 (1.7%) were dyads made by 17 family physicians. Of the 1205 retrieved synopses, 6 (0.5%) were known item type dyads. However, dyads also occurred serendipitously. In the single knowledge resource we studied, email alerts containing research-based synopses were rarely retrieved. Our findings help us to better understand the effect of push on pull and to improve the integration of research-based information within electronic resources for clinicians.
22132658	Digital imaging in dentistry.
Todays FDA  2011 Sep-Oct
Information technology is vital to operations, marketing, accounting, finance and administration. One of the most exciting and quickly evolving technologies in the modern dental office is digital applications. The dentist is often the business manager, information technology officer and strategic planning chief for his small business. The information systems triangle applies directly to this critical manager supported by properly trained ancillary staff and good equipment. With emerging technology driving all medical disciplines and the rapid pace at which it emerges, it is vital for the contemporary practitioner to keep abreast of the newest information technology developments. This article compares the strategic and operational advantages of digital applications, specifically imaging. The focus of this paper will be on digital radiography (DR), 3D computerized tomography, digital photography and digitally-driven CAD/CAM to what are now considered obsolescing modalities and contemplates what may arrive in the future. It is the purpose of this essay to succinctly evaluate the decisions involved in the role, application and implications of employing this tool in the dental environment
21846511	Towards evidence-based medicine in surgical practice: best BETs.
Int J Surg 20110809 2011
Surgeons are faced with the dilemma that many clinical questions in their daily practice to do not have universally agreed answers, but patients increasingly demand the 'best practice' from their doctors. In addition time pressures mean that clinicians are unable to keep up with the full spectrum of published research. We have adopted an approach first pioneered in emergency medicine, namely the Best Evidence Topic or Best BET. Clinicians select a clinical scenario from their daily practice that highlights an area of controversy. From this, a three-part question is generated and this is used to search Medline and other appropriate databases for relevant papers. Once the relevant papers are found, these papers are critically appraised, the relevant data to answer the question is extracted, tabulated and summarised. A clinical bottom line is reached after this process. The resulting BETs, written by practising surgeons can then provide robust evidence-based answers to important clinical questions asked during our daily practice.
21993430	Systematic review and empirical comparison of contemporaneous EQ-5D and SF-6D group mean scores.
Med Decis Making 20111012 2011 Nov-Dec
Group mean estimates and their underlying distributions are the focus of assessment for cost and outcome variables in economic evaluation. Research focusing on the comparability of alternative preference-based measures of health-related quality of life has typically focused on analysis of individual-level data within specific clinical specialties or community-based samples. To explore the relationship between group mean scores for the EQ-5D and SF-6D across the utility scoring range. Studies were identified via a systematic search of 13 online electronic databases, a review of reference lists of included papers, and hand searches of key journals. Studies were included if they reported contemporaneous mean EQ-5D and SF-6D health state scores. All (sub)group comparisons of group mean EQ-5D and SF-6D scores identifiable from text, tables, or figures were extracted from identified studies. A total of 921 group mean comparisons were extracted from 56 studies. The nature of the relationship between the paired scores was examined using ranked scatter graphs and analysis of agreement. Systematic differences in group mean estimates were observed at both ends of the utility scale. At the lower (upper) end of the scale, the SF-6D (EQ-5D) provides higher mean utility estimates. These findings show that group mean EQ-5D and SF-6D scores are not directly comparable. This raises serious concerns about the cross-study comparability of economic evaluations that differ in the choice of preference-based measures, although the review focuses on 2 of the available instruments only. Further work is needed to address the practical implications of noninterchangeable utility estimates for cost-per-QALY estimates and decision making.
21967002	Energy-band engineering for improved charge retention in fully self-aligned double floating-gate single-electron memories.
Nano Lett. 20111017 2011Nov9
We present a new fully self-aligned single-electron memory with a single pair of nano floating gates, made of different materials (Si and Ge). The energy barrier that prevents stored charge leakage is induced not only by quantum effects but also by the conduction-band offset that arises between Ge and Si. The dimensions and position of each floating gate are well-defined and controlled. The devices exhibit a long retention time and single-electron injection at room temperature.
21985530	Memory and threshold resistance switching in Ni/NiO core-shell nanowires.
Nano Lett. 20111013 2011Nov9
We report on the first controlled alternation between memory and threshold resistance switching (RS) in single Ni/NiO core-shell nanowires by setting the compliance current (I(CC)) at room temperature. The memory RS is triggered by a high I(CC), while the threshold RS appears by setting a low I(CC), and the Reset process is achieved without setting a I(CC). In combination with first-principles calculations, the physical mechanisms for the memory and threshold RS are fully discussed and attributed to the formation of an oxygen vacancy (Vo) chain conductive filament and the electrical field induced breakdown without forming a conductive filament, respectively. Migration of oxygen vacancies can be activated by appropriate Joule heating, and it is energetically favorable to form conductive chains rather than random distributions due to the Vo-Vo interaction, which results in the nonvolatile switching from the off- to the on-state. For the Reset process, large Joule heating reorders the oxygen vacancies by breaking the Vo-Vo interactions and thus rupturing the conductive filaments, which are responsible for the switching from on- to off-states. This deeper understanding of the driving mechanisms responsible for the threshold and memory RS provides guidelines for the scaling, reliability, and reproducibility of NiO-based nonvolatile memory devices.
21992670	Modeling healthcare authorization and claim submissions using the openEHR dual-model approach.
BMC Med Inform Decis Mak 20111012 2011
The TISS standard is a set of mandatory forms and electronic messages for healthcare authorization and claim submissions among healthcare plans and providers in Brazil. It is not based on formal models as the new generation of health informatics standards suggests. The objective of this paper is to model the TISS in terms of the openEHR archetype-based approach and integrate it into a patient-centered EHR architecture. Three approaches were adopted to model TISS. In the first approach, a set of archetypes was designed using ENTRY subclasses. In the second one, a set of archetypes was designed using exclusively ADMIN_ENTRY and CLUSTERs as their root classes. In the third approach, the openEHR ADMIN_ENTRY is extended with classes designed for authorization and claim submissions, and an ISM_TRANSITION attribute is added to the COMPOSITION class. Another set of archetypes was designed based on this model. For all three approaches, templates were designed to represent the TISS forms. The archetypes based on the openEHR RM (Reference Model) can represent all TISS data structures. The extended model adds subclasses and an attribute to the COMPOSITION class to represent information on authorization and claim submissions. The archetypes based on all three approaches have similar structures, although rooted in different classes. The extended openEHR RM model is more semantically aligned with the concepts involved in a claim submission, but may disrupt interoperability with other systems and the current tools must be adapted to deal with it. Modeling the TISS standard by means of the openEHR approach makes it aligned with ISO recommendations and provides a solid foundation on which the TISS can evolve. Although there are few administrative archetypes available, the openEHR RM is expressive enough to represent the TISS standard. This paper focuses on the TISS but its results may be extended to other billing processes. A complete communication architecture to simulate the exchange of TISS data between systems according to the openEHR approach still needs to be designed and implemented.
21301923	Five levels of PACS modularity: integrating 3D and other advanced visualization tools.
J Digit Imaging  2011Dec
The current array of PACS products and 3D visualization tools presents a wide range of options for applying advanced visualization methods in clinical radiology. The emergence of server-based rendering techniques creates new opportunities for raising the level of clinical image review. However, best-of-breed implementations of core PACS technology, volumetric image navigation, and application-specific 3D packages will, in general, be supplied by different vendors. Integration issues should be carefully considered before deploying such systems. This work presents a classification scheme describing five tiers of PACS modularity and integration with advanced visualization tools, with the goals of characterizing current options for such integration, providing an approach for evaluating such systems, and discussing possible future architectures. These five levels of increasing PACS modularity begin with what was until recently the dominant model for integrating advanced visualization into the clinical radiologist's workflow, consisting of a dedicated stand-alone post-processing workstation in the reading room. Introduction of context-sharing, thin clients using server-based rendering, archive integration, and user-level application hosting at successive levels of the hierarchy lead to a modularized imaging architecture, which promotes user interface integration, resource efficiency, system performance, supportability, and flexibility. These technical factors and system metrics are discussed in the context of the proposed five-level classification scheme.
21424328	Use of a rich internet application solution to present medical images.
J Digit Imaging  2011Dec
Browser with Rich Internet Application (RIA) Web pages could be a powerful user interface for handling sophisticated data and applications. Then the RIA solutions would be a potential method for viewing and manipulating the most data generated in clinical processes, which can accomplish the main functionalities as general picture archiving and communication system (PACS) viewing systems. The aim of this study is to apply the RIA technology to present medical images. Both Digital Imaging and Communications in Medicine (DICOM) and non-DICOM data can be handled by our RIA solutions. Some clinical data that are especially difficult to present using PACS viewing systems, such as ECG waveform, pathology virtual slide microscopic image, and radiotherapy plan, are as well demonstrated. Consequently, clinicians can use browser as a unique interface for acquiring all the clinical data located in different departments and information systems. And the data could be presented appropriately and processed freely by adopting the RIA technologies.
22073539	[Information retrieval and reading routines in medical students].
Duodecim  2011
For a physician working as an expert continuous following of scientific literature is required. We elucidated the competence of 5th and 6th year students for the development of expertise. The mean time spent on reading medical literature was seven hours a week. The most important source of information for the students were websites with short quidelines and introductions written in students' own language. International original articles or English textbooks were not so much appreciated and seldom read. The present curricula in our medical schools do not encourage the student to search and acquire knowledge wider than their patients themselves do.
21880546	MtbSD--a comprehensive structural database for Mycobacterium tuberculosis.
Tuberculosis (Edinb) 20110830 2011Nov
The Mycobacterium tuberculosis Structural Database (MtbSD) (http://bmi.icmr.org.in/mtbsd/MtbSD.php) is a relational database for the study of protein structures of M. tuberculosis. It currently holds information on description, reaction catalyzed and domains involved, active sites, structural homologues and similarities between bound and cognate ligands, for all the 857 protein structures that are available for M. tb proteins. The database will be a valuable resource for TB researchers to select the appropriate protein-ligand complex of a given protein for molecular modelling, docking, virtual screening and structure-based drug designing.
22086320	[Gastroenterology 2.0: useful resources for the gastroenterologist available on the Web 2.0].
Rev Gastroenterol Peru  2011 Jul-Sep
The term Web 2.0 refers to the use of Internet applications which enable the users to share, participate and collaborate together on information. The objective of this study is to check different applications that use Web 2.0, which could help the gastroenterologist in his daily practice. The applications that will be checked include: blogs, microblogging, RSS, podcasts, wikis and social networks. "Gastroenterology 2.0" represents the applications, services, and tools based on Web 2.0, which are of easy use and easily accessible - to consumers, patients, gastroenterologists and other health professionals, as well as researchers. Although several studies have shown the benefits these technologies have on the medical practice, it is necessary to conduct further studies to demonstrate the use of these applications on improving health.
22094613	Clinical intelligence.
J Nurs Adm  2011Dec
Clinical intelligence is an emerging field in healthcare that will change nursing practice, driving clinical outcomes and operational efficiencies. Clinical intelligence is essential to healthcare organizations in fully realizing the value of the growing amount of data being generated through electronic health records and other clinical information systems. This article discusses the asset of clinical data, the multiple roles of nurses in maximizing the value of this asset, and a vision for nursing's future in regards to clinical intelligence.
21969471	Knowledge sharing and collaboration in translational research, and the DC-THERA Directory.
Brief. Bioinformatics 20111003 2011Nov
Biomedical research relies increasingly on large collections of data sets and knowledge whose generation, representation and analysis often require large collaborative and interdisciplinary efforts. This dimension of 'big data' research calls for the development of computational tools to manage such a vast amount of data, as well as tools that can improve communication and access to information from collaborating researchers and from the wider community. Whenever research projects have a defined temporal scope, an additional issue of data management arises, namely how the knowledge generated within the project can be made available beyond its boundaries and life-time. DC-THERA is a European 'Network of Excellence' (NoE) that spawned a very large collaborative and interdisciplinary research community, focusing on the development of novel immunotherapies derived from fundamental research in dendritic cell immunobiology. In this article we introduce the DC-THERA Directory, which is an information system designed to support knowledge management for this research community and beyond. We present how the use of metadata and Semantic Web technologies can effectively help to organize the knowledge generated by modern collaborative research, how these technologies can enable effective data management solutions during and beyond the project lifecycle, and how resources such as the DC-THERA Directory fit into the larger context of e-science.
22027554	Controlled vocabularies and semantics in systems biology.
Mol. Syst. Biol. 20111025 2011
The use of computational modeling to describe and analyze biological systems is at the heart of systems biology. Model structures, simulation descriptions and numerical results can be encoded in structured formats, but there is an increasing need to provide an additional semantic layer. Semantic information adds meaning to components of structured descriptions to help identify and interpret them unambiguously. Ontologies are one of the tools frequently used for this purpose. We describe here three ontologies created specifically to address the needs of the systems biology community. The Systems Biology Ontology (SBO) provides semantic information about the model components. The Kinetic Simulation Algorithm Ontology (KiSAO) supplies information about existing algorithms available for the simulation of systems biology models, their characterization and interrelationships. The Terminology for the Description of Dynamics (TEDDY) categorizes dynamical features of the simulation results and general systems behavior. The provision of semantic information extends a model's longevity and facilitates its reuse. It provides useful insight into the biology of modeled processes, and may be used to make informed decisions on subsequent simulation experiments.
20421683	Radiance transfer biclustering for real-time all-frequency biscale rendering.
IEEE Trans Vis Comput Graph  2011Jan
We present a real-time algorithm to render all-frequency radiance transfer at both macroscale and mesoscale. At a mesoscale, the shading is computed on a per-pixel basis by integrating the product of the local incident radiance and a bidirectional texture function. While at a macroscale, the precomputed transfer matrix, which transfers the global incident radiance to the local incident radiance at each vertex, is losslessly compressed by a novel biclustering technique. The biclustering is directly applied on the radiance transfer represented in a pixel basis, on which the BTF is naturally defined. It exploits the coherence in the transfer matrix and a property of matrix element values to reduce both storage and runtime computation cost. Our new algorithm renders at real-time frame rates realistic materials and shadows under all-frequency direct environment lighting. Comparisons show that our algorithm is able to generate images that compare favorably with reference ray tracing results, and has obvious advantages over alternative methods in storage and preprocessing time.
22039361	Accelerated Profile HMM Searches.
PLoS Comput. Biol. 20111020 2011Oct
Profile hidden Markov models (profile HMMs) and probabilistic inference methods have made important contributions to the theory of sequence database homology search. However, practical use of profile HMM methods has been hindered by the computational expense of existing software implementations. Here I describe an acceleration heuristic for profile HMMs, the "multiple segment Viterbi" (MSV) algorithm. The MSV algorithm computes an optimal sum of multiple ungapped local alignment segments using a striped vector-parallel approach previously described for fast Smith/Waterman alignment. MSV scores follow the same statistical distribution as gapped optimal local alignment scores, allowing rapid evaluation of significance of an MSV score and thus facilitating its use as a heuristic filter. I also describe a 20-fold acceleration of the standard profile HMM Forward/Backward algorithms using a method I call "sparse rescaling". These methods are assembled in a pipeline in which high-scoring MSV hits are passed on for reanalysis with the full HMM Forward/Backward algorithm. This accelerated pipeline is implemented in the freely available HMMER3 software package. Performance benchmarks show that the use of the heuristic MSV filter sacrifices negligible sensitivity compared to unaccelerated profile HMM searches. HMMER3 is substantially more sensitive and 100- to 1000-fold faster than HMMER2. HMMER3 is now about as fast as BLAST for protein searches.
21827463	Comparisons of familial DNA database searching strategies.
J. Forensic Sci. 20110809 2011Nov
The current familial searching strategies are generally based on either Identity-By-State (IBS) (i.e., number of shared alleles) or likelihood ratio (i.e., kinship index [KI]) assessments. In this study, the expected IBS match probabilities given relationships and the logic of the likelihood ratio method were addressed. Further, the false-positive and false-negative rates of the strategies were compared analytically or by simulations using Caucasian population data of the 13 CODIS Short Tandem Repeat (STR). IBS ≥ 15, IBS ≥ 16, KI ≥ 1000, or KI ≥ 10,000 were found to be good thresholds for balancing false-positive and false-negative rates. IBS ≥ 17 and/or KI ≥ 1,000,000 can exclude the majority of candidate profiles in the database, either related or not, and may be an initial screening option if a small candidate list is desired. Polices combining both IBS and KI can provide higher accuracy. Typing additional STRs can provide better searching performance, and lineage markers can be extremely useful for reducing false rates.
22043737	Staying on your your feet.
Healthc Inform  2011Oct
CIOs are hard at work coming up with the most effective and affordable strategies for protecting electronic data as their hospitals move forward on electronic medical records. While the rise of cloud computing and declining network costs are offering new opportunities in dealing with potential disasters, many find there is no substitute for good planning and constant testing.
21523446	Toward cognitivist ontologies : on the role of selective attention for upper ontologies.
Cogn Process 20110427 2011Nov
Ontologies play a key role in modern information society although there are still many fundamental questions regarding their structure to be answered. In this paper, some of these are presented, and it is argued that they require a shift from realist to cognitivist ontologies, with ontology design crucially depending on taking both cognitive and linguistic aspects into consideration. A detailed discussion of central parts of a proposed cognitivist upper ontology based on qualitative representations of selective attention is presented.
21803149	REMiner: a tool for unbiased mining and analysis of repetitive elements and their arrangement structures of large chromosomes.
Genomics 20110722 2011Nov
Repetitive elements (REs) constitute a substantial portion of the genomes of human and other species; however, the RE profiles (type, density, and arrangement) within the individual genomes have not been fully characterized. In this study, we developed an RE analysis tool, called REMiner, for a chromosome-wide investigation into the occurrence of individual REs and arrangement of clusters of REs, and REMiner's functional features were examined using the human chromosome Y. The algorithm implemented by REMiner focused on unbiased mining of REs in large chromosomes and data interface within a viewer. The data from the chromosome demonstrated that REMiner is an efficient tool in regard to its capacity for a large query size and the availability of a high-resolution viewer, featuring instant retrieval of alignment data and control of magnification and identity ratio. The chromosome-wide survey identified a diverse population of ordered RE arrangements, which may participate in the genome biology.
21859616	Bag-of-features based medical image retrieval via multiple assignment and visual words weighting.
IEEE Trans Med Imaging 20110818 2011Nov
Bag-of-features based approaches have become prominent for image retrieval and image classification tasks in the past decade. Such methods represent an image as a collection of local features, such as image patches and key points with scale invariant feature transform (SIFT) descriptors. To improve the bag-of-features methods, we first model the assignments of local descriptors as contribution functions, and then propose a novel multiple assignment strategy. Assuming the local features can be reconstructed by their neighboring visual words in a vocabulary, reconstruction weights can be solved by quadratic programming. The weights are then used to build contribution functions, resulting in a novel assignment method, called quadratic programming (QP) assignment. We further propose a novel visual word weighting method. The discriminative power of each visual word is analyzed by the sub-similarity function in the bin that corresponds to the visual word. Each sub-similarity function is then treated as a weak classifier. A strong classifier is learned by boosting methods that combine those weak classifiers. The weighting factors of the visual words are learned accordingly. We evaluate the proposed methods on medical image retrieval tasks. The methods are tested on three well-known data sets, i.e., the ImageCLEFmed data set, the 304 CT Set, and the basal-cell carcinoma image set. Experimental results demonstrate that the proposed QP assignment outperforms the traditional nearest neighbor assignment, the multiple assignment, and the soft assignment, whereas the proposed boosting based weighting strategy outperforms the state-of-the-art weighting methods, such as the term frequency weights and the term frequency-inverse document frequency weights.
22051123	Cost-effective ways of delivering enquiry services: a rapid review.
Health Info Libr J  2011Dec
In the recent times of recession and budget cuts, it is more important than ever for library and information services to deliver cost-effective services. This rapid review aims to examine the evidence for the most cost-effective ways of delivering enquiry services. A literature search was conducted on LISA (Library and Information Sciences Abstracts) and MEDLINE. Searches were limited to 2007 onwards. Eight studies met the inclusion criteria. The studies covered hospital and academic libraries in the USA and Canada. Services analysed were 'point-of-care' librarian consultations, staffing models for reference desks and virtual/digital reference services. Transferable lessons, relevant to health library and information services generally, can be drawn from this rapid review. These suggest that 'point-of-care' librarians for primary care practitioners are a cost-effective way of answering questions. Reference desks can be cost-effectively staffed by student employees or general reference staff, although librarian referral must be provided for more complex and subject-specific enquiries. However, it is not possible to draw any conclusions on virtual/digital reference services because of the limited literature available. Further case analysis studies measuring specific services, particularly enquiry services within a health library and information context, are required.
22051124	A review and rationalisation of journal subscriptions undertaken by a library and information service in a mental health trust in north-east England in 2009.
Health Info Libr J 20110817 2011Dec
To describe the methods and processes used in an evaluation of local journal subscriptions in a mental health trust and to suggest possible further areas of investigation were similar exercises to be undertaken again. Results from a user questionnaire were analysed along with e-journal usage statistics and data from local document supply activity. Journal reviews can yield surprising results. Carrying out a user survey is valuable in highlighting awareness of e-resources more generally and thus in providing evidence for marketing/information literacy initiatives. Future journal reviews should undertake impact analysis as potent evidence for continued expenditure on journals in this age of austerity.
22051125	Integrating evidence-based practice and information literacy skills in teaching physical and occupational therapy students.
Health Info Libr J 20110804 2011Dec
To ensure that physical and occupational therapy graduates develop evidence-based practice (EBP) competencies, their academic training must promote EBP skills, such as posing a clinical question and retrieving relevant literature, and the information literacy skills needed to practice these EBP skills. This article describes the collaborative process and outcome of integrating EBP and information literacy early in a professional physical therapy and occupational therapy programme. The liaison librarian and a faculty member designed an instructional activity that included a lecture, workshop and assignment that integrated EBP skills and information literacy skills in the first year of the programme. The assignment was designed to assess students' ability to conduct a search independently. The lecture and workshop were successful in their objectives, as 101 of the 104 students received at least 8 out of 10 points on the search assignment. The teaching activities developed for the students in this course appear to have achieved the goal of teaching students the EBP research cycle so that they might begin to emulate it. The collaboration between the faculty member and the librarian was integral to the success of this endeavour. Future work will include the evaluation of students' long-term retention of information literacy objectives.
22051126	Effectiveness of bibliographic searches performed by paediatric residents and interns assisted by librarians. A randomised controlled trial.
Health Info Libr J 20110816 2011Dec
Considerable barriers still prevent paediatricians from successfully using information retrieval technology. To verify whether the assistance of biomedical librarians significantly improves the outcomes of searches performed by paediatricians in biomedical databases using real-life clinical scenarios. In a controlled trial at a paediatric teaching hospital, nine residents and interns were randomly allocated to an assisted search group and nine to a non-assisted (control) group. Each participant searched PubMed and other online sources, performing pre-determined tasks including the formulation of a clinical question, retrieval and selection of bibliographic records. In the assisted group, participants were supported by a librarian with ≥5 years of experience. The primary outcome was the success of search sessions, scored against a specific assessment tool. The median score of the assisted group was 73.6 points interquartile range (IQR = 13.4) vs. 50.4 (IQR = 17.1) of the control group. The difference between median values in the results was 23.2 points (95% CI 4.8-33.2), in favour of the assisted group (P-value, Mann-Whitney U test: 0.013). The study has found quantitative evidence of a significant difference in search performance between paediatric residents or interns assisted by a librarian and those searching the literature alone.
22051129	A study of the information seeking behaviour of hospital pharmacists: empirical evidence from Greece.
Health Info Libr J  2011Dec
Hospital pharmacists need access to high-quality information in order to constantly update their knowledge and improve their skills. In their modern role, they are expected to address three types of challenges: scientific, organizational and administrative, thus having an increased need for adequate information and library services. This study investigates the information-seeking behaviour of public hospital pharmacists providing evidence from Greece that could be used to encourage the development of effective information hospital services and study the links between the information seeking behaviour of hospital pharmacists and their modern scientific and professional role. An empirical research was conducted between January and February 2010 with the development and distribution of a structured questionnaire. The questionnaire was filled in and returned by 88 public hospital pharmacists from a total of 286 working in all Greek public hospitals, providing a response rate of 31%. The hospital pharmacists in Greece are in search of scientific information and, more particularly, pharmaceutical information (e.g., drug indications, storage, dosage and prices). The Internet and the National Organization of Medicines are their main information sources, while the lack of time and organized information are the main obstacles they have to face when seeking information. The modern professional role of hospital pharmacists as invaluable contributors to efficient and safer healthcare services may be further supported through the development of specialized libraries and information services within Greek public hospitals.
20951519	Measurement of renal volume using respiratory-gated MRI in subjects without known kidney disease: intraobserver, interobserver, and interstudy reproducibility.
Eur J Radiol 20101015 2011Dec
Since renal volume is to be considered in managing renal diseases, a reproducible technique is needed. Our aim was to estimate intraobserver, interobserver, and interstudy reproducibility of renal volume measurement in subjects without known kidney disease using magnetic resonance (MR) imaging. We studied 20 patients (age range 33-82 years) without known renal disease using 1.5-T MR imaging with a respiratory-gated two-dimensional coronal balanced steady state free precession sequence. Each patient repeated the study after 1h. Two readers independently segmented the area of both kidneys of the first study, subtracting cysts. After 1 week, the first reader segmented the second study and repeated the segmentation of the first study. The volume of each kidney was obtained by multiplying the renal area on each slice by the slice thickness and summing all the partial volumes. Reproducibility was assessed by Bland-Altman and Wilcoxon statistics. The coefficient of repeatability (CoR) was summed to the absolute value of bias; the ratio between this sum and the mean of the two data sets was used as a measure of variability while its complement to 100% was used as a measure of reproducibility. Acquisition time was 2-3 min. Segmentation time was 20-25 min. Intraobserver variability results in a CoR of 7 ml and in a reproducibility of 95%, interobserver variability 8.8-9.8 ml and 87-88%, interstudy variability 9.8-10.6 ml and 91-93%, respectively. Considering both the effect of observer and the repetition of the study, the reproducibility was 83-87%. Renal volume measurement by MR imaging is highly reproducible.
22057223	An online resource of digital stories about cancer genetics: qualitative study of patient preferences and information needs.
J. Med. Internet Res. 20110930 2011
The Cancer Genetics Service for Wales (CGSW) was established in 1998 as an all-Wales service for individuals with concerns about their family history of cancer. CGSW offers a range of services such as risk assessment, genetic counseling, and genetic testing. Individuals referred to cancer genetics services often have unmet information and support needs, and they value access to practical and experiential information from other patients and health professionals. As a result of the lifelong nature of genetic conditions, a fundamental challenge is to meet the ongoing needs of these patients by providing easily accessible and reliable information. Our aims were to explore how the long-term information and support needs of CGSW patients could be met and to assess whether an online bank of digital stories about cancer genetics would be acceptable to patients. In 2009, CGSW organized patient panels across Wales. During these events, 169 patients were asked for their feedback about a potential online resource of digital stories from CGSW patients and staff. A total of 75 patients registered to take part in the project and 23 people from across Wales agreed to share their story. All participants took part in a follow-up interview. Patient preferences for an online collection of cancer genetics stories were collected at the patient panels. Key topics to be covered by the stories were identified, and this feedback informed the development of the website to ensure that patients' needs would be met. The 23 patient storytellers were aged between 28 and 75 years, and 19 were female. The digital stories reflect patients' experiences within CGSW and the implications of living with or at risk of cancer. Follow-up interviews with patient storytellers showed that they shared their experiences as a means of helping other patients and to increase understanding of the cancer genetics service. Digital stories were also collected from 12 members of staff working at CGSW. The digital stories provide reliable and easily accessible information about cancer genetics and are hosted on the StoryBank website (www.cancergeneticsstorybank.co.uk). The Internet is one mechanism through which the long-term information and support needs of cancer genetics patients can be met. The StoryBank is one of the first places where patient and staff stories have been allied to every aspect of a patient pathway through a service and provides patients with an experiential perspective of the cancer genetics "journey." The StoryBank was developed in direct response to patient feedback and is an innovative example of patient involvement in service development. The stories are a useful resource for newly referred patients, current patients, the general public, and health care professionals.
21288607	Third year nursing students' understanding of how to find and evaluate information from bibliographic databases and Internet sites.
Nurse Educ Today 20110201 2011Nov
The aim of this study was to increase undergraduate nursing students' knowledge of finding and evaluating information from selected bibliographic databases and Internet sites. A quasi-experimental design was adopted. The 2004 autumn cohort (n=480) was divided into two approximately equal groups at the beginning of their studies. One group was subjected to a greater number of assignments requiring them to find and evaluate bibliographic and Internet-based information. The assignments were spread throughout the curriculum. Questionnaires were used to collect data. The low response rate makes generalizing the findings difficult. Only small differences were demonstrated between the knowledge of the revised assignment group and that of the other students. Both groups had a poor understanding of the use of important search and evaluation techniques. The results indicate that strategies proven in one context are not necessarily as effective in a new context and that more research is needed into which learning activities best enhance the development of information literacy skills during undergraduate nursing education.
21992066	Challenges in the association of human single nucleotide polymorphism mentions with unique database identifiers.
BMC Bioinformatics 20110705 2011
Most information on genomic variations and their associations with phenotypes are covered exclusively in scientific publications rather than in structured databases. These texts commonly describe variations using natural language; database identifiers are seldom mentioned. This complicates the retrieval of variations, associated articles, as well as information extraction, e. g. the search for biological implications. To overcome these challenges, procedures to map textual mentions of variations to database identifiers need to be developed. This article describes a workflow for normalization of variation mentions, i.e. the association of them to unique database identifiers. Common pitfalls in the interpretation of single nucleotide polymorphism (SNP) mentions are highlighted and discussed. The developed normalization procedure achieves a precision of 98.1 % and a recall of 67.5% for unambiguous association of variation mentions with dbSNP identifiers on a text corpus based on 296 MEDLINE abstracts containing 527 mentions of SNPs. The annotated corpus is freely available at http://www.scai.fraunhofer.de/snp-normalization-corpus.html. Comparable approaches usually focus on variations mentioned on the protein sequence and neglect problems for other SNP mentions. The results presented here indicate that normalizing SNPs described on DNA level is more difficult than the normalization of SNPs described on protein level. The challenges associated with normalization are exemplified with ambiguities and errors, which occur in this corpus.
20845223	Automatically assisting human memory: a SenseCam browser.
Memory 20110601 2011Oct
SenseCams have many potential applications as tools for lifelogging, including the possibility of use as a memory rehabilitation tool. Given that a SenseCam can log hundreds of thousands of images per year, it is critical that these be presented to the viewer in a manner that supports the aims of memory rehabilitation. In this article we report a software browser constructed with the aim of using the characteristics of memory to organise SenseCam images into a form that makes the wealth of information stored on SenseCam more accessible. To enable a large amount of visual information to be easily and quickly assimilated by a user, we apply a series of automatic content analysis techniques to structure the images into "events", suggest their relative importance, and select representative images for each. This minimises effort when browsing and searching. We provide anecdotes on use of such a system and emphasise the need for SenseCam images to be meaningfully sorted using such a browser.
22003712	Retrieval evaluation and distance learning from perceived similarity between endomicroscopy videos.
Med Image Comput Comput Assist Interv  2011
Evaluating content-based retrieval (CBR) is challenging because it requires an adequate ground-truth. When the available groundtruth is limited to textual metadata such as pathological classes, retrieval results can only be evaluated indirectly, for example in terms of classification performance. In this study we first present a tool to generate perceived similarity ground-truth that enables direct evaluation of endomicroscopic video retrieval. This tool uses a four-points Likert scale and collects subjective pairwise similarities perceived by multiple expert observers. We then evaluate against the generated ground-truth a previously developed dense bag-of-visual-words method for endomicroscopic video retrieval. Confirming the results of previous indirect evaluation based on classification, our direct evaluation shows that this method significantly outperforms several other state-of-the-art CBR methods. In a second step, we propose to improve the CBR method by learning an adjusted similarity metric from the perceived similarity ground-truth. By minimizing a margin-based cost function that differentiates similar and dissimilar video pairs, we learn a weight vector applied to the visual word signatures of videos. Using cross-validation, we demonstrate that the learned similarity distance is significantly better correlated with the perceived similarity than the original visual-word-based distance.
21332302	Readability of online health information: implications for health literacy.
Inform Health Soc Care 20110218 2011Dec
Accessibility is one of six quality criteria articulated by the European Commission in its code of conduct for health websites. Readability plays an integral part in determining a website's accessibility. Health information that is hard to read may remain inaccessible to people with low health literacy. This study aimed to calculate the readability of websites on various causes of disease. The names of 22 health conditions were entered into five search engines, and the readability of the first 10 results for each search were evaluated using Gunning FOG, SMOG, Flesch-Kincaid and Flesch Reading Ease tests (n=352). Readability was stratified and assessed by search term, search term complexity, top-level domain and paragraph position. The mean reading grade was 12.30, and the mean FRE was 46.08, scores considered 'difficult'. Websites on certain topics were found to be even harder to read than average. Where conditions had multiple names, searching for the simplest one led to the most readable results. Websites with .gov and .nhs TLDs were the most readable while .edu sites were the least. Within texts, a trend of increasing difficulty was found with concluding paragraphs being the hardest to read. It was also found that some of the most frequent search results (such as Wikipedia pages) were amongst the hardest to read. Health professionals, with the help of public and specialised libraries, need to create and direct patients towards high-quality, plain language health information in multiple languages.
21332304	'The web is not enough, it's a base'--an explorative study of what needs a web-based support system for young carers must meet.
Inform Health Soc Care 20110218 2011Dec
The aim of this study was to gain knowledge about the specific needs that a web-based support system for young carers (YCs) must meet. Twelve young people with experience of caring for and supporting a close friend, partner or relative with mental illness (MI) were interviewed about their life situation, support needs and opinions about a hypothetical web-based support system. The transcribed interviews were analysed using content analysis. The analysed data were organised into three themes relating to support needs, each including a number of sub-themes: knowledge--understanding MI, managing the mentally ill person and self-care; communication--shared experiences, advice and feedback, and befriending; and outside involvement--acute relief, structured help and health care commitments. Web-based support for YCs may be a suitable way to meet the need for knowledge and to meet some of the needs for communication. We have outlined a concept of a geographically anchored web support to meet the need for befriending, facilitate connections to health and social care, and increase understanding and interaction between the parties involved. Further research is needed to corroborate the results.
21676941	Handling anticipated exceptions in clinical care: investigating clinician use of 'exit strategies' in an electronic health records system.
J Am Med Inform Assoc 20110614 2011 Nov-Dec
Unpredictable yet frequently occurring exception situations pervade clinical care. Handling them properly often requires aberrant actions temporarily departing from normal practice. In this study, the authors investigated several exception-handling procedures provided in an electronic health records system for facilitating clinical documentation, which the authors refer to as 'data entry exit strategies.' Through a longitudinal analysis of computer-recorded usage data, the authors found that (1) utilization of the exit strategies was not affected by postimplementation system maturity or patient visit volume, suggesting clinicians' needs to 'exit' unwanted situations are persistent; and (2) clinician type and gender are strong predictors of exit-strategy usage. Drilldown analyses further revealed that the exit strategies were judiciously used and enabled actions that would be otherwise difficult or impossible. However, many data entries recorded via them could have been 'properly' documented, yet were not, and a considerable proportion containing temporary or incomplete information was never subsequently amended. These findings may have significant implications for the design of safer and more user-friendly point-of-care information systems for healthcare.
21709162	Design and evaluation of a wireless electronic health records system for field care in mass casualty settings.
J Am Med Inform Assoc 20110627 2011 Nov-Dec
There is growing interest in the use of technology to enhance the tracking and quality of clinical information available for patients in disaster settings. This paper describes the design and evaluation of the Wireless Internet Information System for Medical Response in Disasters (WIISARD). WIISARD combined advanced networking technology with electronic triage tags that reported victims' position and recorded medical information, with wireless pulse-oximeters that monitored patient vital signs, and a wireless electronic medical record (EMR) for disaster care. The EMR system included WiFi handheld devices with barcode scanners (used by front-line responders) and computer tablets with role-tailored software (used by managers of the triage, treatment, transport and medical communications teams). An additional software system provided situational awareness for the incident commander. The WIISARD system was evaluated in a large-scale simulation exercise designed for training first responders. A randomized trial was overlaid on this exercise with 100 simulated victims, 50 in a control pathway (paper-based), and 50 in completely electronic WIISARD pathway. All patients in the electronic pathway were cared for within the WIISARD system without paper-based workarounds. WIISARD reduced the rate of the missing and/or duplicated patient identifiers (0% vs 47%, p&lt;0.001). The total time of the field was nearly identical (38:20 vs 38:23, IQR 26:53-1:05:32 vs 18:55-57:22). Overall, the results of WIISARD show that wireless EMR systems for care of the victims of disasters would be complex to develop but potentially feasible to build and deploy, and likely to improve the quality of information available for the delivery of care during disasters.
21816957	Point-of-care clinical documentation: assessment of a bladder cancer informatics tool (eCancerCareBladder): a randomized controlled study of efficacy, efficiency and user friendliness compared with standard electronic medical records.
J Am Med Inform Assoc 20110804 2011 Nov-Dec
To compare the use of structured reporting software and the standard electronic medical records (EMR) in the management of patients with bladder cancer. The use of a human factors laboratory to study management of disease using simulated clinical scenarios was also assessed. eCancerCare(Bladder) and the EMR were used to retrieve data and produce clinical reports. Twelve participants (four attending staff, four fellows, and four residents) used either eCancerCare(Bladder) or the EMR in two clinical scenarios simulating cystoscopy surveillance visits for bladder cancer follow-up. Time to retrieve and quality of review of the patient history; time to produce and completeness of a cystoscopy report. Finally, participants provided a global assessment of their computer literacy, familiarity with the two systems, and system preference. eCancerCare(Bladder) was faster for data retrieval (scenario 1: 146 s vs 245 s, p=0.019; scenario 2: 306 vs 415 s, NS), but non-significantly slower to generate a clinical report. The quality of the report was better in the eCancerCare(Bladder) system (scenario 1: p&lt;0.001; scenario 2: p=0.11). User satisfaction was higher with the eCancerCare(Bladder) system, and 11/12 participants preferred to use this system. The small sample size affected the power of our study to detect differences. Use of a specific data management tool does not appear to significantly reduce user time, but the results suggest improvement in the level of care and documentation and preference by users. Also, the use of simulated scenarios in a laboratory setting appears to be a valid method for comparing the usability of clinical software.
21898825	UniCarbKB: putting the pieces together for glycomics research.
Proteomics 20110919 2011Nov
Despite the success of several international initiatives the glycosciences still lack a managed infrastructure that contributes to the advancement of research through the provision of comprehensive structural and experimental glycan data collections. UniCarbKB is an initiative that aims to promote the creation of an online information storage and search platform for glycomics and glycobiology research. The knowledgebase will offer a freely accessible and information-rich resource supported by querying interfaces, annotation technologies and the adoption of common standards to integrate structural, experimental and functional data. The UniCarbKB framework endeavors to support the growth of glycobioinformatics and the dissemination of knowledge through the provision of an open and unified portal to encourage the sharing of data. In order to achieve this, the framework is committed to the development of tools and procedures that support data annotation, and expanding interoperability through cross-referencing of existing databases. Database URL: http://www.unicarbkb.org.
22016670	Impact and user satisfaction of a clinical information portal embedded in an electronic health record.
Perspect Health Inf Manag 20111001 2011
In 2008, a clinical information tool was developed and embedded in the electronic health record system of an academic medical center. In 2009, the initial information tool, Clinical-e, was superseded by a portal called Clinical Focus, with a single search box enabling a federated search of selected online information resources. To measure the usefulness and impact of Clinical Focus, a survey was used to gather feedback about users' experience with this clinical resource. The survey determined what type of clinicians were using this tool and assessed user satisfaction and perceived impact on patient care decision making. Initial survey results suggest the majority of respondents found Clinical Focus easy to navigate, the content easy to read, and the retrieved information relevant and complete. The majority would recommend Clinical Focus to their colleagues. Results indicate that this tool is a promising area for future development.
22021495	How to perform a critically appraised topic: part 2, appraise, evaluate, generate, and recommend.
AJR Am J Roentgenol  2011Nov
This article continues the discussion of a critically appraised topic started in Part 1. A critically appraised topic is a practical tool for learning and applying critical appraisal skills. This article outlines steps 4-7 involved in performing a critically appraised topic for studies of diagnostic tests: Appraise, Appraise the literature; Evaluate, evaluate the strength of the evidence from the literature; Generate, generate graphs of conditional probability; and Recommend, draw conclusions and make recommendations. For steps 4-7 of performing a critically appraised topic, the main study results are summarized and translated into clinically useful measures of accuracy, efficacy, or risk.
21978256	Mining the ChEMBL database: an efficient chemoinformatics workflow for assembling an ion channel-focused screening library.
J Chem Inf Model 20111006 2011Oct24
The ChEMBL database was mined to efficiently assemble an ion channel-focused screening library. The compiled library consists of 3241 compounds representing 123 templates across nine ion channel categories. Compounds in the screening library are annotated with their respective ion channel category to facilitate back-tracing of prospective molecular targets from phenotypic screening results. The established workflow is adaptable to the construction of focused screening libraries for other therapeutic target classes with diverse recognition motifs.
21282492	Where do college students get health information? Believability and use of health information sources.
Health Promot Pract 20110131 2011Sep
This study aims to identify predictors of use of health information sources among U.S. college students. For this purpose, the Spring 2006 American College Health Association-National College Health Assessment (ACHA-NCHA) database of 94,806 students at 117 colleges and universities was used. Univariate and multivariable analyses of survey data were conducted. The four most believable sources of health information as indicated by survey respondents were health center medical staff, health educators, faculty or coursework, and parents. Health center medical staff, health educators, and faculty or coursework were underutilized in relation to their perceived believability, whereas parents were both used and believed at high frequencies. In general, older students, females, full time students, and Black and Hispanic students were more likely to use information from one of the four health sources. However, there was considerable subgroup variability, especially in the use of parents as a health information source. The authors conclude that information on use and believability of health information sources can help colleges to design more effective health information campaigns.
21224267	Effectiveness of search strategies for qualitative research about barriers and facilitators of program delivery.
Eval Health Prof 20110110 2011Sep
Electronic database search strategies have developed substantially over the course of the past two decades, but their optimal use within a broader search strategy remains unclear. This article evaluates the use of a range of search strategies to identify qualitative evidence on the implementation of cardiovascular disease (CVD) prevention programs. Within the time-limited context of the production of a policy-relevant systematic review, the authors found the protocol-driven, targeted, and reference-checking search strategies to be the most effective, while obtaining authors' suggestions proved to be a resource-intensive process with negligible results. Weaknesses in the indexing of qualitative research in electronic literature databases mean that the sensitivity of searches may need to be reduced to allow time for other search strategies to be implemented. Expert knowledge may be optimally used through involving experts in the design and implementation of a search strategy, rather than solely as a source of citations.
21966383	Persistence and availability of Web services in computational biology.
PLoS ONE 20110922 2011
We have conducted a study on the long-term availability of bioinformatics Web services: an observation of 927 Web services published in the annual Nucleic Acids Research Web Server Issues between 2003 and 2009. We found that 72% of Web sites are still available at the published addresses, only 9% of services are completely unavailable. Older addresses often redirect to new pages. We checked the functionality of all available services: for 33%, we could not test functionality because there was no example data or a related problem; 13% were truly no longer working as expected; we could positively confirm functionality only for 45% of all services. Additionally, we conducted a survey among 872 Web Server Issue corresponding authors; 274 replied. 78% of all respondents indicate their services have been developed solely by students and researchers without a permanent position. Consequently, these services are in danger of falling into disrepair after the original developers move to another institution, and indeed, for 24% of services, there is no plan for maintenance, according to the respondents. We introduce a Web service quality scoring system that correlates with the number of citations: services with a high score are cited 1.8 times more often than low-scoring services. We have identified key characteristics that are predictive of a service's survival, providing reviewers, editors, and Web service developers with the means to assess or improve Web services. A Web service conforming to these criteria receives more citations and provides more reliable service for its users. The most effective way of ensuring continued access to a service is a persistent Web address, offered either by the publishing journal, or created on the authors' own initiative, for example at http://bioweb.me. The community would benefit the most from a policy requiring any source code needed to reproduce results to be deposited in a public repository.
21949364	InfoBiology by printed arrays of microorganism colonies for timed and on-demand release of messages.
Proc. Natl. Acad. Sci. U.S.A. 20110926 2011Oct4
This paper presents a proof-of-principle method, called InfoBiology, to write and encode data using arrays of genetically engineered strains of Escherichia coli with fluorescent proteins (FPs) as phenotypic markers. In InfoBiology, we encode, send, and release information using living organisms as carriers of data. Genetically engineered systems offer exquisite control of both genotype and phenotype. Living systems also offer the possibility for timed release of information as phenotypic features can take hours or days to develop. We use growth media and chemically induced gene expression as cipher keys or "biociphers" to develop encoded messages. The messages, called Steganography by Printed Arrays of Microbes (SPAM), consist of a matrix of spots generated by seven strains of E. coli, with each strain expressing a different FP. The coding scheme for these arrays relies on strings of paired, septenary digits, where each pair represents an alphanumeric character. In addition, the photophysical properties of the FPs offer another method for ciphering messages. Unique combinations of excited and emitted wavelengths generate distinct fluorescent patterns from the Steganography by Printed Arrays of Microbes (SPAM). This paper shows a new form of steganography based on information from engineered living systems. The combination of bio- and "photociphers" along with controlled timed-release exemplify the capabilities of InfoBiology, which could enable biometrics, communication through compromised channels, easy-to-read barcoding of biological products, or provide a deterrent to counterfeiting.
21975942	Searching NCBI Databases Using Entrez.
Curr Protoc Hum Genet  2011Oct
One of the most widely used interfaces for the retrieval of information from biological databases is the NCBI Entrez system. Entrez capitalizes on the fact that there are pre-existing, logical relationships between the individual entries found in numerous public databases. The existence of such natural connections, mostly biological in nature, argued for the development of a method through which all the information about a particular biological entity could be found without having to sequentially visit and query disparate databases. Two basic protocols describe simple, text-based searches, illustrating the types of information that can be retrieved through the Entrez system. An alternate protocol builds upon the first basic protocol, using additional, built-in features of the Entrez system, and providing alternative ways to issue the initial query. The support protocol reviews how to save frequently issued queries. Finally, Cn3D, a structure visualization tool, is also discussed.
21673581	Consideration of patient preferences and challenges in storage and access of pharmacogenetic test results.
Genet. Med.  2011Oct
Pharmacogenetic testing is one of the primary drivers of personalized medicine. The use of pharmacogenetic testing may provide a lifetime of benefits through tailoring drug dosing and selection of multiple medications to improve therapeutic outcomes and reduce adverse responses. We aimed to assess public interest and concerns regarding sharing and storage of pharmacogenetic test results that would facilitate the reuse of pharmacogenetic data across a lifetime of care. We conducted a random-digit-dial phone survey of a sample of the US public. We achieved an overall response rate of 42% (n = 1139). Most respondents indicated that they were extremely or somewhat comfortable allowing their pharmacogenetic test results to be shared with other doctors involved in their care management (90% ± 2.18%); significantly fewer respondents (74% ± 3.27%) indicated that they were extremely or somewhat comfortable sharing results with their pharmacist (P &lt; 0.0001). Patients, pharmacists, and physicians will all be critical players in the pharmacotherapy process. Patients are supportive of sharing pharmacogenetic test results with physicians and pharmacists and personally maintaining their test results. However, further study is needed to understand which options are needed for sharing, appropriate storage, and patient education about the relevance of pharmacogenetic test results to promote consideration of this information by other prescribing practitioners.
21980353	pubmed2ensembl: a resource for mining the biological literature on genes.
PLoS ONE 20110929 2011
The last two decades have witnessed a dramatic acceleration in the production of genomic sequence information and publication of biomedical articles. Despite the fact that genome sequence data and publications are two of the most heavily relied-upon sources of information for many biologists, very little effort has been made to systematically integrate data from genomic sequences directly with the biological literature. For a limited number of model organisms dedicated teams manually curate publications about genes; however for species with no such dedicated staff many thousands of articles are never mapped to genes or genomic regions. To overcome the lack of integration between genomic data and biological literature, we have developed pubmed2ensembl (http://www.pubmed2ensembl.org), an extension to the BioMart system that links over 2,000,000 articles in PubMed to nearly 150,000 genes in Ensembl from 50 species. We use several sources of curated (e.g., Entrez Gene) and automatically generated (e.g., gene names extracted through text-mining on MEDLINE records) sources of gene-publication links, allowing users to filter and combine different data sources to suit their individual needs for information extraction and biological discovery. In addition to extending the Ensembl BioMart database to include published information on genes, we also implemented a scripting language for automated BioMart construction and a novel BioMart interface that allows text-based queries to be performed against PubMed and PubMed Central documents in conjunction with constraints on genomic features. Finally, we illustrate the potential of pubmed2ensembl through typical use cases that involve integrated queries across the biomedical literature and genomic data. By allowing biologists to find the relevant literature on specific genomic regions or sets of functionally related genes more easily, pubmed2ensembl offers a much-needed genome informatics inspired solution to accessing the ever-increasing biomedical literature.
21989123	A robust approach to optimizing multi-source information for enhancing genomics retrieval performance.
BMC Bioinformatics 20110727 2011
The users desire to be provided short, specific answers to questions and put them in context by linking original sources from the biomedical literature. Through the use of information retrieval technologies, information systems retrieve information to index data based on all kinds of pre-defined searching techniques/functions such that various ranking strategies are designed depending on different sources. In this paper, we propose a robust approach to optimizing multi-source information for improving genomics retrieval performance. In the proposed approach, we first consider a common scenario for a metasearch system that has access to multiple baselines with retrieving and ranking documents/passages by their own models. Then, given selected baselines from multiple sources, we investigate three modified fusion methods in the proposed approach, reciprocal, CombMNZ and CombSUM, to re-rank the candidates as the outputs for evaluation. Our empirical study on both 2007 and 2006 genomics data sets demonstrates the viability of the proposed approach for obtaining better performance. Furthermore, the experimental results show that the reciprocal method provides notable improvements on the individual baseline, especially on the passage2-level MAP and the aspect-level MAP. From the extensive experiments on two TREC genomics data sets, we draw the following conclusions. For the three fusion methods proposed in the robust approach, the reciprocal method outperforms the CombMNZ and CombSUM methods obviously, and CombSUM works well on the passage2-level when compared with CombMNZ. Based on the multiple sources of DFR, BM25 and language model, we can observe that the alliance of giants achieves the best result. Meanwhile, under the same combination, the better the baseline performance is, the more contribution the baseline provides. These conclusions are very useful to direct the fusion work in the field of biomedical information retrieval.
21989180	Promoting ranking diversity for genomics search with relevance-novelty combined model.
BMC Bioinformatics 20110727 2011
In the biomedical domain, the desired information of a question (query) asked by biologists usually is a list of a certain type of entities covering different aspects that are related to the question, such as genes, proteins, diseases, mutations, etc. Hence it is important for a biomedical information retrieval system to be able to provide comprehensive and diverse answers to fulfill biologists' information needs. However, traditional retrieval models assume that the relevance of a document is independent of the relevance of other documents. This assumption may result in high redundancy and low diversity in the retrieval ranked lists. In this paper, we propose a relevance-novelty combined model, named RelNov model, based on the framework of an undirected graphical model. It consists of two component models, namely the aspect-term relevance model and the aspect-term novelty model. They model the relevance of a document and the novelty of a document respectively. We show that our approach can achieve 16.4% improvement over the highest aspect level MAP reported in the TREC 2007 Genomics track, and 9.8% improvement over the highest passage level MAP reported in the TREC 2007 Genomics track. The proposed combination model which models aspects, terms, topic relevance and document novelty as potential functions is demonstrated to be effective in promoting ranking diversity as well as in improving relevance of ranked lists for genomics search. We also show that the use of aspect plays an important role in the model. Moreover, the proposed model can integrate various different relevance and novelty measures easily.
21775198	A review of ECG storage formats.
Int J Med Inform 20110719 2011Oct
The interoperability of the Electrocardiogram (ECG) between heterogeneous systems has been facilitated by not one, but a number of predefined open storage formats. To improve the techniques currently used, it is important to define the similarities and the differences between these ECG storage formats. This paper presents a review of 9 formats used to store the ECG. Three of the predominant formats, namely, SCP-ECG, DICOM-ECG, and HL7 aECG are reviewed in detail along with the undertaking of a SWOT analysis. The remaining formats have been examined to a lesser extent as they are not as predominant in the literature. This study suggests that a plethora of open ECG formats, all aiming to promote interoperability has the opposite effect of adding more complexity. This paper discusses whether a format supporting a variety of diagnostic modalities is more advantageous than a format that only supports the ECG. It is conclusive that a general purpose format such as DICOM solves more interoperability issues, however, no general purpose format currently exists that fulfils the requirements of all users. As a result, the healthcare industry has been bombarded with custom storage formats, i.e., a format for storing the resting ECG, a format for storing the ambulatory ECG, a format for storing the ECG in clinical trials, a format for storing ECG data on mobile devices etc. This study then examines which implementation method is more suited to encode ECG data, i.e. binary or XML. Binary encoding has been used in the past to store the ECG, however, unlike binary, XML files are human readable, searchable and provide a better form of semantics. Based on analysis within this work it is speculated that XML may overtake binary as the preferred implementation method for encoding ECG data since it has already made a huge impact in the healthcare industry. It can be concluded that there is a wide range of vastly different techniques used to store the ECG. Although the specifications of these formats are openly available, neither has been internationally adopted to be used with all ECG machines. Therefore, there remains a lack of global interoperability of ECG information.
21689872	What are we reading? A study of downloaded and cited articles from the British Journal of Oral and Maxillofacial Surgery in 2010.
Br J Oral Maxillofac Surg  2011Oct
A large number of papers related to oral and maxillofacial surgery are published in many specialist journals. With the ever-increasing use of the internet it is easy to download them as part of a journal subscription on a fee per paper basis, or in some cases for free. Online access to the British Journal of Oral and Maxillofacial Surgery (BJOMS) is free to British Association (BAOMS) members with a $30 fee per paper download for non-members. Many colleagues use the online version of the journal, and this provides valuable information about downloading trends. Other data on articles that have been cited in subsequent publications are also readily available, and they form the basis for the calculation of a journal's impact factor. We evaluated the top 50 downloaded papers from the BJOMS website in 2010 to ascertain which articles were being read online. We also obtained data on the number of citations for papers published in 2009-2010 to see whether these papers were similar to the articles being downloaded. In 2010 there were over 360000 downloaded articles. The most popular papers were leading articles, reviews, and full length articles; only one short communication featured in the top 50 downloads. The papers most cited in subsequent publications were full length articles and leading articles or reviews, which represent 80% of the total citations of the 50 papers. Ten papers were in both the top 50 downloaded and most cited lists. We discuss the implications of this study for the journal and our readers.
21463704	Semantic similarity estimation in the biomedical domain: an ontology-based information-theoretic perspective.
J Biomed Inform 20110402 2011Oct
Semantic similarity estimation is an important component of analysing natural language resources like clinical records. Proper understanding of concept semantics allows for improved use and integration of heterogeneous clinical sources as well as higher information retrieval accuracy. Semantic similarity has been the focus of much research, which has led to the definition of heterogeneous measures using different theoretical principles and knowledge resources in a variety of contexts and application domains. In this paper, we study several of these measures, in addition to other similarity coefficients (not necessarily framed in a semantic context) that may be useful in determining the similarity of sets of terms. In order to make them easier to interpret and improve their applicability and accuracy, we propose a framework grounded in information theory that allows the measures studied to be uniformly redefined. Our framework is based on approximating concept semantics in terms of Information Content (IC). We also propose computing IC in a scalable and efficient manner from the taxonomical knowledge modelled in biomedical ontologies. As a result, new semantic similarity measures expressed in terms of concept Information Content are presented. These measures are evaluated and compared to related works using a benchmark of medical terms and a standard biomedical ontology. We found that an information-theoretical redefinition of well-known semantic measures and similarity coefficients, and an intrinsic estimation of concept IC result in noticeable improvements in their accuracy.
21545844	Class proximity measures--dissimilarity-based classification and display of high-dimensional data.
J Biomed Inform 20110427 2011Oct
For two-class problems, we introduce and construct mappings of high-dimensional instances into dissimilarity (distance)-based Class-Proximity Planes. The Class Proximity Projections are extensions of our earlier relative distance plane mapping, and thus provide a more general and unified approach to the simultaneous classification and visualization of many-feature datasets. The mappings display all L-dimensional instances in two-dimensional coordinate systems, whose two axes represent the two distances of the instances to various pre-defined proximity measures of the two classes. The Class Proximity mappings provide a variety of different perspectives of the dataset to be classified and visualized. We report and compare the classification and visualization results obtained with various Class Proximity Projections and their combinations on four datasets from the UCI data base, as well as on a particular high-dimensional biomedical dataset.
21545845	Using a shallow linguistic kernel for drug-drug interaction extraction.
J Biomed Inform 20110424 2011Oct
A drug-drug interaction (DDI) occurs when one drug influences the level or activity of another drug. Information Extraction (IE) techniques can provide health care professionals with an interesting way to reduce time spent reviewing the literature for potential drug-drug interactions. Nevertheless, no approach has been proposed to the problem of extracting DDIs in biomedical texts. In this article, we study whether a machine learning-based method is appropriate for DDI extraction in biomedical texts and whether the results provided are superior to those obtained from our previously proposed pattern-based approach. The method proposed here for DDI extraction is based on a supervised machine learning technique, more specifically, the shallow linguistic kernel proposed in Giuliano et al. (2006). Since no benchmark corpus was available to evaluate our approach to DDI extraction, we created the first such corpus, DrugDDI, annotated with 3169 DDIs. We performed several experiments varying the configuration parameters of the shallow linguistic kernel. The model that maximizes the F-measure was evaluated on the test data of the DrugDDI corpus, achieving a precision of 51.03%, a recall of 72.82% and an F-measure of 60.01%. To the best of our knowledge, this work has proposed the first full solution for the automatic extraction of DDIs from biomedical texts. Our study confirms that the shallow linguistic kernel outperforms our previous pattern-based approach. Additionally, it is our hope that the DrugDDI corpus will allow researchers to explore new solutions to the DDI extraction problem.
21549857	Deriving a probabilistic syntacto-semantic grammar for biomedicine based on domain-specific terminologies.
J Biomed Inform 20110428 2011Oct
Biomedical natural language processing (BioNLP) is a useful technique that unlocks valuable information stored in textual data for practice and/or research. Syntactic parsing is a critical component of BioNLP applications that rely on correctly determining the sentence and phrase structure of free text. In addition to dealing with the vast amount of domain-specific terms, a robust biomedical parser needs to model the semantic grammar to obtain viable syntactic structures. With either a rule-based or corpus-based approach, the grammar engineering process requires substantial time and knowledge from experts, and does not always yield a semantically transferable grammar. To reduce the human effort and to promote semantic transferability, we propose an automated method for deriving a probabilistic grammar based on a training corpus consisting of concept strings and semantic classes from the Unified Medical Language System (UMLS), a comprehensive terminology resource widely used by the community. The grammar is designed to specify noun phrases only due to the nominal nature of the majority of biomedical terminological concepts. Evaluated on manually parsed clinical notes, the derived grammar achieved a recall of 0.644, precision of 0.737, and average cross-bracketing of 0.61, which demonstrated better performance than a control grammar with the semantic information removed. Error analysis revealed shortcomings that could be addressed to improve performance. The results indicated the feasibility of an approach which automatically incorporates terminology semantics in the building of an operational grammar. Although the current performance of the unsupervised solution does not adequately replace manual engineering, we believe once the performance issues are addressed, it could serve as an aide in a semi-supervised solution.
21575741	Degree centrality for semantic abstraction summarization of therapeutic studies.
J Biomed Inform 20110508 2011Oct
Automatic summarization has been proposed to help manage the results of biomedical information retrieval systems. Semantic MEDLINE, for example, summarizes semantic predications representing assertions in MEDLINE citations. Results are presented as a graph which maintains links to the original citations. Graphs summarizing more than 500 citations are hard to read and navigate, however. We exploit graph theory for focusing these large graphs. The method is based on degree centrality, which measures connectedness in a graph. Four categories of clinical concepts related to treatment of disease were identified and presented as a summary of input text. A baseline was created using term frequency of occurrence. The system was evaluated on summaries for treatment of five diseases compared to a reference standard produced manually by two physicians. The results showed that recall for system results was 72%, precision was 73%, and F-score was 0.72. The system F-score was considerably higher than that for the baseline (0.47).
21933571	A study on building data warehouse of hospital information system.
Chin. Med. J.  2011Aug
Existing hospital information systems with simple statistical functions cannot meet current management needs. It is well known that hospital resources are distributed with private property rights among hospitals, such as in the case of the regional coordination of medical services. In this study, to integrate and make full use of medical data effectively, we propose a data warehouse modeling method for the hospital information system. The method can also be employed for a distributed-hospital medical service system. To ensure that hospital information supports the diverse needs of health care, the framework of the hospital information system has three layers: datacenter layer, system-function layer, and user-interface layer. This paper discusses the role of a data warehouse management system in handling hospital information from the establishment of the data theme to the design of a data model to the establishment of a data warehouse. Online analytical processing tools assist user-friendly multidimensional analysis from a number of different angles to extract the required data and information. Use of the data warehouse improves online analytical processing and mitigates deficiencies in the decision support system. The hospital information system based on a data warehouse effectively employs statistical analysis and data mining technology to handle massive quantities of historical data, and summarizes from clinical and hospital information for decision making. This paper proposes the use of a data warehouse for a hospital information system, specifically a data warehouse for the theme of hospital information to determine latitude, modeling and so on. The processing of patient information is given as an example that demonstrates the usefulness of this method in the case of hospital information management. Data warehouse technology is an evolving technology, and more and more decision support information extracted by data mining and with decision-making technology is required for further research.
21935487	Using noun phrases for navigating biomedical literature on Pubmed: how many updates are we losing track of?
PLoS ONE 20110914 2011
Author-supplied citations are a fraction of the related literature for a paper. The "related citations" on PubMed is typically dozens or hundreds of results long, and does not offer hints why these results are related. Using noun phrases derived from the sentences of the paper, we show it is possible to more transparently navigate to PubMed updates through search terms that can associate a paper with its citations. The algorithm to generate these search terms involved automatically extracting noun phrases from the paper using natural language processing tools, and ranking them by the number of occurrences in the paper compared to the number of occurrences on the web. We define search queries having at least one instance of overlap between the author-supplied citations of the paper and the top 20 search results as citation validated (CV). When the overlapping citations were written by same authors as the paper itself, we define it as CV-S and different authors is defined as CV-D. For a systematic sample of 883 papers on PubMed Central, at least one of the search terms for 86% of the papers is CV-D versus 65% for the top 20 PubMed "related citations." We hypothesize these quantities computed for the 20 million papers on PubMed to differ within 5% of these percentages. Averaged across all 883 papers, 5 search terms are CV-D, and 10 search terms are CV-S, and 6 unique citations validate these searches. Potentially related literature uncovered by citation-validated searches (either CV-S or CV-D) are on the order of ten per paper--many more if the remaining searches that are not citation-validated are taken into account. The significance and relationship of each search result to the paper can only be vetted and explained by a researcher with knowledge of or interest in that paper.
21937464	Exploring digital divides: an examination of eHealth technology use in health information seeking, communication and personal health information management in the USA.
Health Informatics J  2011Sep
Recent government initiatives to deploy health information technology in the USA, coupled with a growing body of scholarly evidence linking online heath information and positive health-related behaviors, indicate a widespread belief that access to health information and health information technologies can help reduce healthcare inequalities. However, it is less clear whether the benefits of greater access to online health information and health information technologies is equitably distributed across population groups, particularly to those who are underserved. To examine this issue, this article employs the 2007 Health Information National Trends Survey (HINTS) to investigate relationships between a variety of socio-economic variables and the use of the web-based technologies for health information seeking, personal health information management and patient-provider communication within the context of the USA. This study reveals interesting patterns in technology adoption, some of which are in line with previous studies, while others are less clear. Whether these patterns indicate early evidence of a narrowing divide in eHealth technology use across population groups as a result of the narrowing divide in Internet access and computer ownership warrants further exploration. In particular, the findings emphasize the need to explore differences in the use of eHealth tools by medically underserved and disadvantaged groups. In so doing, it will be important to explore other psychosocial variables, such as health literacy, that may be better predictors of health consumers' eHealth technology adoption.
21937354	Opportunities and challenges of cloud computing to improve health care services.
J. Med. Internet Res. 20110921 2011
Cloud computing is a new way of delivering computing resources and services. Many managers and experts believe that it can improve health care services, benefit health care research, and change the face of health information technology. However, as with any innovation, cloud computing should be rigorously evaluated before its widespread adoption. This paper discusses the concept and its current place in health care, and uses 4 aspects (management, technology, security, and legal) to evaluate the opportunities and challenges of this computing model. Strategic planning that could be used by a health organization to determine its direction, strategy, and resource allocation when it has decided to migrate from traditional to cloud-based health services is also discussed.
21950131	[Knowledgebase as a tool for monitoring post-genomic medico-biological research].
Vestn. Akad. Med. Nauk SSSR  2011
On-going molecular biological researches are characterized by exponential increase in the amount of experimental data which dictates the necessity to develop relevant information technologies. The knowledge-based approach appears to be a most promising and flexible tool for analytical processing biomedical information that allows new relationships between study objects to be established. In this article we analyze the work of researchers based at the Institute of Biomedical Chemistry with the materials of the PubMed biomedical library. A list of publication IDs for a half-year period has been compiled to elucidate individual profiles of research activities. Statistical analysis of medical subject headings (MeSH) reveals typical profiles of research of interest for separate divisions of the Institute. The proposed approach may be recommended as a means for improving the efficiency and coordination of biomedical research.
21803806	Gee Fu: a sequence version and web-services database tool for genomic assembly, genome feature and NGS data.
Bioinformatics 20110729 2011Oct1
Scientists now use high-throughput sequencing technologies and short-read assembly methods to create draft genome assemblies in just days. Tools and pipelines like the assembler, and the workflow management environments make it easy for a non-specialist to implement complicated pipelines to produce genome assemblies and annotations very quickly. Such accessibility results in a proliferation of assemblies and associated files, often for many organisms. These assemblies get used as a working reference by lots of different workers, from a bioinformatician doing gene prediction or a bench scientist designing primers for PCR. Here we describe Gee Fu, a database tool for genomic assembly and feature data, including next-generation sequence alignments. Gee Fu is an instance of a Ruby-On-Rails web application on a feature database that provides web and console interfaces for input, visualization of feature data via AnnoJ, access to data through a web-service interface, an API for direct data access by Ruby scripts and access to feature data stored in BAM files. Gee Fu provides a platform for storing and sharing different versions of an assembly and associated features that can be accessed and updated by bench biologists and bioinformaticians in ways that are easy and useful for each. http://tinyurl.com/geefu dan.maclean@tsl.ac.uk.
21813478	GlobalMIT: learning globally optimal dynamic bayesian network with the mutual information test criterion.
Bioinformatics 20110803 2011Oct1
Dynamic Bayesian networks (DBN) are widely applied in modeling various biological networks including the gene regulatory network (GRN). Due to the NP-hard nature of learning static Bayesian network structure, most methods for learning DBN also employ either local search such as hill climbing, or a meta stochastic global optimization framework such as genetic algorithm or simulated annealing. This article presents GlobalMIT, a toolbox for learning the globally optimal DBN structure from gene expression data. We propose using a recently introduced information theoretic-based scoring metric named mutual information test (MIT). With MIT, the task of learning the globally optimal DBN is efficiently achieved in polynomial time. The toolbox, implemented in Matlab and C++, is available at http://code.google.com/p/globalmit. vinh.nguyen@monash.edu; madhu.chetty@monash.edu Supplementary data is available at Bioinformatics online.
21824973	A biclustering algorithm for extracting bit-patterns from binary datasets.
Bioinformatics 20110808 2011Oct1
Binary datasets represent a compact and simple way to store data about the relationships between a group of objects and their possible properties. In the last few years, different biclustering algorithms have been specially developed to be applied to binary datasets. Several approaches based on matrix factorization, suffix trees or divide-and-conquer techniques have been proposed to extract useful biclusters from binary data, and these approaches provide information about the distribution of patterns and intrinsic correlations. A novel approach to extracting biclusters from binary datasets, BiBit, is introduced here. The results obtained from different experiments with synthetic data reveal the excellent performance and the robustness of BiBit to density and size of input data. Also, BiBit is applied to a central nervous system embryonic tumor gene expression dataset to test the quality of the results. A novel gene expression preprocessing methodology, based on expression level layers, and the selective search performed by BiBit, based on a very fast bit-pattern processing technique, provide very satisfactory results in quality and computational cost. The power of biclustering in finding genes involved simultaneously in different cancer processes is also shown. Finally, a comparison with Bimax, one of the most cited binary biclustering algorithms, shows that BiBit is faster while providing essentially the same results. The source and binary codes, the datasets used in the experiments and the results can be found at: http://www.upo.es/eps/bigs/BiBit.html dsrodbae@upo.es Supplementary data are available at Bioinformatics online.
21834535	Identifying compound-target associations by combining bioactivity profile similarity search and public databases mining.
J Chem Inf Model 20110818 2011Sep26
Molecular target identification is of central importance to drug discovery. Here, we developed a computational approach, named bioactivity profile similarity search (BASS), for associating targets to small molecules by using the known target annotations of related compounds from public databases. To evaluate BASS, a bioactivity profile database was constructed using 4296 compounds that were commonly tested in the US National Cancer Institute 60 human tumor cell line anticancer drug screen (NCI-60). Each compound was used as a query to search against the entire bioactivity profile database, and reference compounds with similar bioactivity profiles above a threshold of 0.75 were considered as neighbor compounds of the query. Potential targets were subsequently linked to the identified neighbor compounds by using the known targets of the query compound. About 45% of the predicted compound-target associations were successfully verified retrospectively, suggesting the possible application of BASS in identifying the targets of uncharacterized compounds and thus providing insight into the study of promiscuity and polypharmacology. Furthermore, BASS identified a significant fraction of structurally diverse compounds with similar bioactivities, indicating its feasibility of "scaffold hopping" in searching novel molecules against the target of interest.
20976611	Creation and storage of standards-based pre-scanning patient questionnaires in PACS as DICOM objects.
J Digit Imaging  2011Oct
Radiology departments around the country have completed the first evolution to digital imaging by becoming filmless. The next step in this evolution is to become truly paperless. Both patient and non-patient paperwork has to be eliminated in order for this transition to occur. A paper-based set of patient pre-scanning questionnaires were replaced with web-based forms for use in an outpatient imaging center. We discuss this process by which questionnaire elements are converted into SNOMED-CT terminology concepts, stored for future use, and sent to PACS in Digital Imaging and Communications in Medicine (DICOM) format to be permanently stored with the relevant study in the DICOM image database.
20978921	Using JPEG 2000 interactive protocol to stream a large image or a large image set.
J Digit Imaging  2011Oct
The electronic health record (EHR) is expected to improve the quality of care by enabling access to relevant information at the diagnostic decision moment. During deployment efforts for including images in the EHR, a main challenge has come up from the need to compare old images with current ones. When old images reside in a different system, they need to be imported for visualization which leads to a problem related to persistency management and information consistency. A solution consisting in avoiding image import is achievable with image streaming. In this paper we present, evaluate, and discuss two medical-specific streaming use cases: displaying a large image such as a digital mammography image and displaying a large set of relatively small images such as a large CT series.
20981467	Dicoogle - an open source peer-to-peer PACS.
J Digit Imaging  2011Oct
Picture Archiving and Communication Systems (PACS) have been widely deployed in healthcare institutions, and they now constitute a normal commodity for practitioners. However, its installation, maintenance, and utilization are still a burden due to their heavy structures, typically supported by centralized computational solutions. In this paper, we present Dicoogle, a PACS archive supported by a document-based indexing system and by peer-to-peer (P2P) protocols. Replacing the traditional database storage (RDBMS) by a documental organization permits gathering and indexing data from file-based repositories, which allows searching the archive through free text queries. As a direct result of this strategy, more information can be extracted from medical imaging repositories, which clearly increases flexibility when compared with current query and retrieval DICOM services. The inclusion of P2P features allows PACS internetworking without the need for a central management framework. Moreover, Dicoogle is easy to install, manage, and use, and it maintains full interoperability with standard DICOM services.
21042830	Development of a research dedicated archival system (TARAS) in a university hospital.
J Digit Imaging  2011Oct
Recent healthcare policies have influenced the manner in which patient data is handled in research projects, and the regulations concerning protected health information have become significantly tighter. Thus, new procedures are needed to facilitate research while protecting the confidentiality of patient data and ensuring the integrity of clinical work in the expanding environment of electronic files and databases. We have addressed this problem in a university hospital setting by developing the Tampere Research Archival System (TARAS), an extensive data warehouse for research purposes. This dynamic system includes numerous integrated and pseudonymized imaging studies and clinical data. In a pilot study on asthma patients, we tested and improved the functionality of the data archival system. TARAS is feasible to use in retrieving, analyzing, and processing both image and non-image data. In this paper, we present a detailed workflow of the implementation process of the data warehouse, paying special attention to administrative, ethical, practical, and data security concerns. The establishment of TARAS will enhance and accelerate research practice at Tampere University Hospital, while also improving the safety of patient information as well as the prospects for national and international research collaboration. We hope that much can be learned from our experience of planning, designing, and implementing a research data warehouse combining imaging studies and medical records in a university hospital.
21861894	De-identifying a public use microdata file from the Canadian national discharge abstract database.
BMC Med Inform Decis Mak 20110823 2011
The Canadian Institute for Health Information (CIHI) collects hospital discharge abstract data (DAD) from Canadian provinces and territories. There are many demands for the disclosure of this data for research and analysis to inform policy making. To expedite the disclosure of data for some of these purposes, the construction of a DAD public use microdata file (PUMF) was considered. Such purposes include: confirming some published results, providing broader feedback to CIHI to improve data quality, training students and fellows, providing an easily accessible data set for researchers to prepare for analyses on the full DAD data set, and serve as a large health data set for computer scientists and statisticians to evaluate analysis and data mining techniques. The objective of this study was to measure the probability of re-identification for records in a PUMF, and to de-identify a national DAD PUMF consisting of 10% of records. Plausible attacks on a PUMF were evaluated. Based on these attacks, the 2008-2009 national DAD was de-identified. A new algorithm was developed to minimize the amount of suppression while maximizing the precision of the data. The acceptable threshold for the probability of correct re-identification of a record was set at between 0.04 and 0.05. Information loss was measured in terms of the extent of suppression and entropy. Two different PUMF files were produced, one with geographic information, and one with no geographic information but more clinical information. At a threshold of 0.05, the maximum proportion of records with the diagnosis code suppressed was 20%, but these suppressions represented only 8-9% of all values in the DAD. Our suppression algorithm has less information loss than a more traditional approach to suppression. Smaller regions, patients with longer stays, and age groups that are infrequently admitted to hospitals tend to be the ones with the highest rates of suppression. The strategies we used to maximize data utility and minimize information loss can result in a PUMF that would be useful for the specific purposes noted earlier. However, to create a more detailed file with less information loss suitable for more complex health services research, the risk would need to be mitigated by requiring the data recipient to commit to a data sharing agreement.
21887336	Enhancing biomedical text summarization using semantic relation extraction.
PLoS ONE 20110826 2011
Automatic text summarization for a biomedical concept can help researchers to get the key points of a certain topic from large amount of biomedical literature efficiently. In this paper, we present a method for generating text summary for a given biomedical concept, e.g., H1N1 disease, from multiple documents based on semantic relation extraction. Our approach includes three stages: 1) We extract semantic relations in each sentence using the semantic knowledge representation tool SemRep. 2) We develop a relation-level retrieval method to select the relations most relevant to each query concept and visualize them in a graphic representation. 3) For relations in the relevant set, we extract informative sentences that can interpret them from the document collection to generate text summary using an information retrieval based method. Our major focus in this work is to investigate the contribution of semantic relation extraction to the task of biomedical text summarization. The experimental results on summarization for a set of diseases show that the introduction of semantic knowledge improves the performance and our results are better than the MEAD system, a well-known tool for text summarization.
21664776	Recent misconceptions about the 'database search problem': a probabilistic analysis using Bayesian networks.
Forensic Sci. Int. 20110612 2011Oct10
This paper analyses and discusses arguments that emerge from a recent discussion about the proper assessment of the evidential value of correspondences observed between the characteristics of a crime stain and those of a sample from a suspect when (i) this latter individual is found as a result of a database search and (ii) remaining database members are excluded as potential sources (because of different analytical characteristics). Using a graphical probability approach (i.e., Bayesian networks), the paper here intends to clarify that there is no need to (i) introduce a correction factor equal to the size of the searched database (i.e., to reduce a likelihood ratio), nor to (ii) adopt a propositional level not directly related to the suspect matching the crime stain (i.e., a proposition of the kind 'some person in (outside) the database is the source of the crime stain' rather than 'the suspect (some other person) is the source of the crime stain'). The present research thus confirms existing literature on the topic that has repeatedly demonstrated that the latter two requirements (i) and (ii) should not be a cause of concern.
21893594	New tools for JCB.
J. Cell Biol.  2011Sep5
New technologies and approaches in cell biology research necessitate new venues for information sharing and publication. JCB continues its support of innovation in publishing with the launch of Tools, a new article type for the description of methods and high-throughput datasets, and of a new interface for the JCB DataViewer for hosting high-content screening datasets in their entirety.
21893701	Bio-Search Computing: integration and global ranking of bioinformatics search results.
J Integr Bioinform 20110906 2011
In the Life Sciences, numerous questions can be addressed only by comprehensively searching different types of data that are inherently ordered, or are associated with ranked confidence values. We previously proposed Search Computing to support the integration of the results of search engines with other data and computational resources. This paper presents how well known bioinformatics resources can be described as search services in the search computing framework and integrated analyses over such services can be carried out. An initial set of bioinformatics services has been described and registered in the search computing framework and a bioinformatics search computing (Bio-SeCo) application using these services has been created. This current prototype application, the available services that it uses, the queries that are supported, the kind of interaction that is therefore made available to the users, and the future scenarios are here described and discussed.
21893923	Machine intelligence for health information: capturing concepts and trends in social media via query expansion.
Stud Health Technol Inform  2011
We aim to improve retrieval of health information from Twitter. The popularity of social media and micro-blogs has emphasised their potential for knowledge discovery and trend building. However, capturing and relating concepts in these short-spoken and lexically extensive sources of information requires search engines with increasing intelligence. Our approach uses query expansion techniques to associate query terms with the most similar Twitter terms to capture trends in the gamut of information. We demonstrated the value, defined as improved precision, of our search engine by considering three search tasks and two independent annotators. We also showed the stability of the engine with an increasing number of tweets; this is crucial as large data sets are needed for capturing trends with high confidence. These results encourage us to continue developing the engine for discovering trends in health information available at Twitter.
21893739	Interoperability driven integration of biomedical data sources.
Stud Health Technol Inform  2011
In this paper, we introduce a data integration methodology that promotes technical, syntactic and semantic interoperability for operational healthcare data sources. ETL processes provide access to different operational databases at the technical level. Furthermore, data instances have they syntax aligned according to biomedical terminologies using natural language processing. Finally, semantic web technologies are used to ensure common meaning and to provide ubiquitous access to the data. The system's performance and solvability assessments were carried out using clinical questions against seven healthcare institutions distributed across Europe. The architecture managed to provide interoperability within the limited heterogeneous grid of hospitals. Preliminary scalability result tests are provided.
21893756	Traceability of patient records usage: barriers and opportunities for improving user interface design and data management.
Stud Health Technol Inform  2011
Although IT governance practices (like ITIL, which recommends on the use of audit logs for proper service level management) are being introduced in many Hospitals to cope with increasing levels of information quality and safety requirements, the standard maturity levels of hospital IT departments is still not enough to reach the level of frequent use of audit logs. This paper aims to address the issues related to the existence of AT in patient records, describe the Hospitals scenario and to produce recommendations. Representatives from four hospitals were interviewed regarding the use of AT in their Hospital IS. Very few AT are known to exist in these hospitals (average of 1 per hospital in an estimate of 21 existing IS). CIOs should to be much more concerned with the existence and maintenance of AT. Recommendations include server clock synchronization and using advanced log visualization tools.
21893777	Can cloud computing benefit health services? - a SWOT analysis.
Stud Health Technol Inform  2011
In this paper, we discuss cloud computing, the current state of cloud computing in healthcare, and the challenges and opportunities of adopting cloud computing in healthcare. A Strengths, Weaknesses, Opportunities and Threats (SWOT) analysis was used to evaluate the feasibility of adopting this computing model in healthcare. The paper concludes that cloud computing could have huge benefits for healthcare but there are a number of issues that will need to be addressed before its widespread use in healthcare.
21893790	Safe storage and multi-modal search for medical images.
Stud Health Technol Inform  2011
Modern hospitals produce enormous amounts of data in all departments, from images, to lab results, medication use, and release letters. Since several years these data are most often produced in digital form, making them accessible for researchers to optimize the outcome of care process and analyze all available data across patients. The Geneva University Hospitals (HUG) are no exception with its daily radiology department's output of over 140'000 images in 2010, with a majority of them being tomographic slices. In this paper we introduce tools for uploading and accessing DICOM images and associated metadata in a secure Grid storage. These data are made available for authorized persons using a Grid security framework, as security is a main problem in secondary use of image data, where images are to be stored outside of the clinical image archive. Our tool combines the security and metadata access of a Grid middleware with the visual search that uses GIFT.
21893792	A nomenclature for the analysis of continuous sensor and other data in the context of health-enabling technologies.
Stud Health Technol Inform  2011
Due to the progress in technology, it is possible to capture continuous sensor data pervasively and ubiquitously. In the area of health-enabling and ambient assisted technologies we are faced with the problem of analyzing these data in order to improve or at least maintain the health status of patients. But due to the interdisciplinarity of this field every discipline makes use of their own analyzing methods. In fact, the choice of a certain analyzing method often solely depends on the set of methods known to the data analyst. It would be an advantage if the data analyst would know about all available analyzing methods and their advantages and disadvantages when applied to the manifold of data. In this paper we propose a nomenclature that structures existing analyzing methods and assists in the choice of a certain method that fits to a given measurement context and a given problem.
21893798	Evaluation of multi-terminology super-concepts for information retrieval.
Stud Health Technol Inform  2011
Following a recent change in the indexing policy for French quality controlled health gateway CISMeF, multiple terminologies are now being used for indexing in addition to MeSH®. To evaluate precision and recall of super-concepts for information retrieval in a multi-terminology paradigm compared to MeSH-only. We evaluate the relevance of resources retrieved by multi-terminology super-concepts and MeSH-only super-concepts queries. Recall was 8-14% higher for multi-terminology super-concepts compared to MeSH only super-concepts. Precision decreased from 0.66 for MeSH only super-concepts to 0.61 for multi-terminology super-concepts. Retrieval performance was found to vary significantly depending on the super-concepts (p&lt;10&lt;sup&gt;-4&lt;/sup&gt;) and indexing methods (manual vs automatic; p&lt;0.004). A multi-terminology paradigm contributes to increase recall but lowers precision. Automated tools for indexing are not accurate enough to allow a very precise information retrieval.
21893800	Populating the i2b2 database with heterogeneous EMR data: a semantic network approach.
Stud Health Technol Inform  2011
In an ongoing effort to share heterogeneous electronic medical record (EMR) data in an i2b2 instance between the University Hospitals Münster and Erlangen for joint cancer research projects, an ontology based system for the mapping of EMR data to a set of common data elements has been developed. The system translates the mappings into local SQL scripts, which are then used to extract, transform and load the facts data from each EMR into the i2b2 database. By using Semantic Web standards, it is the authors' goal to reuse the laboriously compiled "mapping knowledge" in future projects, such as a comprehensive cancer ontology or even a hospital-wide clinical ontology.
21893807	Service delivery for e-Health applications.
Stud Health Technol Inform  2011
E-Health applications have to take the business perspective into account. This is achieved by adding a fourth layer reflecting organizational and business processes to an existing three layer model for IT-system functionality and management. This approach is used for designing a state-wide e-Health service delivery allowing for distributed responsibilities: clinical organizations act on the fourth layer and have established mutual cooperation in this state-wide approach based on collectively outsourced IT-system services. As a result, no clinical organization can take a dominant role based on operating the IT-system infrastructure. The implementation relies on a central infrastructure with extended means to guarantee service delivery: (i) established redundancy within the system architecture, (ii) actively controlled network and application availability, (iii) automated routine performance tests fulfilling regulatory requirements and (iv) hub-to-spoke and end-to-end authentication. As a result, about half of the hospitals and some practices of the state have signed-up to the services and guarantee long-term sustainability by sharing the infrastructural costs. Collaboration takes place for more than 1000 patients per month based on second opinion, online consultation and proxy services for weekend and night shifts.
21893816	Roogle: an information retrieval engine for clinical data warehouse.
Stud Health Technol Inform  2011
High amount of relevant information is contained in reports stored in the electronic patient records and associated metadata. R-oogle is a project aiming at developing information retrieval engines adapted to these reports and designed for clinicians. The system consists in a data warehouse (full-text reports and structured data) imported from two different hospital information systems. Information retrieval is performed using metadata-based semantic and full-text search methods (as Google). Applications may be biomarkers identification in a translational approach, search of specific cases, and constitution of cohorts, professional practice evaluation, and quality control assessment.
21893817	Truecasing clinical narratives.
Stud Health Technol Inform  2011
Truecasing, or capitalization, is the rewriting of each word of an input text with its proper case information. Many medical texts, especially those from legacy systems, are still written entirely in capitalized letters, hampering their readability. We present a pilot study that uses the World Wide Web as a corpus in order to support automatic truecasing. The texts under scrutiny were German-language pathology reports. By submitting token bigrams to the Google Web search engine we collected enough case information so that we achieved 81.3% accuracy for acronyms and 98.5% accuracy for normal words. This is all the more impressive as only half of the words used in this corpus existed in a standard medical dictionary due to the excessive use of ad-hoc single-word nominal compounds in German. Our system performed less satisfactory for spelling correction, and in three cases the proposed word substitutions altered the meaning of the input sentence. For the routine deployment of this method the dependency on a (black box) search engine must be overcome, for example by using cloud-based Web n-gram services.
21893836	Ontology-based framework for electronic health records interoperability.
Stud Health Technol Inform  2011
The use of Electronic Health Records (EHR) is wide spread in healthcare. One of the most challenging tasks for EHR systems is to achieve computable semantic interoperability. To address EHR interoperability, a number of standardization efforts are progressing, however these standards are either incomplete in terms of functionality or lacking specification of precise meaning of underlying data. This paper describes an interoperable EHR framework that uses an ontology-based approach to facilitate exchange of information and knowledge among EHR. Based on the proposed framework, an interoperability scenario between a Personal Health Record System, an EHR and a Laboratory System is described.
21893838	A formal analysis of HL7 version 2.x.
Stud Health Technol Inform  2011
Working interoperability not only requires harmonized system's architectures, but also the same interpretation of technical specifications in order to guide the development processes. But sometimes a specification has not made the underlying model explicit which would enable a coherent understanding. This paper analyses the structures of the HL7 Version 2.x communication standard's family and presents an UML class diagram for it.
21893839	Simplifying HL7 Version 3 messages.
Stud Health Technol Inform  2011
HL7 Version 3 offers a semantically robust method for healthcare interoperability but has been criticized as overly complex to implement. This paper reviews initiatives to simplify HL7 Version 3 messaging and presents a novel approach based on semantic mapping. Based on user-defined definitions, precise transforms between simple and full messages are automatically generated. Systems can be interfaced with the simple messages and achieve interoperability with full Version 3 messages through the transforms. This reduces the costs of HL7 interfacing and will encourage better uptake of HL7 Version 3 and CDA.
21893843	Large scale healthcare data integration and analysis using the semantic web.
Stud Health Technol Inform  2011
Healthcare data interoperability can only be achieved when the semantics of the content is well defined and consistently implemented across heterogeneous data sources. Achieving these objectives of interoperability requires the collaboration of experts from several domains. This paper describes tooling that integrates Semantic Web technologies with common tools to facilitate cross-domain collaborative development for the purposes of data interoperability. Our approach is divided into stages of data harmonization and representation, model transformation, and instance generation. We applied our approach on Hypergenes, an EU funded project, where we use our method to the Essential Hypertension disease model using a CDA template. Our domain expert partners include clinical providers, clinical domain researchers, healthcare information technology experts, and a variety of clinical data consumers. We show that bringing Semantic Web technologies into the healthcare interoperability toolkit increases opportunities for beneficial collaboration thus improving patient care and clinical research outcomes.
21893852	Representing knowledge, data and concepts for EHRS using DCM.
Stud Health Technol Inform  2011
With the move towards next generations of Electronic Health Record Systems (EHRS), the focus changes from administrative and data retrieval and data entry system capabilities towards clinical functions. The representation of the clinical knowledge and evidence base into EHRS becomes an important asset for health care, with its own challenges. Clinician's do want EHRS support but do not want to standardize care, they do want unified terminology and structured data entry but also free text. In addition, information modelers challenge each other for the best solution, and care pathways and other workflows seem to differ for each situation. Such diverging approaches add complexity to the already difficult situation around Information Technology in health care, the EHRS in particular. This paper argues that a change is necessary to adopt Detailed Clinical Modeling as a method to organize clinical knowledge, represent concepts and define data in such a manner that it allows for semantics to be exchanged without being trapped in a specific technology. DCM help to fulfill the requirements for the enter data once, reuse multiple times paradigm for EHRS.
21893857	The Archetype-enabled EHR system ZK-ARCHE - integrating the ISO/EN 13606 standard and IHE XDS profile.
Stud Health Technol Inform  2011
The EHR system ZK-ARCHE automatically generates forms from ISO/EN 13606 archetypes. For this purpose the archetypes are augmented with components of the reference model to achieve so-called "comprehensive archetypes". Data collected via the forms are stored in a list which associates each value with the path of the corresponding comprehensive archetype node coded as W3C XPath. From this list archetype-conformant EHR extracts can be created. The system is embedded with the IHE XDS profile to allow direct data exchange in an environment of distributed data storage.
21893860	What is the coverage of SNOMED CT®on scientific medical corpora?
Stud Health Technol Inform  2011
This paper reports on the results of a large scale mapping of SNOMED CT on scientific medical corpora. The aim is to automatically access the validity, reliability and coverage of the Swedish SNOMED-CT translation, the largest, most extensive available resource of medical terminology. The method described here is based on the generation of predominantly safe harbor term variants which together with simple linguistic processing and the already available SNOMED term content are mapped to large corpora. The results show that term variations are very frequent and this may have implication on technological applications (such as indexing and information retrieval, decision support systems, text mining) using SNOMED CT. Naïve approaches to terminology mapping and indexing would critically affect the performance, success and results of such applications. SNOMED CT appears not well-suited for automatically capturing the enormous variety of concepts in scientific corpora (only 6,3% of all SNOMED terms could be directly matched to the corpus) unless extensive variant forms are generated and fuzzy and partial matching techniques are applied with the risk of allowing the recognition of a large number of false positives and spurious results.
21893862	Recording associated disorders using SNOMED CT.
Stud Health Technol Inform  2011
Multidisciplinary communication about patients with multiple and often interrelated diseases is of utmost importance to guarantee high quality of care. In this paper we focus on storing into the electronic medical record patients' disorders which are associated with each other, taking into account the role of SNOMED CT. The objectives of this paper are to design and discuss possibilities to appropriately record the associations between two disorders as defined in SNOMED CT and to get insight into the use of the relationship "associated with" in SNOMED CT and its consequences for data reuse. Our study showed that textual and concept-based reproducible recording of reusable data is hampered due to incorrect or incomplete modeling of associations between disorders in SNOMED CT. A possible solution for this is to record constituting characteristics of concepts directly into the record, instead of only being represented in the terminology. Further research on binding of information models and terminologies is needed.
21893865	Metadata - an international standard for clinical knowledge resources.
Stud Health Technol Inform  2011
This paper describes a new European and International standard, ISO 13119 Health informatics - Clinical knowledge resources - Metadata that is intended for both health professionals and patients/citizens. This standard aims to facilitate two issues: 1) How to find relevant documents that are appropriate for the reader and situation and 2) How to ensure that the found knowledge documents have a sufficient or at least declared quality management? Example of use is provided from the European Centre for Disease Control and Prevention.
21893867	Model driven development of clinical information sytems using openEHR.
Stud Health Technol Inform  2011
openEHR and the recent international standard (ISO 13606) defined a model driven software development methodology for health information systems. However there is little evidence in the literature describing implementation; especially for desktop clinical applications. This paper presents an implementation pathway using .Net/C# technology for Microsoft Windows desktop platforms. An endoscopy reporting application driven by openEHR Archetypes and Templates has been developed. A set of novel GUI directives has been defined and presented which guides the automatic graphical user interface generator to render widgets properly. We also reveal the development steps and important design decisions; from modelling to the final software product. This might provide guidance for other developers and form evidence required for the adoption of these standards for vendors and national programs alike.
21893868	A metadata-based patient register for cooperative clinical research: a case study in acute myeloid leukemia.
Stud Health Technol Inform  2011
In many medical indications clinical research is organized within study groups which provide and maintain the clinical infrastructure for their randomized clinical trials. Each group also manages a data center where high quality databases store the study specific individual patient data. Sharing this data between study groups is not straightforward. Therefore, a concept is needed which allows to represent a detailed overview on the information available across the cooperating groups. We propose a metadata based patient register and describe a first prototype. It provides information about available patient data sets to interested research partners while the typical register approach only collects a predefined limited core data set. This register implementation enables cooperative groups to allocate clinical data for future research projects in distributed data sources beyond the restrictions of core data sets. Additionally, it supports the research network in communication and data standardization and complies with a governance structure which is compatible with ethical aspects, privacy protection, and patient rights.
21893874	The ONCO-I2b2 project: integrating biobank information and clinical data to support translational research in oncology.
Stud Health Technol Inform  2011
The University of Pavia and the IRCCS Fondazione Salvatore Maugeri of Pavia (FSM), has recently started an IT initiative to support clinical research in oncology, called ONCO-i2b2. ONCO-i2b2, funded by the Lombardia region, grounds on the software developed by the Informatics for Integrating Biology and the Bedside (i2b2) NIH project. Using i2b2 and new software modules purposely designed, data coming from multiple sources are integrated and jointly queried. The core of the integration process stands in retrieving and merging data from the biobank management software and from the FSM hospital information system. The integration process is based on a ontology of the problem domain and on open-source software integration modules. A Natural Language Processing module has been implemented, too. This module automatically extracts clinical information of oncology patients from unstructured medical records. The system currently manages more than two thousands patients and will be further implemented and improved in the next two years.
21893878	Information technology solutions to support translational research on inherited cardiomyopathies.
Stud Health Technol Inform  2011
The INHERITANCE project, funded by the European Commission, is aimed at studying genetic or inherited Dilated cardiomyopathies (DCM) and at understanding the impact and management of the condition within families that suffer from heart conditions that are caused by DCMs. The project is supported by a number of advanced biomedical informatics tools, including data warehousing, automated literature search and decision support. The paper describes the design of these tools and the current status of implementation.
21775306	GaggleBridge: collaborative data analysis.
Bioinformatics 20110719 2011Sep15
Tools aiding in collaborative data analysis are becoming ever more important as researchers work together over long distances. We present an extension to the Gaggle framework, which has been widely adopted as a tool to enable data exchange between different analysis programs on one computer. Our program, GaggleBridge, transparently extends this functionality to allow data exchange between Gaggle users at different geographic locations using network communication. GaggleBridge can automatically set up SSH tunnels to traverse firewalls while adding some security features to the Gaggle communication. GaggleBridge is available as open-source software implemented in the Java language at http://it.inf.uni-tuebingen.de/gb. florian.battke@uni-tuebingen.de Supplementary data are available at Bioinformatics online.
21784796	phyloMeta: a program for phylogenetic comparative analyses with meta-analysis.
Bioinformatics 20110722 2011Sep15
phyloMeta is an easy to use console program for integrating phylogenetic information into meta-analysis. It is designed to help ecologists, evolutionary biologists and conservation biologists analyze effect size data extracted from published studies in a comparative phylogenetic context. This software estimates phylogenetic versions of all the traditional meta-analytical statistics used for: pooling effect sizes with weighted regressions; evaluating the homogeneity of these effect sizes; performing moderator tests akin to ANOVA style analyses; and analyzing data with fixed- and random-effects models. phyloMeta is developed in C/C++ and can be used via command line in MS Windows environments. phyloMeta can be obtained freely as an executable on the web at http://lajeunesse.myweb.usf.edu/publications lajeunesse@usf.edu.
21901739	Using the Saccharomyces Genome Database (SGD) for analysis of genomic information.
Curr Protoc Bioinformatics  2011Sep
Analysis of genomic data requires access to software tools that place the sequence-derived information in the context of biology. The Saccharomyces Genome Database (SGD) integrates functional information about budding yeast genes and their products with a set of analysis tools that facilitate exploring their biological details. This unit describes how the various types of functional data available at SGD can be searched, retrieved, and analyzed. Starting with the guided tour of the SGD Home page and Locus Summary page, this unit highlights how to retrieve data using YeastMine, how to visualize genomic information with GBrowse, how to explore gene expression patterns with SPELL, and how to use Gene Ontology tools to characterize large-scale datasets.
21871970	A quality alert and call for improved curation of public chemistry databases.
Drug Discov. Today 20110730 2011Sep
In the last ten years, public online databases have rapidly become trusted valuable resources upon which researchers rely for their chemical structures and data for use in cheminformatics, bioinformatics, systems biology, translational medicine and now drug repositioning or repurposing efforts. Their utility depends on the quality of the underlying molecular structures used. Unfortunately, the quality of much of the chemical structure-based data introduced to the public domain is poor. As an example we describe some of the errors found in the recently released NIH Chemical Genomics Center 'NPC browser' database as an example. There is an urgent need for government funded data curation to improve the quality of internet chemistry and to limit the proliferation of errors and wasted efforts.
21635690	Surfing for mouth guards: assessing quality of online information.
Dent Traumatol 20110603 2011Oct
The Internet is an easily accessible and commonly used source of health-related information, but evaluations of the quality of this information within the dental trauma field are still lacking. The aims of this study are (i) to present the most current scientific knowledge regarding mouth guards used in sport activities, (ii) to suggest a scoring system to evaluate the quality of information pertaining to mouth guard protection related to World Wide Web sites and (iii) to employ this scoring system when seeking reliable mouth guard-related websites. First, an Internet search using the keywords 'athletic injuries/prevention and control' and 'mouth protector' or 'mouth guards' in English was performed on PubMed, Cochrane, SvedMed+ and Web of Science to identify scientific knowledge about mouth guards. Second, an Internet search using the keywords 'consumer health information Internet', 'Internet information public health' and 'web usage-seeking behaviour' was performed on PubMed and Web of Science to obtain scientific articles seeking to evaluate the quality of health information on the Web. Based on the articles found in the second search, two scoring systems were selected. Then, an Internet search using the keywords 'mouth protector', 'mouth guards' and 'gum shields' in English was performed on the search engines Google, MSN and Yahoo. The websites selected were evaluated for reliability and accuracy. Of the 223 websites retrieved, 39 were designated valid and evaluated. Nine sites scored 22 or higher. The mean total score of the 39 websites was 14.2. Fourteen websites scored higher than the mean total score, and 25 websites scored less. The highest total score, presented by a Public Institution Web site (Health Canada), was 31 from a maximum possible score of 34, and the lowest score was 0. This study shows that there is a high amount of information about mouth guards on the Internet but that the quality of this information varies. It should be the responsibility of health care professionals to suggest and provide reliable Internet URL addresses to patients. In addition, an appropriate search terminology and search strategy should be made available to persons who want to search beyond the recommended sites.
21806808	Identifying quality improvement intervention publications--a comparison of electronic search strategies.
Implement Sci 20110801 2011
The evidence base for quality improvement (QI) interventions is expanding rapidly. The diversity of the initiatives and the inconsistency in labeling these as QI interventions makes it challenging for researchers, policymakers, and QI practitioners to access the literature systematically and to identify relevant publications. We evaluated search strategies developed for MEDLINE (Ovid) and PubMed based on free text words, Medical subject headings (MeSH), QI intervention components, continuous quality improvement (CQI) methods, and combinations of the strategies. Three sets of pertinent QI intervention publications were used for validation. Two independent expert reviewers screened publications for relevance. We compared the yield, recall rate, and precision of the search strategies for the identification of QI publications and for a subset of empirical studies on effects of QI interventions. The search yields ranged from 2,221 to 216,167 publications. Mean recall rates for reference publications ranged from 5% to 53% for strategies with yields of 50,000 publications or fewer. The 'best case' strategy, a simple text word search with high face validity ('quality' AND 'improv*' AND 'intervention*') identified 44%, 24%, and 62% of influential intervention articles selected by Agency for Healthcare Research and Quality (AHRQ) experts, a set of exemplar articles provided by members of the Standards for Quality Improvement Reporting Excellence (SQUIRE) group, and a sample from the Cochrane Effective Practice and Organization of Care Group (EPOC) register of studies, respectively. We applied the search strategy to a PubMed search for articles published in 10 pertinent journals in a three-year period which retrieved 183 publications. Among these, 67% were deemed relevant to QI by at least one of two independent raters. Forty percent were classified as empirical studies reporting on a QI intervention. The presented search terms and operating characteristics can be used to guide the identification of QI intervention publications. Even with extensive iterative development, we achieved only moderate recall rates of reference publications. Consensus development on QI reporting and initiatives to develop QI-relevant MeSH terms are urgently needed.
21712248	BRISK--research-oriented storage kit for biology-related data.
Bioinformatics 20110627 2011Sep1
In genetic science, large-scale international research collaborations represent a growing trend. These collaborations have demanding and challenging database, storage, retrieval and communication needs. These studies typically involve demographic and clinical data, in addition to the results from numerous genomic studies (omics studies) such as gene expression, eQTL, genome-wide association and methylation studies, which present numerous challenges, thus the need for data integration platforms that can handle these complex data structures. Inefficient methods of data transfer and access control still plague research collaboration. As science becomes more and more collaborative in nature, the need for a system that adequately manages data sharing becomes paramount. Biology-Related Information Storage Kit (BRISK) is a package of several web-based data management tools that provide a cohesive data integration and management platform. It was specifically designed to provide the architecture necessary to promote collaboration and expedite data sharing between scientists. The software, documentation, Java source code and demo are available at http://genapha.icapture.ubc.ca/brisk/index.jsp. BRISK was developed in Java, and tested on an Apache Tomcat 6 server with a MySQL database. denise.daley@hli.ubc.ca.
21775307	Simple high-throughput annotation pipeline (SHAP).
Bioinformatics 20110719 2011Sep1
SHAP (simple high-throughput annotation pipeline) is a lightweight and scalable sequence annotation pipeline capable of supporting research efforts that generate or utilize large volumes of DNA sequence data. The software provides Grid capable analysis, relational storage and Web-based full-text searching of annotation results. Implemented in Java, SHAP recognizes the limited resources of many smaller research groups. Source code is freely available under GPLv3 at https://sourceforge.net/projects/shap. matt.demaere@unsw.edu.au; r.cavicchioli@unsw.edu.au.
21854150	A comparison of answer retrieval through four evidence-based textbooks (ACP PIER, Essential Evidence Plus, First Consult, and UpToDate): a randomized controlled trial.
Med Teach  2011
The efficacy of bedside information products has not been properly evaluated, particularly in developing countries. To compare four evidence-based textbooks by comparing efficacy of their use by clinical residents, as measured by the proportion of questions for which relevant answers could be obtained within 20 min, the time to reach the answer and user satisfaction. One hundred and twelve residents were taught information mastery basics and were randomly allocated to four groups to use: (1) ACP PIER, (2) Essential Evidence Plus (formerly InfoRetriever), (3) First Consult, and (4) UpToDate. Participants received 3 of 24 questions randomly to retrieve the answers from the assigned textbook. Retrieved answers and time-to-answers were recorded by special designed software, and the researchers determined if each recorded answer was relevant. The rate of answer retrieval was 86% in UpToDate, 69% in First Consult, 49% in ACP PIER, and 45% in Essential Evidence Plus (p &lt; 0.001). The mean time-to-answer was 14.6 min using UpToDate, 15.9 min using First Consult, 16.3 min using Essential Evidence Plus, and 17.3 min using ACP PIER (p &lt; 0.001). UpToDate seems more comprehensive in content and also faster than the other three evidence-based textbooks. Thus, it may be considered as one of the best sources for answering clinicians' questions at the point of care.
21858142	To compare PubMed Clinical Queries and UpToDate in teaching information mastery to clinical residents: a crossover randomized controlled trial.
PLoS ONE 20110812 2011
To compare PubMed Clinical Queries and UpToDate regarding the amount and speed of information retrieval and users' satisfaction. A cross-over randomized trial was conducted in February 2009 in Tehran University of Medical Sciences that included 44 year-one or two residents who participated in an information mastery workshop. A one-hour lecture on the principles of information mastery was organized followed by self learning slide shows before using each database. Subsequently, participants were randomly assigned to answer 2 clinical scenarios using either UpToDate or PubMed Clinical Queries then crossed to use the other database to answer 2 different clinical scenarios. The proportion of relevantly answered clinical scenarios, time to answer retrieval, and users' satisfaction were measured in each database. Based on intention-to-treat analysis, participants retrieved the answer of 67 (76%) questions using UpToDate and 38 (43%) questions using PubMed Clinical Queries (P&lt;0.001). The median time to answer retrieval was 17 min (95% CI: 16 to 18) using UpToDate compared to 29 min (95% CI: 26 to 32) using PubMed Clinical Queries (P&lt;0.001). The satisfaction with the accuracy of retrieved answers, interaction with UpToDate and also overall satisfaction were higher among UpToDate users compared to PubMed Clinical Queries users (P&lt;0.001). For first time users, using UpToDate compared to Pubmed Clinical Queries can lead to not only a higher proportion of relevant answer retrieval within a shorter time, but also a higher users' satisfaction. So, addition of tutoring pre-appraised sources such as UpToDate to the information mastery curricula seems to be highly efficient.
21859035	2D/3D fetal cardiac dataset segmentation using a deformable model.
Med Phys  2011Jul
To segment the fetal heart in order to facilitate the 3D assessment of the cardiac function and structure. Ultrasound acquisition typically results in drop-out artifacts of the chamber walls. The authors outline a level set deformable model to automatically delineate the small fetal cardiac chambers. The level set is penalized from growing into an adjacent cardiac compartment using a novel collision detection term. The region based model allows simultaneous segmentation of all four cardiac chambers from a user defined seed point placed in each chamber. The segmented boundaries are automatically penalized from intersecting at walls with signal dropout. Root mean square errors of the perpendicular distances between the algorithm's delineation and manual tracings are within 2 mm which is less than 10% of the length of a typical fetal heart. The ejection fractions were determined from the 3D datasets. We validate the algorithm using a physical phantom and obtain volumes that are comparable to those from physically determined means. The algorithm segments volumes with an error of within 13% as determined using a physical phantom. Our original work in fetal cardiac segmentation compares automatic and manual tracings to a physical phantom and also measures inter observer variation.
21862746	Automated identification of postoperative complications within an electronic medical record using natural language processing.
JAMA  2011Aug24
Currently most automated methods to identify patient safety occurrences rely on administrative data codes; however, free-text searches of electronic medical records could represent an additional surveillance approach. To evaluate a natural language processing search-approach to identify postoperative surgical complications within a comprehensive electronic medical record. Cross-sectional study involving 2974 patients undergoing inpatient surgical procedures at 6 Veterans Health Administration (VHA) medical centers from 1999 to 2006. Postoperative occurrences of acute renal failure requiring dialysis, deep vein thrombosis, pulmonary embolism, sepsis, pneumonia, or myocardial infarction identified through medical record review as part of the VA Surgical Quality Improvement Program. We determined the sensitivity and specificity of the natural language processing approach to identify these complications and compared its performance with patient safety indicators that use discharge coding information. The proportion of postoperative events for each sample was 2% (39 of 1924) for acute renal failure requiring dialysis, 0.7% (18 of 2327) for pulmonary embolism, 1% (29 of 2327) for deep vein thrombosis, 7% (61 of 866) for sepsis, 16% (222 of 1405) for pneumonia, and 2% (35 of 1822) for myocardial infarction. Natural language processing correctly identified 82% (95% confidence interval [CI], 67%-91%) of acute renal failure cases compared with 38% (95% CI, 25%-54%) for patient safety indicators. Similar results were obtained for venous thromboembolism (59%, 95% CI, 44%-72% vs 46%, 95% CI, 32%-60%), pneumonia (64%, 95% CI, 58%-70% vs 5%, 95% CI, 3%-9%), sepsis (89%, 95% CI, 78%-94% vs 34%, 95% CI, 24%-47%), and postoperative myocardial infarction (91%, 95% CI, 78%-97%) vs 89%, 95% CI, 74%-96%). Both natural language processing and patient safety indicators were highly specific for these diagnoses. Among patients undergoing inpatient surgical procedures at VA medical centers, natural language processing analysis of electronic medical records to identify postoperative complications had higher sensitivity and lower specificity compared with patient safety indicators based on discharge coding.
21699833	Temporal relationship of atrial tachyarrhythmias, cerebrovascular events, and systemic emboli based on stored device data: a subgroup analysis of TRENDS.
Heart Rhythm 20110423 2011Sep
The temporal relationship between atrial tachyarrhythmias (atrial tachycardia [AT] and atrial fibrillation [AF]) and cerebrovascular events/systemic emboli (CVE/SE) is unknown. The purpose of this study was to evaluate this relationship using stored AT/AF diagnostic data from implanted devices in patients with and those without AF. The TRENDS study enrolled 2,486 patients with an indication for an implantable device, at least one stroke risk factor, and available device data. The current study includes the subgroup of 40 (1.6%) patients enrolled in TRENDS who experienced CVE/SE. AT/AF was detected prior to CVE/SE in 20 (50%) of 40 patients. Other than average and maximum daily AT/AF burden and duration of device monitoring prior to CVE/SE, no statistically significant differences were found between patients with and those without AT/AF prior to CVE/SE. For the 20 patients with AT/AF detected prior to CVE/SE, 9 (45%) did not have any AT/AF in the 30 days prior to CVE/SE. Therefore, 29 (73%) of 40 patients with CVE/SE had zero AT/AF burden within 30 days prior to CVE/SE. Fourteen (70%) of the 20 patients with AT/AF detected prior to CVE/SE were not in AT/AF at diagnosis of CVE/SE. The last episode of AT/AF in these 14 patients was 168 ± 199 days (range 3-642 days) before CVE/SE. The majority of CVE/SE in this population did not occur proximal to recent AT/AF episodes. These data imply that the mechanisms of CVE/SE in patients with implantable devices may importantly involve mechanisms other than cardioembolism due to atrial tachyarrhythmias.
21876686	Multistrategy self-organizing map learning for classification problems.
Comput Intell Neurosci 20110816 2011
Multistrategy Learning of Self-Organizing Map (SOM) and Particle Swarm Optimization (PSO) is commonly implemented in clustering domain due to its capabilities in handling complex data characteristics. However, some of these multistrategy learning architectures have weaknesses such as slow convergence time always being trapped in the local minima. This paper proposes multistrategy learning of SOM lattice structure with Particle Swarm Optimisation which is called ESOMPSO for solving various classification problems. The enhancement of SOM lattice structure is implemented by introducing a new hexagon formulation for better mapping quality in data classification and labeling. The weights of the enhanced SOM are optimised using PSO to obtain better output quality. The proposed method has been tested on various standard datasets with substantial comparisons with existing SOM network and various distance measurement. The results show that our proposed method yields a promising result with better average accuracy and quantisation errors compared to the other methods as well as convincing significant test.
21775258	SortNet: learning to rank by a neural preference function.
IEEE Trans Neural Netw 20110718 2011Sep
Relevance ranking consists in sorting a set of objects with respect to a given criterion. However, in personalized retrieval systems, the relevance criteria may usually vary among different users and may not be predefined. In this case, ranking algorithms that adapt their behavior from users' feedbacks must be devised. Two main approaches are proposed in the literature for learning to rank: the use of a scoring function, learned by examples, that evaluates a feature-based representation of each object yielding an absolute relevance score, a pairwise approach, where a preference function is learned to determine the object that has to be ranked first in a given pair. In this paper, we present a preference learning method for learning to rank. A neural network, the comparative neural network (CmpNN), is trained from examples to approximate the comparison function for a pair of objects. The CmpNN adopts a particular architecture designed to implement the symmetries naturally present in a preference function. The learned preference function can be embedded as the comparator into a classical sorting algorithm to provide a global ranking of a set of objects. To improve the ranking performances, an active-learning procedure is devised, that aims at selecting the most informative patterns in the training set. The proposed algorithm is evaluated on the LETOR dataset showing promising performances in comparison with other state-of-the-art algorithms.
21880266	[Uncertainty of long term preservation of digital documents and how to cope with it].
Med Sci (Paris) 20110831 2011 Aug-Sep
The development of digital technologies in all activities sectors of our society leads to a growing number of digital documents. A significant part of these documents needs to be durably preserved. This long term preservation has to face the short life expectancy of the technologies and the digital storage media. Large national organizations have already take this problem into account and set up teams, skills and means to face this challenge. At the opposite, the small structures, doctor's offices, individuals, students, etc. are not generally aware of the problem or are stripped to face there. A certain number of simple actions, not requiring specific skills in data processing can nevertheless be undertaken. Without important expenditure, they increase to a significant degree, the security level of the documents over the long term.
21644507	Probabilistic consensus scoring improves tandem mass spectrometry peptide identification.
J. Proteome Res. 20110623 2011Aug5
Database search is a standard technique for identifying peptides from their tandem mass spectra. To increase the number of correctly identified peptides, we suggest a probabilistic framework that allows the combination of scores from different search engines into a joint consensus score. Central to the approach is a novel method to estimate scores for peptides not found by an individual search engine. This approach allows the estimation of p-values for each candidate peptide and their combination across all search engines. The consensus approach works better than any single search engine across all different instrument types considered in this study. Improvements vary strongly from platform to platform and from search engine to search engine. Compared to the industry standard MASCOT, our approach can identify up to 60% more peptides. The software for consensus predictions is implemented in C++ as part of OpenMS, a software framework for mass spectrometry. The source code is available in the current development version of OpenMS and can easily be used as a command line application or via a graphical pipeline designer TOPPAS.
21764755	Google effects on memory: cognitive consequences of having information at our fingertips.
Science 20110714 2011Aug5
The advent of the Internet, with sophisticated algorithmic search engines, has made accessing information as easy as lifting a finger. No longer do we have to make costly efforts to find the things we want. We can "Google" the old classmate, find articles online, or look up the actor who was on the tip of our tongue. The results of four studies suggest that when faced with difficult questions, people are primed to think about computers and that when people expect to have future access to information, they have lower rates of recall of the information itself and enhanced recall instead for where to access it. The Internet has become a primary form of external or transactive memory, where information is stored collectively outside ourselves.
21613969	A multifunctional online research portal for facilitation of simulation-based research: a report from the EXPRESS pediatric simulation research collaborative.
Simul Healthc  2011Aug
Simulation-based research requires the coordinated effort of research teams to design projects, recruit subjects, and carry out performance assessments of individuals or teams. These efforts can often be labor intensive, time consuming, and logistically challenging, especially in the context of multicenter simulation-based research trials. We have developed a multifunctional, internet-based research portal for facilitation of simulation-based research. This free portal, accessible from www.cesei.org, is capable of managing the research process by helping researchers to design their project, setup data collection using customized assessment tools, upload videos for performance assessment, and finally, download data-filled spreadsheets for statistical analysis. The research portal has been used successfully to manage the first major project of the EXPRESS research collaborative, a multicenter research study involving 15 recruitment sites and more than 400 subjects. The use of the research portal has enabled us to simplify and streamline the management of our multicenter research studies. We envision that this portal will permit novice and expert researchers alike to carry out their simulation-based research projects in a coordinated and time-efficient fashion, thus ultimately helping to enhance their overall research productivity.
21831220	Developing a geographic search filter to identify randomised controlled trials in Africa: finding the optimal balance between sensitivity and precision.
Health Info Libr J 20110330 2011Sep
Research on identifying trials using geographic filters is limited. To test the sensitivity and precision of a filter to identify African randomised controlled trials (RCTs). We searched medline and embase for RCTs published in 2004 using a Cochrane filter for RCTs. The search was limited to HIV/AIDS but irrespective of location. Two investigators independently identified African RCTs from the retrieved records forming a reference set. We then repeated the search using an African geographic filter comprising country and regional terms forming the filter set. We compared the sensitivity and precision of the sets. The medline reference set comprised 1799 records with 23 African RCTs; for embase, the reference set comprised 763 records with 37 African RCTs. The medline filter set comprised 180 records with 17 African RCTs; the embase filter set comprised 98 records with 27 African RCTs. Sensitivity of the filter was 74% (medline) and 73% (embase). Addition of the filter improved precision from 1.3% to 9.4% (medline) and from 5% to 28% (embase). The African filter improved precision with some loss in sensitivity. Incomplete reporting of trial location in electronic bibliographic records restricts efficiency of geographic filters. Prospective trial registration should alleviate this.
21756325	S3QL: a distributed domain specific language for controlled semantic integration of life sciences data.
BMC Bioinformatics 20110714 2011
The value and usefulness of data increases when it is explicitly interlinked with related data. This is the core principle of Linked Data. For life sciences researchers, harnessing the power of Linked Data to improve biological discovery is still challenged by a need to keep pace with rapidly evolving domains and requirements for collaboration and control as well as with the reference semantic web ontologies and standards. Knowledge organization systems (KOSs) can provide an abstraction for publishing biological discoveries as Linked Data without complicating transactions with contextual minutia such as provenance and access control.We have previously described the Simple Sloppy Semantic Database (S3DB) as an efficient model for creating knowledge organization systems using Linked Data best practices with explicit distinction between domain and instantiation and support for a permission control mechanism that automatically migrates between the two. In this report we present a domain specific language, the S3DB query language (S3QL), to operate on its underlying core model and facilitate management of Linked Data. Reflecting the data driven nature of our approach, S3QL has been implemented as an application programming interface for S3DB systems hosting biomedical data, and its syntax was subsequently generalized beyond the S3DB core model. This achievement is illustrated with the assembly of an S3QL query to manage entities from the Simple Knowledge Organization System. The illustrative use cases include gastrointestinal clinical trials, genomic characterization of cancer by The Cancer Genome Atlas (TCGA) and molecular epidemiology of infectious diseases. S3QL was found to provide a convenient mechanism to represent context for interoperation between public and private datasets hosted at biomedical research institutions and linked data formalisms.
21796321	Rewritable multicolor fluorescent patterns for multistate memory devices with high data storage capacity.
Chem. Commun. (Camb.) 20110728 2011Sep14
We report a branched polyethyleneimine (BPEI)-quantum dot (QD) based rewritable fluorescent system with a multicolor recording mode, in which BPEI is both QD-multicolor patterning "writer" and data erasing "remover". This method could write distinct colors from size-tailored QDs to represent large numbers of logic states for high data storage capacity.
21841211	Toward real-time Monte Carlo simulation using a commercial cloud computing infrastructure.
Phys Med Biol 20110812 2011Sep7
Monte Carlo (MC) methods are the gold standard for modeling photon and electron transport in a heterogeneous medium; however, their computational cost prohibits their routine use in the clinic. Cloud computing, wherein computing resources are allocated on-demand from a third party, is a new approach for high performance computing and is implemented to perform ultra-fast MC calculation in radiation therapy. We deployed the EGS5 MC package in a commercial cloud environment. Launched from a single local computer with Internet access, a Python script allocates a remote virtual cluster. A handshaking protocol designates master and worker nodes. The EGS5 binaries and the simulation data are initially loaded onto the master node. The simulation is then distributed among independent worker nodes via the message passing interface, and the results aggregated on the local computer for display and data analysis. The described approach is evaluated for pencil beams and broad beams of high-energy electrons and photons. The output of cloud-based MC simulation is identical to that produced by single-threaded implementation. For 1 million electrons, a simulation that takes 2.58 h on a local computer can be executed in 3.3 min on the cloud with 100 nodes, a 47× speed-up. Simulation time scales inversely with the number of parallel nodes. The parallelization overhead is also negligible for large simulations. Cloud computing represents one of the most important recent advances in supercomputing technology and provides a promising platform for substantially improved MC simulation. In addition to the significant speed up, cloud computing builds a layer of abstraction for high performance parallel computing, which may change the way dose calculations are performed and radiation treatment plans are completed.
21431246	Utilizing IHE-based Electronic Health Record systems for secondary use.
Methods Inf Med 20110321 2011
Due to the increasing adoption of Electronic Health Records (EHRs) for primary use, the number of electronic documents stored in such systems will soar in the near future. In order to benefit from this development in secondary fields such as medical research, it is important to define requirements for the secondary use of EHR data. Furthermore, analyses of the extent to which an IHE (Integrating the Healthcare Enterprise)-based architecture would fulfill these requirements could provide further information on upcoming obstacles for the secondary use of EHRs. A catalog of eight core requirements for secondary use of EHR data was deduced from the published literature, the risk analysis of the IHE profile MPQ (Multi-Patient Queries) and the analysis of relevant questions. The IHE-based architecture for cross-domain, patient-centered document sharing was extended to a cross-patient architecture. We propose an IHE-based architecture for cross-patient and cross-domain secondary use of EHR data. Evaluation of this architecture concerning the eight core requirements revealed positive fulfillment of six and the partial fulfillment of two requirements. Although not regarded as a primary goal in modern electronic healthcare, the re-use of existing electronic medical documents in EHRs for research and other fields of secondary application holds enormous potential for the future. Further research in this respect is necessary.
21845286	A model-driven privacy compliance decision support for medical data sharing in Europe.
Methods Inf Med 20110726 2011
Clinical practitioners and medical researchers often have to share health data with other colleagues across Europe. Privacy compliance in this context is very important but challenging. Automated privacy guidelines are a practical way of increasing users' awareness of privacy obligations and help eliminating unintentional breaches of privacy. In this paper we present an ontology-plus-rules based approach to privacy decision support for the sharing of patient data across European platforms. We use ontologies to model the required domain and context information about data sharing and privacy requirements. In addition, we use a set of Semantic Web Rule Language rules to reason about legal privacy requirements that are applicable to a specific context of data disclosure. We make the complete set invocable through the use of a semantic web application acting as an interactive privacy guideline system can then invoke the full model in order to provide decision support. When asked, the system will generate privacy reports applicable to a specific case of data disclosure described by the user. Also reports showing guidelines per Member State may be obtained. The advantage of this approach lies in the expressiveness and extensibility of the modelling and inference languages adopted and the ability they confer to reason with complex requirements interpreted from high level regulations. However, the system cannot at this stage fully simulate the role of an ethics committee or review board.
21794173	Teaching evidence based medicine literature searching skills to medical students during the clinical years - a protocol for a randomised controlled trial.
BMC Med Educ 20110728 2011
Two of the key steps in evidence based medicine (EBM) are being able to construct a clinical question and effectively search the literature to source relevant information. No evidence currently exists that informs whether such skills should be taught to medical students during their pre-clinical years, or delivered to include both the pre-clinical and clinical years of study. This is an important component of curriculum design as the level of clinical maturity of students can affect their perception of the importance and uptake of EBM principles in practice. A randomised controlled trial will be conducted to identify the effectiveness of delivering a formal workshop in EBM literature searching skills to third year medical students entering their clinical years of study. The primary outcome of EBM competency in literature searching skills will be evaluated using the Fresno tool. This trial will provide novel information on the effectiveness of delivering a formal education workshop in evidence based medicine literature searching skills during the clinical years of study. The result of this study will also identify the impact of teaching EBM literature searching skills to medical students during the clinical years of study.
21680559	Retrieval of diagnostic and treatment studies for clinical use through PubMed and PubMed's Clinical Queries filters.
J Am Med Inform Assoc 20110615 2011 Sep-Oct
Clinical Queries filters were developed to improve the retrieval of high-quality studies in searches on clinical matters. The study objective was to determine the yield of relevant citations and physician satisfaction while searching for diagnostic and treatment studies using the Clinical Queries page of PubMed compared with searching PubMed without these filters. Forty practicing physicians, presented with standardized treatment and diagnosis questions and one question of their choosing, entered search terms which were processed in a random, blinded fashion through PubMed alone and PubMed Clinical Queries. Participants rated search retrievals for applicability to the question at hand and satisfaction. For treatment, the primary outcome of retrieval of relevant articles was not significantly different between the groups, but a higher proportion of articles from the Clinical Queries searches met methodologic criteria (p=0.049), and more articles were published in core internal medicine journals (p=0.056). For diagnosis, the filtered results returned more relevant articles (p=0.031) and fewer irrelevant articles (overall retrieval less, p=0.023); participants needed to screen fewer articles before arriving at the first relevant citation (p&lt;0.05). Relevance was also influenced by content terms used by participants in searching. Participants varied greatly in their search performance. Clinical Queries filtered searches returned more high-quality studies, though the retrieval of relevant articles was only statistically different between the groups for diagnosis questions. Retrieving clinically important research studies from Medline is a challenging task for physicians. Methodological search filters can improve search retrieval.
21846786	Natural language processing: an introduction.
J Am Med Inform Assoc  2011 Sep-Oct
To provide an overview and tutorial of natural language processing (NLP) and modern NLP-system design. This tutorial targets the medical informatics generalist who has limited acquaintance with the principles behind NLP and/or limited knowledge of the current state of the art. We describe the historical evolution of NLP, and summarize common NLP sub-problems in this extensive field. We then provide a synopsis of selected highlights of medical NLP efforts. After providing a brief description of common machine-learning approaches that are being used for diverse NLP sub-problems, we discuss how modern NLP architectures are designed, with a summary of the Apache Foundation's Unstructured Information Management Architecture. We finally consider possible future directions for NLP, and reflect on the possible impact of IBM Watson on the medical field.
21849714	Putting encyclopaedia knowledge into structural form: finite state transducers approach.
J Integr Bioinform 20110818 2011
In biology and functional genomics in particular, understanding the dependence and interplay between different genome and ecological characteristics of organisms is a very challenging problem. There are some public databases which combine this kind of information, but there is still much more information about microbes and other organisms that reside in unstructured and semi-structured documents, such as encyclopaedias. In this paper we present a method for extracting information from semi-structured resources, such as encyclopaedias, based on finite state transducers, consisting of two clearly distinguished phases. The first phase strongly relies on the analysis of the document structure and it is used for locating records of data in the text. The second phase is based on the finite state transducers created for extracting the data, which can be modified so as to achieve the preferred efficiency and it is used for extracting the particular characteristic from the text. We show how the two phase method is applied to the text of the encyclopaedia "Systematic Bacteriology". A fully structured database with genotype and phenotype characteristics of organisms has been created from the encyclopaedia unstructured descriptions.
21419864	A comparison of evaluation metrics for biomedical journals, articles, and websites in terms of sensitivity to topic.
J Biomed Inform 20110317 2011Aug
Evaluating the biomedical literature and health-related websites for quality are challenging information retrieval tasks. Current commonly used methods include impact factor for journals, PubMed's clinical query filters and machine learning-based filter models for articles, and PageRank for websites. Previous work has focused on the average performance of these methods without considering the topic, and it is unknown how performance varies for specific topics or focused searches. Clinicians, researchers, and users should be aware when expected performance is not achieved for specific topics. The present work analyzes the behavior of these methods for a variety of topics. Impact factor, clinical query filters, and PageRank vary widely across different topics while a topic-specific impact factor and machine learning-based filter models are more stable. The results demonstrate that a method may perform excellently on average but struggle when used on a number of narrower topics. Topic-adjusted metrics and other topic robust methods have an advantage in such situations. Users of traditional topic-sensitive metrics should be aware of their limitations.
21212442	Controlled search term vocabularies for finding articles relevant to injury prevention and safety promotion.
Inj. Prev. 20110106 2011Aug
To assess the usefulness of the controlled vocabularies of PubMed/MEDLINE and PsycINFO for finding articles on injury prevention and safety promotion (IPSP) topics and to identify specific indexing problems that can contribute to incomplete retrieval. Professional reference librarians provided search strategies for finding articles relevant to five topics pertaining to the injury prevention field in the two bibliographic databases. The results of implementing these search strategies were compared with the results of a presumptive gold standard-serial textword searches on the same topics. The index terms assigned to the articles that were missed by the librarian strategies were examined. The search products of the librarian-constructed search strategies identified 34-91% of the IPSP-relevant articles that were identified through serial textword searches of the two databases. Specific indexing issues were found to contribute to this loss. Librarians bring expertise to searching, but irregular or incomplete indexing can limit the product of even well-constructed searches for articles on IPSP topics.
21789201	Interoperability between biomedical ontologies through relation expansion, upper-level ontologies and automatic reasoning.
PLoS ONE 20110718 2011
Researchers design ontologies as a means to accurately annotate and integrate experimental data across heterogeneous and disparate data- and knowledge bases. Formal ontologies make the semantics of terms and relations explicit such that automated reasoning can be used to verify the consistency of knowledge. However, many biomedical ontologies do not sufficiently formalize the semantics of their relations and are therefore limited with respect to automated reasoning for large scale data integration and knowledge discovery. We describe a method to improve automated reasoning over biomedical ontologies and identify several thousand contradictory class definitions. Our approach aligns terms in biomedical ontologies with foundational classes in a top-level ontology and formalizes composite relations as class expressions. We describe the semi-automated repair of contradictions and demonstrate expressive queries over interoperable ontologies. Our work forms an important cornerstone for data integration, automatic inference and knowledge discovery based on formal representations of knowledge. Our results and analysis software are available at http://bioonto.de/pmwiki.php/Main/ReasonableOntologies.
21694568	Reproducibility of literature search reporting in medical education reviews.
Acad Med  2011Aug
Medical education literature has been found to lack key components of scientific reporting, including adequate descriptions of literature searches, thus preventing medical educators from replicating and building on previous scholarship. The purpose of this study was to examine the reproducibility of search strategies as reported in medical education literature reviews. The authors searched for and identified literature reviews published in 2009 in Academic Medicine, Teaching and Learning in Medicine, and Medical Education. They searched for citations whose titles included the words "meta-analysis," "systematic literature review," "systematic review," or "literature review," or whose publication type MEDLINE listed as "meta-analysis" or "review." The authors created a checklist to identify key characteristics of literature searches and of literature search reporting within the full text of the reviews. The authors deemed searches reproducible only if the review reported both a search date and Boolean operators. Of the 34 reviews meeting the inclusion criteria, 19 (56%) explicitly described a literature search and mentioned MEDLINE; however, only 14 (41%) also mentioned searches of nonmedical databases. Eighteen reviews (53%) listed search terms, but only 6 (18%) listed Medical Subject Headings, and only 2 (6%) mentioned Boolean operators. Fifteen (44%) noted the use of limits. None of the reviews included reproducible searches. According to this analysis, literature search strategies in medical education reviews are highly variable and generally not reproducible. The authors provide recommendations to facilitate future high-quality, transparent, and reproducible literature searches.
21795237	Perceived threat and corroboration: key factors that improve a predictive model of trust in internet-based health information and advice.
J. Med. Internet Res. 20110727 2011
How do people decide which sites to use when seeking health advice online? We can assume, from related work in e-commerce, that general design factors known to affect trust in the site are important, but in this paper we also address the impact of factors specific to the health domain. The current study aimed to (1) assess the factorial structure of a general measure of Web trust, (2) model how the resultant factors predicted trust in, and readiness to act on, the advice found on health-related websites, and (3) test whether adding variables from social cognition models to capture elements of the response to threatening, online health-risk information enhanced the prediction of these outcomes. Participants were asked to recall a site they had used to search for health-related information and to think of that site when answering an online questionnaire. The questionnaire consisted of a general Web trust questionnaire plus items assessing appraisals of the site, including threat appraisals, information checking, and corroboration. It was promoted on the hungersite.com website. The URL was distributed via Yahoo and local print media. We assessed the factorial structure of the measures using principal components analysis and modeled how well they predicted the outcome measures using structural equation modeling (SEM) with EQS software. We report an analysis of the responses of participants who searched for health advice for themselves (N = 561). Analysis of the general Web trust questionnaire revealed 4 factors: information quality, personalization, impartiality, and credible design. In the final SEM model, information quality and impartiality were direct predictors of trust. However, variables specific to eHealth (perceived threat, coping, and corroboration) added substantially to the ability of the model to predict variance in trust and readiness to act on advice on the site. The final model achieved a satisfactory fit: χ(2) (5) = 10.8 (P = .21), comparative fit index = .99, root mean square error of approximation = .052. The model accounted for 66% of the variance in trust and 49% of the variance in readiness to act on the advice. Adding variables specific to eHealth enhanced the ability of a model of trust to predict trust and readiness to act on advice.
21800638	[University research about overweight, obesity and physical activity--a Belgian paradigm].
Rech Soins Infirm  2011Jun
To focus on the university medical research record in Belgium, retrieved through university search filter in a context of Body Mass Index (BMI), overweight, obesity and physical activity. A generic model of Belgian university search filter was created and applied in MEDLINE (through PubMed). 'Place of publication' and language characteristics were identified. The performance of the university constructed filter was evaluated in two stages: first, by comparing it with the results of a manual search (gold standard; sensitivity, specificity) and second, by calculating agreement percentages. The evaluation of the filter revealed specificity and sensitivity of 100% in the field of BMI, overweight, obesity and physical activity. Globally the agreement percentage was 97.23%. The construction of university filter with high sensitivity and specificity permitted systematic application and rapid retrieval of desired information in MEDLINE (PubMed).
21800983	Knowledge and utilization of the United States National Library of Medicine's biomedical information products and services among African health sciences librarians.
Med Ref Serv Q  2011
The United States National Library of Medicine (NLM) has the largest collection of biomedical information products and services in the world. Little is known of the extent to which librarians in sub-Saharan Africa are aware of and use these resources. The study's aim was to assess knowledge and frequency of use of NLM's biomedical information products and services among African librarians. Forty-three of the 50 delegates at the 11th biannual Congress of the Association of Health Information and Libraries in Africa (AHILA) participated in the study. The findings showed that participants' knowledge of NLM information products and services was low and that there is a need for increased awareness and training in the use of NLM's information products and services in order for users on the African continent to effectively benefit from them.
21800986	An introduction to QR Codes: linking libraries and mobile patrons.
Med Ref Serv Q  2011
QR codes, or "Quick Response" codes, are two-dimensional barcodes that can be scanned by mobile smartphone cameras. These codes can be used to provide fast access to URLs, telephone numbers, and short passages of text. With the rapid adoption of smartphones, librarians are able to use QR codes to promote services and help library users find materials quickly and independently. This article will explain what QR codes are, discuss how they can be used in the library, and describe issues surrounding their use. A list of resources for generating and scanning QR codes is also provided.
21800987	The systematic review team: contributions of the health sciences librarian.
Med Ref Serv Q  2011
While the role of the librarian as an expert searcher in the systematic review process is widely recognized, librarians also can be enlisted to help systematic review teams with other challenges. This article reviews the contributions of librarians to systematic reviews, including communicating methods of the review process, collaboratively formulating the research question and exclusion criteria, formulating the search strategy on a variety of databases, documenting the searches, record keeping, and writing the search methodology. It also discusses challenges encountered such as irregular timelines, providing education, communication, and learning new technologies for record keeping. Rewards include building relationships with researchers, expanding professional expertise, and receiving recognition for contributions to health care outcomes.
21805806	[Searching for and processing professional and scientific data 2/2].
Soins  2011Jun
Innovation combined with the rapid obsolescence of knowledge means nurses constantly have to update the scientific knowledge needed for their decision-making. Assessing current knowledge on a subject requires not only an efficient strategy for searching for documents but also the ability to recognize the value of the content of the selected references.
21752612	A semantic graph-based approach to biomedical summarisation.
Artif Intell Med 20110712 2011Sep
Access to the vast body of research literature that is available in biomedicine and related fields may be improved by automatic summarisation. This paper presents a method for summarising biomedical scientific literature that takes into consideration the characteristics of the domain and the type of documents. To address the problem of identifying salient sentences in biomedical texts, concepts and relations derived from the Unified Medical Language System (UMLS) are arranged to construct a semantic graph that represents the document. A degree-based clustering algorithm is then used to identify different themes or topics within the text. Different heuristics for sentence selection, intended to generate different types of summaries, are tested. A real document case is drawn up to illustrate how the method works. A large-scale evaluation is performed using the recall-oriented understudy for gisting-evaluation (ROUGE) metrics. The results are compared with those achieved by three well-known summarisers (two research prototypes and a commercial application) and two baselines. Our method significantly outperforms all summarisers and baselines. The best of our heuristics achieves an improvement in performance of almost 7.7 percentage units in the ROUGE-1 score over the LexRank summariser (0.7862 versus 0.7302). A qualitative analysis of the summaries also shows that our method succeeds in identifying sentences that cover the main topic of the document and also considers other secondary or "satellite" information that might be relevant to the user. The method proposed is proved to be an efficient approach to biomedical literature summarisation, which confirms that the use of concepts rather than terms can be very useful in automatic summarisation, especially when dealing with highly specialised domains.
21814662	Data extraction and feedback - does this lead to change in patient care?
Aust Fam Physician  2011Aug
BACKGROUND Computers enable general practitioners to collate clinical data within their practices. The improvements that this can make to clinical care remain the subject of enquiry. OBJECTIVE Does the analysis of clinical data for the purpose of instigating quality improvement strategies in general practice, with support from a local division of general practice, lead to positive changes in measures of care after 12 months? DISCUSSION This study demonstrated that, in this setting, the collection and analysis of clinical data, with support from a division of general practice, led to modest increases in the recording of information rather than improvements in clinical outcomes.
21815099	Microbial genome analysis and comparisons: Web-based protocols and resources.
Methods Mol. Biol.  2011
Fully annotated genome sequences of many microorganisms are publicly available as a resource. However, in-depth analysis of these genomes using specialized tools is required to derive meaningful information. We describe here the utility of three powerful publicly available genome databases and analysis tools. Protocols outlined here are particularly useful for performing pairwise genome comparisons between closely related microorganisms to identify similarities and unique features, for example to identify genes specific to a pathogenic strain of Escherichia coli compared to a nonpathogenic strain.
21747455	Sensor noise informed representation of hyperspectral data, with benefits for image storage and processing.
Opt Express  2011Jul4
Many types of hyperspectral image processing can benefit from knowledge of noise levels in the data, which can be derived from sensor physics. Surprisingly, such information is rarely provided or exploited. Usually, the image data are represented as radiance values, but this representation can lead to suboptimal results, for example in spectral difference metrics. Also, radiance data do not provide an appropriate baseline for calculation of image compression ratios. This paper defines two alternative representations of hyperspectral image data, aiming to make sensor noise accessible to image processing. A "corrected raw data" representation is proportional to the photoelectron count and can be processed like radiance data, while also offering simpler estimation of noise and somewhat more compact storage. A variance-stabilized representation is obtained by square-root transformation of the photodetector signal to make the noise signal-independent and constant across all bands while also reducing data volume by almost a factor 2. Then the data size is comparable to the fundamental information capacity of the sensor, giving a more appropriate measure of uncompressed data size. It is noted that the variance-stabilized representation has parallels in other fields of imaging. The alternative data representations provide an opportunity to reformulate hyperspectral processing algorithms to take actual sensor noise into account.
21747499	Holographic diversity interferometry for optical storage.
Opt Express  2011Jul4
This study proposes holographic diversity interferometry (HDI), a system that combines information from spatially dispersed plural image sensors to reconstruct complex amplitude distributions of light signals. HDI can be used to generate four holographic interference fringes having different phases, thus enabling optical phase detection in a single measurement. Unlike conventional phase-shifting digital holography, this system does not require piezoelectric elements and phase shift arrays. In order to confirm the effectiveness of HDI, we generated optical signals having multilevel phases and amplitudes by using two SLMs and performed an experiment for detection and demodulation with HDI.
21664156	Potential of fluid-attenuated inversion recovery (FLAIR) in identification of temporomandibular joint effusion compared with T2-weighted images.
Oral Surg Oral Med Oral Pathol Oral Radiol Endod 20110612 2011Aug
The purpose of this study was to determine the potential of fluid-attenuated inversion recovery (FLAIR) sequence images in the identification of joint effusion (JE) compared with T2-weighted images. A total of 31 joints (28 patients) with JE were investigated by magnetic resonance imaging (MRI). Regions of interest were placed over JE, cerebrospinal fluid (CSF), and gray matter (GM) on T2-weighted and FLAIR images and their signal intensities compared. The signal intensity ratios (SIRs) of JE and CSF were calculated with GM as the reference point. The Pearson product-moment correlation coefficient was used for the statistical analysis. The SIR of JE showed a strong correlation between T2-weighted and FLAIR images. However, no correlation was observed for CSF. The average suppression ratio for JE was lower than that for CSF. MRI using FLAIR sequences revealed that JE was not just water content, but a fluid accumulation containing elements such as protein. Further studies are needed, and FLAIR sequences could be useful for the diagnosis of pain and symptoms of the temporomandibular joint (TMJ).
21750706	GenExp: an interactive web-based genomic DAS client with client-side data rendering.
PLoS ONE 20110705 2011
The Distributed Annotation System (DAS) offers a standard protocol for sharing and integrating annotations on biological sequences. There are more than 1000 DAS sources available and the number is steadily increasing. Clients are an essential part of the DAS system and integrate data from several independent sources in order to create a useful representation to the user. While web-based DAS clients exist, most of them do not have direct interaction capabilities such as dragging and zooming with the mouse. Here we present GenExp, a web based and fully interactive visual DAS client. GenExp is a genome oriented DAS client capable of creating informative representations of genomic data zooming out from base level to complete chromosomes. It proposes a novel approach to genomic data rendering and uses the latest HTML5 web technologies to create the data representation inside the client browser. Thanks to client-side rendering most position changes do not need a network request to the server and so responses to zooming and panning are almost immediate. In GenExp it is possible to explore the genome intuitively moving it with the mouse just like geographical map applications. Additionally, in GenExp it is possible to have more than one data viewer at the same time and to save the current state of the application to revisit it later on. GenExp is a new interactive web-based client for DAS and addresses some of the short-comings of the existing clients. It uses client-side data rendering techniques resulting in easier genome browsing and exploration. GenExp is open source under the GPL license and it is freely available at http://gralggen.lsi.upc.edu/recerca/genexp.
21750734	Integrated management and visualization of electronic tag data with Tagbase.
PLoS ONE 20110705 2011
Electronic tags have been used widely for more than a decade in studies of diverse marine species. However, despite significant investment in tagging programs and hardware, data management aspects have received insufficient attention, leaving researchers without a comprehensive toolset to manage their data easily. The growing volume of these data holdings, the large diversity of tag types and data formats, and the general lack of data management resources are not only complicating integration and synthesis of electronic tagging data in support of resource management applications but potentially threatening the integrity and longer-term access to these valuable datasets. To address this critical gap, Tagbase has been developed as a well-rounded, yet accessible data management solution for electronic tagging applications. It is based on a unified relational model that accommodates a suite of manufacturer tag data formats in addition to deployment metadata and reprocessed geopositions. Tagbase includes an integrated set of tools for importing tag datasets into the system effortlessly, and provides reporting utilities to interactively view standard outputs in graphical and tabular form. Data from the system can also be easily exported or dynamically coupled to GIS and other analysis packages. Tagbase is scalable and has been ported to a range of database management systems to support the needs of the tagging community, from individual investigators to large scale tagging programs. Tagbase represents a mature initiative with users at several institutions involved in marine electronic tagging research.
21761651	Optimal data-driven sparse parameterization of diffeomorphisms for population analysis.
Inf Process Med Imaging  2011
In this paper, we propose a novel approach for intensity based atlas construction from a population of anatomical images, that estimates not only a template representative image but also a common optimal parameterization of the anatomical variations evident in the population. First, we introduce a discrete parameterization of large diffeomorphic deformations based on a finite set of control points, so that deformations are characterized by a low dimensional geometric descriptor. Second, we optimally estimate the position of the control points in the template image domain. As a consequence, control points move to where they are needed most to capture the geometric variability evident in the population. Third, the optimal number of control points is estimated by using a log - L1 sparsity penalty. The estimation of the template image, the template-to-subject mappings and their optimal parameterization is done via a single gradient descent optimization, and at the same computational cost as independent template-to-subject registrations. We present results that show that the anatomical variability of the population can be encoded efficiently with these compact and adapted geometric descriptors.
21761652	Learning an atlas of a cognitive process in its functional geometry.
Inf Process Med Imaging  2011
In this paper we construct an atlas that captures functional characteristics of a cognitive process from a population of individuals. The functional connectivity is encoded in a low-dimensional embedding space derived from a diffusion process on a graph that represents correlations of fMRI time courses. The atlas is represented by a common prior distribution for the embedded fMRI signals of all subjects. The atlas is not directly coupled to the anatomical space, and can represent functional networks that are variable in their spatial distribution. We derive an algorithm for fitting this generative model to the observed data in a population. Our results in a language fMRI study demonstrate that the method identifies coherent and functionally equivalent regions across subjects.
21761668	Personalization of pictorial structures for anatomical landmark localization.
Inf Process Med Imaging  2011
We propose a method for accurately localizing anatomical landmarks in 3D medical volumes based on dense matching of parts-based graphical models. Our novel approach replaces population mean models by jointly leveraging weighted combinations of labeled exemplars (both spatial and appearance) to obtain personalized models for the localization of arbitrary landmarks in upper body images. We compare the method to a baseline population-mean graphical model and atlas-based deformable registration optimized for CT-CT registration, by measuring the localization accuracy of 22 anatomical landmarks in clinical 3D CT volumes, using a database of 83 lung cancer patients. The average mean localization error across all landmarks is 2.35 voxels. Our proposed method outperforms deformable registration by 73%, 93% for the most improved landmark. Compared to the baseline population-mean graphical model, the average improvement of localization accuracy is 32%; 67% for the most improved landmark.
21761677	Large deformation diffeomorphic metric mapping of orientation distribution functions.
Inf Process Med Imaging  2011
We propose a novel large deformation diffeomorphic registration algorithm to align high angular resolution diffusion images (HARDI) characterized by Orientation Distribution Functions (ODF). Our proposed algorithm seeks an optimal diffeomorphism of large deformation between two ODF fields in a spatial volume domain and at the same time, locally reorients an ODF in a manner such that it remains consistent with the surrounding anatomical structure. We first extend ODFs traditionally defined in a unit sphere to a generalized ODF defined in R3. This makes it easy for an affine transformation as well as a diffeomorphic group action to be applied on the ODF. We then construct a Riemannian space of the generalized ODFs and incorporate its Riemannian metric for the similarity of ODFs into a variational problem defined under the large deformation diffeomorphic metric mapping (LDDMM) framework. We finally derive the gradient of the cost function in both Riemannian spaces of diffeomorphisms and the generalized ODFs, and present its numerical implementation. Both synthetic and real brain HARDI data are used to illustrate the performance of our registration algorithm.
21761679	Dissimilarity-based classification of anatomical tree structures.
Inf Process Med Imaging  2011
A novel method for classification of abnormality in anatomical tree structures is presented. A tree is classified based on direct comparisons with other trees in a dissimilarity-based classification scheme. The pair-wise dissimilarity measure between two trees is based on a linear assignment between the branch feature vectors representing those trees. Hereby, localized information in the branches is collectively used in classification and variations in feature values across the tree are taken into account. An approximate anatomical correspondence between matched branches can be achieved by including anatomical features in the branch feature vectors. The proposed approach is applied to classify airway trees in computed tomography images of subjects with and without chronic obstructive pulmonary disease (COPD). Using the wall area percentage (WA%), a common measure of airway abnormality in COPD, as well as anatomical features to characterize each branch, an area under the receiver operating characteristic curve of 0.912 is achieved. This is significantly better than computing the average WA%.
21761684	Detection of crossing white matter fibers with high-order tensors and rank-k decompositions.
Inf Process Med Imaging  2011
Fundamental to high angular resolution diffusion imaging (HARDI), is the estimation of a positive-semidefinite orientation distribution function (ODF) and extracting the diffusion properties (e.g., fiber directions). In this work we show that these two goals can be achieved efficiently by using homogeneous polynomials to represent the ODF in the spherical deconvolution approach, as was proposed in the Cartesian Tensor-ODF (CT-ODF) formulation. Based on this formulation we first suggest an estimation method for positive-semidefinite ODF by solving a linear programming problem that does not require special parameterization of the ODF. We also propose a rank-k tensor decomposition, known as CP decomposition, to extract the fibers information from the estimated ODF. We show that this decomposition is superior to the fiber direction estimation via ODF maxima detection as it enables one to reach the full fiber separation resolution of the estimation technique. We assess the accuracy of this new framework by applying it to synthetic and experimentally obtained HARDI data.
21761685	Nonnegative factorization of diffusion tensor images and its applications.
Inf Process Med Imaging  2011
This paper proposes a novel method for computing linear basis images from tensor-valued image data. As a generalization of the nonnegative matrix factorization, the proposed method aims to approximate a collection of diffusion tensor images using nonnegative linear combinations of basis tensor images. An efficient iterative optimization algorithm is proposed to solve this factorization problem. We present two applications: the DTI segmentation problem and a novel approach to discover informative and common parts in a collection of diffusion tensor images. The proposed method has been validated using both synthetic and real data, and experimental results have shown that it offers a competitive alternative to current state-of-the-arts in terms of accuracy and efficiency.
21697129	JSBML: a flexible Java library for working with SBML.
Bioinformatics 20110622 2011Aug1
The specifications of the Systems Biology Markup Language (SBML) define standards for storing and exchanging computer models of biological processes in text files. In order to perform model simulations, graphical visualizations and other software manipulations, an in-memory representation of SBML is required. We developed JSBML for this purpose. In contrast to prior implementations of SBML APIs, JSBML has been designed from the ground up for the Java programming language, and can therefore be used on all platforms supported by a Java Runtime Environment. This offers important benefits for Java users, including the ability to distribute software as Java Web Start applications. JSBML supports all SBML Levels and Versions through Level 3 Version 1, and we have strived to maintain the highest possible degree of compatibility with the popular library libSBML. JSBML also supports modules that can facilitate the development of plugins for end user applications, as well as ease migration from a libSBML-based backend. Source code, binaries and documentation for JSBML can be freely obtained under the terms of the LGPL 2.1 from the website http://sbml.org/Software/JSBML.
20706859	Automatic monitoring of localized skin dose with fluoroscopic and interventional procedures.
J Digit Imaging  2011Aug
This software tool locates and computes the intensity of radiation skin dose resulting from fluoroscopically guided interventional procedures. It is comprised of multiple modules. Using standardized body specific geometric values, a software module defines a set of male and female patients arbitarily positioned on a fluoroscopy table. Simulated X-ray angiographic (XA) equipment includes XRII and digital detectors with or without bi-plane configurations and left and right facing tables. Skin dose estimates are localized by computing the exposure to each 0.01 × 0.01 m(2) on the surface of a patient irradiated by the X-ray beam. Digital Imaging and Communications in Medicine (DICOM) Structured Report Dose data sent to a modular dosimetry database automatically extracts the 11 XA tags necessary for peak skin dose computation. Skin dose calculation software uses these tags (gantry angles, air kerma at the patient entrance reference point, etc.) and applies appropriate corrections of exposure and beam location based on each irradiation event (fluoroscopy and acquistions). A physicist screen records the initial validation of the accuracy, patient and equipment geometry, DICOM compliance, exposure output calibration, backscatter factor, and table and pad attenuation once per system. A technologist screen specifies patient positioning, patient height and weight, and physician user. Peak skin dose is computed and localized; additionally, fluoroscopy duration and kerma area product values are electronically recorded and sent to the XA database. This approach fully addresses current limitations in meeting accreditation criteria, eliminates the need for paper logs at a XA console, and provides a method where automated ALARA montoring is possible including email and pager alerts.
20844917	Managing biomedical image metadata for search and retrieval of similar images.
J Digit Imaging  2011Aug
Radiology images are generally disconnected from the metadata describing their contents, such as imaging observations ("semantic" metadata), which are usually described in text reports that are not directly linked to the images. We developed a system, the Biomedical Image Metadata Manager (BIMM) to (1) address the problem of managing biomedical image metadata and (2) facilitate the retrieval of similar images using semantic feature metadata. Our approach allows radiologists, researchers, and students to take advantage of the vast and growing repositories of medical image data by explicitly linking images to their associated metadata in a relational database that is globally accessible through a Web application. BIMM receives input in the form of standard-based metadata files using Web service and parses and stores the metadata in a relational database allowing efficient data query and maintenance capabilities. Upon querying BIMM for images, 2D regions of interest (ROIs) stored as metadata are automatically rendered onto preview images included in search results. The system's "match observations" function retrieves images with similar ROIs based on specific semantic features describing imaging observation characteristics (IOCs). We demonstrate that the system, using IOCs alone, can accurately retrieve images with diagnoses matching the query images, and we evaluate its performance on a set of annotated liver lesion images. BIMM has several potential applications, e.g., computer-aided detection and diagnosis, content-based image retrieval, automating medical analysis protocols, and gathering population statistics like disease prevalences. The system provides a framework for decision support systems, potentially improving their diagnostic accuracy and selection of appropriate therapies.
21768655	Web2.0 paves new ways for collaborative and exploratory analysis of chemical compounds in spectrometry data.
J Integr Bioinform 20110718 2011
In nowadays life science projects, sharing data and data interpretation is becoming increasingly important. This considerably calls for novel information technology approaches, which enable the integration of expert knowledge from different disciplines in combination with advanced data analysis facilities in a collaborative manner. Since the recent development of web technologies offers scientific communities new ways for cooperation and communication, we propose a fully web-based software approach for the collaborative analysis of bioimage data and demonstrate the applicability of Web2.0 techniques to ion mobility spectrometry image data. Our approach allows collaborating experts to easily share, explore and discuss complex image data without any installation of software packages. Scientists only need a username and a password to get access to our system and can directly start exploring and analyzing their data.
21661046	X-PROP: a fast and robust diffusion-weighted propeller technique.
Magn Reson Med 20110609 2011Aug
Diffusion-weighted imaging (DWI) has shown great benefits in clinical MR exams. However, current DWI techniques have shortcomings of sensitivity to distortion or long scan times or combinations of the two. Diffusion-weighted echo-planar imaging (EPI) is fast but suffers from severe geometric distortion. Periodically rotated overlapping parallel lines with enhanced reconstruction diffusion-weighted imaging (PROPELLER DWI) is free of geometric distortion, but the scan time is usually long and imposes high Specific Absorption Rate (SAR) especially at high fields. TurboPROP was proposed to accelerate the scan by combining signal from gradient echoes, but the off-resonance artifacts from gradient echoes can still degrade the image quality. In this study, a new method called X-PROP is presented. Similar to TurboPROP, it uses gradient echoes to reduce the scan time. By separating the gradient and spin echoes into individual blades and removing the off-resonance phase, the off-resonance artifacts in X-PROP are minimized. Special reconstruction processes are applied on these blades to correct for the motion artifacts. In vivo results show its advantages over EPI, PROPELLER DWI, and TurboPROP techniques.
21772260	Retrieval, alignment, and clustering of computational models based on semantic annotations.
Mol. Syst. Biol. 20110719 2011
The exploding number of computational models produced by Systems Biologists over the last years is an invitation to structure and exploit this new wealth of information. Researchers would like to trace models relevant to specific scientific questions, to explore their biological content, to align and combine them, and to match them with experimental data. To automate these processes, it is essential to consider semantic annotations, which describe their biological meaning. As a prerequisite for a wide range of computational methods, we propose general and flexible similarity measures for Systems Biology models computed from semantic annotations. By using these measures and a large extensible ontology, we implement a platform that can retrieve, cluster, and align Systems Biology models and experimental data sets. At present, its major application is the search for relevant models in the BioModels Database, starting from initial models, data sets, or lists of biological concepts. Beyond similarity searches, the representation of models by semantic feature vectors may pave the way for visualisation, exploration, and statistical analysis of large collections of models and corresponding data.
21774164	Encryption and the loss of patient data.
J Policy Anal Manage  2011Summer
Fast-paced IT advances have made it increasingly possible and useful for firms to collect data on their customers on an unprecedented scale. One downside of this is that firms can experience negative publicity and financial damage if their data are breached. This is particularly the case in the medical sector, where we find empirical evidence that increased digitization of patient data is associated with more data breaches. The encryption of customer data is often presented as a potential solution, because encryption acts as a disincentive for potential malicious hackers, and can minimize the risk of breached data being put to malicious use. However, encryption both requires careful data management policies to be successful and does not ward off the insider threat. Indeed, we find no empirical evidence of a decrease in publicized instances of data loss associated with the use of encryption. Instead, there are actually increases in the cases of publicized data loss due to internal fraud or loss of computer equipment.
21285952	High efficiency coherent optical memory with warm rubidium vapour.
Nat Commun 20110201 2011
By harnessing aspects of quantum mechanics, communication and information processing could be radically transformed. Promising forms of quantum information technology include optical quantum cryptographic systems and computing using photons for quantum logic operations. As with current information processing systems, some form of memory will be required. Quantum repeaters, which are required for long distance quantum key distribution, require quantum optical memory as do deterministic logic gates for optical quantum computing. Here, we present results from a coherent optical memory based on warm rubidium vapour and show 87% efficient recall of light pulses, the highest efficiency measured to date for any coherent optical memory suitable for quantum information applications. We also show storage and recall of up to 20 pulses from our system. These results show that simple warm atomic vapour systems have clear potential as a platform for quantum memory.
21776801	Real time 4D IMRT treatment planning based on a dynamic virtual patient model: proof of concept.
Med Phys  2011May
To develop a novel four-dimensional (4D) intensity modulated radiation therapy (IMRT) treatment planning methodology based on dynamic virtual patient models. The 4D model-based planning (4DMP) is a predictive tracking method which consists of two main steps: (1) predicting the 3D deformable motion of the target and critical structures as a function of time during treatment delivery; (2) adjusting the delivery beam apertures formed by the dynamic multi-leaf collimators (DMLC) to account for the motion. The key feature of 4DMP is the application of a dynamic virtual patient model in motion prediction, treatment beam adjustment, and dose calculation. A lung case was chosen to demonstrate the feasibility of the 4DMP. For the lung case, a dynamic virtual patient model (4D model) was first developed based on the patient's 4DCT images. The 4D model was capable of simulating respiratory motion of different patterns. A model-based registration method was then applied to convert the 4D model into a set of deformation maps and 4DCT images for dosimetric purposes. Based on the 4D model, 4DMP treatment plans with different respiratory motion scenarios were developed. The quality of 4DMP plans was then compared with two other commonly used 4D planning methods: maximum intensity projection (MIP) and planning on individual phases (IP). Under regular periodic motion, 4DMP offered similar target coverage as MIP with much better normal tissue sparing. At breathing amplitude of 2 cm, the lung V20 was 23.9% for a MIP plan and 16.7% for a 4DMP plan. The plan quality was comparable between 4DMP and IP: PTV V97 was 93.8% for the IP plan and 93.6% for the 4DMP plan. Lung V20 of the 4DMP plan was 2.1% lower than that of the IP plan and Dmax to cord was 2.2 Gy higher. Under a real time irregular breathing pattern, 4DMP had the best plan quality. PTV V97 was 90.4% for a MIP plan, 88.6% for an IP plan and 94.1% for a 4DMP plan. Lung V20 was 20.1% for the MIP plan, 17.8% for the IP plan and 17.5% for the 4DMP plan. The deliverability of the real time 4DMP plan was proved by calculating the maximum leaf speed of the DMLC. The 4D model-based planning, which applies dynamic virtual patient models in IMRT treatment planning, can account for the real time deformable motion of the tumor under different breathing conditions. Under regular motion, the quality of 4DMP plans was comparable with IP and superior to MIP. Under realistic motion in which breathing amplitude and period change, 4DMP gave the best plan quality of the three 4D treatment planning techniques.
21779994	In silico knowledge and content tracking.
Methods Mol. Biol.  2011
This chapter gives a brief overview of text-mining techniques to extract knowledge from large text collections. It describes the basis pipeline of how to come from text to relationships between biological concepts and the problems that are encountered at each step in the pipeline. We first explain how words in text are recognized as concepts. Second, concepts are associated with each other using 2×2 contingency tables and test statistics. Third, we explain that it is possible to extract indirect links between concepts using the direct links taken from 2×2 table analyses. This we call implicit information extraction. Fourth, the validation techniques to evaluate a text-mining system such as ROC curves and retrospective studies are discussed. We conclude by examining how text information can be combined with other non-textual data sources such as microarray expression data and what the future directions are for text-mining within the Internet.
21779995	Application of gene ontology to gene identification.
Methods Mol. Biol.  2011
Candidate gene identification deals with associating genes to underlying biological phenomena, such as diseases and specific disorders. It has been shown that classes of diseases with similar phenotypes are caused by functionally related genes. Currently, a fair amount of knowledge about the functional characterization can be found across several public databases; however, functional descriptors can be ambiguous, domain specific, and context dependent. In order to cope with these issues, the Gene Ontology (GO) project developed a bio-ontology of broad scope and wide applicability. Thus, the structured and controlled vocabulary of terms provided by the GO project describing the biological roles of gene products can be very helpful in candidate gene identification approaches. The method presented here uses GO annotation data in order to identify the most meaningful functional aspects occurring in a given set of related gene products. The method measures this meaningfulness by calculating an e-value based on the frequency of annotation of each GO term in the set of gene products versus the total frequency of annotation. Then after selecting a GO term related to the underlying biological phenomena being studied, the method uses semantic similarity to rank the given gene products that are annotated to the term. This enables the user to further narrow down the list of gene products and identify those that are more likely of interest.
21780002	In silico prediction of transcriptional factor-binding sites.
Methods Mol. Biol.  2011
The recognition of transcription factor binding sites (TFBSs) is the first step on the way to deciphering the DNA regulatory code. A large variety of computational approaches and corresponding in silico tools for TFBS recognition are available, each having their own advantages and shortcomings. This chapter provides a brief tutorial to assist end users in the application of these tools for functional characterization of genes.
21411285	Optimized retrieval of primary care clinical prediction rules from MEDLINE to establish a Web-based register.
J Clin Epidemiol 20110316 2011Aug
Identifying clinical prediction rules (CPRs) for primary care from electronic databases is difficult. This study aims to identify a search filter to optimize retrieval of these to establish a register of CPRs for the Cochrane Primary Health Care field. Thirty primary care journals were manually searched for CPRs. This was compared with electronic search filters using alternative methodologies: (1) textword searching; (2) proximity searching; (3) inclusion terms using specific phrases and truncation; (4) exclusion terms; and (5) combinations of methodologies. We manually searched 6,344 articles, revealing 41 CPRs. Across the 45 search filters, sensitivities ranged from 12% to 98%, whereas specificities ranged from 43% to 100%. There was generally a trade-off between the sensitivity and specificity of each filter (i.e., the number of CPRs and total number of articles retrieved). Combining textword searching with the inclusion terms (using specific phrases) resulted in the highest sensitivity (98%) but lower specificity (59%) than other methods. The associated precision (2%) and accuracy (60%) were also low. The novel use of combining textword searching with inclusion terms was considered the most appropriate for updating a register of primary care CPRs where sensitivity has to be optimized.
21712355	Enriching the trustworthiness of health-related web pages.
Health Informatics J  2011Jun
We present an experimental mechanism for enriching web content with quality metadata. This mechanism is based on a simple and well-known initiative in the field of the health-related web, the HONcode. The Resource Description Framework (RDF) format and the Dublin Core Metadata Element Set were used to formalize these metadata. The model of trust proposed is based on a quality model for health-related web pages that has been tested in practice over a period of thirteen years. Our model has been explored in the context of a project to develop a research tool that automatically detects the occurrence of quality criteria in health-related web pages.
21712356	Developing tools and resources for the biomedical domain of the Greek language.
Health Informatics J  2011Jun
This paper presents the design and implementation of terminological and specialized textual resources that were produced in the framework of the Greek research project "IATROLEXI". The aim of the project was to create the critical infrastructure for the Greek language, i.e. linguistic resources and tools for use in high level Natural Language Processing (NLP) applications in the domain of biomedicine. The project was built upon existing resources developed by the project partners and further enhanced within its framework, i.e. a Greek morphological lexicon of about 100,000 words, and language processing tools such as a lemmatiser and a morphosyntactic tagger. Christos Tsalidis, Additionally, it developed new assets, such as a specialized corpus of biomedical texts and an ontology of medical terminology.
21712357	A medical ontology for intelligent web-based skin lesions image retrieval.
Health Informatics J  2011Jun
Researchers have applied increasing efforts towards providing formal computational frameworks to consolidate the plethora of concepts and relations used in the medical domain. In the domain of skin related diseases, the variability of semantic features contained within digital skin images is a major barrier to the medical understanding of the symptoms and development of early skin cancers. The desideratum of making these standards machine-readable has led to their formalization in ontologies. In this work, in an attempt to enhance an existing Core Ontology for skin lesion images, hand-coded from image features, high quality images were analyzed by an autonomous ontology creation engine. We show that by exploiting agglomerative clustering methods with distance criteria upon the existing ontological structure, the original domain model could be enhanced with new instances, attributes and even relations, thus allowing for better classification and retrieval of skin lesion categories from the web.
21716507	Fabrication of phase-change Ge2Sb2Te5 nano-rings.
Opt Express  2011Jun20
Phase-change material Ge2Sb2Te5 rings with nanometer-scale thickness have been fabricated using the photo-thermal effect of a focused laser beam followed by differential chemical etching. Laser irradiation conditions and etching process parameters are varied to control the geometric characteristics of the rings. We demonstrate the possibility of arranging the rings in specific geometric patterns, and also their release from the original substrate.
21721252	[Knowledge-based technologies in proteomics].
Bioorg. Khim.  2011 Mar-Apr
Proteomic technologies enable to identify thousands of proteins in biological samples. These data require appropriate means for storage, dissemination and analytical processing to decipher the new knowledge. Automatic processing of high-efficient experiment results is powered by the controlled vocabularies, such as Medical Subjects Headings and GeneOntology. While ontology and vocabularies undergo constant evolution, it is necessary to provide the centralized storage of proteomic data for further revision in accordance with the updated knowledge domain. Proteomic repositories like PRIDE, The Global Proteome Machine, PeptideAtlas etc. are available to harbor the wealth of mass spectral data and appropriate protein identifications. The existing repositories facilitate the development of knowledge extraction technologies to compare the list of identified proteins with the GeneOntology annotations, Medical Subjects Headings, metabolic and regulatory pathways. This paper reviews modern analytical tools that exploit the knowledge-based technologies for proteome research.
21450907	Open access, readership, citations: a randomized controlled trial of scientific journal publishing.
FASEB J. 20110330 2011Jul
Does free access to journal articles result in greater diffusion of scientific knowledge? Using a randomized controlled trial of open access publishing, involving 36 participating journals in the sciences, social sciences, and humanities, we report on the effects of free access on article downloads and citations. Articles placed in the open access condition (n=712) received significantly more downloads and reached a broader audience within the first year, yet were cited no more frequently, nor earlier, than subscription-access control articles (n=2533) within 3 yr. These results may be explained by social stratification, a process that concentrates scientific authors at a small number of elite research universities with excellent access to the scientific literature. The real beneficiaries of open access publishing may not be the research community but communities of practice that consume, but rarely contribute to, the corpus of literature.
21605356	Extracting scientific articles from a large digital archive: BioStor and the Biodiversity Heritage Library.
BMC Bioinformatics 20110523 2011
The Biodiversity Heritage Library (BHL) is a large digital archive of legacy biological literature, comprising over 31 million pages scanned from books, monographs, and journals. During the digitisation process basic metadata about the scanned items is recorded, but not article-level metadata. Given that the article is the standard unit of citation, this makes it difficult to locate cited literature in BHL. Adding the ability to easily find articles in BHL would greatly enhance the value of the archive. A service was developed to locate articles in BHL based on matching article metadata to BHL metadata using approximate string matching, regular expressions, and string alignment. This article locating service is exposed as a standard OpenURL resolver on the BioStor web site http://biostor.org/openurl/. This resolver can be used on the web, or called by bibliographic tools that support OpenURL. BioStor provides tools for extracting, annotating, and visualising articles from the Biodiversity Heritage Library. BioStor is available from http://biostor.org/.
21726696	Technological updates in dental photography.
Dent. Clin. North Am. 20110408 2011Jul
Digital photography is a constantly evolving medium that can be used in dentistry for a number of applications including documentation and patient education. In the past 5 years, it has become standard professional practice for photographers to shoot in raw format, organize and edit in Adobe Photoshop Lightroom, and archive files using portable hard drives and off-site storage. Concurrently, cameras have increased resolution, improved antidust technology, and added versatile flash accessories for macro imaging. Adopting professional photographic practices and taking advantage of technological developments in a dental practice can be an invaluable tool in education and documentation.
21728772	Challenges in archiving electronic bioanalytical data supporting GLP studies in a CRO.
Bioanalysis  2011Jul
The purpose of this article is to articulate the fundamental issues of archiving electronic GLP data in a CRO environment. CROs struggle to address the absolute requirement of archiving GLP studies electronically. The difficulty of adhering to this is partly due to the wide variety of systems and types of electronic data. Often the end solution to this complicated issue is printing the data at the end of the study and in turn archiving the paper data, foregoing or ignoring the fact that scientific decisions were made while reviewing electronic data. Paper data and electronic data can be different. For example, chromatographic resolution or being able to focus in on detailed integration of chromatograms can significantly change the perspective of the data. While the core goal of archiving according to GLP principles are met, the reality is much different. Businesses purchasing CRO capabilities and regulatory agencies have been quite clear that when it comes to auditing the data that the electronic record is the preferred and often required source of the information for auditing and review purposes. The fact is that paper data do not always provide the flexibility, sensitivity and complete data context that an electronic record can provide. The ability to adhere to compliance standards due to the electronic data cannot be undervalued and by printing the data after all the acquisition, decisions, scientific judgments and review, do a complete disservice to the multidimensional (such as colors for deactivated or changed data, audit trails, 'zooming-in functionality', direct data links and snapshots) data in an electronic system.
21729115	A learning-based approach for performing an in-depth literature search using MEDLINE.
J Clin Pharm Ther 20101026 2011Aug
Exhaustive literature searching is a core requirement for developing guidelines for evidence-based practice. MEDLINE is typically used. Searching requires the user to identify appropriate search terms, called Medical Subject Headings (MeSH) and refine the search to retrieve relevant articles. The objective of this study was to develop and test a learning algorithm for conducting a thorough literature search. A learning algorithm to effectively utilize MeSH terms is presented. This algorithm creates combinations of available MeSH terms from which a search is conducted. The algorithm was applied to search MEDLINE (January 1950 to Janaury 2008) focusing on the impact of pharmaceutical care in HIV-infected patients. The number of relevant articles retrieved from the learning algorithm search was then compared against a static search with a fixed set of keywords implemented by an independent user. The learning algorithm retrieved 1670 articles with six relevant articles identified. The static search retrieved a total of 49 articles, with three being relevant. These three articles were also located from the learning algorithm-based search. WHAT IS KNOWN AND CONCLUSION: Performing a literature search for retrieving evidence-based studies can be a daunting and error-prone process. The introduction of automatic, learning tools for searching is desirable and we present a possible approach.
21406485	Evaluating the 'next generation' of cell salvage--will it make a difference?
Perfusion 20110315 2011Jul
Donor blood supplies are diminishing, becoming more costly and these transfusions lead to higher mortality in cardiac patients. The transfusion risks and the literature highlight the need for an alternative similar to cell salvage to be routinely considered. The Xtra is the first cell saver to be launched since 2001 and will undoubtedly initiate evolution towards the 'next generation' of cell savers. It is also the first to be launched in a new era where the demand for electronic perfusion data management (EPDM) has grown. The user interface (UI) was easy to use. The increased data entry options improved the quality of the recordable data. The integrated data management system (DMS) was comprehensive. Data was easy to manage and enabled central data compilation, which reduces repeated data, the risk of inconsistent data inventory and provides the potential for research and analyses. The haematocrit of the processed blood is a key quality indicator for cell salvage. The comparison of the manufacturer's integrated protocol, Popt, to our team's own protocol showed that Popt delivered a higher haematocrit on its '1st bowl' (59.1% compared to 57.3%) and its 'total process' end product haematocrit was 0.68% higher. The Popt cycle took an average of 330s, whereas our own settings completed in just over 300s. The Xtra is a device which will lead the evolution of 'next generation' cell saver technology. The user interface and data management system provide export options and the ability to record the level of data required for good EPDM. This is essential to 'future proof' cell salvage technology. The manufacturer's integrated protocol achieved a higher end product haematocrit than our perfusion team's best practice. The design of the Xtra is contemporary, but the DMS equips this cell saver for the new era that faces both Perfusion and Cardiac Surgery.
21447497	Automatic detection of omissions in medication lists.
J Am Med Inform Assoc 20110329 2011 Jul-Aug
Evidence suggests that the medication lists of patients are often incomplete and could negatively affect patient outcomes. In this article, the authors propose the application of collaborative filtering methods to the medication reconciliation task. Given a current medication list for a patient, the authors employ collaborative filtering approaches to predict drugs the patient could be taking but are missing from their observed list. The collaborative filtering approach presented in this paper emerges from the insight that an omission in a medication list is analogous to an item a consumer might purchase from a product list. Online retailers use collaborative filtering to recommend relevant products using retrospective purchase data. In this article, the authors argue that patient information in electronic medical records, combined with artificial intelligence methods, can enhance medication reconciliation. The authors formulate the detection of omissions in medication lists as a collaborative filtering problem. Detection of omissions is accomplished using several machine-learning approaches. The effectiveness of these approaches is evaluated using medication data from three long-term care centers. The authors also propose several decision-theoretic extensions to the methodology for incorporating medical knowledge into recommendations. Results show that collaborative filtering identifies the missing drug in the top-10 list about 40-50% of the time and the therapeutic class of the missing drug 50%-65% of the time at the three clinics in this study. Results suggest that collaborative filtering can be a valuable tool for reconciling medication lists, complementing currently recommended process-driven approaches. However, a one-size-fits-all approach is not optimal, and consideration should be given to context (eg, types of patients and drug regimens) and consequence (eg, the impact of omission on outcomes).
21515543	Enabling collaborative research using the Biomedical Informatics Research Network (BIRN).
J Am Med Inform Assoc 20110422 2011 Jul-Aug
As biomedical technology becomes increasingly sophisticated, researchers can probe ever more subtle effects with the added requirement that the investigation of small effects often requires the acquisition of large amounts of data. In biomedicine, these data are often acquired at, and later shared between, multiple sites. There are both technological and sociological hurdles to be overcome for data to be passed between researchers and later made accessible to the larger scientific community. The goal of the Biomedical Informatics Research Network (BIRN) is to address the challenges inherent in biomedical data sharing. BIRN tools are grouped into 'capabilities' and are available in the areas of data management, data security, information integration, and knowledge engineering. BIRN has a user-driven focus and employs a layered architectural approach that promotes reuse of infrastructure. BIRN tools are designed to be modular and therefore can work with pre-existing tools. BIRN users can choose the capabilities most useful for their application, while not having to ensure that their project conforms to a monolithic architecture. BIRN has implemented a new software-based data-sharing infrastructure that has been put to use in many different domains within biomedicine. BIRN is actively involved in outreach to the broader biomedical community to form working partnerships. BIRN's mission is to provide capabilities and services related to data sharing to the biomedical research community. It does this by forming partnerships and solving specific, user-driven problems whose solutions are then available for use by other groups.
21515544	Normalized names for clinical drugs: RxNorm at 6 years.
J Am Med Inform Assoc 20110421 2011 Jul-Aug
In the 6 years since the National Library of Medicine began monthly releases of RxNorm, RxNorm has become a central resource for communicating about clinical drugs and supporting interoperation between drug vocabularies. Built on the idea of a normalized name for a medication at a given level of abstraction, RxNorm provides a set of names and relationships based on 11 different external source vocabularies. The standard model enables decision support to take place for a variety of uses at the appropriate level of abstraction. With the incorporation of National Drug File Reference Terminology (NDF-RT) from the Veterans Administration, even more sophisticated decision support has become possible. While related products such as RxTerms, RxNav, MyMedicationList, and MyRxPad have been recognized as helpful for various uses, tasks such as identifying exactly what is and is not on the market remain a challenge.
21527408	Metrics associated with NIH funding: a high-level view.
J Am Med Inform Assoc 20110427 2011 Jul-Aug
To introduce the availability of grant-to-article linkage data associated with National Institutes of Health (NIH) grants and to perform a high-level analysis of the publication outputs and impacts associated with those grants. Articles were linked to the grants they acknowledge using the grant acknowledgment strings in PubMed using a parsing and matching process as embodied in the NIH Scientific Publication Information Retrieval &amp; Evaluation System system. Additional data from PubMed and citation counts from Scopus were added to the linkage data. The data comprise 2,572,576 records from 1980 to 2009. The data show that synergies between NIH institutes are increasing over time; 29% of current articles acknowledge grants from multiple institutes. The median time lag to publication for a new grant is 3 years. Each grant contributes to approximately 1.7 articles per year, averaged over all grant types. Articles acknowledging US Public Health Service (PHS, which includes NIH) funding are cited twice as much as US-authored articles acknowledging no funding source. Articles acknowledging both PHS funding and a non-US government funding source receive on average 40% more citations that those acknowledging PHS funding sources alone. The US PHS is effective at funding research with a higher-than-average impact. The data are amenable to further and much more detailed analysis.
21672913	Bridging the integration gap between imaging and information systems: a uniform data concept for content-based image retrieval in computer-aided diagnosis.
J Am Med Inform Assoc  2011 Jul-Aug
It is widely accepted that content-based image retrieval (CBIR) can be extremely useful for computer-aided diagnosis (CAD). However, CBIR has not been established in clinical practice yet. As a widely unattended gap of integration, a unified data concept for CBIR-based CAD results and reporting is lacking. Picture archiving and communication systems and the workflow of radiologists must be considered for successful data integration to be achieved. We suggest that CBIR systems applied to CAD should integrate their results in a picture archiving and communication systems environment such as Digital Imaging and Communications in Medicine (DICOM) structured reporting documents. A sample DICOM structured reporting template adaptable to CBIR and an appropriate integration scheme is presented. The proposed CBIR data concept may foster the promulgation of CBIR systems in clinical environments and, thereby, improve the diagnostic process.
21569281	DAS writeback: a collaborative annotation system.
BMC Bioinformatics 20110510 2011
Centralised resources such as GenBank and UniProt are perfect examples of the major international efforts that have been made to integrate and share biological information. However, additional data that adds value to these resources needs a simple and rapid route to public access. The Distributed Annotation System (DAS) provides an adequate environment to integrate genomic and proteomic information from multiple sources, making this information accessible to the community. DAS offers a way to distribute and access information but it does not provide domain experts with the mechanisms to participate in the curation process of the available biological entities and their annotations. We designed and developed a Collaborative Annotation System for proteins called DAS Writeback. DAS writeback is a protocol extension of DAS to provide the functionalities of adding, editing and deleting annotations. We implemented this new specification as extensions of both a DAS server and a DAS client. The architecture was designed with the involvement of the DAS community and it was improved after performing usability experiments emulating a real annotation task. We demonstrate that DAS Writeback is effective, usable and will provide the appropriate environment for the creation and evolution of community protein annotation.
21619655	LabKey Server NAb: a tool for analyzing, visualizing and sharing results from neutralizing antibody assays.
BMC Immunol. 20110527 2011
Multiple types of assays allow sensitive detection of virus-specific neutralizing antibodies. For example, the extent of antibody neutralization of HIV-1, SIV and SHIV can be measured in the TZM-bl cell line through the degree of luciferase reporter gene expression after infection. In the past, neutralization curves and titers for this standard assay have been calculated using an Excel macro. Updating all instances of such a macro with new techniques can be unwieldy and introduce non-uniformity across multi-lab teams. Using Excel also poses challenges in centrally storing, sharing and associating raw data files and results. We present LabKey Server's NAb tool for organizing, analyzing and securely sharing data, files and results for neutralizing antibody (NAb) assays, including the luciferase-based TZM-bl NAb assay. The customizable tool supports high-throughput experiments and includes a graphical plate template designer, allowing researchers to quickly adapt calculations to new plate layouts. The tool calculates the percent neutralization for each serum dilution based on luminescence measurements, fits a range of neutralization curves to titration results and uses these curves to estimate the neutralizing antibody titers for benchmark dilutions. Results, curve visualizations and raw data files are stored in a database and shared through a secure, web-based interface. NAb results can be integrated with other data sources based on sample identifiers. It is simple to make results public after publication by updating folder security settings. Standardized tools for analyzing, archiving and sharing assay results can improve the reproducibility, comparability and reliability of results obtained across many labs. LabKey Server and its NAb tool are freely available as open source software at http://www.labkey.com under the Apache 2.0 license. Many members of the HIV research community can also access the LabKey Server NAb tool without installing the software by using the Atlas Science Portal (https://atlas.scharp.org). Atlas is an installation of LabKey Server.
21545721	GenDrux: a biomedical literature search system to identify gene expression-based drug sensitivity in breast cancer.
BMC Med Inform Decis Mak 20110505 2011
This paper describes the development of a web-based tool, GenDrux, which extracts and presents (over the Internet) information related to the disease-gene-drug nexus. This information is archived from the relevant biomedical literature using automated methods. GenDrux is designed to alleviate the difficulties of manually processing the vast biomedical literature to identify disease-gene-drug relationships. GenDrux will evolve with the literature without additional algorithmic modifications. GenDrux, a pilot system, is developed in the domain of breast cancer and can be accessed at http://www.microarray.uab.edu/drug_gene.pl. GenDrux can be queried based on drug, gene and/or disease name. From over 8,000 relevant abstracts from the biomedical literature related to breast cancer, we have archived a corpus of more than 4,000 articles that depict gene expression-drug activity relationships for breast cancer and related cancers. The archiving process has been automated. The successful development, implementation, and evaluation of this and similar systems when created may provide clinicians with a tool for literature management, clinical decision making, thus setting the platform for personalized therapy in the future.
21586134	A formal MIM specification and tools for the common exchange of MIM diagrams: an XML-Based format, an API, and a validation method.
BMC Bioinformatics 20110517 2011
The Molecular Interaction Map (MIM) notation offers a standard set of symbols and rules on their usage for the depiction of cellular signaling network diagrams. Such diagrams are essential for disseminating biological information in a concise manner. A lack of software tools for the notation restricts wider usage of the notation. Development of software is facilitated by a more detailed specification regarding software requirements than has previously existed for the MIM notation. A formal implementation of the MIM notation was developed based on a core set of previously defined glyphs. This implementation provides a detailed specification of the properties of the elements of the MIM notation. Building upon this specification, a machine-readable format is provided as a standardized mechanism for the storage and exchange of MIM diagrams. This new format is accompanied by a Java-based application programming interface to help software developers to integrate MIM support into software projects. A validation mechanism is also provided to determine whether MIM datasets are in accordance with syntax rules provided by the new specification. The work presented here provides key foundational components to promote software development for the MIM notation. These components will speed up the development of interoperable tools supporting the MIM notation and will aid in the translation of data stored in MIM diagrams to other standardized formats. Several projects utilizing this implementation of the notation are outlined herein. The MIM specification is available as an additional file to this publication. Source code, libraries, documentation, and examples are available at http://discover.nci.nih.gov/mim.
21551150	Creating views on integrated multidomain data.
Bioinformatics 20110506 2011Jul1
Modern data acquisition methods in biology allow the procurement of different types of data in increasing quantity, facilitating a comprehensive view of biological systems. As data are usually gathered and interpreted by separate domain scientists, it is hard to grasp multidomain properties and structures. Consequently, there is a need for the integration of biological data from different sources and of different types in one application, providing various visualization approaches. In this article, methods for the integration and visualization of multimodal biological data are presented. This is achieved based on two graphs representing the meta-relations between biological data and the measurement combinations, respectively. Both graphs are linked and serve as different views of the integrated data with navigation and exploration possibilities. Data can be combined and visualized multifariously, resulting in views of the integrated biological data. http://vanted.ipk-gatersleben.de/hive/. rohn@ipk-gatersleben.de.
21685060	MeSH: a window into full text for document summarization.
Bioinformatics  2011Jul1
Previous research in the biomedical text-mining domain has historically been limited to titles, abstracts and metadata available in MEDLINE records. Recent research initiatives such as TREC Genomics and BioCreAtIvE strongly point to the merits of moving beyond abstracts and into the realm of full texts. Full texts are, however, more expensive to process not only in terms of resources needed but also in terms of accuracy. Since full texts contain embellishments that elaborate, contextualize, contrast, supplement, etc., there is greater risk for false positives. Motivated by this, we explore an approach that offers a compromise between the extremes of abstracts and full texts. Specifically, we create reduced versions of full text documents that contain only important portions. In the long-term, our goal is to explore the use of such summaries for functions such as document retrieval and information extraction. Here, we focus on designing summarization strategies. In particular, we explore the use of MeSH terms, manually assigned to documents by trained annotators, as clues to select important text segments from the full text documents. Our experiments confirm the ability of our approach to pick the important text portions. Using the ROUGE measures for evaluation, we were able to achieve maximum ROUGE-1, ROUGE-2 and ROUGE-SU4 F-scores of 0.4150, 0.1435 and 0.1782, respectively, for our MeSH term-based method versus the maximum baseline scores of 0.3815, 0.1353 and 0.1428, respectively. Using a MeSH profile-based strategy, we were able to achieve maximum ROUGE F-scores of 0.4320, 0.1497 and 0.1887, respectively. Human evaluation of the baselines and our proposed strategies further corroborates the ability of our method to select important sentences from the full texts. sanmitra-bhattacharya@uiowa.edu; padmini-srinivasan@uiowa.edu.
21685064	RINQ: Reference-based Indexing for Network Queries.
Bioinformatics  2011Jul1
We consider the problem of similarity queries in biological network databases. Given a database of networks, similarity query returns all the database networks whose similarity (i.e. alignment score) to a given query network is at least a specified similarity cutoff value. Alignment of two networks is a very costly operation, which makes exhaustive comparison of all the database networks with a query impractical. To tackle this problem, we develop a novel indexing method, named RINQ (Reference-based Indexing for Biological Network Queries). Our method uses a set of reference networks to eliminate a large portion of the database quickly for each query. A reference network is a small biological network. We precompute and store the alignments of all the references with all the database networks. When our database is queried, we align the query network with all the reference networks. Using these alignments, we calculate a lower bound and an approximate upper bound to the alignment score of each database network with the query network. With the help of upper and lower bounds, we eliminate the majority of the database networks without aligning them to the query network. We also quickly identify a small portion of these as guaranteed to be similar to the query. We perform pairwise alignment only for the remaining networks. We also propose a supervised method to pick references that have a large chance of filtering the unpromising database networks. Extensive experimental evaluation suggests that (i) our method reduced the running time of a single query on a database of around 300 networks from over 2 days to only 8 h; (ii) our method outperformed the state of the art method Closure Tree and SAGA by a factor of three or more; and (iii) our method successfully identified statistically and biologically significant relationships across networks and organisms.
21685102	Multi-view methods for protein structure comparison using latent dirichlet allocation.
Bioinformatics  2011Jul1
With rapidly expanding protein structure databases, efficiently retrieving structures similar to a given protein is an important problem. It involves two major issues: (i) effective protein structure representation that captures inherent relationship between fragments and facilitates efficient comparison between the structures and (ii) effective framework to address different retrieval requirements. Recently, researchers proposed vector space model of proteins using bag of fragments representation (FragBag), which corresponds to the basic information retrieval model. In this article, we propose an improved representation of protein structures using latent dirichlet allocation topic model. Another important requirement is to retrieve proteins, whether they are either close or remote homologs. In order to meet diverse objectives, we propose multi-viewpoint based framework that combines multiple representations and retrieval techniques. We compare the proposed representation and retrieval framework on the benchmark dataset developed by Kolodny and co-workers. The results indicate that the proposed techniques outperform state-of-the-art methods. http://www.cse.iitm.ac.in/~ashishvt/research/protein-lda/. ashishvt@cse.iitm.ac.in.
21687575	Craniux: a LabVIEW-based modular software framework for brain-machine interface research.
Comput Intell Neurosci 20110407 2011
This paper presents "Craniux," an open-access, open-source software framework for brain-machine interface (BMI) research. Developed in LabVIEW, a high-level graphical programming environment, Craniux offers both out-of-the-box functionality and a modular BMI software framework that is easily extendable. Specifically, it allows researchers to take advantage of multiple features inherent to the LabVIEW environment for on-the-fly data visualization, parallel processing, multithreading, and data saving. This paper introduces the basic features and system architecture of Craniux and describes the validation of the system under real-time BMI operation using simulated and real electrocorticographic (ECoG) signals. Our results indicate that Craniux is able to operate consistently in real time, enabling a seamless work flow to achieve brain control of cursor movement. The Craniux software framework is made available to the scientific research community to provide a LabVIEW-based BMI software platform for future BMI research and development.
21685585	Extracting clinical information to support medical decision based on standards.
Stud Health Technol Inform  2011
The paper presents a method connecting medical databases to a medical decision system, and describes a service created to extract the necessary information that is transferred based on standards. The medical decision can be improved based on many inputs from different medical locations. The developed solution is described for a concrete case concerning the management for chronic pelvic pain, based on the information retrieved from diverse healthcare databases.
21685618	Health multi-terminology portal: a semantic added-value for patient safety.
Stud Health Technol Inform  2011
Since the mid-90s, several quality-controlled health gateways were developed. In France, CISMeF is the leading health gateway. It indexes Internet resources from the main institutions, using the MeSH thesaurus and the Dublin Core metadata element set. Since 2005, the CISMeF Information System (IS) includes 24 health terminologies, classifications and thesauri for indexing and information retrieval. This work aims at creating a Health Multi-Terminology Portal (HMTP) and connect it to the CISMeF Terminology Database mainly for searching concepts and terms among all the health controlled vocabularies available in French (or in English and translated in French) and browsing it dynamically. To integrate the terminologies in the CISMeF IS, three steps are necessary: (1) designing a meta-model into which each terminology can be integrated, (2) developing a process to include terminologies into the HMTP, (3) building and integrating existing and new inter-terminology mappings into the HMTP. A total of 24 terminologies are included in the HMTP, with 575,300 concepts, 852,000 synonyms, 222,800 definitions and 1,180,000 relations. Heightteen of these terminologies are not included yet in the UMLS among them, some from the World Health Organization. Since January 2010, HMTP is daily used by CISMeF librarians to index in multi-terminology mode. A health multiterminology portal is a valuable tool helping the indexing and the retrieval of resources from a quality-controlled patient safety gateway. It can also be very useful for teaching or performing audits in terminology management.
21600222	Functionality and metagraph disintegration in boolean networks.
J. Theor. Biol. 20110513 2011Aug7
We study regulatory networks of N genes giving rise to a vector expression profile v(t) in which each gene is Boolean; either on or off at any time. We require a network to produce a particular time sequence v(t) for t∈1,…,T and parameterize the complexity of such a genetic function by its duration T. We establish a number of new results regarding how functional complexity constrains genetic regulatory networks and their evolution. We find that the number of networks which generate a function decreases approximately exponentially with its complexity T and show there is a corresponding weakening of the robustness of those networks to mutations. These results suggest a limit on the functional complexity T of typical networks that is polynomial in N. However, we are also able to prove the existence of a, presumably small, class of networks in which this scales exponentially with N. We demonstrate that an increase in functional complexity T drives what we describe as a metagraph disintegration effect, breaking up the space of networks previously connected by neutral mutations and contrast this with what is found with less restrictive definitions of functionality. Our findings show how functional complexity could be a factor in shaping the evolutionary landscape and how the evolutionary history of a species constrains its future functionality. Finally we extend our analysis to functions with more exotic topologies in expression space, including "stars" and "trees". We quantify how the properties of networks that give rise to these functions differ from those that produce linear functional paths with the same overall duration T.
21701677	Remote data retrieval for bioinformatics applications: an agent migration approach.
PLoS ONE 20110620 2011
Some of the approaches have been developed to retrieve data automatically from one or multiple remote biological data sources. However, most of them require researchers to remain online and wait for returned results. The latter not only requires highly available network connection, but also may cause the network overload. Moreover, so far none of the existing approaches has been designed to address the following problems when retrieving the remote data in a mobile network environment: (1) the resources of mobile devices are limited; (2) network connection is relatively of low quality; and (3) mobile users are not always online. To address the aforementioned problems, we integrate an agent migration approach with a multi-agent system to overcome the high latency or limited bandwidth problem by moving their computations to the required resources or services. More importantly, the approach is fit for the mobile computing environments. Presented in this paper are also the system architecture, the migration strategy, as well as the security authentication of agent migration. As a demonstration, the remote data retrieval from GenBank was used to illustrate the feasibility of the proposed approach.
21337020	Importance of data management in a long-term biological monitoring program.
Environ Manage 20110220 2011Jun
The long-term Biological Monitoring and Abatement Program (BMAP) has always needed to collect and retain high-quality data on which to base its assessments of ecological status of streams and their recovery after remediation. Its formal quality assurance, data processing, and data management components all contribute to meeting this need. The Quality Assurance Program comprehensively addresses requirements from various institutions, funders, and regulators, and includes a data management component. Centralized data management began a few years into the program when an existing relational database was adapted and extended to handle biological data. The database's main data tables and several key reference tables are described. One of the most important related activities supporting long-term analyses was the establishing of standards for sampling site names, taxonomic identification, flagging, and other components. The implemented relational database supports the transmittal of data to the Oak Ridge Environmental Information System (OREIS) as the permanent repository. We also discuss some limitations to our implementation. Some types of program data were not easily accommodated in the central systems, and many possible data-sharing and integration options are not easily accessible to investigators. From our experience we offer data management advice to other biologically oriented long-term environmental sampling and analysis programs.
21079273	Face recognition using nearest feature space embedding.
IEEE Trans Pattern Anal Mach Intell  2011Jun
Face recognition algorithms often have to solve problems such as facial pose, illumination, and expression (PIE). To reduce the impacts, many researchers have been trying to find the best discriminant transformation in eigenspaces, either linear or nonlinear, to obtain better recognition results. Various researchers have also designed novel matching algorithms to reduce the PIE effects. In this study, a nearest feature space embedding (called NFS embedding) algorithm is proposed for face recognition. The distance between a point and the nearest feature line (NFL) or the NFS is embedded in the transformation through the discriminant analysis. Three factors, including class separability, neighborhood structure preservation, and NFS measurement, were considered to find the most effective and discriminating transformation in eigenspaces. The proposed method was evaluated by several benchmark databases and compared with several state-of-the-art algorithms. According to the compared results, the proposed method outperformed the other algorithms.
21498285	A state-of-the-art pipeline for postmortem CT and MRI visualization: from data acquisition to interactive image interpretation at autopsy.
Acta Radiol 20110317 2011Jun1
The importance of autopsy procedures leading to the establishment of the cause of death is well-known. A recent addition to the autopsy work flow is the possibility of conducting postmortem imaging, in its 3D version also called virtual autopsy (VA), using multidetector computed tomography (MDCT) or magnetic resonance imagining (MRI) data from scans of cadavers displayed with direct volume rendering (DVR) 3D techniques. The use of the data and their workflow are presented. Data acquisition was performed and high quality data-sets with submillimeter precision were acquired. New data acquisition techniques such as dual-energy CT (DECT) and quantitative MRI, then were implemented and provided additional information. Particular findings hardly visualized in conventional autopsy can rather easy be seen at the full body CT, such as air distribution, e.g. pneumothorax, pneumopericardium, air embolism, and wound channels. MRI shows natural deaths such as myocardial infarctions. Interactive visualization of these 3D data-sets can provide valuable insight into the corpses and enables non-invasive diagnostic procedures. In postmortem CT imaging, not being limited by a patient depending radiation dose limit the data-sets can, however, be generated with such a high resolution that they become difficult to handle in today's archive retrieval and interactive visualization systems, specifically in the case of full body scans. To take full advantage of these new technologies the postmortem workflow needs to be tailored to the demands and opportunities that the new technologies allow.
21633942	Searching NCBI databases using Entrez.
Curr Protoc Bioinformatics  2011Jun
One of the most widely used interfaces for the retrieval of information from biological databases is the NCBI Entrez system. Entrez capitalizes on the fact that there are pre-existing, logical relationships between the individual entries found in numerous public databases. The existence of such natural connections, mostly biological in nature, argued for the development of a method through which all the information about a particular biological entity could be found without having to sequentially visit and query disparate databases. Two basic protocols describe simple, text-based searches, illustrating the types of information that can be retrieved through the Entrez system. An alternate protocol builds upon the first basic protocol, using additional, built-in features of the Entrez system, and providing alternative ways to issue the initial query. The support protocol reviews how to save frequently issued queries. Finally, Cn3D, a structure visualization tool, is also discussed.
21538091	Drug discovery using very large numbers of patents: general strategy with extensive use of match and edit operations.
J. Comput. Aided Mol. Des. 20110503 2011May
A patent data base of 6.7 million compounds generated by a very high performance computer (Blue Gene) requires new techniques for exploitation when extensive use of chemical similarity is involved. Such exploitation includes the taxonomic classification of chemical themes, and data mining to assess mutual information between themes and companies. Importantly, we also launch candidates that evolve by "natural selection" as failure of partial match against the patent data base and their ability to bind to the protein target appropriately, by simulation on Blue Gene. An unusual feature of our method is that algorithms and workflows rely on dynamic interaction between match-and-edit instructions, which in practice are regular expressions. Similarity testing by these uses SMILES strings and, less frequently, graph or connectivity representations. Examining how this performs in high throughput, we note that chemical similarity and novelty are human concepts that largely have meaning by utility in specific contexts. For some purposes, mutual information involving chemical themes might be a better concept.
21629144	A content analysis of parents' written communication of needs and expectations for emergency care of their children.
Pediatr Emerg Care  2011Jun
We investigated the potential value of information shared by parents on a written form designed to capture needs and expectations for care to an emergency department (ED) system that values patient-centeredness. We conducted a retrospective content analysis of parent-completed written forms collected during an improvement project focused on parent-provider communication in a pediatric ED. The primary outcome was potential value of the completed forms to a patient-centered ED system, defined as a form that was legible, included observations that mapped to medical problems, and included reasonable parental requests. We analyzed variation in potential value and other form attributes across a priori-defined visit type and acuity. Visit type was validated by a separate, blinded medical record review. A random stratified sample of 1008 forms was established from 6937 parent-completed forms collected during the 6-month improvement project; 995 of 1008 forms had matching medical records; 922 (92.7%) of 995 forms demonstrated potential value; 990 (99.5%) of 995 forms were legible; 948 (95.3%) of 995 forms included observations that mapped to a medical problem, and 599 (93.3%) of 642 forms contained reasonable parental requests. There was good agreement between the form and medical record for visit type (κ = 0.62). The potential value of forms did not vary significantly across visit type (88.2%-92.8%) or acuity (88.9%-93.4%). Information shared by parents on written forms designed to capture needs and expectations provides potential value to a patient-centered ED system. The high level of informational value is consistent across patient type and acuity level.
21466461	From data processing to multivariate validation--essential steps in extracting interpretable information from metabolomics data.
Curr Pharm Biotechnol  2011Jul
In metabolomics studies there is a clear increase of data. This indicates the necessity of both having a battery of suitable analysis methods and validation procedures able to handle large amounts of data. In this review, an overview of the metabolomics data processing pipeline is presented. A selection of recently developed and most cited data processing methods is discussed. In addition, commonly used chemometric and machine learning analysis methods as well as validation approaches are described.
21455801	Orthopaedic Web Links (OWL): a way to find professional orthopaedic information on the internet.
Clin. Orthop. Relat. Res. 20110401 2011Jul
Finding useful high-grade professional orthopaedic information on the Internet is often difficult. Orthopaedic Web Links (OWL) is a searchable database of vetted online orthopaedic resources. OWL uses a subject directory (OWL Directory) and a custom search engine (OWL Web) to provide a list of resources. The most effective way to find readily accessible, full text on-subject material suitable for education of an orthopaedic surgeon or trainee has not been defined. We therefore (1) proposed a method for selecting topics and evaluating searches and (2) compared the search results from an orthopaedic-specific directory (OWL Directory), a custom search engine (OWL Web), and standard Google searches. A scoring system for evaluation of the search results was developed for standardized comparison. Single words and sets of three words from randomly selected examination questions provided the search strings to compare the three strategies. For single keyword searches, the OWL Directory scored highest (16.4/50) of the three methods. For the three keywords searches, OWL Web had the highest mean score (26.0/50), followed by Google (22.8/50), and the OWL Directory (1.0/50). OWL Web searches had higher scores than Google searches, while returning 800 times fewer search results. The OWL Directory of orthopaedic subjects on the Internet provides a simple browsable category structure to find information. The OWL Web search engine scored higher than Google and resulted in a greater proportion of valid, on-subject, and accessible resources in the search results.
21658288	Building a biomedical tokenizer using the token lattice design pattern and the adapted Viterbi algorithm.
BMC Bioinformatics 20110609 2011
Tokenization is an important component of language processing yet there is no widely accepted tokenization method for English texts, including biomedical texts. Other than rule based techniques, tokenization in the biomedical domain has been regarded as a classification task. Biomedical classifier-based tokenizers either split or join textual objects through classification to form tokens. The idiosyncratic nature of each biomedical tokenizer's output complicates adoption and reuse. Furthermore, biomedical tokenizers generally lack guidance on how to apply an existing tokenizer to a new domain (subdomain). We identify and complete a novel tokenizer design pattern and suggest a systematic approach to tokenizer creation. We implement a tokenizer based on our design pattern that combines regular expressions and machine learning. Our machine learning approach differs from the previous split-join classification approaches. We evaluate our approach against three other tokenizers on the task of tokenizing biomedical text. Medpost and our adapted Viterbi tokenizer performed best with a 92.9% and 92.4% accuracy respectively. Our evaluation of our design pattern and guidelines supports our claim that the design pattern and guidelines are a viable approach to tokenizer construction (producing tokenizers matching leading custom-built tokenizers in a particular domain). Our evaluation also demonstrates that ambiguous tokenizations can be disambiguated through POS tagging. In doing so, POS tag sequences and training data have a significant impact on proper text tokenization.
21658293	Machine learning with naturally labeled data for identifying abbreviation definitions.
BMC Bioinformatics 20110609 2011
The rapid growth of biomedical literature requires accurate text analysis and text processing tools. Detecting abbreviations and identifying their definitions is an important component of such tools. Most existing approaches for the abbreviation definition identification task employ rule-based methods. While achieving high precision, rule-based methods are limited to the rules defined and fail to capture many uncommon definition patterns. Supervised learning techniques, which offer more flexibility in detecting abbreviation definitions, have also been applied to the problem. However, they require manually labeled training data. In this work, we develop a machine learning algorithm for abbreviation definition identification in text which makes use of what we term naturally labeled data. Positive training examples are naturally occurring potential abbreviation-definition pairs in text. Negative training examples are generated by randomly mixing potential abbreviations with unrelated potential definitions. The machine learner is trained to distinguish between these two sets of examples. Then, the learned feature weights are used to identify the abbreviation full form. This approach does not require manually labeled training data. We evaluate the performance of our algorithm on the Ab3P, BIOADI and Medstract corpora. Our system demonstrated results that compare favourably to the existing Ab3P and BIOADI systems. We achieve an F-measure of 91.36% on Ab3P corpus, and an F-measure of 87.13% on BIOADI corpus which are superior to the results reported by Ab3P and BIOADI systems. Moreover, we outperform these systems in terms of recall, which is one of our goals.
21658294	A structural SVM approach for reference parsing.
BMC Bioinformatics 20110609 2011
Automated extraction of bibliographic data, such as article titles, author names, abstracts, and references is essential to the affordable creation of large citation databases. References, typically appearing at the end of journal articles, can also provide valuable information for extracting other bibliographic data. Therefore, parsing individual reference to extract author, title, journal, year, etc. is sometimes a necessary preprocessing step in building citation-indexing systems. The regular structure in references enables us to consider reference parsing a sequence learning problem and to study structural Support Vector Machine (structural SVM), a newly developed structured learning algorithm on parsing references. In this study, we implemented structural SVM and used two types of contextual features to compare structural SVM with conventional SVM. Both methods achieve above 98% token classification accuracy and above 95% overall chunk-level accuracy for reference parsing. We also compared SVM and structural SVM to Conditional Random Field (CRF). The experimental results show that structural SVM and CRF achieve similar accuracies at token- and chunk-levels. When only basic observation features are used for each token, structural SVM achieves higher performance compared to SVM since it utilizes the contextual label features. However, when the contextual observation features from neighboring tokens are combined, SVM performance improves greatly, and is close to that of structural SVM after adding the second order contextual observation features. The comparison of these two methods with CRF using the same set of binary features show that both structural SVM and CRF perform better than SVM, indicating their stronger sequence learning ability in reference parsing.
21586248	[Successful training course in academic research in gynaecology and obstetrics].
Ugeskr. Laeg.  2011May16
In 2003 The National Board of Health, Denmark, enforced a compulsory training course in academic research as part of the specialist training for doctors. The trainees must learn to search and evaluate relevant literature and be able to implement the results in clinical practice. The structure of the 20 days of academic training in gynaecology and obstetrics is described. In 2008, 17 doctors participated in these sessions. We performed a questionnaire survey to explore the implementation. The academic training was well rated by the participants with respect to content, form and outcome.
21507258	PheMaDB: a solution for storage, retrieval, and analysis of high throughput phenotype data.
BMC Bioinformatics 20110420 2011
OmniLog™ phenotype microarrays (PMs) have the capability to measure and compare the growth responses of biological samples upon exposure to hundreds of growth conditions such as different metabolites and antibiotics over a time course of hours to days. In order to manage the large amount of data produced from the OmniLog™ instrument, PheMaDB (Phenotype Microarray DataBase), a web-based relational database, was designed. PheMaDB enables efficient storage, retrieval and rapid analysis of the OmniLog™ PM data. PheMaDB allows the user to quickly identify records of interest for data analysis by filtering with a hierarchical ordering of Project, Strain, Phenotype, Replicate, and Temperature. PheMaDB then provides various statistical analysis options to identify specific growth pattern characteristics of the experimental strains, such as: outlier analysis, negative controls analysis (signal/background calibration), bar plots, pearson's correlation matrix, growth curve profile search, k-means clustering, and a heat map plot. This web-based database management system allows for both easy data sharing among multiple users and robust tools to phenotype organisms of interest. PheMaDB is an open source system standardized for OmniLog™ PM data. PheMaDB could facilitate the banking and sharing of phenotype data. The source code is available for download at http://phemadb.sourceforge.net.
21118778	Video tracking based on sequential particle filtering on graphs.
IEEE Trans Image Process 20101129 2011Jun
In this paper, we develop a novel solution for particle filtering on general graphs. We provide an exact solution for particle filtering on directed cycle-free graphs. The proposed approach relies on a partial-order relation in an antichain decomposition that forms a high-order Markov chain over the partitioned graph. We subsequently derive a closed-form sequential updating scheme for conditional density propagation using particle filtering on directed cycle-free graphs. We also provide an approximate solution for particle filtering on general graphs by splitting graphs with cycles into multiple directed cycle-free subgraphs. We then use the sequential updating scheme by alternating among the directed cycle-free subgraphs to obtain an estimate of the density propagation. We rely on the proposed method for particle filtering on general graphs for two video tracking applications: 1) object tracking using high-order Markov chains; and 2) distributed multiple object tracking based on multi-object graphical interaction models. Experimental results demonstrate the improved performance of the proposed approach to particle filtering on graphs compared with existing methods for video tracking.
21189243	Text segmentation for MRC document compression.
IEEE Trans Image Process 20101223 2011Jun
The mixed raster content (MRC) standard (ITU-T T.44) specifies a framework for document compression which can dramatically improve the compression/quality tradeoff as compared to traditional lossy image compression algorithms. The key to MRC compression is the separation of the document into foreground and background layers, represented as a binary mask. Therefore, the resulting quality and compression ratio of a MRC document encoder is highly dependent upon the segmentation algorithm used to compute the binary mask. In this paper, we propose a novel multiscale segmentation scheme for MRC document encoding based upon the sequential application of two algorithms. The first algorithm, cost optimized segmentation (COS), is a blockwise segmentation algorithm formulated in a global cost optimization framework. The second algorithm, connected component classification (CCC), refines the initial segmentation by classifying feature vectors of connected components using an Markov random field (MRF) model. The combined COS/CCC segmentation algorithms are then incorporated into a multiscale framework in order to improve the segmentation accuracy of text with varying size. In comparisons to state-of-the-art commercial MRC products and selected segmentation algorithms in the literature, we show that the new algorithm achieves greater accuracy of text detection but with a lower false detection rate of nontext features. We also demonstrate that the proposed segmentation algorithm can improve the quality of decoded documents while simultaneously lowering the bit rate.
21193378	Nonlocal PDEs-based morphology on weighted graphs for image and data processing.
IEEE Trans Image Process 20101230 2011Jun
Mathematical morphology (MM) offers a wide range of operators to address various image processing problems. These operators can be defined in terms of algebraic (discrete) sets or as partial differential equations (PDEs). In this paper, we introduce a nonlocal PDEs-based morphological framework defined on weighted graphs. We present and analyze a set of operators that leads to a family of discretized morphological PDEs on weighted graphs. Our formulation introduces nonlocal patch-based configurations for image processing and extends PDEs-based approach to the processing of arbitrary data such as nonuniform high dimensional data. Finally, we show the potentialities of our methodology in order to process, segment and classify images and arbitrary data.
21592914	Perceptual segmentation: combining image segmentation with object tagging.
IEEE Trans Image Process  2011Jun
Human observers understand the content of an image intuitively. Based upon image content, they perform many image-related tasks, such as creating slide shows and photo albums, and organizing their image archives. For example, to select photos for an album, people assess image quality based upon the main objects in the image. They modify colors in an image based upon the color of important objects, such as sky, grass or skin. Serious photographers might modify each object separately. Photo applications, in contrast, use low-level descriptors to guide similar tasks. Typical descriptors, such as color histograms, noise level, JPEG artifacts and overall sharpness, can guide an imaging application and safeguard against blunders. However, there is a gap between the outcome of such operations and the same task performed by a person. We believe that the gap can be bridged by automatically understanding the content of the image. This paper presents algorithms for automatic tagging of perceptual objects in images, including sky, skin, and foliage, which constitutes an important step toward this goal.
21594856	[The use of bibliographic information resources and Web 2.0 by neuropaediatricians].
Rev Neurol  2011Jun16
To determine the state of knowledge and use of the main sources of bibliographic information and Web 2.0 resources in a sample of pediatricians linked professionally to child neurology. Anonymous opinion survey to 44 pediatricians (36 neuropediatric staffs and 8 residents) with two sections: sources of bibliographic information: (25 questions) and Web 2.0 resources (14 questions). The most consulted journals are Revista de Neurología and Anales de Pediatría. All use PubMed database and less frequently �?ndice Médico Español (40%) and Embase (27%); less than 20% use of other international and national databases. 81% of respondents used the Cochrane Library, and less frequently other sources of evidence-based medicine: Tripdatabase (39%), National Guideline Clearinghouse (37%), Excelencia Clínica (12%) and Sumsearch (3%). 45% regularly receive some e-TOC (electronic table of contents) of biomedical journals, but only 7% reported having used the RSS (really system syndication). The places to start searching for information are PubMed (55%) and Google (23%). The four resources most used of Web 2.0 are YouTube (73%), Facebook (43%), Picasa (27%) and blogs (25%). We don't found differences in response between the group of minus or equal to 34 and major or equal to 35 years. Knowledge of the patterns of use of information databases and Web 2.0 resources can identify the limitations and opportunities for improvement in the field of pediatric neurology training and information.
21510860	GET WELL: an automated surveillance system for gaining new epidemiological knowledge.
BMC Public Health 20110421 2011
The assumption behind the presented work is that the information people search for on the internet reflects the disease status in society. By having access to this source of information, epidemiologists can get a valuable complement to the traditional surveillance and potentially get new and timely epidemiological insights. For this purpose, the Swedish Institute for Infectious Disease Control collaborates with a medical web site in Sweden. We built an application consisting of two conceptual parts. One part allows for trends, based on user specified requests, to be extracted from anonymous web query data from a Swedish medical web site. The second conceptual part permits tailored analyses of particular diseases, where more complex statistical methods are applied to the data. To evaluate the epidemiological relevance of the output, we compared Google search data and search data from the medical web site. In the paper, we give concrete examples of the output from the web query-based system. We also present results from the comparison between data from the search engine Google and search data from the national medical web site. The application is in regular use at the Swedish Institute for Infectious Disease Control. A system based on web queries is flexible in that it can be adapted to any disease; we get information on other individuals than those who seek medical care; and the data do not suffer from reporting delays. Although Google data are based on a substantially larger search volume, search patterns obtained from the medical web site may still convey more information from an epidemiological perspective. Furthermore we can see advantages with having full access to the raw data.
21545748	Assessment of function and clinical utility of alcohol and other drug web sites: an observational, qualitative study.
BMC Public Health 20110505 2011
The increasing popularity and use of the internet makes it an attractive option for providing health information and treatment, including alcohol/other drug use. There is limited research examining how people identify and access information about alcohol or other drug (AOD) use online, or how they assess the usefulness of the information presented. This study examined the strategies that individuals used to identify and navigate a range of AOD websites, along with the attitudes concerning presentation and content. Members of the general community in Brisbane and Roma (Queensland, Australia) were invited to participate in a 30-minute search of the internet for sites related to AOD use, followed by a focus group discussion. Fifty one subjects participated in the study across nine focus groups. Participants spent a maximum of 6.5 minutes on any one website, and less if the user was under 25 years of age. Time spent was as little as 2 minutes if the website was not the first accessed. Participants recommended that AOD-related websites should have an engaging home or index page, which quickly and accurately portrayed the site's objectives, and provided clear site navigation options. Website content should clearly match the title and description of the site that is used by internet search engines. Participants supported the development of a portal for AOD websites, suggesting that it would greatly facilitate access and navigation.Treatment programs delivered online were initially viewed with caution. This appeared to be due to limited understanding of what constituted online treatment, including its potential efficacy. A range of recommendations arise from this study regarding the design and development of websites, particularly those related to AOD use. These include prudent use of text and information on any one webpage, the use of graphics and colours, and clear, uncluttered navigation options. Implications for future website development are discussed.
20566253	Trends in PACS architecture.
Eur J Radiol 20100620 2011May
Radiological Picture Archiving and Communication Systems (PACS) have only relatively recently become abundant. Many hospitals have made the transition to PACS about a decade ago. During that decade requirements and available technology have changed considerably. In this paper we look at factors that influence the design of tomorrow's systems, especially those in larger multidisciplinary hospitals. We discuss their impact on PACS architecture (a technological perspective) as well as their impact on radiology (a management perspective). We emphasize that many of these influencing factors originate outside radiology and that radiology has little impact on these factors. That makes it the more important for managers in radiology to be aware of architectural aspects and it may change cooperation of radiology with, among others, the hospital's central IT department.
20580180	IHE profiles applied to regional PACS.
Eur J Radiol 20100626 2011May
PACS has been widely adopted as an image storage solution that perfectly fits the radiology department workflow and that can be easily extended to other hospital departments. Integrations with other hospital systems, like the Radiology Information System, the Hospital Information System and the Electronic Patient Record are fully achieved but still challenging aims. PACS also creates the perfect environment for teleradiology and teleworking setups. One step further is the regional PACS concept where different hospitals or health care enterprises share the images in an integrated Electronic Patient Record. Among the different solutions available to share images between different hospitals IHE (Integrating the Healthcare Enterprise) organization presents the Cross Enterprise Document Sharing profile (XDS) which allows sharing images from different hospitals even if they have different PACS vendors. Adopting XDS has multiple advantages, images do not need to be duplicated in a central archive to be shared among the different healthcare enterprises, they only need to be indexed and published in a central document registry. In the XDS profile IHE defines the mechanisms to publish and index the images in the central document registry. It also defines the mechanisms that each hospital will use to retrieve those images regardless on the Hospital PACS they are stored.
20605693	The quest for standards in medical imaging.
Eur J Radiol 20100601 2011May
This article focuses on standards supporting interoperability and system integration in the medical imaging domain. We introduce the basic concepts and actors and we review the most salient achievements in this domain, especially with the DICOM standard, and the definition of IHE integration profiles. We analyze and discuss what was successful, and what could still be more widely adopted by industry. We then sketch out a perspective of what should be done next, based on our vision of new requirements for the next decade. In particular, we discuss the challenges of a more explicit sharing of image and image processing semantics, and we discuss the help that semantic web technologies (and especially ontologies) may bring to achieving this goal.
20619986	PACS infrastructure supporting e-learning.
Eur J Radiol 20100710 2011May
Digital imaging is becoming predominant in radiology. This has implications for teaching support, because conventional film-based concepts are now obsolete. The IHE Teaching File and Clinical Study Export (TCE) profile provides an excellent platform to enhance PACS infrastructure with educational functionality. This can be supplemented with dedicated e-learning tools.
20634012	The future of PACS in healthcare enterprises.
Eur J Radiol 20100714 2011May
Picture Archiving and Communication System (PACS), which was originally designed as a tool for facilitating radiologists in interpreting images more efficiently, is evolving into a hospital-integrated system storing diagnostic imaging information that often reaches far beyond Radiology. The continuous evolution of PACS technology has led to a gradual broadening of its applications, ranging from teleradiology to CAD (Computer-Assisted Diagnosis) and multidimensional imaging, and is moving into the direction of providing access to image data outside the Radiology department, so to reach all the branches of the healthcare enterprise. New perspectives have been created thanks to new technologies (such as holographic media and GRID computing) that are likely due to expand PACS-based applications even further, improving patient care and enhancing overall productivity.
21601675	Developer's and user's guide to Clotho v2.0 A software platform for the creation of synthetic biological systems.
Meth. Enzymol.  2011
To design the complex systems that synthetic biologists propose to create, software tools must be developed. Critical to success is the enablement of collaboration across our community such that individual tools that perform specific tasks combine with other tools to provide multiplicative benefits. This will require standardization of the form of the data that exists within the field (Parts, Strains, measurements, etc.), a software environment that enables communication between tools, and a sharing mechanism for distributing the tools. Additionally, this data model must describe the data in a sufficiently rigorous and validated form such that meaningful layers of abstraction can be built upon the base. Herein, we describe a software platform called "Clotho" which provides such a data model, and the plugin and sharing mechanisms needed for a rich tool environment. This document provides a tutorial for users of Clotho and information for software developers who wish to contribute new tools (known as "Apps") to it.
21496247	CDAO-store: ontology-driven data integration for phylogenetic analysis.
BMC Bioinformatics 20110415 2011
The Comparative Data Analysis Ontology (CDAO) is an ontology developed, as part of the EvoInfo and EvoIO groups supported by the National Evolutionary Synthesis Center, to provide semantic descriptions of data and transformations commonly found in the domain of phylogenetic analysis. The core concepts of the ontology enable the description of phylogenetic trees and associated character data matrices. Using CDAO as the semantic back-end, we developed a triple-store, named CDAO-Store. CDAO-Store is a RDF-based store of phylogenetic data, including a complete import of TreeBASE. CDAO-Store provides a programmatic interface, in the form of web services, and a web-based front-end, to perform both user-defined as well as domain-specific queries; domain-specific queries include search for nearest common ancestors, minimum spanning clades, filter multiple trees in the store by size, author, taxa, tree identifier, algorithm or method. In addition, CDAO-Store provides a visualization front-end, called CDAO-Explorer, which can be used to view both character data matrices and trees extracted from the CDAO-Store. CDAO-Store provides import capabilities, enabling the addition of new data to the triple-store; files in PHYLIP, MEGA, nexml, and NEXUS formats can be imported and their CDAO representations added to the triple-store. CDAO-Store is made up of a versatile and integrated set of tools to support phylogenetic analysis. To the best of our knowledge, CDAO-Store is the first semantically-aware repository of phylogenetic data with domain-specific querying capabilities. The portal to CDAO-Store is available at http://www.cs.nmsu.edu/~cdaostore.
21609401	Clinical long-term studies: data collection and assessment.
J Dtsch Dermatol Ges  2011Jun
Short-term randomized controlled clinical trials often provide the basis of regulatory approval for a new drug application, while long-term clinical studies are essential for monitoring the (long-term) effectiveness and safety of a drug. As the duration of a study increases and the number of patients continuing in the study declines, missing data become more of a problem, as they may bias the results. Therefore, standard analytical strategies used in short-term trials (intention-to-treat, per-protocol) may not always be appropriate for data generated in long-term studies. In this article, commonly used analytical approaches in the assessment of clinical trial data will be reviewed. Given their specific characteristics, regulatory authorities and expert guidelines suggest to use several of these approaches in parallel to correctly interpret the data of long-term clinical studies and to come to better informed decisions.
21609166	Teaching evidence-based medicine: Impact on students' literature use and inpatient clinical documentation.
Med Teach  2011
Effective teaching of evidence-based medicine (EBM) to medical students is important for lifelong self-directed learning. We implemented a brief workshop designed to teach literature searching skills to third-year medical students. We assessed its impact on students' utilization of EBM resources during their clinical rotation and the quality of EBM integration in inpatient notes. We developed a physician-led, hands-on workshop to introduce EBM resources to all internal medicine clerks. Pre- and post-workshop measures included student's attitudes to EBM, citations of EBM resources in their clinical notes, and quality of the EBM component of the discussion in the note. Computer log analysis recorded students' online search attempts. After the workshop, students reported improved comfort using EBM and increased utilization of EBM resources. EBM integration into the discussion component of the notes also showed significant improvement. Computer log analysis of students' searches demonstrated increased utilization of EBM resources following the workshop. We describe the successful implementation of a workshop designed to teach third-year medical students how to perform an efficient EBM literature search. We demonstrated improvements in students' confidence regarding EBM, increased utilization of EBM resources, and improved integration of EBM into inpatient notes.
20620126	Disaster victim investigation recommendations from two simulated mass disaster scenarios utilized for user acceptance testing CODIS 6.0.
Forensic Sci Int Genet 20100709 2011Aug
Members of the National DNA Data Bank (NDDB) of Canada designed and searched two simulated mass disaster (MD) scenarios for User Acceptance Testing (UAT) of the Combined DNA Index System (CODIS) 6.0, developed by the Federal Bureau of Investigation (FBI) and the US Department of Justice. A simulated airplane MD and inland Tsunami MD were designed representing a closed and open environment respectively. An in-house software program was written to randomly generate DNA profiles from a mock Caucasian population database. As part of the UAT, these two MDs were searched separately using CODIS 6.0. The new options available for identity and pedigree searching in addition to the inclusion of mitochondrial DNA (mtDNA) and Y-STR (short tandem repeat) information in CODIS 6.0, led to rapid identification of all victims. A Joint Pedigree Likelihood Ratio (JPLR) was calculated from the pedigree searches and ranks were stored in Rank Manager providing confidence to the user in assigning an Unidentified Human Remain (UHR) to a pedigree tree. Analyses of the results indicated that primary relatives were more useful in Disaster Victim Identification (DVI) compared to secondary or tertiary relatives and that inclusion of mtDNA and/or Y-STR technologies helped to link family units together as shown by the software searches. It is recommended that UHRs have as many informative loci possible to assist with their identification. CODIS 6.0 is a valuable technological tool for rapidly and confidently identifying victims of mass disasters.
20655289	Low template STR typing: effect of replicate number and consensus method on genotyping reliability and DNA database search results.
Forensic Sci Int Genet 20100722 2011Aug
To analyze DNA samples with very low DNA concentrations, various methods have been developed that sensitize short tandem repeat (STR) typing. Sensitized DNA typing is accompanied by stochastic amplification effects, such as allele drop-outs and drop-ins. Therefore low template (LT) DNA profiles are interpreted with care. One can either try to infer the genotype by a consensus method that uses alleles confirmed in replicate analyses, or one can use a statistical model to evaluate the strength of the evidence in a direct comparison with a known DNA profile. In this study we focused on the first strategy and we show that the procedure by which the consensus profile is assembled will affect genotyping reliability. In order to gain insight in the roles of replicate number and requested level of reproducibility, we generated six independent amplifications of samples of known donors. The LT methods included both increased cycling and enhanced capillary electrophoresis (CE) injection [1]. Consensus profiles were assembled from two to six of the replications using four methods: composite (include all alleles), n-1 (include alleles detected in all but one replicate), n/2 (include alleles detected in at least half of the replicates) and 2× (include alleles detected twice). We compared the consensus DNA profiles with the DNA profile of the known donor, studied the stochastic amplification effects and examined the effect of the consensus procedure on DNA database search results. From all these analyses we conclude that the accuracy of LT DNA typing and the efficiency of database searching improve when the number of replicates is increased and the consensus method is n/2. The most functional number of replicates within this n/2 method is four (although a replicate number of three suffices for samples showing &gt;25% of the alleles in standard STR typing). This approach was also the optimal strategy for the analysis of 2-person mixtures, although modified search strategies may be needed to retrieve the minor component in database searches. From the database searches follows the recommendation to specifically mark LT DNA profiles when entering them into the DNA database.
21593546	Encoding visual information in retinal ganglion cells with prosthetic stimulation.
J Neural Eng 20110518 2011Jun
Retinal prostheses aim to restore functional vision to those blinded by outer retinal diseases using electric stimulation of surviving retinal neurons. The ability to replicate the spatiotemporal pattern of ganglion cell spike trains present under normal viewing conditions is presumably an important factor for restoring high-quality vision. In order to replicate such activity with a retinal prosthesis, it is important to consider both how visual information is encoded in ganglion cell spike trains, and how retinal neurons respond to electric stimulation. The goal of the current review is to bring together these two concepts in order to guide the development of more effective stimulation strategies. We review the experiments to date that have studied how retinal neurons respond to electric stimulation and discuss these findings in the context of known retinal signaling strategies. The results from such in vitro studies reveal the advantages and disadvantages of activating the ganglion cell directly with the electric stimulus (direct activation) as compared to activation of neurons that are presynaptic to the ganglion cell (indirect activation). While direct activation allows high temporal but low spatial resolution, indirect activation yields improved spatial resolution but poor temporal resolution. Finally, we use knowledge gained from in vitro experiments to infer the patterns of elicited activity in ongoing human trials, providing insights into some of the factors limiting the quality of prosthetic vision.
21191794	Assessment of breast cancer tumour size using six different methods.
Eur Radiol 20101230 2011Jun
Tumour size estimates using mammography (MG), conventional ultrasound (US), compound imaging (CI) and real-time elastography (RTE) were compared with histopathological specimen sizes. The largest diameters of 97 malignant breast lesions were measured. Two US and CI measurements were made: US1/CI1 (hypoechoic nucleus only) and US2/CI2 (hypoechoic nucleus plus hyperechoic halo). Measurements were compared with histopathological tumour sizes using linear regression and Bland-Altman plots. Size prediction was best with ultrasound (US/CI/RTE: R (2) 0.31-0.36); mammography was poorer (R(2) = 0.19). The most accurate method was US2, while US1 and CI1 were poorest. Bland-Altman plots showed better size estimation with US2, CI2 and RTE, with low variation, while mammography showed greatest variability. Smaller tumours were better assessed than larger ones. CI2 and US2 performed best for ductal tumours and RTE for lobular cancers. Tumour size prediction accuracy did not correlate significantly with breast density, but on MG tumours were more difficult to detect in high-density tissue. The size of ductal tumours is best predicted with US2 and CI2, while for lobular cancers RTE is best. Hyperechoic tumour surroundings should be included in US and CI measurements and RTE used as an additional technique in the clinical staging process.
21553535	[Information extraction methodology used in electronic medical records].
Zhongguo Yi Liao Qi Xie Za Zhi  2011Jan
We try to use information extraction technology in some parts of the medical records and extract disease information to accumulate experience for extracting complete information from medical records. This paper attempts to use dictionary and rules to achieve the named entity recognition. Information extraction is based on shallow parsing and use pattern sentence matching method with the help of a 3 levels finite state automaton.
21553662	Electronic matching of HIV/AIDS and hepatitis C surveillance registries in three states.
Public Health Rep  2011 May-Jun
Both HIV and hepatitis C virus (HCV) can be transmitted through percutaneous exposure to blood in similar high-risk populations. HCV and HIV/AIDS surveillance databases were matched in Colorado, Connecticut, and Oregon to measure the frequency of co-infection and to characterize coinfected people. We defined a case of HCV infection as a person with a reactive antibody for hepatitis C, medical diagnosis, positive viral-load test result, or positive genotype reported to any of three state health departments from the start of each state's hepatitis C registry through June 30, 2008. We defined a case of HIV/AIDS as a person diagnosed and living with HIV/AIDS at the start of each state's respective hepatitis C registry through June 30, 2008. HIV/AIDS and hepatitis C datasets were matched using Link King, public domain record linkage and consolidation software, and all potential matches were manually reviewed before acceptance as a match. The proportion of reported hepatitis C cases co-infected with HIV/ AIDS was 1.8% in Oregon, 1.9% in Colorado, and 4.9% in Connecticut. Conversely, the proportion of HIV/AIDS cases co-infected with hepatitis C was consistently higher in the three states: 4.4% in Oregon, 9.7% in Colorado, and 23.6% in Connecticut. Electronic matching of registries is a potentially useful and efficient way to transfer information from one registry to another. In addition, it can provide a measure of the public health burden of HIV/AIDS and hepatitis C co-infection and provide insight into prevention and medical care needs for respective states.
21558151	Multi-source and ontology-based retrieval engine for maize mutant phenotypes.
Database (Oxford) 20110510 2011
Model Organism Databases, including the various plant genome databases, collect and enable access to massive amounts of heterogeneous information, including sequence data, gene product information, images of mutant phenotypes, etc, as well as textual descriptions of many of these entities. While a variety of basic browsing and search capabilities are available to allow researchers to query and peruse the names and attributes of phenotypic data, next-generation search mechanisms that allow querying and ranking of text descriptions are much less common. In addition, the plant community needs an innovative way to leverage the existing links in these databases to search groups of text descriptions simultaneously. Furthermore, though much time and effort have been afforded to the development of plant-related ontologies, the knowledge embedded in these ontologies remains largely unused in available plant search mechanisms. Addressing these issues, we have developed a unique search engine for mutant phenotypes from MaizeGDB. This advanced search mechanism integrates various text description sources in MaizeGDB to aid a user in retrieving desired mutant phenotype information. Currently, descriptions of mutant phenotypes, loci and gene products are utilized collectively for each search, though expansion of the search mechanism to include other sources is straightforward. The retrieval engine, to our knowledge, is the first engine to exploit the content and structure of available domain ontologies, currently the Plant and Gene Ontologies, to expand and enrich retrieval results in major plant genomic databases. Database URL: http:www.PhenomicsWorld.org/QBTA.php.
21560920	NVivo 8 and consistency in data analysis: reflecting on the use of a qualitative data analysis program.
Nurse Res  2011
Qualitative data analysis is a complex process and demands clear thinking on the part of the analyst. However, a number of deficiencies may obstruct the research analyst during the process, leading to inconsistencies occurring. This paper is a reflection on the use of a qualitative data analysis program, NVivo 8, and its usefulness in identifying consistency and inconsistency during the coding process. The author was conducting a large-scale study of providers and users of mental health services in Ireland. He used NVivo 8 to store, code and analyse the data and this paper reflects some of his observations during the study. The demands placed on the analyst in trying to balance the mechanics of working through a qualitative data analysis program, while simultaneously remaining conscious of the value of all sources are highlighted. NVivo 8 as a qualitative data analysis program is a challenging but valuable means for advancing the robustness of qualitative research. Pitfalls can be avoided during analysis by running queries as the analyst progresses from tree node to tree node rather than leaving it to a stage whereby data analysis is well advanced.
21564495	Evidence-based information-seeking skills of junior doctors entering the workforce: an evaluation of the impact of information literacy training during pre-clinical years.
Health Info Libr J 20110408 2011Jun
To investigate the extent to which junior doctors in their first clinical positions retained information literacy skills taught as part of their undergraduate education. Participants drawn from different training cohorts were interviewed about their recall of the instruction they had received, and their confidence in retrieving and evaluating information for clinical decision making. They completed a search based on a scenario related to their specialty. Their self-assessment of their competency in conducting and evaluating a search was compared with an evaluation of their skills by an experienced observer. Most participants recalled the training they received but had not retained high-level search skills, and lacked skills in identifying and applying best evidence. There was no apparent link between the type of training given and subsequent skill level. Those whose postgraduate education required these skills were more successful in retrieving and appraising information. Commitment to evidence-based medicine from clinicians at all levels in the profession is needed to increase the information seeking skills of clinicians entering the work force.
21564496	Website creation and resource management: developing collaborative strategies for asynchronous interaction with library users.
Health Info Libr J 20101203 2011Jun
To make electronic resources available to library users while effectively harnessing intellectual capital within the library, ultimately fostering the library's use of technology to interact asynchronously with its patrons (users). The methods used in the project included: (1) developing a new library website to facilitate the creation, management, accessibility, maintenance and dissemination of library resources; and (2) establishing ownership by those who participated in the project, while creating effective work allocation strategies through the implementation of a content management system that allowed the library to manage cost, complexity and interoperability. Preliminary results indicate that contributors to the system benefit from an increased understanding of the library's resources and add content valuable to library patrons. These strategies have helped promote the manageable creation and maintenance of electronic content in accomplishing the library's goal of interacting with its patrons. Establishment of a contributive system for adding to the library's electronic resources and electronic content has been successful. Further work will look at improving asynchronous interaction, particularly highlighting accessibility of electronic content and resources.
21564497	Medicine and literature: a section in a medical university library.
Health Info Libr J 20110306 2011Jun
In 2007, the Alberto Malliani Medical Library of the Università degli Studi in Milan decided to order some novels for its students. The library purchased 24 titles written by famous authors and planned to add others in the future. The proposal for this action was made by a professor, with whom the library had previously co-operated in organising meetings for students. This article summarises the results of this experiment over 4 years, from its conception, to determine whether any positive outcome has resulted, including how library users welcomed this addition to the library and evaluation of its economic sustainability. Data from July 2007 to December 2010 are presented in terms of the initial purchases, costs, and volume of lending for this section of the library; in addition, readers' preferences are examined. A university medical library can act as a stimulus to its students' new or renewed interest in literature, assuming that the novels, biographies, and short stories can contribute positively to the training of medical students. Therefore, setting up a special section can be useful, even though the costs of this section must be limited. A questionnaire could be the method of gathering information about users' assessment.
21564499	If you cannot beat them, join them! Using Health 2.0 and popular Internet applications to improve information literacy.
Health Info Libr J 20110331 2011Jun
The popularity of Health 2.0 technologies has grown exponentially in recent years. They are increasingly being used to inform and support professional practice. This article discusses the use of the health facet of Web 2.0 applications by health professionals. In particular, it considers their value in the delivery of information literacy agendas by health librarians for health professionals.
21574322	[Searching for and processing professional and scientific data].
Soins  2011Apr
Carry out a relevant documentary search requires a methodical approach: definition of the subject of the search, selection of the most relevant sources of information and knowledge of the main databases. Some tips and pointers will help to make a search more efficient, notably the selection and combination of suitable descriptors relevant to the subject of the search.
21504866	Difficulties in finding DNA mutations and associated phenotypic data in web resources using simple, uncomplicated search terms, and a suggested solution.
Hum. Genomics  2011Mar
DNA mutation data currently reside in many online databases, which differ markedly in the terminology used to describe or define the mutation and also in completeness of content, potentially making it difficult both to locate a mutation of interest and to find sought-after data (eg phenotypic effect). To highlight the current deficiencies in the accessibility of web-based genetic variation information, we examined the ease with which various resources could be interrogated for five model mutations, using a set of simple search terms relating to the change in amino acid or nucleotide. Fifteen databases were investigated for the time and/or number of mouse clicks; clicks required to find the mutations; availability of phenotype data; the procedure for finding information; and site layout. Google and PubMed were also examined. The three locus-specific databases (LSDBs) generally yielded positive outcomes, but the 12 genome-wide databases gave poorer results, with most proving not to be searchable and only three yielding successful outcomes. Google and PubMed searches found some mutations and provided patchy information on phenotype. The results show that many web-based resources are not currently configured for fast and easy access to comprehensive mutation data, with only the isolated LSDBs providing optimal outcomes. Centralising this information within a common repository, coupled with a simple, all-inclusive interrogation process, would improve searching for all gene variation data.
20479491	A dual-bound algorithm for very fast and exact template matching.
IEEE Trans Pattern Anal Mach Intell  2011Mar
Recently proposed fast template matching techniques employ rejection schemes derived from lower bounds on the match measure. This paper generalizes that idea and shows that in addition to lower bounds, upper bounds on the match measure can be used to accelerate the search. An algorithm is proposed that utilizes both lower and upper bounds to detect the k best matches in an image. The performance of this dual-bound algorithm is guaranteed; it always detects the k best matches. Theoretical analysis and experimental results show that its runtime compares favorably with previously proposed real-time exact template-matching schemes.
20714020	Large displacement optical flow: descriptor matching in variational motion estimation.
IEEE Trans Pattern Anal Mach Intell  2011Mar
Optical flow estimation is classically marked by the requirement of dense sampling in time. While coarse-to-fine warping schemes have somehow relaxed this constraint, there is an inherent dependency between the scale of structures and the velocity that can be estimated. This particularly renders the estimation of detailed human motion problematic, as small body parts can move very fast. In this paper, we present a way to approach this problem by integrating rich descriptors into the variational optical flow setting. This way we can estimate a dense optical flow field with almost the same high accuracy as known from variational optical flow, while reaching out to new domains of motion analysis where the requirement of dense sampling in time is no longer satisfied.
21509667	Acupuncture for treating attention deficit hyperactivity disorder: a systematic review and meta-analysis.
Chin J Integr Med 20110421 2011Apr
To assess the effectiveness of acupuncture as a treatment option for attention deficit hyperactivity disorder (ADHD). The literatures were searched using 15 databases, including MEDLINE, AMED, CINAHL, EMBASE, PsycInfo, the Cochrane Central Register of Controlled Trials, the Cochrane Database of Systematic Reviews, six Korean medical databases and two Chinese databases without language restritions. Prospective controlled clinical studies of any type of acupuncture therapy for ADHD autistic patients were included. Trials in which acupuncture was part of a complex intervention were also included. All articles were read by two independent reviewers, who extracted data from the articles according to predefined criteria. Risk of bias was assessed using the Cochrane risk of bias tool. Of 114 articles, only three randomized clinical trials (RCTs) met our inclusion criteria. One RCT found that electroacupuncture (EA) plus behavioural treatment was superior to sham EA plus behavioural treatment. Two RCTs reported a significant benefit of acupuncture or auricular acupuncture over conventional drug therapies. Limited evidence exists for the effectiveness of acupuncture as a symptomatic treatment of ADHD. Given that the risk of bias of the included studies was high, firm conclusions cannot be drawn.
20530810	Robust bilayer segmentation and motion/depth estimation with a handheld camera.
IEEE Trans Pattern Anal Mach Intell  2011Mar
Extracting high-quality dynamic foreground layers from a video sequence is a challenging problem due to the coupling of color, motion, and occlusion. Many approaches assume that the background scene is static or undergoes the planar perspective transformation. In this paper, we relax these restrictions and present a comprehensive system for accurately computing object motion, layer, and depth information. A novel algorithm that combines different clues to extract the foreground layer is proposed, where a voting-like scheme robust to outliers is employed in optimization. The system is capable of handling difficult examples in which the background is nonplanar and the camera freely moves during video capturing. Our work finds several applications, such as high-quality view interpolation and video editing.
21047715	An iterative shrinkage approach to total-variation image restoration.
IEEE Trans Image Process 20101101 2011May
The problem of restoration of digital images from their degraded measurements plays a central role in a multitude of practically important applications. A particularly challenging instance of this problem occurs in the case when the degradation phenomenon is modeled by an ill-conditioned operator. In such a situation, the presence of noise makes it impossible to recover a valuable approximation of the image of interest without using some a priori information about its properties. Such a priori information--commonly referred to as simply priors--is essential for image restoration, rendering it stable and robust to noise. Moreover, using the priors makes the recovered images exhibit some plausible features of their original counterpart. Particularly, if the original image is known to be a piecewise smooth function, one of the standard priors used in this case is defined by the Rudin-Osher-Fatemi model, which results in total variation (TV) based image restoration. The current arsenal of algorithms for TV-based image restoration is vast. In this present paper, a different approach to the solution of the problem is proposed based upon the method of iterative shrinkage (aka iterated thresholding). In the proposed method, the TV-based image restoration is performed through a recursive application of two simple procedures, viz. linear filtering and soft thresholding. Therefore, the method can be identified as belonging to the group of first-order algorithms which are efficient in dealing with images of relatively large sizes. Another valuable feature of the proposed method consists in its working directly with the TV functional, rather then with its smoothed versions. Moreover, the method provides a single solution for both isotropic and anisotropic definitions of the TV functional, thereby establishing a useful connection between the two formulae. Finally, a number of standard examples of image deblurring are demonstrated, in which the proposed method can provide restoration results of superior quality as compared to the case of sparse-wavelet deconvolution.
21078572	Practical bounds on image denoising: from estimation to information.
IEEE Trans Image Process 20101115 2011May
Recently, in a previous work, we proposed a way to bound how well any given image can be denoised. The bound was computed directly from the noise-free image that was assumed to be available. In this work, we extend the formulation to the more practical case where no ground truth is available. We show that the parameters of the bounds, namely the cluster covariances and level of redundancy for patches in the image, can be estimated directly from the noise corrupted image. Further, we analyze the bounds formulation to show that these two parameters are interdependent and they, along with the bounds formulation as a whole, have a nice information-theoretic interpretation as well. The results are verified through a variety of well-motivated experiments.
21078576	High dynamic range image display with halo and clipping prevention.
IEEE Trans Image Process 20101115 2011May
The dynamic range of an image is defined as the ratio between the highest and the lowest luminance level. In a high dynamic range (HDR) image, this value exceeds the capabilities of conventional display devices; as a consequence, dedicated visualization techniques are required. In particular, it is possible to process an HDR image in order to reduce its dynamic range without producing a significant change in the visual sensation experienced by the observer. In this paper, we propose a dynamic range reduction algorithm that produces high-quality results with a low computational cost and a limited number of parameters. The algorithm belongs to the category of methods based upon the Retinex theory of vision and was specifically designed in order to prevent the formation of common artifacts, such as halos around the sharp edges and clipping of the highlights, that often affect methods of this kind. After a detailed analysis of the state of the art, we shall describe the method and compare the results and performance with those of two techniques recently proposed in the literature and one commercial software.
21078577	Information content weighting for perceptual image quality assessment.
IEEE Trans Image Process 20101115 2011May
Many state-of-the-art perceptual image quality assessment (IQA) algorithms share a common two-stage structure: local quality/distortion measurement followed by pooling. While significant progress has been made in measuring local image quality/distortion, the pooling stage is often done in ad-hoc ways, lacking theoretical principles and reliable computational models. This paper aims to test the hypothesis that when viewing natural images, the optimal perceptual weights for pooling should be proportional to local information content, which can be estimated in units of bit using advanced statistical models of natural images. Our extensive studies based upon six publicly-available subject-rated image databases concluded with three useful findings. First, information content weighting leads to consistent improvement in the performance of IQA algorithms. Second, surprisingly, with information content weighting, even the widely criticized peak signal-to-noise-ratio can be converted to a competitive perceptual quality measure when compared with state-of-the-art algorithms. Third, the best overall performance is achieved by combining information content weighting with multiscale structural similarity measures.
21513174	Medicaid program; federal funding for Medicaid eligibility determination and enrollment activities. Final rule.
Fed Regist  2011Apr19
This final rule will revise Medicaid regulations for Mechanized Claims Processing and Information Retrieval Systems. We are also modifying our regulations so that the enhanced Federal financial participation (FFP) is available for design, development and installation or enhancement of eligibility determination systems until December 31, 2015. This final rule also imposes certain defined standards and conditions in terms of timeliness, accuracy, efficiency, and integrity for mechanized claims processing and information retrieval systems in order to receive enhanced FFP.
21472891	A new face and new challenges for Online Mendelian Inheritance in Man (OMIM®).
Hum. Mutat. 20110405 2011May
OMIM's task of cataloging the association between human phenotypes and their causative genes (the Morbid Map of the Genome) and classifying and naming newly recognized disorders is growing rapidly. Establishing the relationship between genotype and phenotype has become increasingly complex. New technologies such as genome-wide association studies (GWAS) and array comparative genomic hybridization (aCGH) define "risk alleles" that are inherently prone to substantial interpretation and modification. In addition, whole exome and genome sequencing are expected to result in many reports of new mendelian disorders and their causative genes. In preparation for the onslaught of new information, we have launched a new Website to allow a more comprehensive and structured view of the contents of OMIM and to improve interconnectivity with complementary clinical and basic science genetics resources. This article focuses on the content of OMIM, the process and intent of disease classification and nosology, and anticipated improvements in our new Website (http://www.omim.org).
21521456	Nursing clinical documentation data retrieval for hospitalized older adults with heart failure: part 2.
Int J Nurs Terminol Classif  2011 Apr-Jun
The study aims to discuss the implications for retrieval of nursing data and building a multiorganizational data warehouse. The method used was a descriptive comparative multisite study of documented care for 302 older adults with heart failure. Unit and patient level variables were retrieved. Data regarding the most identified variables were retrievable electronically. Important linkages among nursing data elements were not present. Data were retrievable and the building of a data warehouse was possible and lessons were learned. When clinical information systems (CISs) are developed, developers and nurses must discuss how standardized data will be entered to ensure retrieval and usefulness in evaluating nursing care. For nursing effectiveness research, CISs must also provide linkages among nursing diagnoses and specific interventions, and nursing-sensitive patient outcomes.
21245417	VeryGene: linking tissue-specific genes to diseases, drugs, and beyond for knowledge discovery.
Physiol. Genomics 20110118 2011Apr27
In addition to many other genes, tissue-specific genes (TSGs) represent a set of genes of great importance for human physiology. However, the links among TSGs, diseases, and potential therapeutic agents are often missing, hidden, or too scattered to find. There is a need to establish a knowledgebase for researchers to share this and additional information in order to speed up discovery and clinical practice. As an initiative toward systems biology, the VeryGene web server was developed to fill this gap. A significant effort has been made to integrate TSGs from two large-scale data analyses with respective information on subcellular localization, Gene Ontology, Reactome, KEGG pathway, Mouse Genome Informatics (MGI) Mammalian Phenotype, disease association, and targeting drugs. The current release carefully selected 3,960 annotated TSGs derived from 127 normal human tissues and cell types, including 5,672 gene-disease and 2,171 drug-target relationships. In addition to being a specialized source for TSGs, VeryGene can be used as a discovery tool by generating novel inferences. Some inherently useful but hidden relations among genes, diseases, drugs, and other important aspects can be inferred to form testable hypotheses. VeryGene is available online at http://www.verygene.com.
21474380	Identity crisis? The need for systematic gene IDs.
Trends Parasitol. 20110405 2011May
Recent years have seen an explosion in the availability of protozoan pathogen genome sequences. Although data regarding the underlying genome sequence remain relatively stable after the initial draft, understanding of specific gene function is increasing rapidly. This dichotomy is reflected in the relative stability of systematic gene identifiers (SysIDs(*)) in genome sequence databases, as compared to evolving and/or conflicting gene and gene product names. GenBank/EMBL/DDBJ accession numbers are important, but most protozoan parasite researchers use organism-based databases such as EuPathDB or GeneDB as their immediate resource for gene-based information because they not only provide sequence information but also functional information and links to references. Reference to SysIDs therefore provides a valuable bridge to this repository of information.
21439897	MEDRank: using graph-based concept ranking to index biomedical texts.
Int J Med Inform 20110325 2011Jun
As the volume of biomedical text increases exponentially, automatic indexing becomes increasingly important. However, existing approaches do not distinguish central (or core) concepts from concepts that were mentioned in passing. We focus on the problem of indexing MEDLINE records, a process that is currently performed by highly trained humans at the National Library of Medicine (NLM). NLM indexers are assisted by a system called the Medical Text Indexer (MTI) that suggests candidate indexing terms. To improve the ability of MTI to select the core terms in MEDLINE abstracts. These core concepts are deemed to be most important and are designated as "major headings" by MEDLINE indexers. We introduce and evaluate a graph-based indexing methodology called MEDRank that generates concept graphs from biomedical text and then ranks the concepts within these graphs to identify the most important ones. We insert a MEDRank step into the MTI and compare MTI's output with and without MEDRank to the MEDLINE indexers' selected terms for a sample of 11,803 PubMed Central articles. We also tested whether human raters prefer terms generated by the MEDLINE indexers, MTI without MEDRank, and MTI with MEDRank for a sample of 36 PubMed Central articles. MEDRank improved recall of major headings designated by 30% over MTI without MEDRank (0.489 vs. 0.376). Overall recall was only slightly (6.5%) higher (0.490 vs. 0.460) as was F(2) (3%, 0.408 vs. 0.396). However, overall precision was 3.9% lower (0.268 vs. 0.279). Human raters preferred terms generated by MTI with MEDRank over terms generated by MTI without MEDRank (by an average of 1.00 more term per article), and preferred terms generated by MTI with MEDRank and the MEDLINE indexers at the same rate. The addition of MEDRank to MTI significantly improved the retrieval of core concepts in MEDLINE abstracts and more closely matched human expectations compared to MTI without MEDRank. In addition, MEDRank slightly improved overall recall and F(2).
21534110	Library links on medical school home pages.
Med Ref Serv Q  2011
The purpose of this study was to assess the websites of American Association of Medical Colleges (AAMC)-member medical schools for the presence of library links. Sixty-one percent (n = 92) of home pages of the 150 member schools of the AAMC contain library links. For the 58 home pages not offering such links, 50 provided a pathway of two or three clicks to a library link. The absence of library links on 39% of AAMC medical school home pages indicates that the designers of those pages did not consider the library to be a primary destination for their visitors.
21534111	Customized USB flash drives used to promote library resources and services to first-year medical and dental students.
Med Ref Serv Q  2011
In line with its institution's commitment to keep new student orientation/welcome events "green," Columbia University's Health Sciences Library (HSL) applied for a National Network of Libraries of Medicine Middle Atlantic Region (NN/LM MAR) Small Projects Award(1) in Spring/Summer 2009 to explore paperless modes of outreach. This article describes the project that resulted from this funding, whose purpose was to determine whether customized 1 GB USB flash drives are a good vehicle for distributing library promotional materials to incoming medical and dental students. This project gave HSL librarians the opportunity to connect with students in a way that had not been done before and to consider how these new students use/view the information the library produces.
21534112	Live and online: using co-streaming to reach users.
Med Ref Serv Q  2011
The increase in distance education students and the changing preferences for online instruction led the Health Sciences Library to seek creative approaches to traditional classroom instruction. Library instructors compared two different class formats: online-only classes and in-person classes with online sections. The second format, called "co-streaming," provided instruction in traditional classroom and virtual environments at the same time. A postclass survey was used to gather users' evaluations of the instruction and the format via which it was offered. This paper examines the user response to, and satisfaction with, the co-streaming classes.
21534114	Health sciences librarians' research on medical students' use of information for their studies at the medical school, University of Queensland, Australia.
Med Ref Serv Q  2011
This study reports the findings of research undertaken by health sciences librarians at the University of Queensland Library into how medical students use information for their studies, particularly resources and services provided by the Library. The methods utilized were an online survey and focus groups. Results indicated that students favor print resources over electronic, value accessing resources on a one-stop basis, and prefer training to be delivered flexibly. The implication of these results for future resource selection, service provision, and instructional design and delivery is discussed.
21469218	Design rules for phase-change materials in data storage applications.
Adv. Mater. Weinheim 20110405 2011May10
Phase-change materials can rapidly and reversibly be switched between an amorphous and a crystalline phase. Since both phases are characterized by very different optical and electrical properties, these materials can be employed for rewritable optical and electrical data storage. Hence, there are considerable efforts to identify suitable materials, and to optimize them with respect to specific applications. Design rules that can explain why the materials identified so far enable phase-change based devices would hence be very beneficial. This article describes materials that have been successfully employed and dicusses common features regarding both typical structures and bonding mechanisms. It is shown that typical structural motifs and electronic properties can be found in the crystalline state that are indicative for resonant bonding, from which the employed contrast originates. The occurence of resonance is linked to the composition, thus providing a design rule for phase-change materials. This understanding helps to unravel characteristic properties such as electrical and thermal conductivity which are discussed in the subsequent section. Then, turning to the transition kinetics between the phases, the current understanding and modeling of the processes of amorphization and crystallization are discussed. Finally, present approaches for improved high-capacity optical discs and fast non-volatile electrical memories, that hold the potential to succeed present-day's Flash memory, are presented.
21534835	Computer use, internet access, and online health searching among Harlem adults.
Am J Health Promot  2011 May-Jun
Computer use, Internet access, and online searching for health information were assessed toward enhancing Internet use for health promotion. Cross-sectional random digit dial landline phone survey. Eight zip codes that comprised Central Harlem/Hamilton Heights and East Harlem in New York City. Adults 18 years and older (N=646). Demographic characteristics, computer use, Internet access, and online searching for health information. Frequencies for categorical variables and means and standard deviations for continuous variables were calculated and compared with analogous findings reported in national surveys from similar time periods. Among Harlem adults, ever computer use and current Internet use were 77% and 52%, respectively. High-speed home Internet connections were somewhat lower for Harlem adults than for U.S. adults overall (43% vs. 68%). Current Internet users in Harlem were more likely to be younger, white vs. black or Hispanic, better educated, and in better self-reported health than non-current users (p&lt;.01). Of those who reported searching online for health information, 74% sought information on medical problems and thought that information found on the Internet affected the way they eat (47%) or exercise (44%). Many Harlem adults currently use the Internet to search for health information. High-speed connections and culturally relevant materials may facilitate health information searching for underserved groups.
21161469	DART, a platform for the creation and registration of cone beam digital tomosynthesis datasets.
Australas Phys Eng Sci Med 20101216 2011Apr
Digital tomosynthesis is an imaging modality that allows for tomographic reconstructions using only a fraction of the images needed for CT reconstruction. Since it offers the advantages of tomographic images with a smaller imaging dose delivered to the patient, the technique offers much promise for use in patient positioning prior to radiation delivery. This paper describes a software environment developed to help in the creation of digital tomosynthesis image sets from digital portal images using three different reconstruction algorithms. The software then allows for use of the tomograms for patient positioning or for dose recalculation if shifts are not applied, possibly as part of an adaptive radiotherapy regimen.
21328403	Pharmacophore alignment search tool: influence of scoring systems on text-based similarity searching.
J Comput Chem 20110215 2011Jun
The text-based similarity searching method Pharmacophore Alignment Search Tool is grounded on pairwise comparisons of potential pharmacophoric points between a query and screening compounds. The underlying scoring matrix is of critical importance for successful virtual screening and hit retrieval from large compound libraries. Here, we compare three conceptually different computational methods for systematic deduction of scoring matrices: assignment-based, alignment-based, and stochastic optimization. All three methods resulted in optimized pharmacophore scoring matrices with significantly superior retrospective performance in comparison with simplistic scoring schemes. Computer-generated similarity matrices of pharmacophoric features turned out to agree well with a manually constructed matrix. We introduce the concept of position-specific scoring to text-based similarity searching so that knowledge about specific ligand-receptor binding patterns can be included and demonstrate its benefit for hit retrieval. The approach was also used for automated pharmacophore elucidation in agonists of peroxisome proliferator activated receptor gamma, successfully identifying key interactions for receptor activation.
21473243	Three PubMed skills to support evidence-based dentistry.
Tex Dent J  2011Feb
The National Library of Medicine's PubMed database can powerfully assist dentists in evidence-based practice. Three useful PubMed skills can improve the efficiency of the clinician's search: (1) Use of MeSH terms; (2) Use of Limits; (3) Use of Clinical Queries.
21473245	Teaching evidence-based practice at the University of Texas Dental Branch at Houston.
Tex Dent J  2011Feb
This brief report outlines the current curriculum for evidence-based practice at The University of Texas Dental Branch at Houston (UTDB). This curriculum is now based on the American Dental Association's Commission on Dental Accreditation 2010 Accreditation Standards for Dental Education Programs. Evidence-based practice is introduced to students in the first-year curriculum. Students learn to be clinically effective through use of the components of evidence-based practice, information search and retrieval, critical thinking (appraisal), and through information resource evaluation and then application to the patient or population. Planned innovations in curriculum include further implementation of evidence-based decision-making in clinical courses, including development of the clinical prescription as a means of demonstrating competence in asking and answering clinical questions, and of the portfolio as a means of demonstrating overall competence.
21473246	Teaching evidence-based practice at the University of Texas Health Science Center at San Antonio dental school.
Tex Dent J  2011Feb
The overarching goal of the Evidence-Based Practice Program at San Antonio is to provide our graduates with life-long learning skills that will enable them to keep up-to-date and equip them with the best possible patient care skills during their 30-40 years of practice. Students are taught to (1) ask focused clinical questions, (2) search the biomedical research literature (PubMed) for the most recent and highest level of evidence, (3) critically evaluate the evidence, and (4) make clinical judgments about the applicability of the evidence for their patients. Students must demonstrate competency with these "just-in-time" learning skills through writing concise one-page Critically Appraised Topics (CATs) on focused clinical questions. The school has established an online searchable library of these Critically Appraised Topics. This library provides students and faculty with rapid, up-to-date evidence-based answers to clinical questions. The long-range plan is to make this online library available to practitioners and the public.
21165536	FDG PET/CT in cancer therapy monitoring: computer-assisted analysis of baseline together with up to two follow-ups.
Nuklearmedizin 20101217 2011
We developed and tested a software tool for computer-assisted analysis of FDG-PET/CT in cancer therapy monitoring. The tool provides automatic semi-quantitative analysis of a baseline scan together with up to two follow-up scans (standardized uptake values, glycolytic volume). The tool also supports visual analysis by local spatial registration which allows display of tumor lesions with the same orientation in all scans. The tool's stability and accuracy was tested at typical everyday image quality. Ten unselected cancer patients in whom three FDG PET/CT scans had been performed were included. A total of 18 lesions were analyzed. Automatic lesion tracking worked properly in all lesions but one. In this lesion local coregistration had to be adjusted manually which, however, is easily performed with the tool. Semi-automatic lesion segmentation and fully automatic semi-quantitative analysis worked properly in all cases. Computer-assisted analysis was significantly less time consuming than manual analysis. The novel software tool appears useful for analysis of FDG-PET/CT in cancer therapy monitoring in clinical routine patient care.
21485181	[Research on DICOM SR].
Sheng Wu Yi Xue Gong Cheng Xue Za Zhi  2011Feb
This paper is aimed to research into the information model of the Digital Imaging and Communication in Medicine (DICOM) Structured Reporting (SR), and to introduce DICOM information object definitions (IODs) and services used for the storage and transmission of SR. The DICOM services are concerned with storage, query, retrieval, and transfer of data, and give a brief introduction to DICOM DIR. DICOM DIR is a file based on medical information. According to the DICOM DIR definition in the DICOM part ten, it may be found that the composite objects referenced in the DICOM SR. So putting forward the management of DICOM files by DICOM DIR sets, It effectively improves the efficiency of the object referenced by SR. This can increase the ability to access the data. For scientific research, medical data mining and applications, DICOM SR can profit the communication of medical information in different hospitals, and this can be useful for the analysis, research, summary, classification and extraction of a large quantity of medical information.
21377835	On the creation of a segmentation library for digitized cervical and lumbar spine radiographs.
Comput Med Imaging Graph 20110305 2011Jun
In this paper, we address the issue of computer-assisted indexing in one specific case, i.e., for the 17,000 digitized images of the spine acquired during the National Health and Nutrition Examination Survey (NHANES). The crucial step in this process is to accurately segment the cervical and lumbar spine in the radiographic images. To that end, we have implemented a unique segmentation system that consists of a suite of spine-customized automatic and semi-automatic statistical shape segmentation algorithms. Using the aforementioned system, we have developed experiments to optimally generate a library of spine segmentations, which currently include 2000 cervical and 2000 lumbar spines. This work is expected to contribute toward the creation of a biomedical Content-Based Image Retrieval system that will allow retrieval of vertebral shapes by using query by image example or query by shape example.
21486887	Collaborative search in electronic health records.
J Am Med Inform Assoc  2011May1
A full-text search engine can be a useful tool for augmenting the reuse value of unstructured narrative data stored in electronic health records (EHR). A prominent barrier to the effective utilization of such tools originates from users' lack of search expertise and/or medical-domain knowledge. To mitigate the issue, the authors experimented with a 'collaborative search' feature through a homegrown EHR search engine that allows users to preserve their search knowledge and share it with others. This feature was inspired by the success of many social information-foraging techniques used on the web that leverage users' collective wisdom to improve the quality and efficiency of information retrieval. The authors conducted an empirical evaluation study over a 4-year period. The user sample consisted of 451 academic researchers, medical practitioners, and hospital administrators. The data were analyzed using a social-network analysis to delineate the structure of the user collaboration networks that mediated the diffusion of knowledge of search. The users embraced the concept with considerable enthusiasm. About half of the EHR searches processed by the system (0.44 million) were based on stored search knowledge; 0.16 million utilized shared knowledge made available by other users. The social-network analysis results also suggest that the user-collaboration networks engendered by the collaborative search feature played an instrumental role in enabling the transfer of search knowledge across people and domains. Applying collaborative search, a social information-foraging technique popularly used on the web, may provide the potential to improve the quality and efficiency of information retrieval in healthcare.
21486890	Data standards for clinical research data collection forms: current status and challenges.
J Am Med Inform Assoc  2011May1
Case report forms (CRFs) are used for structured-data collection in clinical research studies. Existing CRF-related standards encompass structural features of forms and data items, content standards, and specifications for using terminologies. This paper reviews existing standards and discusses their current limitations. Because clinical research is highly protocol-specific, forms-development processes are more easily standardized than is CRF content. Tools that support retrieval and reuse of existing items will enable standards adoption in clinical research applications. Such tools will depend upon formal relationships between items and terminological standards. Future standards adoption will depend upon standardized approaches for bridging generic structural standards and domain-specific content standards. Clinical research informatics can help define tools requirements in terms of workflow support for research activities, reconcile the perspectives of varied clinical research stakeholders, and coordinate standards efforts toward interoperability across healthcare and research data collection.
21426576	Standard requirements for GCP-compliant data management in multinational clinical trials.
Trials 20110322 2011
A recent survey has shown that data management in clinical trials performed by academic trial units still faces many difficulties (e.g. heterogeneity of software products, deficits in quality management, limited human and financial resources and the complexity of running a local computer centre). Unfortunately, no specific, practical and open standard for both GCP-compliant data management and the underlying IT-infrastructure is available to improve the situation. For that reason the "Working Group on Data Centres" of the European Clinical Research Infrastructures Network (ECRIN) has developed a standard specifying the requirements for high quality GCP-compliant data management in multinational clinical trials. International, European and national regulations and guidelines relevant to GCP, data security and IT infrastructures, as well as ECRIN documents produced previously, were evaluated to provide a starting point for the development of standard requirements. The requirements were produced by expert consensus of the ECRIN Working group on Data Centres, using a structured and standardised process. The requirements were divided into two main parts: an IT part covering standards for the underlying IT infrastructure and computer systems in general, and a Data Management (DM) part covering requirements for data management applications in clinical trials. The standard developed includes 115 IT requirements, split into 15 separate sections, 107 DM requirements (in 12 sections) and 13 other requirements (2 sections). Sections IT01 to IT05 deal with the basic IT infrastructure while IT06 and IT07 cover validation and local software development. IT08 to IT015 concern the aspects of IT systems that directly support clinical trial management. Sections DM01 to DM03 cover the implementation of a specific clinical data management application, i.e. for a specific trial, whilst DM04 to DM12 address the data management of trials across the unit. Section IN01 is dedicated to international aspects and ST01 to the competence of a trials unit's staff. The standard is intended to provide an open and widely used set of requirements for GCP-compliant data management, particularly in academic trial units. It is the intention that ECRIN will use these requirements as the basis for the certification of ECRIN data centres.
21489220	A linguistic rule-based approach to extract drug-drug interactions from pharmacological documents.
BMC Bioinformatics 20110329 2011
A drug-drug interaction (DDI) occurs when one drug influences the level or activity of another drug. The increasing volume of the scientific literature overwhelms health care professionals trying to be kept up-to-date with all published studies on DDI. This paper describes a hybrid linguistic approach to DDI extraction that combines shallow parsing and syntactic simplification with pattern matching. Appositions and coordinate structures are interpreted based on shallow syntactic parsing provided by the UMLS MetaMap tool (MMTx). Subsequently, complex and compound sentences are broken down into clauses from which simple sentences are generated by a set of simplification rules. A pharmacist defined a set of domain-specific lexical patterns to capture the most common expressions of DDI in texts. These lexical patterns are matched with the generated sentences in order to extract DDIs. We have performed different experiments to analyze the performance of the different processes. The lexical patterns achieve a reasonable precision (67.30%), but very low recall (14.07%). The inclusion of appositions and coordinate structures helps to improve the recall (25.70%), however, precision is lower (48.69%). The detection of clauses does not improve the performance. Information Extraction (IE) techniques can provide an interesting way of reducing the time spent by health care professionals on reviewing the literature. Nevertheless, no approach has been carried out to extract DDI from texts. To the best of our knowledge, this work proposes the first integral solution for the automatic extraction of DDI from biomedical texts.
21489224	Automatic classification of sentences to support Evidence Based Medicine.
BMC Bioinformatics 20110329 2011
Given a set of pre-defined medical categories used in Evidence Based Medicine, we aim to automatically annotate sentences in medical abstracts with these labels. We constructed a corpus of 1,000 medical abstracts annotated by hand with specified medical categories (e.g. Intervention, Outcome). We explored the use of various features based on lexical, semantic, structural, and sequential information in the data, using Conditional Random Fields (CRF) for classification. For the classification tasks over all labels, our systems achieved micro-averaged f-scores of 80.9% and 66.9% over datasets of structured and unstructured abstracts respectively, using sequential features. In labeling only the key sentences, our systems produced f-scores of 89.3% and 74.0% over structured and unstructured abstracts respectively, using the same sequential features. The results over an external dataset were lower (f-scores of 63.1% for all labels, and 83.8% for key sentences). Of the features we used, the best for classifying any given sentence in an abstract were based on unigrams, section headings, and sequential information from preceding sentences. These features resulted in improved performance over a simple bag-of-words approach, and outperformed feature sets used in previous work.
21489225	Processing SPARQL queries with regular expressions in RDF databases.
BMC Bioinformatics 20110329 2011
As the Resource Description Framework (RDF) data model is widely used for modeling and sharing a lot of online bioinformatics resources such as Uniprot (dev.isb-sib.ch/projects/uniprot-rdf) or Bio2RDF (bio2rdf.org), SPARQL - a W3C recommendation query for RDF databases - has become an important query language for querying the bioinformatics knowledge bases. Moreover, due to the diversity of users' requests for extracting information from the RDF data as well as the lack of users' knowledge about the exact value of each fact in the RDF databases, it is desirable to use the SPARQL query with regular expression patterns for querying the RDF data. To the best of our knowledge, there is currently no work that efficiently supports regular expression processing in SPARQL over RDF databases. Most of the existing techniques for processing regular expressions are designed for querying a text corpus, or only for supporting the matching over the paths in an RDF graph. In this paper, we propose a novel framework for supporting regular expression processing in SPARQL query. Our contributions can be summarized as follows. 1) We propose an efficient framework for processing SPARQL queries with regular expression patterns in RDF databases. 2) We propose a cost model in order to adapt the proposed framework in the existing query optimizers. 3) We build a prototype for the proposed framework in C++ and conduct extensive experiments demonstrating the efficiency and effectiveness of our technique. Experiments with a full-blown RDF engine show that our framework outperforms the existing ones by up to two orders of magnitude in processing SPARQL queries with regular expression patterns.
21414991	Customizable views on semantically integrated networks for systems biology.
Bioinformatics 20110316 2011May1
The rise of high-throughput technologies in the post-genomic era has led to the production of large amounts of biological data. Many of these datasets are freely available on the Internet. Making optimal use of these data is a significant challenge for bioinformaticians. Various strategies for integrating data have been proposed to address this challenge. One of the most promising approaches is the development of semantically rich integrated datasets. Although well suited to computational manipulation, such integrated datasets are typically too large and complex for easy visualization and interactive exploration. We have created an integrated dataset for Saccharomyces cerevisiae using the semantic data integration tool Ondex, and have developed a view-based visualization technique that allows for concise graphical representations of the integrated data. The technique was implemented in a plug-in for Cytoscape, called OndexView. We used OndexView to investigate telomere maintenance in S. cerevisiae. The Ondex yeast dataset and the OndexView plug-in for Cytoscape are accessible at http://bsu.ncl.ac.uk/ondexview.
21376296	[Store-and-forward teledermatology: assessment of validity in a series of 2000 observations].
Actas Dermosifiliogr 20110303 2011May
The aim of this study was to assess the validity of store-and-forward teledermatology as a tool to support physicians in primary care and hospital emergency services and reduce the requirement for face-to-face appointments. Diagnostic validity and the approach chosen for patient management (face-to-face vs teledermatology) were compared according to patient origin and diagnostic group. Digital images from 100 patients were assessed by 20 different dermatologists and the diagnoses offered were compared with those provided in face-to-face appointments (gold standard). The proposed management of the different groups of patients was also compared. The percentage complete agreement was 69.05% (95% confidence interval [CI], 66.9%-71.0%). The aggregate agreement was 87.80% (95% CI, 86.1%-89.0%). When questioned about appropriate management of the patients, observers elected face-to-face consultation in 60% of patients (95% CI, 58%-61%) and teledermatology in 40% (95% CI, 38%-41%). Diagnostic validity was higher in patients from primary care (76.1% complete agreement and 91.8% aggregate agreement) than those from hospital emergency services (61.8% complete agreement, 83.4% aggregate agreement) (p &lt; 0.001) and teledermatology was also chosen more often in patients from primary care compared with those from emergency services (42% vs 38%; p=0.003). In terms of diagnostic group, higher validity was observed for patients with infectious diseases (73.3% complete agreement and 91.3% aggregate agreement) compared to those with inflammatory disease (70.8% complete agreement and 86.4% aggregate agreement) or tumors (63.0% complete agreement and 87.2% aggregate agreement) (p &lt;0.001). Teledermatology was also chosen more often in patients with infectious diseases (52%) than in those with inflammatory disease (40%) or tumors (28%) (p &lt;0.001). Store-and-forward teledermatology has a high level of diagnostic validity, particularly in those cases referred from primary care and in infectious diseases. It can be considered useful for the diagnosis and management of patients at a distance and would reduce the requirement for face-to-face consultation by 40%.
20851207	A practical method for transforming free-text eligibility criteria into computable criteria.
J Biomed Inform 20100917 2011Apr
Formalizing eligibility criteria in a computer-interpretable language would facilitate eligibility determination for study subjects and the identification of studies on similar patient populations. Because such formalization is extremely labor intensive, we transform the problem from one of fully capturing the semantics of criteria directly in a formal expression language to one of annotating free-text criteria in a format called ERGO annotation. The annotation can be done manually, or it can be partially automated using natural-language processing techniques. We evaluated our approach in three ways. First, we assessed the extent to which ERGO annotations capture the semantics of 1000 eligibility criteria randomly drawn from ClinicalTrials.gov. Second, we demonstrated the practicality of the annotation process in a feasibility study. Finally, we demonstrate the computability of ERGO annotation by using it to (1) structure a library of eligibility criteria, (2) search for studies enrolling specified study populations, and (3) screen patients for potential eligibility for a study. We therefore demonstrate a new and practical method for incrementally capturing the semantics of free-text eligibility criteria into computable form.
21094696	Semi-automatic semantic annotation of PubMed queries: a study on quality, efficiency, satisfaction.
J Biomed Inform 20101120 2011Apr
Information processing algorithms require significant amounts of annotated data for training and testing. The availability of such data is often hindered by the complexity and high cost of production. In this paper, we investigate the benefits of a state-of-the-art tool to help with the semantic annotation of a large set of biomedical queries. Seven annotators were recruited to annotate a set of 10,000 PubMed® queries with 16 biomedical and bibliographic categories. About half of the queries were annotated from scratch, while the other half were automatically pre-annotated and manually corrected. The impact of the automatic pre-annotations was assessed on several aspects of the task: time, number of actions, annotator satisfaction, inter-annotator agreement, quality and number of the resulting annotations. The analysis of annotation results showed that the number of required hand annotations is 28.9% less when using pre-annotated results from automatic tools. As a result, the overall annotation time was substantially lower when pre-annotations were used, while inter-annotator agreement was significantly higher. In addition, there was no statistically significant difference in the semantic distribution or number of annotations produced when pre-annotations were used. The annotated query corpus is freely available to the research community. This study shows that automatic pre-annotations are found helpful by most annotators. Our experience suggests using an automatic tool to assist large-scale manual annotation projects. This helps speed-up the annotation time and improve annotation consistency while maintaining high quality of the final annotations.
21167957	Use of Medical Subject Headings (MeSH) in Portuguese for categorizing web-based healthcare content.
J Biomed Inform 20101216 2011Apr
Internet users are increasingly using the worldwide web to search for information relating to their health. This situation makes it necessary to create specialized tools capable of supporting users in their searches. To apply and compare strategies that were developed to investigate the use of the Portuguese version of Medical Subject Headings (MeSH) for constructing an automated classifier for Brazilian Portuguese-language web-based content within or outside of the field of healthcare, focusing on the lay public. 3658 Brazilian web pages were used to train the classifier and 606 Brazilian web pages were used to validate it. The strategies proposed were constructed using content-based vector methods for text classification, such that Naive Bayes was used for the task of classifying vector patterns with characteristics obtained through the proposed strategies. A strategy named InDeCS was developed specifically to adapt MeSH for the problem that was put forward. This approach achieved better accuracy for this pattern classification task (0.94 sensitivity, specificity and area under the ROC curve). Because of the significant results achieved by InDeCS, this tool has been successfully applied to the Brazilian healthcare search portal known as Busca Saúde. Furthermore, it could be shown that MeSH presents important results when used for the task of classifying web-based content focusing on the lay public. It was also possible to show from this study that MeSH was able to map out mutable non-deterministic characteristics of the web.
21256977	AskHERMES: An online question answering system for complex clinical questions.
J Biomed Inform 20110121 2011Apr
Clinical questions are often long and complex and take many forms. We have built a clinical question answering system named AskHERMES to perform robust semantic analysis on complex clinical questions and output question-focused extractive summaries as answers. This paper describes the system architecture and a preliminary evaluation of AskHERMES, which implements innovative approaches in question analysis, summarization, and answer presentation. Five types of resources were indexed in this system: MEDLINE abstracts, PubMed Central full-text articles, eMedicine documents, clinical guidelines and Wikipedia articles. We compared the AskHERMES system with Google (Google and Google Scholar) and UpToDate and asked physicians to score the three systems by ease of use, quality of answer, time spent, and overall performance. AskHERMES allows physicians to enter a question in a natural way with minimal query formulation and allows physicians to efficiently navigate among all the answer sentences to quickly meet their information needs. In contrast, physicians need to formulate queries to search for information in Google and UpToDate. The development of the AskHERMES system is still at an early stage, and the knowledge resource is limited compared with Google or UpToDate. Nevertheless, the evaluation results show that AskHERMES' performance is comparable to the other systems. In particular, when answering complex clinical questions, it demonstrates the potential to outperform both Google and UpToDate systems. AskHERMES, available at http://www.AskHERMES.org, has the potential to help physicians practice evidence-based medicine and improve the quality of patient care.
21262390	A comparison of two methods for retrieving ICD-9-CM data: the effect of using an ontology-based method for handling terminology changes.
J Biomed Inform 20110122 2011Apr
Most existing controlled terminologies can be characterized as collections of terms, wherein the terms are arranged in a simple list or organized in a hierarchy. These kinds of terminologies are considered useful for standardizing terms and encoding data and are currently used in many existing information systems. However, they suffer from a number of limitations that make data reuse difficult. Relatively recently, it has been proposed that formal ontological methods can be applied to some of the problems of terminological design. Biomedical ontologies organize concepts (embodiments of knowledge about biomedical reality) whereas terminologies organize terms (what is used to code patient data at a certain point in time, based on the particular terminology version). However, the application of these methods to existing terminologies is not straightforward. The use of these terminologies is firmly entrenched in many systems, and what might seem to be a simple option of replacing these terminologies is not possible. Moreover, these terminologies evolve over time in order to suit the needs of users. Any methodology must therefore take these constraints into consideration, hence the need for formal methods of managing changes. Along these lines, we have developed a formal representation of the concept-term relation, around which we have also developed a methodology for management of terminology changes. The objective of this study was to determine whether our methodology would result in improved retrieval of data. Comparison of two methods for retrieving data encoded with terms from the International Classification of Diseases (ICD-9-CM), based on their recall when retrieving data for ICD-9-CM terms whose codes had changed but which had retained their original meaning (code change). Recall and interclass correlation coefficient. Statistically significant differences were detected (p&lt;0.05) with the McNemar test for two terms whose codes had changed. Furthermore, when all the cases are combined in an overall category, our method also performs statistically significantly better (p&lt;0.05). Our study shows that an ontology-based ICD-9-CM data retrieval method that takes into account the effects of terminology changes performs better on recall than one that does not in the retrieval of data for terms whose codes had changed but which retained their original meaning.
21431599	Enhancing medical research efficiency by using concept maps.
Adv. Exp. Med. Biol.  2011
Even with today's advances in technology, the processes involved in medical research continue to be both time consuming and labor intensive. We have built an experimental integrated tool to convert the textual information available to the researchers into a concept map using the Web Ontology Language as an intermediate source of information. This tool is based on building semantic models using concept maps. The labor-intensive sequence of processes involved in medical research is suitably replaced by using this tool built by a suitable integration of concept maps and Web Ontology Language. We analyzed this tool by considering the example of linking vitamin D deficiency with prostate cancer. This tool is intended to provide a faster solution in building relations and concepts based on the existing facts.
21437291	Clustering more than two million biomedical publications: comparing the accuracies of nine text-based similarity approaches.
PLoS ONE 20110317 2011
We investigate the accuracy of different similarity approaches for clustering over two million biomedical documents. Clustering large sets of text documents is important for a variety of information needs and applications such as collection management and navigation, summary and analysis. The few comparisons of clustering results from different similarity approaches have focused on small literature sets and have given conflicting results. Our study was designed to seek a robust answer to the question of which similarity approach would generate the most coherent clusters of a biomedical literature set of over two million documents. We used a corpus of 2.15 million recent (2004-2008) records from MEDLINE, and generated nine different document-document similarity matrices from information extracted from their bibliographic records, including titles, abstracts and subject headings. The nine approaches were comprised of five different analytical techniques with two data sources. The five analytical techniques are cosine similarity using term frequency-inverse document frequency vectors (tf-idf cosine), latent semantic analysis (LSA), topic modeling, and two Poisson-based language models--BM25 and PMRA (PubMed Related Articles). The two data sources were a) MeSH subject headings, and b) words from titles and abstracts. Each similarity matrix was filtered to keep the top-n highest similarities per document and then clustered using a combination of graph layout and average-link clustering. Cluster results from the nine similarity approaches were compared using (1) within-cluster textual coherence based on the Jensen-Shannon divergence, and (2) two concentration measures based on grant-to-article linkages indexed in MEDLINE. PubMed's own related article approach (PMRA) generated the most coherent and most concentrated cluster solution of the nine text-based similarity approaches tested, followed closely by the BM25 approach using titles and abstracts. Approaches using only MeSH subject headings were not competitive with those based on titles and abstracts.
21414791	A remote desktop-based telemedicine system.
J Clin Neurosci  2011May
We describe the construction of an almost cost-free telemedicine system using a remote desktop and the public internet system. Using this system, real-time images in Digital Imaging and Communication in Medicine (DICOM) format were observed with their original spatial resolution without any delay. The system is secure for the protection of personal information. There is no risk of viral infection to either the host or the viewer's computer. No specific license, contract or initial investment is necessary. This system may partially alleviate the present shortage of specialists.
21401061	PeakML/mzMatch: a file format, Java library, R library, and tool-chain for mass spectrometry data analysis.
Anal. Chem. 20110314 2011Apr1
The recent proliferation of high-resolution mass spectrometers has generated a wealth of new data analysis methods. However, flexible integration of these methods into configurations best suited to the research question is hampered by heterogeneous file formats and monolithic software development. The mzXML, mzData, and mzML file formats have enabled uniform access to unprocessed raw data. In this paper we present our efforts to produce an equally simple and powerful format, PeakML, to uniformly exchange processed intermediary and result data. To demonstrate the versatility of PeakML, we have developed an open source Java toolkit for processing, filtering, and annotating mass spectra in a customizable pipeline (mzMatch), as well as a user-friendly data visualization environment (PeakML Viewer). The PeakML format in particular enables the flexible exchange of processed data between software created by different groups or companies, as we illustrate by providing a PeakML-based integration of the widely used XCMS package with mzMatch data processing tools. As an added advantage, downstream analysis can benefit from direct access to the full mass trace information underlying summarized mass spectrometry results, providing the user with the means to rapidly verify results. The PeakML/mzMatch software is freely available at http://mzmatch.sourceforge.net, with documentation, tutorials, and a community forum.
21147785	Brazilian external occupational dose management system.
Radiat Prot Dosimetry 20101208 2011Mar
Brazil, a large country, with more than 120,000 workers under individual monitoring for ionising radiation, developed, more than 20 y ago, a centralised data bank for external occupational dose. This old database, however, presented some problems and does not satisfy any more Brazilian present needs, not allowing dose analysis reports, for example. Therefore, a new system that reduces manual tasks, provides system communication support, manages reports and improves data storage management is being developed. This paper describes this new web-based information system, named Brazilian External Occupational Dose Management database system--GDOSE.
21410174	Nanomechanical properties of advanced plasma polymerized coatings for mechanical data storage.
J Phys Chem B 20110316 2011Apr7
In this paper we report on the unprecedented deformation behavior of stratified ultrathin polymer films. The mechanical behavior of layered nanoscale films composed of 8-12 nm thin plasma polymerized hexamethyldisiloxane (ppHMDSO) films on a 70 nm thick film of polystyrene was unveiled by atomic force microscopy nanoindentation. In particular, we observed transitions from the deformation of a thin plate under point load to an elastic contact of a paraboloid of revolution, followed by an elastic-plastic contact for polystyrene and finally an elastic contact for silicon. The different deformation modes were identified on the basis of force-penetration data and atomic force microscopy images of residual indents. A clear threshold was observed for the onset of plastic deformation of the films at loads larger than 2 μN. The measured force curves are in agreement with an elastic and elastic-plastic contact mechanics model, taking the amount of deformation and the geometry of the layer that presumably contributed more to the overall deformation into account. This study shows that the complex deformation behavior of advanced soft matter systems with nanoscale dimensions can be successfully unraveled.
21224340	Chloroplast 2010: a database for large-scale phenotypic screening of Arabidopsis mutants.
Plant Physiol. 20110111 2011Apr
Large-scale phenotypic screening presents challenges and opportunities not encountered in typical forward or reverse genetics projects. We describe a modular database and laboratory information management system that was implemented in support of the Chloroplast 2010 Project, an Arabidopsis (Arabidopsis thaliana) reverse genetics phenotypic screen of more than 5,000 mutants (http://bioinfo.bch.msu.edu/2010_LIMS; www.plastid.msu.edu). The software and laboratory work environment were designed to minimize operator error and detect systematic process errors. The database uses Ruby on Rails and Flash technologies to present complex quantitative and qualitative data and pedigree information in a flexible user interface. Examples are presented where the database was used to find opportunities for process changes that improved data quality. We also describe the use of the data-analysis tools to discover mutants defective in enzymes of leucine catabolism (heteromeric mitochondrial 3-methylcrotonyl-coenzyme A carboxylase [At1g03090 and At4g34030] and putative hydroxymethylglutaryl-coenzyme A lyase [At2g26800]) based upon a syndrome of pleiotropic seed amino acid phenotypes that resembles previously described isovaleryl coenzyme A dehydrogenase (At3g45300) mutants. In vitro assay results support the computational annotation of At2g26800 as hydroxymethylglutaryl-coenzyme A lyase.
21418606	Appearance frequency modulated gene set enrichment testing.
BMC Bioinformatics 20110320 2011
Gene set enrichment testing has helped bridge the gap from an individual gene to a systems biology interpretation of microarray data. Although gene sets are defined a priori based on biological knowledge, current methods for gene set enrichment testing treat all genes equal. It is well-known that some genes, such as those responsible for housekeeping functions, appear in many pathways, whereas other genes are more specialized and play a unique role in a single pathway. Drawing inspiration from the field of information retrieval, we have developed and present here an approach to incorporate gene appearance frequency (in KEGG pathways) into two current methods, Gene Set Enrichment Analysis (GSEA) and logistic regression-based LRpath framework, to generate more reproducible and biologically meaningful results. Two breast cancer microarray datasets were analyzed to identify gene sets differentially expressed between histological grade 1 and 3 breast cancer. The correlation of Normalized Enrichment Scores (NES) between gene sets, generated by the original GSEA and GSEA with the appearance frequency of genes incorporated (GSEA-AF), was compared. GSEA-AF resulted in higher correlation between experiments and more overlapping top gene sets. Several cancer related gene sets achieved higher NES in GSEA-AF as well. The same datasets were also analyzed by LRpath and LRpath with the appearance frequency of genes incorporated (LRpath-AF). Two well-studied lung cancer datasets were also analyzed in the same manner to demonstrate the validity of the method, and similar results were obtained. We introduce an alternative way to integrate KEGG PATHWAY information into gene set enrichment testing. The performance of GSEA and LRpath can be enhanced with the integration of appearance frequency of genes. We conclude that, generally, gene set analysis methods with the integration of information from KEGG PATHWAY performs better both statistically and biologically.
21464862	Risk factors for bladder cancer: challenges of conducting a literature search using PubMed.
Perspect Health Inf Manag 20110401 2011
The objective of this study was to assess the risk factors for bladder cancer using PubMed articles from January 2000 to December 2009. The study also aimed to describe the challenges encountered in the methodology of a literature search for bladder cancer risk factors using PubMed. Twenty-six categories of risk factors for bladder cancer were identified using the National Cancer Institute Web site and the Medical Subject Headings (MeSH) Web site. A total of 1,338 PubMed searches were run using the term "urinary bladder cancer" and a risk factor term (e.g., "cigarette smoking") and were screened to identify 260 articles for final analysis. The search strategy had an overall precision of 3.42 percent, relative recall of 12.64 percent, and an F-measure of 5.39 percent. Although search terms derived from MeSH had the highest overall precision and recall, the differences did not reach significance, which indicates that for generalized, free-text searches of the PubMed database, the searchers' own terms are generally as effective as MeSH terms.
21390321	Standard biological parts knowledgebase.
PLoS ONE 20110224 2011
We have created the Knowledgebase of Standard Biological Parts (SBPkb) as a publically accessible Semantic Web resource for synthetic biology (sbolstandard.org). The SBPkb allows researchers to query and retrieve standard biological parts for research and use in synthetic biology. Its initial version includes all of the information about parts stored in the Registry of Standard Biological Parts (partsregistry.org). SBPkb transforms this information so that it is computable, using our semantic framework for synthetic biology parts. This framework, known as SBOL-semantic, was built as part of the Synthetic Biology Open Language (SBOL), a project of the Synthetic Biology Data Exchange Group. SBOL-semantic represents commonly used synthetic biology entities, and its purpose is to improve the distribution and exchange of descriptions of biological parts. In this paper, we describe the data, our methods for transformation to SBPkb, and finally, we demonstrate the value of our knowledgebase with a set of sample queries. We use RDF technology and SPARQL queries to retrieve candidate "promoter" parts that are known to be both negatively and positively regulated. This method provides new web based data access to perform searches for parts that are not currently possible.
21393941	Medical literature search dot com.
Indian J Dermatol Venereol Leprol  2011 Mar-Apr
The Internet provides a quick access to a plethora of the medical literature, in the form of journals, databases, dictionaries, textbooks, indexes, and e-journals, thereby allowing access to more varied, individualized, and systematic educational opportunities. Web search engine is a tool designed to search for information on the World Wide Web, which may be in the form of web pages, images, information, and other types of files. Search engines for internet-based search of medical literature include Google, Google scholar, Yahoo search engine, etc., and databases include MEDLINE, PubMed, MEDLARS, etc. Commercial web resources (Medscape, MedConnect, MedicineNet) add to the list of resource databases providing some of their content for open access. Several web-libraries (Medical matrix, Emory libraries) have been developed as meta-sites, providing useful links to health resources globally. Availability of specific dermatology-related websites (DermIs, DermNet, and Genamics Jornalseek) is useful addition to the ever growing list of web-based resources. A researcher must keep in mind the strengths and limitations of a particular search engine/database while searching for a particular type of data. Knowledge about types of literature and levels of detail available, user interface, ease of access, reputable content, and period of time covered allow their optimal use and maximal utility in the field of medicine.
21299447	Short-course adjuvant trastuzumab therapy in early stage breast cancer in Finland: cost-effectiveness and value of information analysis based on the 5-year follow-up results of the FinHer Trial.
Acta Oncol 20110208 2011Apr
Trastuzumab is a standard treatment of HER2-positive early breast cancer in many countries, and it is usually given as a one year adjuvant treatment. However, its cost-effectiveness has not been assessed in Finland. The Finland Herceptin (FinHer) trial has compared a shorter 9-week treatment protocol against no trastuzumab with promising results. The aim of this study was to assess the potential cost-effectiveness of the 9-week treatment based on the recently published five-year follow-up results of the FinHer trial. An evaluation model of breast cancer treatment was constructed using fitted survival estimates and a long-term Markov model. The cost-effectiveness of 9-week adjuvant treatment was assessed in a Finnish setting, compared to treatment without trastuzumab. The analysis was performed from a societal perspective, and a 3% discount rate was applied for future costs and outcomes. Value of information analysis was performed to estimate the potential value of further research. According to the probabilistic analysis, the incremental cost-effectiveness ratio was €12 000 per quality adjusted life year (QALY), and €9300 per life year gained (LYG), when comparing adjuvant trastuzumab therapy to standard treatment without trastuzumab. The modelled incremental outcomes for trastuzumab treatment were 0.66 QALY and 0.85 LYG for a lifetime perspective. Value of information analysis showed that additional research on treatment effects would be most valuable for reducing uncertainty in the adoption decision. Adjuvant 9-week trastuzumab is likely to be a cost-effective treatment in the Finnish setting. Results from an ongoing trial comparing adjuvant 9-week treatment with the 12-month treatment will play a key role in addressing the uncertainty related to the treatment effect and potential cost-effectiveness of these two treatment protocols.
21400687	Disease and phenotype data at Ensembl.
Curr Protoc Hum Genet  2011Apr
Biological databases are an important resource for the life sciences community. Accessing the hundreds of databases supporting molecular biology and related fields is a daunting and time-consuming task. Integrating this information into one access point is a necessity for the life sciences community, which includes researchers focusing on human disease. Here we discuss the Ensembl genome browser, which acts as a single entry point with Graphical User Interface to data from multiple projects, including OMIM, dbSNP, and the NHGRI GWAS catalog. Ensembl provides a comprehensive source of annotation for the human genome, along with other species of biomedical interest. In this unit, we explore how to use the Ensembl genome browser in example queries related to human genetic diseases. Support protocols demonstrate quick sequence export using the BioMart tool.
21056022	SAM: String-based sequence search algorithm for mitochondrial DNA database queries.
Forensic Sci Int Genet 20101105 2011Mar
The analysis of the haploid mitochondrial (mt) genome has numerous applications in forensic and population genetics, as well as in disease studies. Although mtDNA haplotypes are usually determined by sequencing, they are rarely reported as a nucleotide string. Traditionally they are presented in a difference-coded position-based format relative to the corrected version of the first sequenced mtDNA. This convention requires recommendations for standardized sequence alignment that is known to vary between scientific disciplines, even between laboratories. As a consequence, database searches that are vital for the interpretation of mtDNA data can suffer from biased results when query and database haplotypes are annotated differently. In the forensic context that would usually lead to underestimation of the absolute and relative frequencies. To address this issue we introduce SAM, a string-based search algorithm that converts query and database sequences to position-free nucleotide strings and thus eliminates the possibility that identical sequences will be missed in a database query. The mere application of a BLAST algorithm would not be a sufficient remedy as it uses a heuristic approach and does not address properties specific to mtDNA, such as phylogenetically stable but also rapidly evolving insertion and deletion events. The software presented here provides additional flexibility to incorporate phylogenetic data, site-specific mutation rates, and other biologically relevant information that would refine the interpretation of mitochondrial DNA data. The manuscript is accompanied by freeware and example data sets that can be used to evaluate the new software (http://stringvalidation.org).
21310488	A bioinformatics pipeline to build a knowledge database for in silico antibody engineering.
Mol. Immunol.  2011Apr
A challenge to antibody engineering is the large number of positions and nature of variation and opposing concerns of introducing unfavorable biochemical properties. While large libraries are quite successful in identifying antibodies with improved binding or activity, still only a fraction of possibilities can be explored and that would require considerable effort. The vast array of natural antibody sequences provides a potential wealth of information on (1) selecting hotspots for variation, and (2) designing mutants to mimic natural variations seen in hotspots. The human immune system can generate an enormous diversity of immunoglobulins against an almost unlimited range of antigens by gene rearrangement of a limited number of germline variable, diversity and joining genes followed by somatic hypermutation and antigen selection. All the antibody sequences in NCBI database can be assigned to different germline genes. As a result, a position specific scoring matrix for each germline gene can be constructed by aligning all its member sequences and calculating the amino acid frequencies for each position. The position specific scoring matrix for each germline gene characterizes "hotspots" and the nature of variations, and thus reduces the sequence space of exploration in antibody engineering. We have developed a bioinformatics pipeline to conduct analysis of human antibody sequences, and generated a comprehensive knowledge database for in silico antibody engineering. The pipeline is fully automatic and the knowledge database can be refreshed anytime by re-running the pipeline. The refresh process is fast, typically taking 1min on a Lenovo ThinkPad T60 laptop with 3G memory. Our knowledge database consists of (1) the individual germline gene usage in generation of natural antibodies; (2) the CDR length distributions; and (3) the position specific scoring matrix for each germline gene. The knowledge database provides comprehensive support for antibody engineering, including de novo library design in selection of favorable germline V gene scaffolds and CDR lengths. In addition, we have also developed a web application framework to present our knowledge database, and the web interface can help people to easily retrieve a variety of information from the knowledge database.
21145629	ILISI® digital index of the Italian scientific literature of nursing.
Nurse Educ Today 20101209 2011Apr
The IPASVI Rome Nursing Board-Centre of Excellence-began the project of building a free accessed database, Ilisi®, where the main Italian nursing and health-related journals could be consulted (including the few peer reviewed, at international level, Italian journals of nursing). Today, it includes the abstracts of more than 2700 articles from 2004 of about 25 Italian journals of nursing and/or related to nursing disciplines. The Ilisi® project has got with Thisi-Italian thesaurus of nursing science-a controlled vocabulary specifically built for nursing science, its tool of feasibility. This project was developed to foster nursing scholarship in Italy and to offer a free controlled database for all stakeholders (students, nurses, other health professionals, and scholars). The abstracts of the articles of these Italian journals are a tool for lifelong learning and constitute a patrimony for nursing science even at a wider level if this patrimony could be translated in English that will be a further step of the project. The project group who developed this database is going to value Italian nursing literature production and implement an electronic tool that, in the near future, might be used by all students and healthcare professionals in the world. Besides, with this project scientific productions by Italian students, nurses could be encouraged. More of them need to be trained in the use of the most frequently used databases, and Ilisi® could be a good training experience for them.
21292796	Computer-assisted system for diagnosis of Alzheimer disease using data base- independent estimation and fluorodeoxyglucose- positron-emission tomography and 3D-stereotactic surface projection.
AJNR Am J Neuroradiol 20110203 2011Mar
Recently, voxel-based statistical parametric images have been developed as additional diagnostic tools for AD. However these methods require the generation of a data base of healthy brain images. The purpose of this study was to produce and evaluate an automatic method using a data base-independent estimation system for the diagnosis of mild AD. We retrospectively selected 66 subjects, including 33 patients with early AD and 33 age-matched healthy volunteers. Individual brain metabolic images were obtained by using FDG-PET. These were transformed by using 3D-SSP. We then produced CADDIES, which compares the parietal and sensorimotor metabolic counts by using t tests. If parietal metabolism was significantly lower than the sensorimotor metabolism, the subject was automatically diagnosed as having AD. The FDG-PET images were also analyzed by using a previous automatic diagnosis system (CAAD) that is dependent on the construction of a "normal data base" of healthy brain images. Diagnostic performance was compared between the 2 methods. The CADDIES demonstrated a sensitivity of 88%, specificity of 79%, and accuracy of 85%, while the CAAD system demonstrated a sensitivity of 70%, specificity of 94%, and accuracy of 82%. The area under the ROC curve of CADDIES was 0.964. The areas under ROC curves of the CAAD method in the parietal and posterior cingulate gyri were 0.843 and 0.939, respectively. The CADDIES method demonstrated a diagnostic accuracy similar to that of the CAAD system. Our results indicate that this method can be applied to the detection of patients with early AD in routine clinical examinations, with the benefit of not requiring the generation of a normal data base.
20182765	Development and evaluation of a low-cost and high-capacity DICOM image data storage system for research.
J Digit Imaging 20100224 2011Apr
Thin-slice CT data, useful for clinical diagnosis and research, is now widely available but is typically discarded in many institutions, after a short period of time due to data storage capacity limitations. We designed and built a low-cost high-capacity Digital Imaging and COmmunication in Medicine (DICOM) storage system able to store thin-slice image data for years, using off-the-shelf consumer hardware components, such as a Macintosh computer, a Windows PC, and network-attached storage units. "Ordinary" hierarchical file systems, instead of a centralized data management system such as relational database, were adopted to manage patient DICOM files by arranging them in directories enabling quick and easy access to the DICOM files of each study by following the directory trees with Windows Explorer via study date and patient ID. Software used for this system was open-source OsiriX and additional programs we developed ourselves, both of which were freely available via the Internet. The initial cost of this system was about $3,600 with an incremental storage cost of about $900 per 1 terabyte (TB). This system has been running since 7th Feb 2008 with the data stored increasing at the rate of about 1.3 TB per month. Total data stored was 21.3 TB on 23rd June 2009. The maintenance workload was found to be about 30 to 60 min once every 2 weeks. In conclusion, this newly developed DICOM storage system is useful for research due to its cost-effectiveness, enormous capacity, high scalability, sufficient reliability, and easy data access.
20204448	Assessment of performance and reliability of computer-aided detection scheme using content-based image retrieval approach and limited reference database.
J Digit Imaging  2011Apr
Content-based image retrieval approach was used in our computer-aided detection (CAD) schemes for breast cancer detection with mammography. In this study, we assessed CAD performance and reliability using a reference database including 1500 positive (breast mass) regions of interest (ROIs) and 1500 normal ROIs. To test the relationship between CAD performance and the similarity level between the queried ROI and the retrieved ROIs, we applied a set of similarity thresholds to the retrieved similar ROIs selected by the CAD schemes for all queried suspicious regions, and used only the ROIs that were above the threshold for assessing CAD performance at each threshold level. Using the leave-one-out testing method, we computed areas under receiver operating characteristic (ROC) curves (A(Z)) to assess CAD performance. The experimental results showed that as threshold increase, (1) less true positive ROIs can be referenced in the database than normal ROIs and (2) the A(Z) value was monotonically increased from 0.854 ± 0.004 to 0.932 ± 0.016. This study suggests that (1) in order to more accurately detect and diagnose subtle masses, a large and diverse database is required, and (2) assessing the reliability of the decision scores based on the similarity measurement is important in application of the CBIR-based CAD schemes when the limited database is used.
20376525	Content-based image retrieval in radiology: current status and future directions.
J Digit Imaging  2011Apr
Diagnostic radiology requires accurate interpretation of complex signals in medical images. Content-based image retrieval (CBIR) techniques could be valuable to radiologists in assessing medical images by identifying similar images in large archives that could assist with decision support. Many advances have occurred in CBIR, and a variety of systems have appeared in nonmedical domains; however, permeation of these methods into radiology has been limited. Our goal in this review is to survey CBIR methods and systems from the perspective of application to radiology and to identify approaches developed in nonmedical applications that could be translated to radiology. Radiology images pose specific challenges compared with images in the consumer domain; they contain varied, rich, and often subtle features that need to be recognized in assessing image similarity. Radiology images also provide rich opportunities for CBIR: rich metadata about image semantics are provided by radiologists, and this information is not yet being used to its fullest advantage in CBIR systems. By integrating pixel-based and metadata-based image feature analysis, substantial advances of CBIR in medicine could ensue, with CBIR systems becoming an important tool in radiology practice.
20386951	Development of a kidney TeleUltrasound consultation system.
J Digit Imaging  2011Apr
This paper presents the development of kidney TeleUltrasound consultation system. The TeleUltrasound system provides an innovative design that aids the acquisition, archiving, and dissemination of medical data and information over the internet as its backbone. The system provides data sharing to allow remote collaboration, viewing, consultation, and diagnosis of medical data. The design is layered upon a standard known as Digital Imaging and Communication in Medicine (DICOM). The DICOM standard defines protocols for exchanging medical images and their associated data. The TeleUltrasound system is an integrated solution for retrieving, processing, and archiving images and providing data storage management using Structured Query Language (SQL) database. Creating a web-based interface is an additional advantage to achieve global accessibility of experts that will widely open the opportunity of greater examination and multiple consultations. This system is equipped with a high level of data security and its performance has been tested with white, black, and gray box techniques. And the result was satisfactory. The overall system has been evaluated by several radiologists in Malaysia, United Arab Emirates, and Sudan, the result is shown within this paper.
20517632	Design of a Web-tool for diagnostic clinical trials handling medical imaging research.
J Digit Imaging  2011Apr
New clinical studies in medicine are based on patients and controls using different imaging diagnostic modalities. Medical information systems are not designed for clinical trials employing clinical imaging. Although commercial software and communication systems focus on storage of image data, they are not suitable for storage and mining of new types of quantitative data. We sought to design a Web-tool to support diagnostic clinical trials involving different experts and hospitals or research centres. The image analysis of this project is based on skeletal X-ray imaging. It involves a computerised image method using quantitative analysis of regions of interest in healthy bone and skeletal metastases. The database is implemented with ASP.NET 3.5 and C# technologies for our Web-based application. For data storage, we chose MySQL v.5.0, one of the most popular open source databases. User logins were necessary, and access to patient data was logged for auditing. For security, all data transmissions were carried over encrypted connections. This Web-tool is available to users scattered at different locations; it allows an efficient organisation and storage of data (case report form) and images and allows each user to know precisely what his task is. The advantages of our Web-tool are as follows: (1) sustainability is guaranteed; (2) network locations for collection of data are secured; (3) all clinical information is stored together with the original images and the results derived from processed images and statistical analysis that enable us to perform retrospective studies; (4) changes are easily incorporated because of the modular architecture; and (5) assessment of trial data collected at different sites is centralised to reduce statistical variance.
20544372	Challenges for data storage in medical imaging research.
J Digit Imaging  2011Apr
Researchers in medical imaging have multiple challenges for storing, indexing, maintaining viability, and sharing their data. Addressing all these concerns requires a constellation of tools, but not all of them need to be local to the site. In particular, the data storage challenges faced by researchers can begin to require professional information technology skills. With limited human resources and funds, the medical imaging researcher may be better served with an outsourcing strategy for some management aspects. This paper outlines an approach to manage the main objectives faced by medical imaging scientists whose work includes processing and data mining on non-standard file formats, and relating those files to the their DICOM standard descendents. The capacity of the approach scales as the researcher's need grows by leveraging the on-demand provisioning ability of cloud computing.
20734101	Will the next generation of PACS be sitting on a cloud?
J Digit Imaging  2011Apr
Cloud computing has gathered significant attention from information technology (IT) vendors in providing massively scalable applications as well as highly managed remote services. What is cloud computing and how will it impact the medical IT market? Will the next generation of picture archiving and communication systems be leveraging cloud technology?
20824303	An automated DICOM database capable of arbitrary data mining (including radiation dose indicators) for quality monitoring.
J Digit Imaging  2011Apr
The U.S. National Press has brought to full public discussion concerns regarding the use of medical radiation, specifically x-ray computed tomography (CT), in diagnosis. A need exists for developing methods whereby assurance is given that all diagnostic medical radiation use is properly prescribed, and all patients' radiation exposure is monitored. The "DICOM Index Tracker©" (DIT) transparently captures desired digital imaging and communications in medicine (DICOM) tags from CT, nuclear imaging equipment, and other DICOM devices across an enterprise. Its initial use is recording, monitoring, and providing automatic alerts to medical professionals of excursions beyond internally determined trigger action levels of radiation. A flexible knowledge base, aware of equipment in use, enables automatic alerts to system administrators of newly identified equipment models or software versions so that DIT can be adapted to the new equipment or software. A dosimetry module accepts mammography breast organ dose, skin air kerma values from XA modalities, exposure indices from computed radiography, etc. upon receipt. The American Association of Physicists in Medicine recommended a methodology for effective dose calculations which are performed with CT units having DICOM structured dose reports. Web interface reporting is provided for accessing the database in real-time. DIT is DICOM-compliant and, thus, is standardized for international comparisons. Automatic alerts currently in use include: email, cell phone text message, and internal pager text messaging. This system extends the utility of DICOM for standardizing the capturing and computing of radiation dose as well as other quality measures.
20889434	Natural language morphology integration in off-line Arabic optical text recognition.
IEEE Trans Syst Man Cybern B Cybern 20100930 2011Apr
In this paper, we propose a new linguistic-based approach called the affixal approach for Arabic word and text image recognition. Most of the existing works in the field integrate the knowledge of the Arabic language in the recognition process in two ways: either in post-recognition using the language of dictionary (dictionary of words) to validate the word hypotheses suggested by the OCR or in the course of the recognition process (recognition directed by a lexicon) using a statistical model of the language (Hidden Markov Model or N-gram). The proposed approach uses the linguistic concepts of the vocabulary to direct and simplify the recognition process. The principal contribution of the proposed approach is to be able to categorize the word hypotheses in words that are either derived or not derived from roots and to characterize morphologically each word hypothesis in order to prepare the text hypotheses for later analyses (for example, syntactic analysis; to filter the sentence hypotheses).
21413063	Two-point dixon method with flexible echo times.
Magn Reson Med 20101116 2011Apr
The two-point Dixon method is a proton chemical shift imaging technique that produces separated water-only and fat-only images from a dual-echo acquisition. It is shown how this can be achieved without the usual constraints on the echo times. A signal model considering spectral broadening of the fat peak is proposed for improved water/fat separation. Phase errors, mostly due to static field inhomogeneity, must be removed prior to least-squares estimation of water and fat. To resolve ambiguity of the phase errors, a corresponding global optimization problem is formulated and solved using a message-passing algorithm. It is shown that the noise in the water and fat estimates matches the Cramér-Rao bounds, and feasibility is demonstrated for in vivo abdominal breath-hold imaging. The water-only images were found to offer superior fat suppression compared with conventional spectrally fat suppressed images.
21413082	Longitudinal analysis of MRI T2 knee cartilage laminar organization in a subset of patients from the osteoarthritis initiative: a texture approach.
Magn Reson Med 20101202 2011Apr
Cartilage magnetic resonance imaging T(2) relaxation time is sensitive to hydration, collagen content, and tissue anisotropy, and a potential imaging-based biomarker for knee osteoarthritis. This longitudinal pilot study presents an improved cartilage flattening technique that facilitates texture analysis using gray-level co-occurrence matrices parallel and perpendicular to the cartilage layers, and the application of this technique to the knee cartilage of 13 subjects of the osteoarthritis initiative at baseline, 1-year follow-up, and 2-year follow-up. Cartilage flattening showed minimum distortion (∼ 0.5 ms) of mean T(2) values between nonflattened and flattened T(2) maps. Gray-level co-occurrence matrices texture analysis of flattened T(2) maps detected a cartilage laminar organization at baseline, 1-year follow-up, and 2-year follow-up by yielding significant (P &lt; 0.05) differences between texture parameters perpendicular and parallel to the cartilage layers. Tendencies showed higher contrast, dissimilarity, angular second moment, and energy perpendicular to the cartilage layers; and higher homogeneity, entropy, variance, and correlation parallel to them. Significant (P &lt; 0.05) longitudinal texture changes were also detected reflecting subtle signs of a laminar disruption. Tendencies showed decreasing contrast, dissimilarity, and entropy; and increasing homogeneity, energy, and correlation. Results of this study warrant further investigation to complete the assessment of the usefulness of the presented methodology in the study of knee osteoarthritis.
21392381	Web-based database for the management of tissue specimens in a transregional histological research facility.
Diagn Pathol 20110310 2011
In the setting of a histological research core facility sample tracking and project specific archiving of tissue specimens and communication of related data is of central importance. Over a 24-month period 10 laboratories from two transregional research centers submitted in excess of 3000 tissue samples for histological processing and evaluation to our core facility. A web based database was set up to overcome the logistical problem of managing samples with inconsistent, duplicate and missing labels and to allow for efficient sample tracking, archiving and communication with the collaborating research laboratories. The database allows the users to remotely generate unique sample identifiers and enter sample annotation prior to sample processing. Furthermore the database facilitates communication about experimental set-up results and media files such as histological images. Our newly constructed web based portal is an important tool for the management of research samples of our histological core facility and facilitates significantly interdisciplinary and transregional research. It is freely available for scientific use.
20923737	1-D transforms for the motion compensation residual.
IEEE Trans Image Process 20101004 2011Apr
Transforms used in image coding are also commonly used to compress prediction residuals in video coding. Prediction residuals have different spatial characteristics from images, and it is useful to develop transforms that are adapted to prediction residuals. In this paper, we explore the differences between the characteristics of images and motion compensated prediction residuals by analyzing their local anisotropic characteristics and develop transforms adapted to the local anisotropic characteristics of these residuals. The analysis indicates that many regions of motion compensated prediction residuals have 1-D anisotropic characteristics and we propose to use 1-D directional transforms for these regions. We present experimental results with one example set of such transforms within the H.264/AVC codec and the results indicate that the proposed transforms can improve the compression efficiency of motion compensated prediction residuals over conventional transforms.
21422592	Unified reconstruction framework for multi-modal medical imaging.
J Xray Sci Technol  2011
Various types of advanced imaging technologies have significantly improved the quality of medical care available to patients. Corresponding medical image reconstruction algorithms, especially 3D reconstruction, play an important role in disease diagnosis and treatment assessment. However, these increasing reconstruction methods are not implemented in a unified software framework, which brings along lots of disadvantages such as breaking connection of different modalities, lack of module reuse and inconvenience to method comparison. This paper discusses reconstruction process from the viewpoint of data flow and implements a free, accelerated, extensible Unified Reconstruction Software Framework (URSF). The software framework is an abstract solution that supports multi-modal image reconstruction. The goal of this framework is to capture the common processing work flow for different modalities and different methods, make the development of reconstruction for new devices much easier, and implement a set of popular reconstruction algorithms, so that it is convenient for researchers to compare against. The overall design and certain key technologies are introduced in detail. Presented experiment examples and practical applications commendably demonstrate the validity of this framework.
21421491	[A distributed storage architecture for regional medical image sharing and cooperation based on HDFS].
Nan Fang Yi Ke Da Xue Xue Bao  2011Mar
Given the importance of regional centers for medical image sharing and cooperation is important for resource balancing, healthcare service enhancement and medical expense reduction, building such regional medical image sharing and cooperation centers faces huge challenges. In this paper we analyze the advantages and weakness of two storage architectures, and designed a hybrid storage architecture combining FC SAN and Hadoop HDFS. A HDFS suitable medical image file format, called S-DICOM, and a set of S-DICOM operating middleware, SDFO (S-DICOM File Operator), was developed. The results of performance testing indicated that this hybrid storage architecture is suitable for storing and managing large volume of medical images.
21421362	RiceRBP: a database of experimentally identified RNA-binding proteins in Oryza sativa L.
Plant Sci. 20100813 2011Feb
RNA-binding proteins play critical roles at multiple steps during gene expression, including mRNA transport and translation. mRNA transport is particularly important in rice (Oryza sativa L.) in order to ensure the proper localization of the prolamine and glutelin seed storage proteins. However, relatively little information is available about RNA-binding proteins that have been isolated or characterized in plants. The RiceRBP database is a novel resource for the analysis of RNA-binding proteins in rice. RiceRBP contains 257 experimentally identified RNA-binding proteins, which are derived from at least 221 distinct rice genes. Many of the identified proteins catalogued in RiceRBP had not previously been annotated or predicted to bind RNA. RiceRBP provides tools to facilitate the analysis of the identified RNA-binding proteins, including information about predicted protein domains, phylogenetic relationships, and expression patterns of the identified genes. Importantly, RiceRBP also contains tools to search and analyze predicted RNA-binding protein orthologs in other plant species. We anticipate that the data and analysis tools provided by RiceRBP should facilitate the study of plant RNA-binding proteins. RiceRBP is available at http://www.bioinformatics2.wsu.edu/RiceRBP.
21057093	Building a metadata framework for sharing feed information in Spain.
J. Anim. Sci. 20101105 2011Mar
Information about the nutritional aspects and uses of feed is of widespread interest, hence systematic efforts of laboratories to obtain it. The way this information is currently being handled leaves something to be desired, underscoring the need to use computerized systems and statistical techniques that allow the management of large volumes of heterogeneous information. This project seeks to develop a structure that will facilitate the exchange and exploitation of information on feeds produced in Spain. To this end, metadata and data mining techniques have been adopted by the Feed Information Service at the University of Cordoba. The structure has been designed to work on the basis of a server-client architecture, in which information is stored on local software (Califa) by its own creators so that it can subsequently be incorporated into a database server where it can be accessed online. Various aspects of the structure are described in this paper: organization (participants and data shared), format (physical features), logistics (data description), quality (reliability of information), legality (correct use of data), and financing (revenue and expenditure). An indication is given of the amount of information accumulated to date, now exceeding 200,000 numerical data and associated metadata, arranged in several thematic databases. The activities carried out highlight the heterogeneous nature of the information produced, as well as the large number of errors and ambiguities that slip through the normal filters and reach the end-user of the data.
20924986	The influence of goal-state access cost on planning during problem solving.
Q J Exp Psychol (Hove) 20100929 2011Mar
Two problem-solving experiments investigated the relationship between planning and the cost of accessing goal-state information using the theoretical framework of the soft constraints hypothesis (Gray &amp; Fu, 2004; Gray, Simms, Fu, &amp; Schoelles, 2006). In Experiment 1, 36 participants were allocated to low, medium, and high access cost conditions and completed a problem-solving version of the Blocks World Task. Both the nature of planning (memory based or display based) and its timing (before or during action) changed with high goal-state access cost (a mouse movement and a 2.5-s delay). In this condition more planning before action was observed, with less planning during action, evidenced by longer first-move latencies, more moves per goal-state inspection, and more short (≤ 0.8 s) and long (&gt;8 s) "preplanned" intermove latencies. Experiment 2 used an eight-puzzle-like transformation task and replicated the effect of goal-state access cost when more complex planning was required, also confirmed by sampled protocol data. Planning before an episode of move making increased with higher goal-state access cost, and planning whilst making moves increased with lower access cost. These novel results are discussed in the context of the soft constraints hypothesis.
20721588	New technologies for information retrieval to achieve situational awareness and higher patient safety in the surgical operating room: the MRI institutional approach and review of the literature.
Surg Endosc 20100819 2011Mar
Technical progress in the operating room (OR) increases constantly, but advanced techniques for error prevention are lacking. It has been the vision to create intelligent OR systems ("autopilot") that not only collect intraoperative data but also interpret whether the course of the operation is normal or deviating from the schedule ("situation awareness"), to recommend the adequate next steps of the intervention, and to identify imminent risky situations. Recently introduced technologies in health care for real-time data acquisition (bar code, radiofrequency identification [RFID], voice and emotion recognition) may have the potential to meet these demands. This report aims to identify, based on the authors' institutional experience and a review of the literature (MEDLINE search 2000-2010), which technologies are currently most promising for providing the required data and to describe their fields of application and potential limitations. Retrieval of information on the functional state of the peripheral devices in the OR is technically feasible by continuous sensor-based data acquisition and online analysis. Using bar code technologies, automatic instrument identification seems conceivable, with information given about the actual part of the procedure and indication of any change in the routine workflow. The dynamics of human activities also comprise key information. A promising technology for continuous personnel tracking is data acquisition with RFID. Emotional data capture and analysis in the OR are difficult. Although technically feasible, nonverbal emotion recognition is difficult to assess. In contrast, emotion recognition by speech seems to be a promising technology for further workflow prediction. The presented technologies are a first step to achieving an increased situational awareness in the OR. However, workflow definition in surgery is feasible only if the procedure is standardized, the peculiarities of the individual patient are taken into account, the level of the surgeon's expertise is regarded, and a comprehensive data capture can be obtained.
21354897	[CT image retrieval of the liver with intrahepatic lesions].
Nan Fang Yi Ke Da Xue Xue Bao  2011Feb
This paper presents a method for global feature extraction and the application of the boostmetric distance metric method for medical image retrieval. The global feature extraction method used the low frequency subband coefficient of the wavelet decomposition based on the non-tensor product coefficient for piecewise Gaussian fitting. The local features were extracted after semi-automatic segmentation of the lesion areas in the images in the database. The experimental verification of the method using 1688 CT images of the liver containing lesions of liver cancer, liver angioma, and liver cyst confirmed that this feature extraction method improved the detection rate of the lesions with good image retrieval performance.
21118769	X-ray categorization and retrieval on the organ and pathology level, using patch-based visual words.
IEEE Trans Med Imaging 20101129 2011Mar
In this study we present an efficient image categorization and retrieval system applied to medical image databases, in particular large radiograph archives. The methodology is based on local patch representation of the image content, using a "bag of visual words" approach. We explore the effects of various parameters on system performance, and show best results using dense sampling of simple features with spatial content, and a nonlinear kernel-based support vector machine (SVM) classifier. In a recent international competition the system was ranked first in discriminating orientation and body regions in X-ray images. In addition to organ-level discrimination, we show an application to pathology-level categorization of chest X-ray data, the most popular examination in radiology. The system discriminates between healthy and pathological cases, and is also shown to successfully identify specific pathologies in a set of chest radiographs taken from a routine hospital examination. This is a first step towards similarity-based categorization, which has a major clinical implications for computer-assisted diagnostics.
21118771	Active volume models for medical image segmentation.
IEEE Trans Med Imaging 20101129 2011Mar
In this paper, we propose a novel predictive model, active volume model (AVM), for object boundary extraction. It is a dynamic "object" model whose manifestation includes a deformable curve or surface representing a shape, a volumetric interior carrying appearance statistics, and an embedded classifier that separates object from background based on current feature information. The model focuses on an accurate representation of the foreground object's attributes, and does not explicitly represent the background. As we will show, however, the model is capable of reasoning about the background statistics thus can detect when is change sufficient to invoke a boundary decision. When applied to object segmentation, the model alternates between two basic operations: 1) deforming according to current region of interest (ROI), which is a binary mask representing the object region predicted by the current model, and 2) predicting ROI according to current appearance statistics of the model. To further improve robustness and accuracy when segmenting multiple objects or an object with multiple parts, we also propose multiple-surface active volume model (MSAVM), which consists of several single-surface AVM models subject to high-level geometric spatial constraints. An AVM's deformation is derived from a linear system based on finite element method (FEM). To keep the model's surface triangulation optimized, surface remeshing is derived from another linear system based on Laplacian mesh optimization (LMO). Thus efficient optimization and fast convergence of the model are achieved by solving two linear systems. Segmentation, validation and comparison results are presented from experiments on a variety of 2-D and 3-D medical images.
21233049	Semisupervised learning using negative labels.
IEEE Trans Neural Netw 20110113 2011Mar
The problem of semisupervised learning has aroused considerable research interests in the past few years. Most of these methods aim to learn from a partially labeled dataset, i.e., they assume that the exact labels of some data are already known. In this paper, we propose to use a novel type of supervision information to guide the process of semisupervised learning, which indicates whether a point does not belong to a specific category. We call this kind of information negative label (NL) and propose a novel approach called NL propagation (NLP) to efficiently make use of this type of information to assist the process of semisupervised learning. Specifically, NLP assumes that nearby points should have similar class indicators. The data labels are propagated under the guidance of NL information and the geometric structure revealed by both labeled and unlabeled points, by employing some specified initialization and parameter matrices. The convergence analysis, out-of-sample extension, parameter determination, computational complexity, and relations to other approaches are presented. We also interpret the proposed approach within the framework of regularization. Promising experimental results on image, digit, spoken letter, and text classification tasks are provided to show the effectiveness of our method.
21370077	Omics technologies, data and bioinformatics principles.
Methods Mol. Biol.  2011
We provide an overview on the state of the art for the Omics technologies, the types of omics data and the bioinformatics resources relevant and related to Omics. We also illustrate the bioinformatics challenges of dealing with high-throughput data. This overview touches several fundamental aspects of Omics and bioinformatics: data standardisation, data sharing, storing Omics data appropriately and exploring Omics data in bioinformatics. Though the principles and concepts presented are true for the various different technological fields, we concentrate in three main Omics fields namely: genomics, transcriptomics and proteomics. Finally we address the integration of Omics data, and provide several useful links for bioinformatics and Omics.
21370094	Integration, warehousing, and analysis strategies of Omics data.
Methods Mol. Biol.  2011
"-Omics" is a current suffix for numerous types of large-scale biological data generation procedures, which naturally demand the development of novel algorithms for data storage and analysis. With next generation genome sequencing burgeoning, it is pivotal to decipher a coding site on the genome, a gene's function, and information on transcripts next to the pure availability of sequence information. To explore a genome and downstream molecular processes, we need umpteen results at the various levels of cellular organization by utilizing different experimental designs, data analysis strategies and methodologies. Here comes the need for controlled vocabularies and data integration to annotate, store, and update the flow of experimental data. This chapter explores key methodologies to merge Omics data by semantic data carriers, discusses controlled vocabularies as eXtensible Markup Languages (XML), and provides practical guidance, databases, and software links supporting the integration of Omics data.
21370095	Integrating Omics data for signaling pathways, interactome reconstruction, and functional analysis.
Methods Mol. Biol.  2011
Omics data and computational approaches are today providing a key to disentangle the complex architecture of living systems. The integration and analysis of data of different nature allows to extract meaningful representations of signaling pathways and protein interactions networks, helpful in achieving an increased understanding of such intricate biochemical processes. We here describe a general workflow and relative hurdles in integrating online Omics data and analyzing reconstructed representations by using the available computational platforms.
21369104	Experimental demonstration and optimisation of a synchronous clock recovery technique for real-time end-to-end optical OFDM transmission at 11.25Gb/s over 25km SSMF.
Opt Express  2011Jan31
A simple, digital signal processing-free, low-cost and robust synchronous clocking technique is proposed and experimentally demonstrated, for the first time, in a 64-QAM-encoded, 11.25Gb/s over 25km SSMF, real-time end-to-end optical OFDM (OOFDM) system using directly modulated DFB laser-based intensity-modulation and direct-detection (IMDD). Detailed experimental investigations show that, in comparison with the common clock approach utilised in previous experimental demonstrations, the proposed clocking technique can be implemented to achieve no system BER performance degradation or optical power budget penalty and more importantly to improve system stability. As a viable synchronous clocking solution for real-time OOFDM transmission, this work is a vital step towards the realisation of practical OOFDM transmission systems and has particular significance for synchronisation of OOFDM multiple access-based passive optical networks where highly accurate synchronisation of all network elements is essential.
21369285	Photochromic polyurethanes for rewritable CGHs in optical testing.
Opt Express  2011Feb28
The development of photochromic Computer Generated Holograms (CGHs) to test any complex optics, such as aspheres and free-form optics, is described. A thermally irreversible photochromic polyurethane has been synthesized to give good thin films with a strong modulation of the optical transmission. The photochromic CGH has been tested with a simple interferometrical configuration showing promising results. The use of photochromic CGHs provides advantages over standard technologies, as rewritability and self developing.
21378643	The RCN's literacy competences for evidence-based practice.
Br J Nurs  2011 Feb 10-23
Professor Alan Glasper discusses the Royal College of Nursing's recently announced information literacy competences, which set out skills required by nurses to deliver safe and effective evidence-based care.
21381452	Reinventing electronic support for physicians. A look at Kaiser Permanente's Patient Panel Support Tool and care delivery: Part 1. Interview by John Degaspari.
Healthc Inform  2011Feb
A Web-based tool that extracts information from the electronic health record helps physicians improve care and manage their entire panel of patients. Two Kaiser Permanente studies examine the effectiveness of the tool in a large, diverse patient population.
21382190	Initial clinical test of a breast-PET scanner.
J Med Imaging Radiat Oncol  2011Feb
The goal of this initial clinical study was to test a new positron emission/tomography imager and biopsy system (PEM/PET) in a small group of selected subjects to assess its clinical imaging capabilities. Specifically, the main task of this study is to determine whether the new system can successfully be used to produce images of known breast cancer and compare them to those acquired by standard techniques. The PEM/PET system consists of two pairs of rotating radiation detectors located beneath a patient table. The scanner has a spatial resolution of ∼2 mm in all three dimensions. The subjects consisted of five patients diagnosed with locally advanced breast cancer ranging in age from 40 to 55 years old scheduled for pre-treatment, conventional whole body PET imaging with F-18 Fluorodeoxyglucose (FDG). The primary lesions were at least 2 cm in diameter. The images from the PEM/PET system demonstrated that this system is capable of identifying some lesions not visible in standard mammograms. Furthermore, while the relatively large lesions imaged in this study where all visualised by a standard whole body PET/CT scanner, some of the morphology of the tumours (ductal infiltration, for example) was better defined with the PEM/PET system. Significantly, these images were obtained immediately following a standard whole body PET scan. The initial testing of the new PEM/PET system demonstrated that the new system is capable of producing good quality breast-PET images compared standard methods.
21384302	German Registry for Acute Aortic Dissection Type A (GERAADA)--new software design, parameters and their definitions.
Thorac Cardiovasc Surg 20110307 2011Mar
The working group for aortic surgery and interventional vascular surgery of the German Society for Thoracic and Cardiovascular Surgery (GSTCVS) initiated the web-based German Registry for Acute Aortic Dissection type A (GERAADA). It is the project's aim to collect standardized data from a large pool of patients with acute aortic dissections type A (AADA) to gain a deeper insight and knowledge to improve surgical therapies and perioperative management for these patients in the future. In addition to new medical insights, the working group has gained more experience over the last 4 years in how to collect valid and high-quality data. This experience led us to revise the database completely. In this article we describe the new version of GERAADA, providing an overview as well as defining the parameters, and explaining the new features. This overview fulfills a request by the users of GERAADA in the participating centers. Since its inception, 50 cardiac centers in Germany, Austria and Switzerland have provided over 2000 records and the first statistical results have been published. GERAADA's new design allows it to stay abreast of changes in medicine and to focus on the essentials necessary for statistically relevant results, while keeping the work load low for the data providers at each cardiac center.
21309339	Community-based information exchange. Community-based hies are a model of interoperability among clinical information systems.
Healthc Inform  2011Jan
Many health providers have focused on community-based health information exchanges (HIEs) as a useful platform to help them take the next steps in implementing electronic health information. Recent research by a healthcare technology advisory firm is an attempt to characterize various HIE models, with an eye on achieving that goal.
21272371	Development of a heart failure filter for Medline: an objective approach using evidence-based clinical practice guidelines as an alternative to hand searching.
BMC Med Res Methodol 20110128 2011
Heart failure is a highly debilitating syndrome with a poor prognosis primarily affecting the elderly. Clinicians wanting timely access to heart failure evidence to provide optimal patient care can face many challenges in locating this evidence. This study developed and validated a search filter of high clinical utility for the retrieval of heart failure articles in OvidSP Medline. A Clinical Advisory Group was established to advise study investigators. The study set of 876 relevant articles from four heart failure clinical practice guidelines was divided into three datasets: a Term Identification Set, a Filter Development Set, and a Filter Validation Set. A further validation set (the Cochrane Validation Set) was formed using studies included in Cochrane heart failure systematic reviews. Candidate search terms were identified via word frequency analysis. The filter was developed by creating combinations of terms and recording their performance in retrieving items from the Filter Development Set. The filter's recall was then validated in both the Filter Validation Set and the Cochrane Validation Set. A precision estimate was obtained post-hoc by running the filter in Medline and screening the first 200 retrievals for relevance to heart failure. The four-term filter achieved a recall of 96.9% in the Filter Development Set; 98.2% in the Filter Validation Set; and 97.8% in the Cochrane Validation Set. Of the first 200 references retrieved by the filter when run in Medline, 150 were deemed relevant and 50 irrelevant. The post-hoc precision estimate was therefore 75%. This study describes an objective method for developing a validated heart failure filter of high recall performance and then testing its precision post-hoc. Clinical practice guidelines were found to be a feasible alternative to hand searching in creating a gold standard for filter development. Guidelines may be especially appropriate given their clinical utility. A validated heart failure filter is now available to support health professionals seeking reliable and efficient access to the heart failure literature.
21133848	Computational polypharmacology with text mining and ontologies.
Curr Pharm Biotechnol  2011Mar1
Huge volumes of data, produced by microarrays and next- generation sequencing, are now at the fingertips of scientists and allow to expand the scope beyond conventional drug de- sign. New promiscuous drugs directed at multiple targets promise increased therapeutic efficacy for treatment of multi- factorial diseases. At the same time, more systematic tests for unwanted side effects are now possible. In this paper, we focus on the application of text mining and ontologies to support experimental drug discovery. Text mining is a high- throughput technique to extract information from millions of scientific documents and web pages. By exploiting the vast number of extracted facts as well as the indirect links between them, text mining and ontologies help to generate new hypotheses on drug target interactions. We review latest applications of text mining and ontologies suitable for target and drug-target interaction discovery in addition to conventional approaches. We conclude that mining the literature on drugs and proteins offers unique opportunities to support the laborious and expensive process of drug development.
21311007	Challenges and opportunities of open data in ecology.
Science  2011Feb11
Ecology is a synthetic discipline benefiting from open access to data from the earth, life, and social sciences. Technological challenges exist, however, due to the dispersed and heterogeneous nature of these data. Standardization of methods and development of robust metadata can increase data access but are not sufficient. Reproducibility of analyses is also important, and executable workflows are addressing this issue by capturing data provenance. Sociological challenges, including inadequate rewards for sharing data, must also be resolved. The establishment of well-curated, federated data repositories will provide a means to preserve data while promoting attribution and acknowledgement of its use.
21311008	Changing the equation on scientific data visualization.
Science  2011Feb11
An essential facet of the data deluge is the need for different types of users to apply visualizations to understand how data analyses and queries relate to each other. Unfortunately, visualization too often becomes an end product of scientific analysis, rather than an exploration tool that scientists can use throughout the research life cycle. However, new database technologies, coupled with emerging Web-based technologies, may hold the key to lowering the cost of visualization generation and allow it to become a more integral part of the scientific process.
21311010	The disappearing third dimension.
Science  2011Feb11
Three-dimensional computing is driving what many would call a revolution in scientific visualization. However, its power and advancement are held back by the absence of sustainable archives for raw data and derivative visualizations. Funding agencies, professional societies, and publishers each have unfulfilled roles in archive design and data management policy.
21311012	More is less: signal processing and the data deluge.
Science  2011Feb11
The data deluge is changing the operating environment of many sensing systems from data-poor to data-rich--so data-rich that we are in jeopardy of being overwhelmed. Managing and exploiting the data deluge require a reinvention of sensor system design and signal processing theory. The potential pay-offs are huge, as the resulting sensor systems will enable radically new information technologies and powerful new tools for scientific discovery.
21311013	Ensuring the data-rich future of the social sciences.
Science  2011Feb11
Massive increases in the availability of informative social science data are making dramatic progress possible in analyzing, understanding, and addressing many major societal problems. Yet the same forces pose severe challenges to the scientific infrastructure supporting data sharing, data management, informatics, statistical methodology, and research ethics and policy, and these are collectively holding back progress. I address these changes and challenges and suggest what can be done.
21311016	On the future of genomic data.
Science  2011Feb11
Many of the challenges in genomics derive from the informatics needed to store and analyze the raw sequencing data that is available from highly multiplexed sequencing technologies. Because single week-long sequencing runs today can produce as much data as did entire genome centers a few years ago, the need to process terabytes of information has become de rigueur for many labs engaged in genomic research. The availability of deep (and large) genomic data sets raises concerns over information access, data security, and subject/patient privacy that must be addressed for the field to continue its rapid advances.
21266060	SciReader enables reading of medical content with instantaneous definitions.
BMC Med Inform Decis Mak 20110125 2011
A major problem patients encounter when reading about health related issues is document interpretation, which limits reading comprehension and therefore negatively impacts health care. Currently, searching for medical definitions from an external source is time consuming, distracting, and negatively impacts reading comprehension and memory of the material. SciReader was built as a Java application with a Flex-based front-end client. The dictionary used by SciReader was built by consolidating data from several sources and generating new definitions with a standardized syntax. The application was evaluated by measuring the percentage of words defined in different documents. A survey was used to test the perceived effect of SciReader on reading time and comprehension. We present SciReader, a web-application that simplifies document interpretation by allowing users to instantaneously view medical, English, and scientific definitions as they read any document. This tool reveals the definitions of any selected word in a small frame at the top of the application. SciReader relies on a dictionary of ~750,000 unique Biomedical and English word definitions. Evaluation of the application shows that it maps ~98% of words in several different types of documents and that most users tested in a survey indicate that the application decreases reading time and increases comprehension. SciReader is a web application useful for reading medical and scientific documents. The program makes jargon-laden content more accessible to patients, educators, health care professionals, and the general public.
21314893	The Library's role and challenges in implementing an e-learning strategy: a case study from northern Australia.
Health Info Libr J 20101209 2011Mar
  The Northern Territory Department of Health and Families' (DHF) Library supports education programs for all staff. DHF is implementing an e-learning strategy, which may be viewed as a vehicle for coordinating the education function throughout the organisation.   The objective of this study is to explore the concept of e-learning in relation to the Library's role in implementing an organisation-wide e-learning strategy.   The main findings of a literature search about the effectiveness of e-learning in health professionals' education, and the responsibility and roles of health librarians in e-learning are described. A case study approach is used to outline the current role and future opportunities and challenges for the Library.   The case study presents the organisation's strategic planning context. Four areas of operational activity which build on the Library's current educational activities are suggested: the integration of library resources 'learning objects' within a Learning Management System; developing online health information literacy training programs; establishing a physical and virtual 'e-Learning Library/Centre'; developing collaborative partnerships, taking on new responsibilities in e-learning development, and creating a new e-learning librarian role.   The study shows that the Library's role is fundamental to developing the organisation's e-learning capacity and implementing an organisation-wide e-learning strategy.
21314894	Evaluation of three point-of-care healthcare databases: BMJ Point-of-Care, Clin-eguide and Nursing Reference Centre.
Health Info Libr J 20101209 2011Mar
  Point of care resources make it easier for clinicians to find answers to questions that arise during a clinical encounter. In order to make informed purchase decisions in times of tight budgets, librarians need to have a better understanding of which resources will meet their patrons' clinical information needs.   The goal of this study was to assess the content, interface and usability of three point-of-care tools: BMJ Point-of-Care, Clin-eguide and Nursing Reference Centre.   A questionnaire designed to gather quantitative and qualitative data was created using Survey Monkey. The survey was distributed to healthcare practitioners in Alberta's two largest health regions, and the data were analysed for emergent themes.   The themes that arose--ease of use, validated content, relevancy to practice--generally echoed those stated in the literature. No one database fared significantly better, due to differing features, content and client preference.   Despite the limitations of the survey, the themes that emerged provide a springboard for future research on the efficacy of information resources used at the point of care, and the need for deeper analysis of these recent additions to the medical information market.
21314896	Searching for randomised controlled trials and clinical controlled trials in Thai online bibliographical biomedical databases.
Health Info Libr J 20101110 2011Mar
  Thailand is a poor but highly literate country in South-East Asia with over 60 million people. A lot of biomedical research is undertaken but dissemination is limited.   To identify relevant Thai bibliographic databases and investigate accessibility, functionality and content, particularly in relation to randomised controlled trials (RCTs) and clinical controlled trials (CCTs).   A systematic search for institutions productive of research and the databases in their libraries. Search each accessible database in both Thai and English, recording the functionality and content. Assess accessibility of the retrieved RCTs or CCTs by comparing to PubMed holdings.   We found 32 different databases (29 accessible in UK) of various sizes, coverage and functionality but many with unique records of RCTs and CCTs (total, n=781). Two hundred and nine of 781 trials were accessible on PubMed (27%). However, 641 of the 781 trial records contain text in both English and Thai (82%) and 112 records were solely English (14%).   Those undertaking comprehensive searches for RCTs/CCTs should, in addition to a PubMed, search the Thai Medical Index and Thai Index Medicus databases, and the Khon Kaen University Library Catalogue.
21314897	Case study: library usage at an Indian medical college.
Health Info Libr J 20101227 2011Mar
This issue's feature column reports on the findings of a small survey of library users carried out in an Indian medical college with a traditional curriculum. The study found that the main reason a student visited the library was to consult text books. Although the majority of students were satisfied with the library facilities, the study suggests that more needs to be done to promote self-directed learning. JM.
21314898	Further developing the library curriculum: skills for life delivery.
Health Info Libr J 20101227 2011Mar
This feature discusses how the library curriculum was developed at Plymouth Hospitals NHS Trust Library Service to aid the delivery of Skills for Life. In particular, the feature describes how, through collaboration with learndirect, literacy and numeracy skills were embedded into the broader library and information skills training programme. The article reports on how the programme resulted in the provision of qualifications and skills development opportunities to NHS staff, and an increase in the NHS library profile.
21142435	A model based on multi-features to enhance healthcare and medical document retrieval.
Inform Health Soc Care 20101208 2011Mar
A major problem in biomedical informatics is the contextual retrieval and ranking of medical and healthcare information. In this article, we present a model for extracting semantic relations among medical and clinical documents. The purpose is to maximise contextual retrieval and ranking performance with minimum input from users. We developed and evaluated a medical search engine that relies on a multi-features similarity model. The indexed documents are represented as a network that reflects the semantic relations among documents to assess topical rankings. The evaluation measurements include the following: recall, precision and R-precision. We used OHSUMED collection to evaluate our work with runs submitted to TREC-9. We provide a comparison of the top five runs that achieved the highest average precision scores. In addition, we used questionnaire-based evaluation to measure the effectiveness of the ranking task. The results indicated that the proposed model achieved a higher average precision in comparison with top-scored runs submitted to TREC-9; the improvement of our model over other methods is statistically significant (p-value &lt;0.0001). Furthermore, a questionnaire-based experiment showed that the proposed model performed quite well in ranking retrieved documents according to their topics.
21317106	How strong are passwords used to protect personal health information in clinical trials?
J. Med. Internet Res. 20110211 2011
Findings and statements about how securely personal health information is managed in clinical research are mixed. The objective of our study was to evaluate the security of practices used to transfer and share sensitive files in clinical trials. Two studies were performed. First, 15 password-protected files that were transmitted by email during regulated Canadian clinical trials were obtained. Commercial password recovery tools were used on these files to try to crack their passwords. Second, interviews with 20 study coordinators were conducted to understand file-sharing practices in clinical trials for files containing personal health information. We were able to crack the passwords for 93% of the files (14/15). Among these, 13 files contained thousands of records with sensitive health information on trial participants. The passwords tended to be relatively weak, using common names of locations, animals, car brands, and obvious numeric sequences. Patient information is commonly shared by email in the context of query resolution. Files containing personal health information are shared by email and, by posting them on shared drives with common passwords, to facilitate collaboration. If files containing sensitive patient information must be transferred by email, mechanisms to encrypt them and to ensure that password strength is high are necessary. More sophisticated collaboration tools are required to allow file sharing without password sharing. We provide recommendations to implement these practices.
21185643	YouTube as a source of information on cardiopulmonary resuscitation.
Resuscitation 20101224 2011Mar
Widespread knowledge of cardiopulmonary resuscitation (CPR) is critical to improving survival in sudden cardiac death. We analyzed YouTube, an Internet video-site which is a growing source of healthcare information for source, content and quality of information about CPR. YouTube was queried using keywords "CPR", "Cardiopulmonary resuscitation", "BLS" and "Basic life support". Videos in English demonstrating CPR technique were included. Videos were classified by upload source, content, structure of course, subject for CPR demonstration, etc. Videos were scored for 'accuracy of demonstration' of CPR steps on a scale of 0-8 and for 'viewability'. Of 800 videos screened 52 met the inclusion criteria with mean duration of 233 (±145)s and view count 37 (±77) per day. 48% (n = 25) videos were by individuals with unspecified credentials. No differences were noted in view count/day, 'accuracy of demonstration' and 'viewability' among videos based on source. No information was provided about scene safety assessment in 65% (n = 34) videos. Only 69% (n = 31/45) videos demonstrated the correct compression-ventilation ratio while 63.5% (n = 33), 34.6% (n = 18) and 40.4% (n = 21) gave information on location, rate and depth of chest compressions respectively. 19% (n = 10) videos incorrectly recommended checking for pulse. Videos judged the best source for CPR information were not the ones most viewed. Information on this platform is unregulated, hence content by trusted sources should be posted to provide accurate and easily accessible information about CPR. YouTube may have a potential role in video-assisted learning of CPR and as source of information for CPR in emergencies.
21261995	Worm Phenotype Ontology: integrating phenotype data within and beyond the C. elegans community.
BMC Bioinformatics 20110124 2011
Caenorhabditis elegans gene-based phenotype information dates back to the 1970's, beginning with Sydney Brenner and the characterization of behavioral and morphological mutant alleles via classical genetics in order to understand nervous system function. Since then C. elegans has become an important genetic model system for the study of basic biological and biomedical principles, largely through the use of phenotype analysis. Because of the growth of C. elegans as a genetically tractable model organism and the development of large-scale analyses, there has been a significant increase of phenotype data that needs to be managed and made accessible to the research community. To do so, a standardized vocabulary is necessary to integrate phenotype data from diverse sources, permit integration with other data types and render the data in a computable form. We describe a hierarchically structured, controlled vocabulary of terms that can be used to standardize phenotype descriptions in C. elegans, namely the Worm Phenotype Ontology (WPO). The WPO is currently comprised of 1,880 phenotype terms, 74% of which have been used in the annotation of phenotypes associated with greater than 18,000 C. elegans genes. The scope of the WPO is not exclusively limited to C. elegans biology, rather it is devised to also incorporate phenotypes observed in related nematode species. We have enriched the value of the WPO by integrating it with other ontologies, thereby increasing the accessibility of worm phenotypes to non-nematode biologists. We are actively developing the WPO to continue to fulfill the evolving needs of the scientific community and hope to engage researchers in this crucial endeavor. We provide a phenotype ontology (WPO) that will help to facilitate data retrieval, and cross-species comparisons within the nematode community. In the larger scientific community, the WPO will permit data integration, and interoperability across the different Model Organism Databases (MODs) and other biological databases. This standardized phenotype ontology will therefore allow for more complex data queries and enhance bioinformatic analyses.
21291558	Methodology in conducting a systematic review of systematic reviews of healthcare interventions.
BMC Med Res Methodol 20110203 2011
Hundreds of studies of maternity care interventions have been published, too many for most people involved in providing maternity care to identify and consider when making decisions. It became apparent that systematic reviews of individual studies were required to appraise, summarise and bring together existing studies in a single place. However, decision makers are increasingly faced by a plethora of such reviews and these are likely to be of variable quality and scope, with more than one review of important topics. Systematic reviews (or overviews) of reviews are a logical and appropriate next step, allowing the findings of separate reviews to be compared and contrasted, providing clinical decision makers with the evidence they need. The methods used to identify and appraise published and unpublished reviews systematically, drawing on our experiences and good practice in the conduct and reporting of systematic reviews are described. The process of identifying and appraising all published reviews allows researchers to describe the quality of this evidence base, summarise and compare the review's conclusions and discuss the strength of these conclusions. Methodological challenges and possible solutions are described within the context of (i) sources, (ii) study selection, (iii) quality assessment (i.e. the extent of searching undertaken for the reviews, description of study selection and inclusion criteria, comparability of included studies, assessment of publication bias and assessment of heterogeneity), (iv) presentation of results, and (v) implications for practice and research. Conducting a systematic review of reviews highlights the usefulness of bringing together a summary of reviews in one place, where there is more than one review on an important topic. The methods described here should help clinicians to review and appraise published reviews systematically, and aid evidence-based clinical decision-making.
21266047	Lin4Neuro: a customized Linux distribution ready for neuroimaging analysis.
BMC Med Imaging 20110125 2011
A variety of neuroimaging software packages have been released from various laboratories worldwide, and many researchers use these packages in combination. Though most of these software packages are freely available, some people find them difficult to install and configure because they are mostly based on UNIX-like operating systems. We developed a live USB-bootable Linux package named "Lin4Neuro." This system includes popular neuroimaging analysis tools. The user interface is customized so that even Windows users can use it intuitively. The boot time of this system was only around 40 seconds. We performed a benchmark test of inhomogeneity correction on 10 subjects of three-dimensional T1-weighted MRI scans. The processing speed of USB-booted Lin4Neuro was as fast as that of the package installed on the hard disk drive. We also installed Lin4Neuro on a virtualization software package that emulates the Linux environment on a Windows-based operation system. Although the processing speed was slower than that under other conditions, it remained comparable. With Lin4Neuro in one's hand, one can access neuroimaging software packages easily, and immediately focus on analyzing data. Lin4Neuro can be a good primer for beginners of neuroimaging analysis or students who are interested in neuroimaging analysis. It also provides a practical means of sharing analysis environments across sites.
20931203	[Documentation of ophthalmological findings in contact lens wearers: software-based symbol library of the Efron grading scale].
Ophthalmologe  2011Feb
To prevent complications through the use of contact lenses the efforts spent on documentation will have to be increased to meet the demands of quality management. How can ophthalmologists benefit from the use of software-based graphical documentation? To make a standardized documentation of contact lens complications possible a software module with a graphically intuitive visualization scheme of the Efron grading scale was developed. The Efron grading scale defines 16 different classes of findings scaled to 5 grades. The software module shows a schematic view of the eye which allows an easy sectoral assignment of all 16 Efron findings. The designated sector can simply be marked with a mouse click. A subsequently appearing context menu offers all 16 Efron findings together with the corresponding 5 grades. Finally the chosen Efron grade is graphically visualized in the corresponding sector. The software module supplies a feasible way for high quality documentation in the clinical routine with illustrative visualization. The practical application of software tools to assist clinical documentation can help to meet the challenges of quality management in contact lens utilization.
21163853	Design and implementation of an institutional case report form library.
Clin Trials 20101216 2011Feb
Case report forms (CRFs) are used to collect data in clinical research. Case report form development represents a significant part of the clinical trial process and can affect study success. Libraries of CRFs can preserve the organizational knowledge and expertise invested in CRF development and expedite the sharing of such knowledge. Although CRF libraries have been advocated, there have been no published accounts reporting institutional experiences with creating and using them. We sought to enhance an existing institutional CRF library by improving information indexing and accessibility. We describe this CRF library and discuss challenges encountered in its development and implementation, as well as future directions for continued work in this area. We transformed an existing but underused and poorly accessible CRF library into a resource capable of supporting and expediting clinical and translational investigation at our institution by (1) expanding access to the entire institution; (2) adding more form attributes for improved information retrieval; and (3) creating a formal information curation and maintenance process. An open-source content management system, Plone (Plone.org), served as the platform for our CRF library. We report results from these three processes. Over the course of this project, the size of the CRF library increased from 160 CRFs comprising an estimated total of 17,000 pages, to 177 CRFs totaling 1.5 gigabytes. Eighty-two of these CRFs are now available to researchers across our institution; 95 CRFs remain within a contractual confidentiality window (usually 5 years from database lock) and are not available to users outside of the Duke Clinical Research Institute (DCRI). Conservative estimates suggest that the library supports an average of 37 investigators per month. The resources needed to curate and maintain the CRF library require less than 10% of the effort of one full-time equivalent employee. Although we succeeded in expanding use of the CRF library, creating awareness of such institutional resources among investigators and research teams remains challenging and requires additional efforts to overcome. Institutions that have not achieved a critical mass of attractive research resources or effective dissemination mechanisms may encounter persistent difficulty attracting researchers to use institutional resources. Further, a useful CRF library requires both an initial investment of resources for development, as well as ongoing maintenance once it is established. CRF libraries can be established and made broadly available to institutional researchers. Curation - that is, indexing newly added forms - is required. Such a resource provides knowledge management capacity for institutions until standards and software are available to support widespread exchange of data and form definitions.
21335818	CvhSlicer: an interactive cross-sectional anatomy navigation system based on high-resolution Chinese visible human data.
Stud Health Technol Inform  2011
We introduce the design and implementation of an interactive system for the navigation of cross-sectional anatomy based on Chinese Visible Human (CVH) data, named CvhSlicer. This system is featured in real-time computation and rendering of high-resolution anatomical images on standard personal computers (PCs) equipped with commodity Graphics Processing Units (GPUs). In order to load the whole-body dataset into the memory of a common PC, several processing steps are first applied to compress the huge CVH data. Thereafter, an adaptive CPU-GPU balancing scheme is performed to dynamically distribute rendering tasks among CPU and GPU based on parameters of computing resources. Experimental results demonstrate that our system can achieve real-time performance and has great potential to be used in anatomy education.
21208982	Tabix: fast retrieval of sequence features from generic TAB-delimited files.
Bioinformatics 20110105 2011Mar1
Tabix is the first generic tool that indexes position sorted files in TAB-delimited formats such as GFF, BED, PSL, SAM and SQL export, and quickly retrieves features overlapping specified regions. Tabix features include few seek function calls per query, data compression with gzip compatibility and direct FTP/HTTP access. Tabix is implemented as a free command-line tool as well as a library in C, Java, Perl and Python. It is particularly useful for manually examining local genomic features on the command line and enables genome viewers to support huge data files and remote custom tracks over networks. http://samtools.sourceforge.net.
21216779	A Java API for working with PubChem datasets.
Bioinformatics 20110106 2011Mar1
PubChem is a public repository of chemical structures and associated biological activities. The PubChem BioAssay database contains assay descriptions, conditions and readouts and biological screening results that have been submitted by the biomedical research community. The PubChem web site and Power User Gateway (PUG) web service allow users to interact with the data and raw files are available via FTP. These resources are helpful to many but there can also be great benefit by using a software API to manipulate the data. Here, we describe a Java API with entity objects mapped to the PubChem Schema and with wrapper functions for calling the NCBI eUtilities and PubChem PUG web services. PubChem BioAssays and associated chemical compounds can then be queried and manipulated in a local relational database. Features include chemical structure searching and generation and display of curve fits from stored dose-response experiments, something that is not yet available within PubChem itself. The aim is to provide researchers with a fast, consistent, queryable local resource from which to manipulate PubChem BioAssays in a database agnostic manner. It is not intended as an end user tool but to provide a platform for further automation and tools development. http://code.google.com/p/pubchemdb.
21342555	GARNET--gene set analysis with exploration of annotation relations.
BMC Bioinformatics 20110215 2011
Gene set analysis is a powerful method of deducing biological meaning for an a priori defined set of genes. Numerous tools have been developed to test statistical enrichment or depletion in specific pathways or gene ontology (GO) terms. Major difficulties towards biological interpretation are integrating diverse types of annotation categories and exploring the relationships between annotation terms of similar information. GARNET (Gene Annotation Relationship NEtwork Tools) is an integrative platform for gene set analysis with many novel features. It includes tools for retrieval of genes from annotation database, statistical analysis &amp; visualization of annotation relationships, and managing gene sets. In an effort to allow access to a full spectrum of amassed biological knowledge, we have integrated a variety of annotation data that include the GO, domain, disease, drug, chromosomal location, and custom-defined annotations. Diverse types of molecular networks (pathways, transcription and microRNA regulations, protein-protein interaction) are also included. The pair-wise relationship between annotation gene sets was calculated using kappa statistics. GARNET consists of three modules--gene set manager, gene set analysis and gene set retrieval, which are tightly integrated to provide virtually automatic analysis for gene sets. A dedicated viewer for annotation network has been developed to facilitate exploration of the related annotations. GARNET (gene annotation relationship network tools) is an integrative platform for diverse types of gene set analysis, where complex relationships among gene annotations can be easily explored with an intuitive network visualization tool (http://garnet.isysbio.org/ or http://ercsb.ewha.ac.kr/garnet/).
21342564	iGEMDOCK: a graphical environment of enhancing GEMDOCK using pharmacological interactions and post-screening analysis.
BMC Bioinformatics 20110215 2011
Pharmacological interactions are useful for understanding ligand binding mechanisms of a therapeutic target. These interactions are often inferred from a set of active compounds that were acquired experimentally. Moreover, most docking programs loosely coupled the stages (binding-site and ligand preparations, virtual screening, and post-screening analysis) of structure-based virtual screening (VS). An integrated VS environment, which provides the friendly interface to seamlessly combine these VS stages and to identify the pharmacological interactions directly from screening compounds, is valuable for drug discovery. We developed an easy-to-use graphic environment, iGEMDOCK, integrating VS stages (from preparations to post-screening analysis). For post-screening analysis, iGEMDOCK provides biological insights by deriving the pharmacological interactions from screening compounds without relying on the experimental data of active compounds. The pharmacological interactions represent conserved interacting residues, which often form binding pockets with specific physico-chemical properties, to play the essential functions of a target protein. Our experimental results show that the pharmacological interactions derived by iGEMDOCK are often hot spots involving in the biological functions. In addition, iGEMDOCK provides the visualizations of the protein-compound interaction profiles and the hierarchical clustering dendrogram of the compounds for post-screening analysis. We have developed iGEMDOCK to facilitate steps from preparations of target proteins and ligand libraries toward post-screening analysis. iGEMDOCK is especially useful for post-screening analysis and inferring pharmacological interactions from screening compounds. We believe that iGEMDOCK is useful for understanding the ligand binding mechanisms and discovering lead compounds. iGEMDOCK is available at http://gemdock.life.nctu.edu.tw/dock/igemdock.php.
21342574	A method for supporting retrieval of articles on protein structure analysis considering users' intention.
BMC Bioinformatics 20110215 2011
In recent years, information about protein structure and function is described in a large amount of articles. However, a naive full-text search by specific keywords often fails to find desired articles, because the articles involve the ambiguous and complicated concepts that cannot be described with uniform representation. For retrieving articles on protein structure and function, it is important to consider the relevance between structural and/or functional concepts by identifying the user's intention. We introduce a scheme of evaluating relevance between articles based on various biological databases and ontologies on structures and functions of proteins. The relevance, which is defined as a path length between concepts on hierarchies, is modified adaptively based on additional articles as a query in order to reflect the user's intention. Also we implemented the retrieval system, in which the user can input some articles as a query and the related articles are retrieved and displayed on the 2D map. The effectiveness of the proposed system was confirmed experimentally by having shown that the users can obtain easily highly related articles which reflect their intention.
21342584	Semantically enabled and statistically supported biological hypothesis testing with tissue microarray databases.
BMC Bioinformatics 20110215 2011
Although many biological databases are applying semantic web technologies, meaningful biological hypothesis testing cannot be easily achieved. Database-driven high throughput genomic hypothesis testing requires both of the capabilities of obtaining semantically relevant experimental data and of performing relevant statistical testing for the retrieved data. Tissue Microarray (TMA) data are semantically rich and contains many biologically important hypotheses waiting for high throughput conclusions. An application-specific ontology was developed for managing TMA and DNA microarray databases by semantic web technologies. Data were represented as Resource Description Framework (RDF) according to the framework of the ontology. Applications for hypothesis testing (Xperanto-RDF) for TMA data were designed and implemented by (1) formulating the syntactic and semantic structures of the hypotheses derived from TMA experiments, (2) formulating SPARQLs to reflect the semantic structures of the hypotheses, and (3) performing statistical test with the result sets returned by the SPARQLs. When a user designs a hypothesis in Xperanto-RDF and submits it, the hypothesis can be tested against TMA experimental data stored in Xperanto-RDF. When we evaluated four previously validated hypotheses as an illustration, all the hypotheses were supported by Xperanto-RDF. We demonstrated the utility of high throughput biological hypothesis testing. We believe that preliminary investigation before performing highly controlled experiment can be benefited.
21208430	CNV-WebStore: online CNV analysis, storage and interpretation.
BMC Bioinformatics 20110105 2011
Microarray technology allows the analysis of genomic aberrations at an ever increasing resolution, making functional interpretation of these vast amounts of data the main bottleneck in routine implementation of high resolution array platforms, and emphasising the need for a centralised and easy to use CNV data management and interpretation system. We present CNV-WebStore, an online platform to streamline the processing and downstream interpretation of microarray data in a clinical context, tailored towards but not limited to the Illumina BeadArray platform. Provided analysis tools include CNV analsyis, parent of origin and uniparental disomy detection. Interpretation tools include data visualisation, gene prioritisation, automated PubMed searching, linking data to several genome browsers and annotation of CNVs based on several public databases. Finally a module is provided for uniform reporting of results. CNV-WebStore is able to present copy number data in an intuitive way to both lab technicians and clinicians, making it a useful tool in daily clinical practice.
19908094	Data management solution for large-volume computed tomography in an existing picture archiving and communication system (PACS).
J Digit Imaging  2011Feb
Multidetector row computed tomography (MDCT) creates massive amounts of data, which can overload a picture archiving and communication system (PACS). To solve this problem, we designed a new data storage and image interpretation system in an existing PACS. Two MDCT image datasets, a thick- and a thin-section dataset, and a single-detector CT thick-section dataset were reconstructed. The thin-section dataset was archived in existing PACS disk space reserved for temporary storage, and the system overwrote the source data to preserve available disk space. The thick-section datasets were archived permanently. Multiplanar reformation (MPR) images were reconstructed from the stored thin-section datasets on the PACS workstation. In regular interpretations by eight radiologists during the same week, the volume of images and the times taken for interpretation of thick-section images with (246 CT examinations) or without (170 CT examinations) thin-section images were recorded, and the diagnostic usefulness of the thin-section images was evaluated. Thin-section datasets and MPR images were used in 79% and 18% of cases, respectively. The radiologists' assessments of this system were useful, though the volume of images and times taken to archive, retrieve, and interpret thick-section images together with thin-section images were significantly greater than the times taken without thin-section images. The limitations were compensated for by the usefulness of thin-section images. This data storage and image interpretation system improves the storage and availability of the thin-section datasets of MDCT and can prevent overloading problems in an existing PACS for the moment.
20354755	Ontology-assisted analysis of Web queries to determine the knowledge radiologists seek.
J Digit Imaging  2011Feb
Radiologists frequently search the Web to find information they need to improve their practice, and knowing the types of information they seek could be useful for evaluating Web resources. Our goal was to develop an automated method to categorize unstructured user queries using a controlled terminology and to infer the type of information users seek. We obtained the query logs from two commonly used Web resources for radiology. We created a computer algorithm to associate RadLex-controlled vocabulary terms with the user queries. Using the RadLex hierarchy, we determined the high-level category associated with each RadLex term to infer the type of information users were seeking. To test the hypothesis that the term category assignments to user queries are non-random, we compared the distributions of the term categories in RadLex with those in user queries using the chi square test. Of the 29,669 unique search terms found in user queries, 15,445 (52%) could be mapped to one or more RadLex terms by our algorithm. Each query contained an average of one to two RadLex terms, and the dominant categories of RadLex terms in user queries were diseases and anatomy. While the same types of RadLex terms were predominant in both RadLex itself and user queries, the distribution of types of terms in user queries and RadLex were significantly different (p &lt; 0.0001). We conclude that RadLex can enable processing and categorization of user queries of Web resources and enable understanding the types of information users seek from radiology knowledge resources on the Web.
20980665	Informatics in radiology: an information model of the DICOM standard.
Radiographics 20101027 2011 Jan-Feb
The Digital Imaging and Communications in Medicine (DICOM) Standard is a key foundational technology for radiology. However, its complexity creates challenges for information system developers because the current DICOM specification requires human interpretation and is subject to nonstandard implementation. To address this problem, a formally sound and computationally accessible information model of the DICOM Standard was created. The DICOM Standard was modeled as an ontology, a machine-accessible and human-interpretable representation that may be viewed and manipulated by information-modeling tools. The DICOM Ontology includes a real-world model and a DICOM entity model. The real-world model describes patients, studies, images, and other features of medical imaging. The DICOM entity model describes connections between real-world entities and the classes that model the corresponding DICOM information entities. The DICOM Ontology was created to support the Cancer Biomedical Informatics Grid (caBIG) initiative, and it may be extended to encompass the entire DICOM Standard and serve as a foundation of medical imaging systems for research and patient care.
21115393	Application of data mining to the identification of critical factors in patient falls using a web-based reporting system.
Int J Med Inform 20101105 2011Feb
The implementation of an information system has become a trend in healthcare institutions. How to identify variables related to patient safety among accumulated data has been viewed as a main issue. The purpose of this study was to identify critical factors related to patient falls through the application of data mining to available data through a hospital information system. Data on a total of 725 patient falls were obtained from a web-based nursing incident reporting system at a medical center in Taiwan. In the process of data mining, feature selection was applied as the first step, after which 10 critical factors were selected to predict the dependent variables (injury versus non-injury). An artificial neural network (ANN) analysis was applied to develop a predictive model and a multivariate stepwise logistic regression was performed for comparison purposes. The ANN model produced the following results: a Receiver-Operating-Character (ROC) curve indicated 77% accuracy, the positive predictive value (PPV) was 68%, and the negative predictive value (NPV) was 72%; while the multivariate stepwise logistic regression only identified 3 variables (fall assessment, anti-psychosis medication and diuretics) as significant predictors with ROC curve of 42%, PPV of 26.24%, and NPV of 87.12%. In addition to medication use such as anti-psychotic and diuretics, nursing intervention where a fall assessment is conducted could represent a critical factor related to outcomes of fall incidence.
21271451	ScienceDirect through SciVerse: a new way to approach Elsevier.
Med Ref Serv Q  2011
SciVerse is the new combined portal from Elsevier that services their ScienceDirect collection, SciTopics, and their Scopus database. Using SciVerse to access ScienceDirect is the specific focus of this review. Along with advanced keyword searching and citation searching options, SciVerse also incorporates a very useful image search feature. The aim seems to be not only to create an interface that provides broad functionality on par with other database search tools that many searchers use regularly but also to create an open platform that could be changed to respond effectively to the needs of customers.
21271452	HTML5: a new standard for the Web.
Med Ref Serv Q  2011
HTML5 is the newest revision of the HTML standard developed by the World Wide Web Consortium (W3C). This new standard adds several exciting news features and capabilities to HTML. This article will briefly discuss the history of HTML standards, explore what changes are in the new HTML5 standard, and what implications it has for information professionals. A list of HTML5 resources and examples will also be provided.
21278375	PHAST and RPHAST: phylogenetic analysis with space/time models.
Brief. Bioinformatics 20101221 2011Jan
The PHylogenetic Analysis with Space/Time models (PHAST) software package consists of a collection of command-line programs and supporting libraries for comparative genomics. PHAST is best known as the engine behind the Conservation tracks in the University of California, Santa Cruz (UCSC) Genome Browser. However, it also includes several other tools for phylogenetic modeling and functional element identification, as well as utilities for manipulating alignments, trees and genomic annotations. PHAST has been in development since 2002 and has now been downloaded more than 1000 times, but so far it has been released only as provisional ('beta') software. Here, we describe the first official release (v1.0) of PHAST, with improved stability, portability and documentation and several new features. We outline the components of the package and detail recent improvements. In addition, we introduce a new interface to the PHAST libraries from the R statistical computing environment, called RPHAST, and illustrate its use in a series of vignettes. We demonstrate that RPHAST can be particularly useful in applications involving both large-scale phylogenomics and complex statistical analyses. The R interface also makes the PHAST libraries acccessible to non-C programmers, and is useful for rapid prototyping. PHAST v1.0 and RPHAST v1.0 are available for download at http://compgen.bscb.cornell.edu/phast, under the terms of an unrestrictive BSD-style license. RPHAST can also be obtained from the Comprehensive R Archive Network (CRAN; http://cran.r-project.org).
21278049	Review of extracting information from the Social Web for health personalization.
J. Med. Internet Res. 20110128 2011
In recent years the Web has come into its own as a social platform where health consumers are actively creating and consuming Web content. Moreover, as the Web matures, consumers are gaining access to personalized applications adapted to their health needs and interests. The creation of personalized Web applications relies on extracted information about the users and the content to personalize. The Social Web itself provides many sources of information that can be used to extract information for personalization apart from traditional Web forms and questionnaires. This paper provides a review of different approaches for extracting information from the Social Web for health personalization. We reviewed research literature across different fields addressing the disclosure of health information in the Social Web, techniques to extract that information, and examples of personalized health applications. In addition, the paper includes a discussion of technical and socioethical challenges related to the extraction of information for health personalization.
21287628	Fingerprint-based structure retrieval using electron density.
Proteins 20110103 2011Mar
We present a computational approach that can quickly search a large protein structural database to identify structures that fit a given electron density, such as determined by cryo-electron microscopy. We use geometric invariants (fingerprints) constructed using 3D Zernike moments to describe the electron density, and reduce the problem of fitting of the structure to the electron density to simple fingerprint comparison. Using this approach, we are able to screen the entire Protein Data Bank and identify structures that fit two experimental electron densities determined by cryo-electron microscopy.
21294836	The systematic review of literature: synthesizing evidence for practice.
J Spec Pediatr Nurs  2011Jan
With current emphasis on evidence-based practice, nurses are searching for answers to questions generated at the bedside. One method to identify and evaluate the existing evidence is to conduct or read a systematic review of the literature. The purpose of this paper is to describe the process of conducting a systematic review of literature. Defining attributes, rationale for, and steps in conducting systematic reviews are presented. Examples from published reviews on pediatric nursing are included. Pediatric nurses may consult professionally prepared systematic reviews, such as The Cochrane Collection, or conduct their own reviews with the help of electronic search engines.
21208091	Web vulnerability study of online pharmacy sites.
Inform Health Soc Care  2011Jan
Consumers are increasingly using online pharmacies, but these sites may not provide an adequate level of security with the consumers' personal data. There is a gap in this research addressing the problems of security vulnerabilities in this industry. The objective is to identify the level of web application security vulnerabilities in online pharmacies and the common types of flaws, thus expanding on prior studies. Technical, managerial and legal recommendations on how to mitigate security issues are presented. The proposed four-step method first consists of choosing an online testing tool. The next steps involve choosing a list of 60 online pharmacy sites to test, and then running the software analysis to compile a list of flaws. Finally, an in-depth analysis is performed on the types of web application vulnerabilities. The majority of sites had serious vulnerabilities, with the majority of flaws being cross-site scripting or old versions of software that have not been updated. A method is proposed for the securing of web pharmacy sites, using a multi-phased approach of technical and managerial techniques together with a thorough understanding of national legal requirements for securing systems.
21146377	An ontology-based comparative anatomy information system.
Artif Intell Med 20101210 2011Jan
This paper describes the design, implementation, and potential use of a comparative anatomy information system (CAIS) for querying on similarities and differences between homologous anatomical structures across species, the knowledge base it operates upon, the method it uses for determining the answers to the queries, and the user interface it employs to present the results. The relevant informatics contributions of our work include (1) the development and application of the structural difference method, a formalism for symbolically representing anatomical similarities and differences across species; (2) the design of the structure of a mapping between the anatomical models of two different species and its application to information about specific structures in humans, mice, and rats; and (3) the design of the internal syntax and semantics of the query language. These contributions provide the foundation for the development of a working system that allows users to submit queries about the similarities and differences between mouse, rat, and human anatomy; delivers result sets that describe those similarities and differences in symbolic terms; and serves as a prototype for the extension of the knowledge base to any number of species. Additionally, we expanded the domain knowledge by identifying medically relevant structural questions for the human, the mouse, and the rat, and made an initial foray into the validation of the application and its content by means of user questionnaires, software testing, and other feedback. The anatomical structures of the species to be compared, as well as the mappings between species, are modeled on templates from the Foundational Model of Anatomy knowledge base, and compared using graph-matching techniques. A graphical user interface allows users to issue queries that retrieve information concerning similarities and differences between structures in the species being examined. Queries from diverse information sources, including domain experts, peer-reviewed articles, and reference books, have been used to test the system and to illustrate its potential use in comparative anatomy studies. 157 test queries were submitted to the CAIS system, and all of them were correctly answered. The interface was evaluated in terms of clarity and ease of use. This testing determined that the application works well, and is fairly intuitive to use, but users want to see more clarification of the meaning of the different types of possible queries. Some of the interface issues will naturally be resolved as we refine our conceptual model to deal with partial and complex homologies in the content. The CAIS system and its associated methods are expected to be useful to biologists and translational medicine researchers. Possible applications range from supporting theoretical work in clarifying and modeling ontogenetic, physiological, pathological, and evolutionary transformations, to concrete techniques for improving the analysis of genotype-phenotype relationships among various animal models in support of a wide array of clinical and scientific initiatives.
21233530	Visual exploration across biomedical databases.
IEEE/ACM Trans Comput Biol Bioinform  2011 Mar-Apr
Though biomedical research often draws on knowledge from a wide variety of fields, few visualization methods for biomedical data incorporate meaningful cross-database exploration. A new approach is offered for visualizing and exploring a query-based subset of multiple heterogeneous biomedical databases. Databases are modeled as an entity-relation graph containing nodes (database records) and links (relationships between records). Users specify a keyword search string to retrieve an initial set of nodes, and then explore intra- and interdatabase links. Results are visualized with user-defined semantic substrates to take advantage of the rich set of attributes usually present in biomedical data. Comments from domain experts indicate that this visualization method is potentially advantageous for biomedical knowledge exploration.
21242735	Leading wound care technology: The ARANZ medical silhouette.
Adv Skin Wound Care  2011Feb
Despite major advances in medical technology and wound care, wound assessment and documentation still rely mainly on rudimentary measures. Many practitioners continue to estimate wound size using maximal length, width, and depth measurements with rulers and probes. Others use acetate tracings or equivalent measures to outline the wound onto a grid to estimate surface area and document wound border changes. Even fewer practitioners seem to routinely photograph wounds with an included scale for more visual records. This article presents a recently developed device that is changing clinical documentation, assessment, auditing, and interservice communication-the ARANZ Medical Silhouette (ARANZ Medical Limited, Christchurch, New Zealand).
21243054	The Bellevue Classification System: nursing's voice upon the library shelves.
J Med Libr Assoc  2011Jan
This article examines the inspiration, construction, and meaning of the Bellevue Classification System (BCS), created during the 1930s for use in the Bellevue School of Nursing Library. Nursing instructor Ann Doyle, with assistance from librarian Mary Casamajor, designed the BCS after consulting with library leaders and examining leading contemporary classification systems, including the Dewey Decimal Classification and Library of Congress, Ballard, and National Health Library classification systems. A close textual reading of the classes, subclasses, and subdivisions of these classification systems against those of the resulting BCS, reveals Doyle's belief that the BCS was created not only to organize the literature, but also to promote the burgeoning intellectualism and professionalism of early twentieth-century American nursing.
21243055	From Hahnemann's hand to your computer screen: building a digital homeopathy collection.
J Med Libr Assoc  2011Jan
The University of California, San Francisco (UCSF), Library holds the unique manuscript of the sixth edition of Samuel Hahnemann's Organon der Heilkunst, the primary text of homeopathy. The manuscript volume is Hahnemann's own copy of the fifth edition of the Organon with his notes for the sixth edition, handwritten throughout the volume. There is a high level of interest in the Organon manuscript, particularly among homeopaths. This led to the decision to present a digital surrogate on the web to make it accessible to a wider audience. Digitizing Hahnemann's manuscript and determining the best method of presentation on the web posed several challenges. Lessons learned in the course of this project will inform future digital projects. This article discusses the historical significance of the sixth edition of Hahnemann's Organon, its context in UCSF's homeopathy collections, and the specifics of developing the online homeopathy collection.
21243059	A validated search assessment tool: assessing practice-based learning and improvement in a residency program.
J Med Libr Assoc  2011Jan
The objective of this study was to validate an assessment instrument for MEDLINE search strategies at an academic medical center. Two approaches were used to investigate if the search assessment tool could capture performance differences in search strategy construction. First, data from an evaluation of MEDLINE searches from a pediatric resident's longitudinal assessment were investigated. Second, a cross-section of search strategies from residents in one incoming class was compared with strategies of residents graduating a year later. MEDLINE search strategies formulated by faculty who had been identified as having search expertise were used as a gold standard comparison. Participants were presented with a clinical scenario and asked to identify the search question and conduct a MEDLINE search. Two librarians rated the blinded search strategies. Search strategy scores were significantly higher for residents who received training than the comparison group with no training. There was no significant difference in search strategy scores between senior residents who received training and faculty experts. The results provide evidence for the validity of the instrument to evaluate MEDLINE search strategies. This assessment tool can measure improvements in information-seeking skills and provide data to fulfill Accreditation Council for Graduate Medical Education competencies.
21248691	Facilitating the analysis of immunological data with visual analytic techniques.
J Vis Exp 20110102 2011
Visual analytics (VA) has emerged as a new way to analyze large dataset through interactive visual display. We demonstrated the utility and the flexibility of a VA approach in the analysis of biological datasets. Examples of these datasets in immunology include flow cytometry, Luminex data, and genotyping (e.g., single nucleotide polymorphism) data. Contrary to the traditional information visualization approach, VA restores the analysis power in the hands of analyst by allowing the analyst to engage in real-time data exploration process. We selected the VA software called Tableau after evaluating several VA tools. Two types of analysis tasks analysis within and between datasets were demonstrated in the video presentation using an approach called paired analysis. Paired analysis, as defined in VA, is an analysis approach in which a VA tool expert works side-by-side with a domain expert during the analysis. The domain expert is the one who understands the significance of the data, and asks the questions that the collected data might address. The tool expert then creates visualizations to help find patterns in the data that might answer these questions. The short lag-time between the hypothesis generation and the rapid visual display of the data is the main advantage of a VA approach.
21249231	Benchmarking ontologies: bigger or better?
PLoS Comput. Biol. 20110113 2011
A scientific ontology is a formal representation of knowledge within a domain, typically including central concepts, their properties, and relations. With the rise of computers and high-throughput data collection, ontologies have become essential to data mining and sharing across communities in the biomedical sciences. Powerful approaches exist for testing the internal consistency of an ontology, but not for assessing the fidelity of its domain representation. We introduce a family of metrics that describe the breadth and depth with which an ontology represents its knowledge domain. We then test these metrics using (1) four of the most common medical ontologies with respect to a corpus of medical documents and (2) seven of the most popular English thesauri with respect to three corpora that sample language from medicine, news, and novels. Here we show that our approach captures the quality of ontological representation and guides efforts to narrow the breach between ontology and collective discourse within a domain. Our results also demonstrate key features of medical ontologies, English thesauri, and discourse from different domains. Medical ontologies have a small intersection, as do English thesauri. Moreover, dialects characteristic of distinct domains vary strikingly as many of the same words are used quite differently in medicine, news, and novels. As ontologies are intended to mirror the state of knowledge, our methods to tighten the fit between ontology and domain will increase their relevance for new areas of biomedical science and improve the accuracy and power of inferences computed across them.
21249186	Figure text extraction in biomedical literature.
PLoS ONE 20110113 2011
Figures are ubiquitous in biomedical full-text articles, and they represent important biomedical knowledge. However, the sheer volume of biomedical publications has made it necessary to develop computational approaches for accessing figures. Therefore, we are developing the Biomedical Figure Search engine (http://figuresearch.askHERMES.org) to allow bioscientists to access figures efficiently. Since text frequently appears in figures, automatically extracting such text may assist the task of mining information from figures. Little research, however, has been conducted exploring text extraction from biomedical figures. We first evaluated an off-the-shelf Optical Character Recognition (OCR) tool on its ability to extract text from figures appearing in biomedical full-text articles. We then developed a Figure Text Extraction Tool (FigTExT) to improve the performance of the OCR tool for figure text extraction through the use of three innovative components: image preprocessing, character recognition, and text correction. We first developed image preprocessing to enhance image quality and to improve text localization. Then we adapted the off-the-shelf OCR tool on the improved text localization for character recognition. Finally, we developed and evaluated a novel text correction framework by taking advantage of figure-specific lexicons. The evaluation on 382 figures (9,643 figure texts in total) randomly selected from PubMed Central full-text articles shows that FigTExT performed with 84% precision, 98% recall, and 90% F1-score for text localization and with 62.5% precision, 51.0% recall and 56.2% F1-score for figure text extraction. When limiting figure texts to those judged by domain experts to be important content, FigTExT performed with 87.3% precision, 68.8% recall, and 77% F1-score. FigTExT significantly improved the performance of the off-the-shelf OCR tool we used, which on its own performed with 36.6% precision, 19.3% recall, and 25.3% F1-score for text extraction. In addition, our results show that FigTExT can extract texts that do not appear in figure captions or other associated text, further suggesting the potential utility of FigTExT for improving figure search.
21208450	Logical development of the cell ontology.
BMC Bioinformatics 20110105 2011
The Cell Ontology (CL) is an ontology for the representation of in vivo cell types. As biological ontologies such as the CL grow in complexity, they become increasingly difficult to use and maintain. By making the information in the ontology computable, we can use automated reasoners to detect errors and assist with classification. Here we report on the generation of computable definitions for the hematopoietic cell types in the CL. Computable definitions for over 340 CL classes have been created using a genus-differentia approach. These define cell types according to multiple axes of classification such as the protein complexes found on the surface of a cell type, the biological processes participated in by a cell type, or the phenotypic characteristics associated with a cell type. We employed automated reasoners to verify the ontology and to reveal mistakes in manual curation. The implementation of this process exposed areas in the ontology where new cell type classes were needed to accommodate species-specific expression of cellular markers. Our use of reasoners also inferred new relationships within the CL, and between the CL and the contributing ontologies. This restructured ontology can be used to identify immune cells by flow cytometry, supports sophisticated biological queries involving cells, and helps generate new hypotheses about cell function based on similarities to other cell types. Use of computable definitions enhances the development of the CL and supports the interoperability of OBO ontologies.
21252399	Non-lambertian reflectance modeling and shape recovery of faces using tensor splines.
IEEE Trans Pattern Anal Mach Intell  2011Mar
Modeling illumination effects and pose variations of a face is of fundamental importance in the field of facial image analysis. Most of the conventional techniques that simultaneously address both of these problems work with the Lambertian assumption and thus fall short of accurately capturing the complex intensity variation that the facial images exhibit or recovering their 3D shape in the presence of specularities and cast shadows. In this paper, we present a novel Tensor-Spline-based framework for facial image analysis. We show that, using this framework, the facial apparent BRDF field can be accurately estimated while seamlessly accounting for cast shadows and specularities. Further, using local neighborhood information, the same framework can be exploited to recover the 3D shape of the face (to handle pose variation). We quantitatively validate the accuracy of the Tensor Spline model using a more general model based on the mixture of single-lobed spherical functions. We demonstrate the effectiveness of our technique by presenting extensive experimental results for face relighting, 3D shape recovery, and face recognition using the Extended Yale B and CMU PIE benchmark data sets.
21252400	Tiny videos: a large data set for nonparametric video retrieval and frame classification.
IEEE Trans Pattern Anal Mach Intell  2011Mar
In this paper, we present a large database of over 50,000 user-labeled videos collected from YouTube. We develop a compact representation called "tiny videos" that achieves high video compression rates while retaining the overall visual appearance of the video as it varies over time. We show that frame sampling using affinity propagation-an exemplar-based clustering algorithm-achieves the best trade-off between compression and video recall. We use this large collection of user-labeled videos in conjunction with simple data mining techniques to perform related video retrieval, as well as classification of images and video frames. The classification results achieved by tiny videos are compared with the tiny images framework [24] for a variety of recognition tasks. The tiny images data set consists of 80 million images collected from the Internet. These are the largest labeled research data sets of videos and images available to date. We show that tiny videos are better suited for classifying scenery and sports activities, while tiny images perform better at recognizing objects. Furthermore, we demonstrate that combining the tiny images and tiny videos data sets improves classification precision in a wider range of categories.
20870391	Storage and recall capabilities of fuzzy morphological associative memories with adjunction-based learning.
Neural Netw 20100909 2011Jan
We recently employed concepts of mathematical morphology to introduce fuzzy morphological associative memories (FMAMs), a broad class of fuzzy associative memories (FAMs). We observed that many well-known FAM models can be classified as belonging to the class of FMAMs. Moreover, we developed a general learning strategy for FMAMs using the concept of adjunction of mathematical morphology. In this paper, we describe the properties of FMAMs with adjunction-based learning. In particular, we characterize the recall phase of these models. Furthermore, we prove several theorems concerning the storage capacity, noise tolerance, fixed points, and convergence of auto-associative FMAMs. These theorems are corroborated by experimental results concerning the reconstruction of noisy images. Finally, we successfully employ FMAMs with adjunction-based learning in order to implement fuzzy rule-based systems in an application to a time-series prediction problem in industry.
20733224	A fast bilinear structure from motion algorithm using a video sequence and inertial sensors.
IEEE Trans Pattern Anal Mach Intell  2011Jan
In this paper, we study the benefits of the availability of a specific form of additional information—the vertical direction (gravity) and the height of the camera, both of which can be conveniently measured using inertial sensors and a monocular video sequence for 3D urban modeling. We show that in the presence of this information, the SfM equations can be rewritten in a bilinear form. This allows us to derive a fast, robust, and scalable SfM algorithm for large scale applications. The SfM algorithm developed in this paper is experimentally demonstrated to have favorable properties compared to the sparse bundle adjustment algorithm. We provide experimental evidence indicating that the proposed algorithm converges in many cases to solutions with lower error than state-of-art implementations of bundle adjustment. We also demonstrate that for the case of large reconstruction problems, the proposed algorithm takes lesser time to reach its solution compared to bundle adjustment. We also present SfM results using our algorithm on the Google StreetView research data set.
21055489	Assigning statistical significance to proteotypic peptides via database searches.
J Proteomics 20101103 2011Feb1
Querying MS/MS spectra against a database containing only proteotypic peptides reduces data analysis time due to reduction of database size. Despite the speed advantage, this search strategy is challenged by issues of statistical significance and coverage. The former requires separating systematically significant identifications from less confident identifications, while the latter arises when the underlying peptide is not present, due to single amino acid polymorphisms (SAPs) or post-translational modifications (PTMs), in the proteotypic peptide libraries searched. To address both issues simultaneously, we have extended RAId's knowledge database to include proteotypic information, utilized RAId's statistical strategy to assign statistical significance to proteotypic peptides, and modified RAId's programs to allow for consideration of proteotypic information during database searches. The extended database alleviates the coverage problem since all annotated modifications, even those that occurred within proteotypic peptides, may be considered. Taking into account the likelihoods of observation, the statistical strategy of RAId provides accurate E-value assignments regardless whether a candidate peptide is proteotypic or not. The advantage of including proteotypic information is evidenced by its superior retrieval performance when compared to regular database searches.
20693107	Case retrieval in medical databases by fusing heterogeneous information.
IEEE Trans Med Imaging 20100805 2011Jan
A novel content-based heterogeneous information retrieval framework, particularly well suited to browse medical databases and support new generation computer aided diagnosis (CADx) systems, is presented in this paper. It was designed to retrieve possibly incomplete documents, consisting of several images and semantic information, from a database; more complex data types such as videos can also be included in the framework. The proposed retrieval method relies on image processing, in order to characterize each individual image in a document by their digital content, and information fusion. Once the available images in a query document are characterized, a degree of match, between the query document and each reference document stored in the database, is defined for each attribute (an image feature or a metadata). A Bayesian network is used to recover missing information if need be. Finally, two novel information fusion methods are proposed to combine these degrees of match, in order to rank the reference documents by decreasing relevance for the query. In the first method, the degrees of match are fused by the Bayesian network itself. In the second method, they are fused by the Dezert-Smarandache theory: the second approach lets us model our confidence in each source of information (i.e., each attribute) and take it into account in the fusion process for a better retrieval performance. The proposed methods were applied to two heterogeneous medical databases, a diabetic retinopathy database and a mammography screening database, for computer aided diagnosis. Precisions at five of 0.809 ± 0.158 and 0.821 ± 0.177, respectively, were obtained for these two databases, which is very promising.
20699207	A new supervised method for blood vessel segmentation in retinal images by using gray-level and moment invariants-based features.
IEEE Trans Med Imaging 20100809 2011Jan
This paper presents a new supervised method for blood vessel detection in digital retinal images. This method uses a neural network (NN) scheme for pixel classification and computes a 7-D vector composed of gray-level and moment invariants-based features for pixel representation. The method was evaluated on the publicly available DRIVE and STARE databases, widely used for this purpose, since they contain retinal images where the vascular structure has been precisely marked by experts. Method performance on both sets of test images is better than other existing solutions in literature. The method proves especially accurate for vessel detection in STARE images. Its application to this database (even when the NN was trained on the DRIVE database) outperforms all analyzed segmentation approaches. Its effectiveness and robustness with different image conditions, together with its simplicity and fast implementation, make this blood vessel segmentation proposal suitable for retinal image computer analyses such as automated screening for early diabetic retinopathy detection.
20972749	Tissue microarrays and digital image analysis.
Methods Mol. Biol.  2011
Tissue microarrays (TMAs) have recently emerged as very valuable tools for high-throughput pathological assessment, especially in the cancer research arena. This important technology, however, has yet to fully penetrate into the area of toxicology. Here, we describe the creation of TMAs representative of samples produced from conventional toxicology studies within a large-scale, multi-institutional pan-European project, PredTox. PredTox, short for Predictive Toxicology, formed part of an EU FP6 Integrated Project, Innovative Medicines for Europe (InnoMed), and aimed to study pre-clinically 16 compounds of known liver and/or kidney toxicity. In more detail, TMAs were constructed from materials corresponding to the full face sections of liver and kidney from rats treated with different drug candidates by members of the consortium. We also describe the process of digital slide scanning of kidney and liver sections, in the context of creating an online resource of histopathological data.
21071788	Video painting with space-time-varying style parameters.
IEEE Trans Vis Comput Graph  2011Jan
Artists use different means of stylization to control the focus on different objects in the scene. This allows them to portray complex meaning and achieve certain artistic effects. Most prior work on painterly rendering of videos, however, uses only a single painting style, with fixed global parameters, irrespective of objects and their layout in the images. This often leads to inadequate artistic control. Moreover, brush stroke orientation is typically assumed to follow an everywhere continuous directional field. In this paper, we propose a video painting system that accounts for the spatial support of objects in the images or videos, and uses this information to specify style parameters and stroke orientation for painterly rendering. Since objects occupy distinct image locations and move relatively smoothly from one video frame to another, our object-based painterly rendering approach is characterized by style parameters that coherently vary in space and time. Space-time-varying style parameters enable more artistic freedom, such as emphasis/de-emphasis, increase or decrease of contrast, exaggeration or abstraction of different objects in the scene in a temporally coherent fashion.
21071790	Automatic metro map layout using multicriteria optimization.
IEEE Trans Vis Comput Graph  2011Jan
This paper describes an automatic mechanism for drawing metro maps. We apply multicriteria optimization to find effective placement of stations with a good line layout and to label the map unambiguously. A number of metrics are defined, which are used in a weighted sum to find a fitness value for a layout of the map. A hill climbing optimizer is used to reduce the fitness value, and find improved map layouts. To avoid local minima, we apply clustering techniques to the map-the hill climber moves both stations and clusters when finding improved layouts. We show the method applied to a number of metro maps, and describe an empirical study that provides some quantitative evidence that automatically-drawn metro maps can help users to find routes more efficiently than either published maps or undistorted maps. Moreover, we have found that, in these cases, study subjects indicate a preference for automatically-drawn maps over the alternatives.
21088323	Product quantization for nearest neighbor search.
IEEE Trans Pattern Anal Mach Intell  2011Jan
This paper introduces a product quantization-based approach for approximate nearest neighbor search. The idea is to decompose the space into a Cartesian product of low-dimensional subspaces and to quantize each subspace separately. A vector is represented by a short code composed of its subspace quantization indices. The euclidean distance between two vectors can be efficiently estimated from their codes. An asymmetric version increases precision, as it computes the approximate distance between a vector and a code. Experimental results show that our approach searches for nearest neighbors efficiently, in particular in combination with an inverted file system. Results for SIFT and GIST image descriptors show excellent search accuracy, outperforming three state-of-the-art approaches. The scalability of our approach is validated on a data set of two billion vectors.
21531961	Medicaid and state regulation of nurse-midwives: the challenge of data retrieval.
Policy Polit Nurs Pract  2010Nov
This article discusses one of four findings of a larger descriptive correlational health policy study, the purpose of which was to investigate relationships among state regulation of nurse-midwifery practice, utilization of certified nurse-midwives (CNM) for Medicaid funded prenatal care, and maternal newborn outcomes. The larger study showed that use of accurate data about CNM practice and subsequent health care outcomes creates a challenge for researchers because of the paucity of data related to services provided by CNMs. Barriers to adequate data collection related to CNM services, specifically those funded by Medicaid, preclude legitimate conclusions about subsequent health care policy. Methods of workforce data collection need to be addressed by health care and health policy groups to facilitate further investigation of the relationships among state regulation of CNM practice, utilization of CNMs for Medicaid-funded prenatal care and maternal newborn outcomes as they affect access to care for vulnerable populations.
22163414	Sensor data fusion for accurate cloud presence prediction using Dempster-Shafer evidence theory.
Sensors (Basel) 20101018 2010
Sensor data fusion technology can be used to best extract useful information from multiple sensor observations. It has been widely applied in various applications such as target tracking, surveillance, robot navigation, signal and image processing. This paper introduces a novel data fusion approach in a multiple radiation sensor environment using Dempster-Shafer evidence theory. The methodology is used to predict cloud presence based on the inputs of radiation sensors. Different radiation data have been used for the cloud prediction. The potential application areas of the algorithm include renewable power for virtual power station where the prediction of cloud presence is the most challenging issue for its photovoltaic output. The algorithm is validated by comparing the predicted cloud presence with the corresponding sunshine occurrence data that were recorded as the benchmark. Our experiments have indicated that comparing to the approaches using individual sensors, the proposed data fusion approach can increase correct rate of cloud prediction by ten percent, and decrease unknown rate of cloud prediction by twenty three percent.
22163472	Dynamic load balancing data centric storage for wireless sensor networks.
Sensors (Basel) 20101117 2010
In this paper, a new data centric storage that is dynamically adapted to the work load changes is proposed. The proposed data centric storage distributes the load of hot spot areas to neighboring sensor nodes by using a multilevel grid technique. The proposed method is also able to use existing routing protocols such as GPSR (Greedy Perimeter Stateless Routing) with small changes. Through simulation, the proposed method enhances the lifetime of sensor networks over one of the state-of-the-art data centric storages. We implement the proposed method based on an operating system for sensor networks, and evaluate the performance through running based on a simulation tool.
22163524	Approximate nearest neighbor search by residual vector quantization.
Sensors (Basel) 20101208 2010
A recently proposed product quantization method is efficient for large scale approximate nearest neighbor search, however, its performance on unstructured vectors is limited. This paper introduces residual vector quantization based approaches that are appropriate for unstructured vectors. Database vectors are quantized by residual vector quantizer. The reproductions are represented by short codes composed of their quantization indices. Euclidean distance between query vector and database vector is approximated by asymmetric distance, i.e., the distance between the query vector and the reproduction of the database vector. An efficient exhaustive search approach is proposed by fast computing the asymmetric distance. A straight forward non-exhaustive search approach is proposed for large scale search. Our approaches are compared to two state-of-the-art methods, spectral hashing and product quantization, on both structured and unstructured datasets. Results show that our approaches obtain the best results in terms of the trade-off between search quality and memory usage.
22219700	Fast scene recognition and camera relocalisation for wide area augmented reality systems.
Sensors (Basel) 20100614 2010
This paper focuses on online scene learning and fast camera relocalisation which are two key problems currently limiting the performance of wide area augmented reality systems. Firstly, we propose to use adaptive random trees to deal with the online scene learning problem. The algorithm can provide more accurate recognition rates than traditional methods, especially with large scale workspaces. Secondly, we use the enhanced PROSAC algorithm to obtain a fast camera relocalisation method. Compared with traditional algorithms, our method can significantly reduce the computation complexity, which facilitates to a large degree the process of online camera relocalisation. Finally, we implement our algorithms in a multithreaded manner by using a parallel-computing scheme. Camera tracking, scene mapping, scene learning and relocalisation are separated into four threads by using multi-CPU hardware architecture. While providing real-time tracking performance, the resulting system also possesses the ability to track multiple maps simultaneously. Some experiments have been conducted to demonstrate the validity of our methods.
22163638	Using RFID to enhance security in off-site data storage.
Sensors (Basel) 20100827 2010
Off-site data storage is one of the most widely used strategies in enterprises of all sizes to improve business continuity. In medium-to-large size enterprises, the off-site data storage processes are usually outsourced to specialized providers. However, outsourcing the storage of critical business information assets raises serious security considerations, some of which are usually either disregarded or incorrectly addressed by service providers. This article reviews these security considerations and presents a radio frequency identification (RFID)-based, off-site, data storage management system specifically designed to address security issues. The system relies on a set of security mechanisms or controls that are arranged in security layers or tiers to balance security requirements with usability and costs. The system has been successfully implemented, deployed and put into production. In addition, an experimental comparison with classical bar-code-based systems is provided, demonstrating the system's benefits in terms of efficiency and failure prevention.
22315570	Effective route maintenance and restoration schemes in mobile ad hoc networks.
Sensors (Basel) 20100121 2010
This study proposes a location-based hybrid routing protocol to improve data packet delivery and to reduce control message overhead in mobile ad hoc networks. In mobile environments, where nodes move continuously at a high speed, it is generally difficult to maintain and restore route paths. Therefore, this study suggests a new flooding mechanism to control route paths. The essence of the proposed scheme is its effective tracking of the destination's location based on the beacon messages of the main route nodes. Through experiments based on an NS-2 simulator, the proposed scheme shows improvements in the data packet delivery ratio and reduces the amount of routing control message overhead compared with existing routing protocols such as AODV, LAR, ZRP and AODV-DFR.
22315572	Vision-based traffic data collection sensor for automotive applications.
Sensors (Basel) 20100122 2010
This paper presents a complete vision sensor onboard a moving vehicle which collects the traffic data in its local area in daytime conditions. The sensor comprises a rear looking and a forward looking camera. Thus, a representative description of the traffic conditions in the local area of the host vehicle can be computed. The proposed sensor detects the number of vehicles (traffic load), their relative positions and their relative velocities in a four-stage process: lane detection, candidates selection, vehicles classification and tracking. Absolute velocities (average road speed) and global positioning are obtained after combining the outputs provided by the vision sensor with the data supplied by the CAN Bus and a GPS sensor. The presented experiments are promising in terms of detection performance and accuracy in order to be validated for applications in the context of the automotive industry.
21176171	Data linkage: a powerful research tool with potential problems.
BMC Health Serv Res 20101222 2010
Policy makers, clinicians and researchers are demonstrating increasing interest in using data linked from multiple sources to support measurement of clinical performance and patient health outcomes. However, the utility of data linkage may be compromised by sub-optimal or incomplete linkage, leading to systematic bias. In this study, we synthesize the evidence identifying participant or population characteristics that can influence the validity and completeness of data linkage and may be associated with systematic bias in reported outcomes. A narrative review, using structured search methods was undertaken. Key words "data linkage" and Mesh term "medical record linkage" were applied to Medline, EMBASE and CINAHL databases between 1991 and 2007. Abstract inclusion criteria were; the article attempted an empirical evaluation of methodological issues relating to data linkage and reported on patient characteristics, the study design included analysis of matched versus unmatched records, and the report was in English. Included articles were grouped thematically according to patient characteristics that were compared between matched and unmatched records. The search identified 1810 articles of which 33 (1.8%) met inclusion criteria. There was marked heterogeneity in study methods and factors investigated. Characteristics that were unevenly distributed among matched and unmatched records were; age (72% of studies), sex (50% of studies), race (64% of studies), geographical/hospital site (93% of studies), socio-economic status (82% of studies) and health status (72% of studies). A number of relevant patient or population factors may be associated with incomplete data linkage resulting in systematic bias in reported clinical outcomes. Readers should consider these factors in interpreting the reported results of data linkage studies.
21192813	SIDEKICK: Genomic data driven analysis and decision-making framework.
BMC Bioinformatics 20101230 2010
Scientists striving to unlock mysteries within complex biological systems face myriad barriers in effectively integrating available information to enhance their understanding. While experimental techniques and available data sources are rapidly evolving, useful information is dispersed across a variety of sources, and sources of the same information often do not use the same format or nomenclature. To harness these expanding resources, scientists need tools that bridge nomenclature differences and allow them to integrate, organize, and evaluate the quality of information without extensive computation. Sidekick, a genomic data driven analysis and decision making framework, is a web-based tool that provides a user-friendly intuitive solution to the problem of information inaccessibility. Sidekick enables scientists without training in computation and data management to pursue answers to research questions like "What are the mechanisms for disease X" or "Does the set of genes associated with disease X also influence other diseases." Sidekick enables the process of combining heterogeneous data, finding and maintaining the most up-to-date data, evaluating data sources, quantifying confidence in results based on evidence, and managing the multi-step research tasks needed to answer these questions. We demonstrate Sidekick's effectiveness by showing how to accomplish a complex published analysis in a fraction of the original time with no computational effort using Sidekick. Sidekick is an easy-to-use web-based tool that organizes and facilitates complex genomic research, allowing scientists to explore genomic relationships and formulate hypotheses without computational effort. Possible analysis steps include gene list discovery, gene-pair list discovery, various enrichments for both types of lists, and convenient list manipulation. Further, Sidekick's ability to characterize pairs of genes offers new ways to approach genomic analysis that traditional single gene lists do not, particularly in areas such as interaction discovery.
21240951	Children as codesigners of new technologies: valuing the imagination to transform what is possible.
New Dir Youth Dev  2010Winter
The technological complexity and richness of a child's environment today is far beyond what any adults today experienced when they were growing up. For example, no adult today knows what it is like to be a four-year-old using his or her first iPhone app or Webkinz account. Therefore, we seek ways to understand what children need in today's new technologies even without ourselves being children. Since 1999, young people ages seven to eleven have been the author's partners in codesigning new educational technologies at the University of Maryland's Human-Computer Interaction Lab. This work has helped inform who children are--what matters to them, what technologies need to be changed, and what needs to be built for the future. This work uses cooperative inquiry, a set of codesign methods that can enable adults and children to share their ideas while minimizing the differences in age and communication styles. This article describes low-tech methods for brainstorming, offering feedback, and supporting creative change in technology prototype designs. Examples of technologies are discussed and insights from children shared.
21240954	YouTube as a participatory culture.
New Dir Youth Dev  2010Winter
There is an explosion of youth subscriptions to original content-media-sharing Web sites such as YouTube. These Web sites combine media production and distribution with social networking features, making them an ideal place to create, connect, collaborate, and circulate. By encouraging youth to become media creators and social networkers, new media platforms such as YouTube offer a participatory culture in which youth can develop, interact, and learn. As youth development researchers, we must be cognizant of this context and critically examine what this platform offers that might be unique to (or redundant of) typical adolescent experiences in other developmental contexts.
21176225	MBAT: a scalable informatics system for unifying digital atlasing workflows.
BMC Bioinformatics 20101222 2010
Digital atlases provide a common semantic and spatial coordinate system that can be leveraged to compare, contrast, and correlate data from disparate sources. As the quality and amount of biological data continues to advance and grow, searching, referencing, and comparing this data with a researcher's own data is essential. However, the integration process is cumbersome and time-consuming due to misaligned data, implicitly defined associations, and incompatible data sources. This work addressing these challenges by providing a unified and adaptable environment to accelerate the workflow to gather, align, and analyze the data. The MouseBIRN Atlasing Toolkit (MBAT) project was developed as a cross-platform, free open-source application that unifies and accelerates the digital atlas workflow. A tiered, plug-in architecture was designed for the neuroinformatics and genomics goals of the project to provide a modular and extensible design. MBAT provides the ability to use a single query to search and retrieve data from multiple data sources, align image data using the user's preferred registration method, composite data from multiple sources in a common space, and link relevant informatics information to the current view of the data or atlas. The workspaces leverage tool plug-ins to extend and allow future extensions of the basic workspace functionality. A wide variety of tool plug-ins were developed that integrate pre-existing as well as newly created technology into each workspace. Novel atlasing features were also developed, such as supporting multiple label sets, dynamic selection and grouping of labels, and synchronized, context-driven display of ontological data. MBAT empowers researchers to discover correlations among disparate data by providing a unified environment for bringing together distributed reference resources, a user's image data, and biological atlases into the same spatial or semantic context. Through its extensible tiered plug-in architecture, MBAT allows researchers to customize all platform components to quickly achieve personalized workflows.
20922480	Which factors predict the time spent answering queries to a drug information centre?
Pharm World Sci 20101005 2010Dec
To develop a model based upon factors able to predict the time spent answering drug-related queries to Norwegian drug information centres (DICs). Drug-related queries received at 5 DICs in Norway from March to May 2007 were randomly assigned to 20 employees until each of them had answered a minimum of five queries. The employees reported the number of drugs involved, the type of literature search performed, and whether the queries were considered judgmental or not, using a specifically developed scoring system. The scores of these three factors were added together to define a workload score for each query. Workload and its individual factors were subsequently related to the measured time spent answering the queries by simple or multiple linear regression analyses. Ninety-six query/answer pairs were analyzed. Workload significantly predicted the time spent answering the queries (adjusted R (2) = 0.22, P &lt; 0.001). Literature search was the individual factor best predicting the time spent answering the queries (adjusted R(2) = 0.17, P &lt; 0.001), and this variable also contributed the most in the multiple regression analyses. The most important workload factor predicting the time spent handling the queries in this study was the type of literature search that had to be performed. The categorisation of queries as judgmental or not, also affected the time spent answering the queries. The number of drugs involved did not significantly influence the time spent answering drug information queries.
21190561	Evolution of gene regulation of pluripotency--the case for wiki tracks at genome browsers.
Biol. Direct 20101229 2010
Experimentally validated data on gene regulation are hard to obtain. In particular, information about transcription factor binding sites in regulatory regions are scattered around in the literature. This impedes their systematic in-context analysis, e.g. the inference of their conservation in evolutionary history. We demonstrate the power of integrative bioinformatics by including curated transcription factor binding site information into the UCSC genome browser, using wiki and custom tracks, which enable easy publication of annotation data. Data integration allows to investigate the evolution of gene regulation of the pluripotency-associated genes Oct4, Sox2 and Nanog. For the first time, experimentally validated transcription factor binding sites in the regulatory regions of all three genes were assembled together based on manual curation of data from 39 publications. Using the UCSC genome browser, these data were then visualized in the context of multi-species conservation based on genomic alignment. We confirm previous hypotheses regarding the evolutionary age of specific regulatory patterns, establishing their "deep homology". We also confirm some other principles of Carroll's "Genetic theory of Morphological Evolution", such as "mosaic pleiotropy", exemplified by the dual role of Sox2 reflected in its regulatory region. We were able to elucidate some aspects of the evolution of gene regulation for three genes associated with pluripotency. Based on the expected return on investment for the community, we encourage other scientists to contribute experimental data on gene regulation (original work as well as data collected for reviews) to the UCSC system, to enable studies of the evolution of gene regulation on a large scale, and to report their findings.
20889438	Efficient design of bio-basis function to predict protein functional sites using kernel-based classifiers.
IEEE Trans Nanobioscience 20100930 2010Dec
In order to apply the powerful kernel-based pattern recognition algorithms such as support vector machines to predict functional sites in proteins, amino acids need encoding prior to input. In this regard, a new string kernel function, termed as the modified bio-basis function, is proposed that maps a nonnumerical sequence space to a numerical feature space. The proposed string kernel function is developed based on the conventional bio-basis function and needs a bio-basis string as a support like conventional kernel function. The concept of zone of influence of a bio-basis string is introduced in the proposed kernel function to take into account the influence of each bio-basis string in nonnumerical sequence space. An efficient method is described to select a set of bio-basis strings for the proposed kernel function, integrating the Fisher ratio and a novel concept of degree of resemblance. The integration enables the method to select a reduced set of relevant and nonredundant bio-basis strings.
21309285	Digital microscopy. Bringing new technology into focus.
Health Devices  2010Jun
Digital microscopy enables the scanning of microscope slides so that they can be viewed, analyzed, and archived on a computer. While the technology is not yet widely accepted by pathologists, a switch to digital microscopy systems seems to be inevitable in the near future.
21374993	[How to get free articles on biomedicine from web sites abroad].
Sheng Wu Yi Xue Gong Cheng Xue Za Zhi  2010Dec
Since "Budapest Open Access Initiative" signed in 2002, there have been more and more open-access journals coming out on Internet, thus brining the best condition to the researchers. It has made better the phenomenon of not having enough foreign language periodicals in our country. The present author has selected part of free biomedicine periodical websites: High Wire Press; PubMed Central; BioMed Central; BioLine International; Free Medical Journals Site; Scientific Electronic Library; Hindawi; Academic Journals Inc; Biomedical Journals from India. And then, analysis and demonstration are made with respect to the search channels of 3 main free electronic periodical websites. It is the intent of this paper to make things convenient for researchers to get information from the websites quickly.
21381412	[Virtual room of gastroenterology].
Acta Gastroenterol. Latinoam.  2010Dec
The amount of published information and its continuing growth can no longer be managed by an individual searcher. One of today's great challenges for the academic researcher and clinician is to find a relevant scientific article using bibliographic search strategies. We aimed to design and build a Virtual Room of Gastroenterology (VRG) generating real-time automated search strategies and producing bibliographic and full text search results. These results update and complement with the latest evidence the Clinical Guideline Program of the World Gastroenterology Organisation. The HTML driven interface provides a series of pre-formulated MeSH based search strategies for each Aula. For each topic between 10 and 20 specific terms, qualifiers and subheadings are identified. The functionality of the VRG is based on the PubMed's characteristic that allows a search strategy to be captured as a web address. The VRG is available in Spanish and English, and the access is free. There are 28 rooms currently available. All together these rooms provide an advanced bibliographic access using more than 900 pre-programmed MeSH driven strategies. In a further very recent development some of the topics of VRG now allow cascade based searches. These searches look at resource sensitive options and possible ethnic difference per topic. The VRG allows significant reductions in time required to design and carry out complex bibliographic searches in the areas of gastroenterology, hepatology and endoscopy. The system updates automatically in real-time thus ensuring the currency of the results.
21396243	The benefits and risks of structuring and coding of patient histories in the electronic clinical record: protocol for a systematic review.
Inform Prim Care  2010
Data in medical records have in part been recorded in structured and coded forms for some decades. However, the patient history is as yet largely recorded in an uncoded format. There is a need to consider the optimal balance of use of free text and coded data in the patient history. This review protocol summarises our plans to identify, critically appraise and synthesise evidence relating to approaches taken to introduce structure and coding within patient histories in electronic health records, and the empirically demonstrated benefits and risks of structuring and coding of patient histories in health records. To determine how structured and coded data are being introduced for the recording of patient histories, the benefits observed where structuring and coding have been introduced and the risks encountered when structuring and coding are introduced. We will search the following databases for evidence of published and unpublished material: CINAHL; EMBASE; Google Scholar; IndMED; LILACS; MEDLINE; NIHR; Paklit and PsycINFO. We will, depending on the study designs employed, use the Cochrane EPOC, Joanna Briggs Institute (JBI) and Newcastle-Ottawa instruments to critically appraise studies. Data synthesis is likely to be undertaken using a narrative approach, although meta-analysis will also be undertaken if appropriate and if the data allow this. This protocol should represent a reproducible approach to reviewing the literature regarding structuring and coding in patient histories. We anticipate that we will be able to report results in early 2011. The review should offer increased clarity and direction on the optimal balance between structuring/coding and free text recording of data relating to the patient history.
21396244	Clinical data extraction and feedback in general practice: a case study from Australian primary care.
Inform Prim Care  2010
Quality improvement in general practice has increasingly focused on the analysis of its clinical databases to guide its improvement strategies. However, general practitioners (GPs) need to be motivated to extract and review their clinical data, and they need skills to do so. This study examines the initial experience of 15 practices in undertaking clinical data extraction and management and the support they were given by their local division of general practice. To explore the uptake of data extraction tools in general practice and understand how divisions of general practice can assist with their uptake. This study was conducted within a single division of general practice within the south-eastern suburbs of metropolitan Melbourne, Australia. Self-selected practices were offered a data extraction program ('tool') free of charge, with ongoing division support. Practice representatives, either GPs, practice nurses or other practice staff members, were given instructions on how to extract data using the data extraction tool. This was followed by discussion with division staff regarding which clinical areas might be focused on. Division staff systematically recorded information about the experience of the practices and collated their clinical data. Fifteen practices, representing 69 GPs, participated. The practices chose from the following areas to work on as quality improvement activities: improving data entry; inactivating patient files for those who no longer attended the practice; correcting demographic information; diabetes and coronary heart disease management. The recording of data, according to the extraction tool, was found to be incomplete. For example, one-third of the patients who had HbA1cs recorded were on target, i.e. &lt;7%, but nearly half the patients with diabetes did not have HbA1cs recorded at all. About half the patients with coronary heart disease were not reported as taking aspirin and one-third were not on a statin. Nearly half the patients who had attended their practice in the previous 30 months did not have smoking status recorded. While data extraction programs provide GPs with useful tools for examining their clinical databases and identifying clinical practice issues which could be improved, external support, such as that provided by divisions, is helpful. Technical barriers, such as the failure of extraction tools to recognise some data and the failure to comprehensively enter data, are impediments, but in spite of these considerable interest exists in the use of clinical data to improve practice.
21485342	Drowned in information yet starved for knowledge; evidence-based dentistry, what's in it for me?
J Am Coll Dent  2010Winter
A graduate of the ADA Evidence-based Dentistry Champions Conference explains what he has learned about the techniques of EBD literature and literature searches. EBD is the area of overlap among the literature, clinical experience, and patient characteristics. This paper focuses on evidence from the literature. Sources of summarized evidence are mentioned that can be accessed via the Internet, especially those that summarize evidence of the greatest research rigor that have been summarized systematically.
21143808	Algorithms and semantic infrastructure for mutation impact extraction and grounding.
BMC Genomics 20101202 2010
Mutation impact extraction is a hitherto unaccomplished task in state of the art mutation extraction systems. Protein mutations and their impacts on protein properties are hidden in scientific literature, making them poorly accessible for protein engineers and inaccessible for phenotype-prediction systems that currently depend on manually curated genomic variation databases. We present the first rule-based approach for the extraction of mutation impacts on protein properties, categorizing their directionality as positive, negative or neutral. Furthermore protein and mutation mentions are grounded to their respective UniProtKB IDs and selected protein properties, namely protein functions to concepts found in the Gene Ontology. The extracted entities are populated to an OWL-DL Mutation Impact ontology facilitating complex querying for mutation impacts using SPARQL. We illustrate retrieval of proteins and mutant sequences for a given direction of impact on specific protein properties. Moreover we provide programmatic access to the data through semantic web services using the SADI (Semantic Automated Discovery and Integration) framework. We address the problem of access to legacy mutation data in unstructured form through the creation of novel mutation impact extraction methods which are evaluated on a corpus of full-text articles on haloalkane dehalogenases, tagged by domain experts. Our approaches show state of the art levels of precision and recall for Mutation Grounding and respectable level of precision but lower recall for the task of Mutant-Impact relation extraction. The system is deployed using text mining and semantic web technologies with the goal of publishing to a broad spectrum of consumers.
21143398	Review article: debriefing critical incidents in the emergency department.
Emerg Med Australas  2010Dec
The impact of work related stressors on emergency clinicians has long been recognized, yet there is little formal research into the benefits of debriefing hospital staff after critical incidents, such as failed resuscitation. This article examines current models of debriefing and their application to emergency staff through a review of the literature. The goal being, to outline best practice, with recommendations for guideline development and future research directives. An electronic database search was conducted in Ovid and Psychinfo. All available abstracts were read and a hand search was completed of the references. Included articles were selected by a panel of two experts. Models and evidence relating to their efficacy were identified from the literature, and detailed evaluation included. The reviewed literature revealed a distinct paucity regarding the efficacy of debriefing of clinicians post CI and in particular randomized controlled trials. Despite this debriefing is perceived as important by emergency clinicians. However evidence presents both benefits and disadvantages to debriefing interventions. In the absence of evidence based practice guidelines, any development of models of debriefing in the emergency healthcare setting should be closely evaluated. And future research directives should aim towards large randomized control trials.
21144455	[Adverse drug reactions or adverse events of Chaihu Injection: a systematic review].
Zhong Xi Yi Jie He Xue Bao  2010Dec
Chaihu Injection (CI), which is widely used in treatment of febrile diseases, is an aqueous solution of Chaihu (Radix Bupleuri Chinensis) or Nanchaihu (Radix Bupleuri Scorzonerifolii) prepared by steam distillation. This study aims at finding out the possible causes for adverse drug reaction or adverse event (ADR/AE) caused by CI and assessing its safety based on existing evidence. Manual search was not conducted. Electronic search was conducted by two authors in China National Knowledge Internet (CNKI) database and Chongqing VIP database (VIP). The search ended in June 30th, 2009. Studies of ADR/AE induced by CI were collected comprehensively without considering language of literature and outcome indicators. Search results were not limited by patient's age, gender, race, primary disease, etc. Interventions were using CI alone or CI combined with other drugs (Chinese herbal medicine decoction or other drugs containing Chaihu were excluded). Two authors conducted data extraction independently. Microsoft Excel software was used to develop data extraction forms. Because of heterogeneity of the studies, only a descriptive analysis was conducted. Totally 83 studies with 203 cases were included in this review. Without the yield data and total amount of using, we cannot tell the incidence of ADR/AE induced by CI as well as assess the risk and safety of CI. The constituent ratio of severe cases was higher in children and old people than in other age groups. For most intramuscular cases, ADR/AE happened in 30 min after injection (constituent ratio of cumulative incidence in 30 min was 93.8%); for intravenously guttae patients, 4 cases of ADR/AE happened in the process of infusion; for first users, constituent ratio of cumulative incidence in 30 min and constituent ratio of cumulative incidence of severe cases in 30 min were higher than cases who had used CI before. Most ADRs/AEs were caused by incorrect use of CI, such as excessive doses (5 cases), intravenously guttae administration (6 cases), and violating incompatibility rules (7 cases). The incidence ratios of ADR and AE for severe and mild cases were 1.7:1 and 1.1:1, respectively; the ratios of the three relevant levels described as definitely related, most probably related and possibly related in the two types (severe and mild) of cases were 25:14:5 and 44:9:16, respectively. Present evidence with low level shows that incorrect use is the main cause of ADR/AE of CI. Whether CI is proper for children and old people still needs further research. Training for correct use of CI is necessary for medical workers. Much improvement in reporting ADR/AE based on "Recommendations for Reporting Adverse Drug Reactions and Adverse Events of Traditional Chinese Medicine" is in need.
21159730	Damming the genomic data flood using a comprehensive analysis and storage data structure.
Database (Oxford) 20101215 2010
Data generation, driven by rapid advances in genomic technologies, is fast outpacing our analysis capabilities. Faced with this flood of data, more hardware and software resources are added to accommodate data sets whose structure has not specifically been designed for analysis. This leads to unnecessarily lengthy processing times and excessive data handling and storage costs. Current efforts to address this have centered on developing new indexing schemas and analysis algorithms, whereas the root of the problem lies in the format of the data itself. We have developed a new data structure for storing and analyzing genotype and phenotype data. By leveraging data normalization techniques, database management system capabilities and the use of a novel multi-table, multidimensional database structure we have eliminated the following: (i) unnecessarily large data set size due to high levels of redundancy, (ii) sequential access to these data sets and (iii) common bottlenecks in analysis times. The resulting novel data structure horizontally divides the data to circumvent traditional problems associated with the use of databases for very large genomic data sets. The resulting data set required 86% less disk space and performed analytical calculations 6248 times faster compared to a standard approach without any loss of information. Database URL: http://castor.pharmacogenomics.ca.
21164846	Blue laser-sensitized photopolymer for a holographic high density data storage system.
Opt Express  2010Nov22
We present a new blue-sensitized photopolymer to achieve a higher storage density compared to green/red-recordable media. Photopolymers are prepared based on a two-chemistry system and their holographic recording properties are investigated. A matrix of long and flexible ether units of an epoxy precursor and a multi-crosslinkable amine hardener enhances energetic sensitivity and suppresses volume shrinkage effectively. Page-wise recording of 961 bits/page of digital data is demonstrated and long term recording stability is also verified for a period of roughly 2 months.
21164995	Three dimensional object recognition with photon counting imagery in the presence of noise.
Opt Express  2010Dec6
Three dimensional (3D) imaging systems have been recently suggested for passive sensing and recognition of objects in photon-starved environments where only a few photons are emitted or reflected from the object. In this paradigm, it is important to make optimal use of limited information carried by photons. We present a statistical framework for 3D passive object recognition in presence of noise. Since in quantum-limited regime, detector dark noise is present, our approach takes into account the effect of noise on information bearing photons. The model is tested when background noise and dark noise sources are present for identifying a target in a 3D scene. It is shown that reliable object recognition is possible in photon-counting domain. The results suggest that with proper translation of physical characteristics of the imaging system into the information processing algorithms, photon-counting imagery can be used for object classification.
21169167	Missing data approaches in eHealth research: simulation study and a tutorial for nonmathematically inclined researchers.
J. Med. Internet Res. 20101219 2010
Missing data is a common nuisance in eHealth research: it is hard to prevent and may invalidate research findings. In this paper several statistical approaches to data "missingness" are discussed and tested in a simulation study. Basic approaches (complete case analysis, mean imputation, and last observation carried forward) and advanced methods (expectation maximization, regression imputation, and multiple imputation) are included in this analysis, and strengths and weaknesses are discussed. The dataset used for the simulation was obtained from a prospective cohort study following participants in an online self-help program for problem drinkers. It contained 124 nonnormally distributed endpoints, that is, daily alcohol consumption counts of the study respondents. Missingness at random (MAR) was induced in a selected variable for 50% of the cases. Validity, reliability, and coverage of the estimates obtained using the different imputation methods were calculated by performing a bootstrapping simulation study. In the performed simulation study, the use of multiple imputation techniques led to accurate results. Differences were found between the 4 tested multiple imputation programs: NORM, MICE, Amelia II, and SPSS MI. Among the tested approaches, Amelia II outperformed the others, led to the smallest deviation from the reference value (Cohen's d = 0.06), and had the largest coverage percentage of the reference confidence interval (96%). The use of multiple imputation improves the validity of the results when analyzing datasets with missing observations. Some of the often-used approaches (LOCF, complete cases analysis) did not perform well, and, hence, we recommend not using these. Accumulating support for the analysis of multiple imputed datasets is seen in more recent versions of some of the widely used statistical software programs making the use of multiple imputation more readily available to less mathematically inclined researchers.
21169168	Content and functionality of alcohol and other drug websites: results of an online survey.
J. Med. Internet Res. 20101219 2010
There is a growing trend for individuals to seek health information from online sources. Alcohol and other drug (AOD) use is a significant health problem worldwide, but access and use of AOD websites is poorly understood. To investigate content and functionality preferences for AOD and other health websites. An anonymous online survey examined general Internet and AOD-specific usage and search behaviors, valued features of AOD and health-related websites (general and interactive website features), indicators of website trustworthiness, valued AOD website tools or functions, and treatment modality preferences. Surveys were obtained from 1214 drug (n = 766) and alcohol website users (n = 448) (mean age 26.2 years, range 16-70). There were no significant differences between alcohol and drug groups on demographic variables, Internet usage, indicators of website trustworthiness, or on preferences for AOD website functionality. A robust website design/navigation, open access, and validated content provision were highly valued by both groups. While attractiveness and pictures or graphics were also valued, high-cost features (videos, animations, games) were minority preferences. Almost half of respondents in both groups were unable to readily access the information they sought. Alcohol website users placed greater importance on several AOD website tools and functions than did those accessing other drug websites: online screening tools (χ²(2) = 15.8, P &lt; .001, n = 985); prevention programs (χ²(2) = 27.5, P &lt; .001, n = 981); tracking functions (χ²(2) = 11.5, P = .003, n = 983); self help treatment programs (χ²(2) = 8.3, P = .02, n = 984); downloadable fact sheets for friends (χ²(2) = 11.6, P = .003, n = 981); or family (χ²(2) = 12.7, P = .002, n = 983). The most preferred online treatment option for both the user groups was an Internet site with email therapist support. Explorations of demographic differences were also performed. While gender did not affect survey responses, younger respondents were more likely to value interactive and social networking features, whereas downloading of credible information was most highly valued by older respondents. Significant deficiencies in the provision of accessible information on AOD websites were identified, an important problem since information seeking was the most common reason for accessing these websites, and, therefore, may be a key avenue for engaging website users in behaviour change. The few differences between AOD website users suggested that both types of websites may have similar features, although alcohol website users may more readily be engaged in screening, prevention and self-help programs, tracking change, and may value fact sheets more highly. While the sociodemographic differences require replication and clarification, these differences support the notion that the design and features of AOD websites should target specific audiences to have maximal impact.
21169169	The ins and outs of an online bipolar education program: a study of program attrition.
J. Med. Internet Res. 20101219 2010
The science of eHealth interventions is rapidly evolving. However, despite positive outcomes, evaluations of eHealth applications have thus far failed to explain the high attrition rates that are associated with some eHealth programs. Patient adherence remains an issue, and the science of attrition is still in its infancy. To our knowledge, there has been no in-depth qualitative study aimed at identifying the reasons for nonadherence to-and attrition from- online interventions. This paper explores the predictors of attrition and participant-reported reasons for nonadherence to an online psycho-education program for people newly diagnosed with a bipolar disorder. As part of an ongoing randomized controlled trial (RCT) evaluating an online psycho-education program for people newly diagnosed with a bipolar disorder, we undertook an in-depth qualitative study to identify participants' reasons for nonadherence to, and attrition from, the online intervention as well as a quantitative study investigating predictors of attrition. Within the RCT, 370 participants were randomly allocated to 1 of 2 active interventions or an attention control condition. Descriptive analyses and chi-square tests were used to explore the completion rates of 358 participants, and standard regression analysis was used to identify predictors of attrition. The data from interviews with a subsample of 39 participants who did not complete the online program were analyzed using "thematic analysis" to identify patterns in reported reasons for attrition. Overall, 26.5% of the sample did not complete their assigned intervention. Standard multiple regression analysis revealed that young age (P= .004), male gender (P= .001), and clinical recruitment setting (P= .001) were significant predictors of attrition (F(7,330)= 8.08, P&lt; .001). Thematic analysis of interview data from the noncompleter subsample revealed that difficulties associated with the acute phases of bipolar disorder, not wanting to think about one's illness, and program factors such as the information being too general and not personally tailored were the major reasons for nonadherence. The dropout rate was equivalent to other Internet interventions and to face-to-face therapy. Findings from our qualitative study provide participant-reported reasons for discontinuing the online intervention, which, in conjunction with the quantitative investigations about predictors, add to understanding about Internet interventions. However, further research is needed to determine whether there are systematic differences between those who complete and those who do not complete eHealth interventions. Ultimately, this may lead to the identification of population subgroups that most benefit from eHealth interventions and to informing the development of strategies to improve adherence. ACTRN12608000411347; http://www.anzctr.org.au/ACTRN12608000411347.aspx (Archived by WebCite at http://www.webcitation.org/5uX4uYwVN).
21169176	How patients with schizophrenia use the internet: qualitative study.
J. Med. Internet Res. 20101219 2010
The Internet is an important source of health information for people with psychiatric conditions. Little is known about the way patients with schizophrenia use the Internet when it comes to issues related to their illness. Data on their specific needs, difficulties, and the consequences related to Internet use are lacking. Our objective was to investigate the nature and subjective consequences of health-related Internet use among patients with schizophrenia. In all, 26 individual semistructured interviews were conducted and analyzed qualitatively in groups of 4 until theoretical saturation was achieved. Study results suggest that the Internet is an influential source of illness-related information for patients with schizophrenia. Many aspects of their behavior around the Internet resemble those of individuals not afflicted by mental illness. Importantly, problems specific to patients with schizophrenia were stimulus overflow, an inability to deal with the abundance of information, difficulties with concentration, lack of energy, paranoid ideas, symptom provocation, and the need to distance themselves from illness-related topics as part of the recovery process. Internet information was subjectively perceived as having the potential to significantly change patients' attitudes toward medication and their relationships with doctors. These findings provide insight into how individuals with schizophrenia handle illness-related Internet information. The data could contribute to the continuous development of Internet-based interventions and offer novel approaches to optimizing traditional treatment options.
21174023	Healthcare students' e-literacy skills.
J Allied Health  2010Fall
To be critical healthcare consumers, patients must learn self-management skills and become active participants in knowledge management and exchange. eHealth literacy is considered critical to the development of these self-management skills. The World Health Organization identifies five core competencies required of all healthcare providers working with persons with chronic conditions, and this paper focuses on the fourth--the ability to employ information and communication technology. To supplement our literature-based argument, we also present findings from a class of first-year masters-level occupational therapy students asked to complete an existing standardized e-health literacy survey, eHEALS, as a learning activity. The eHEALS revealed that students reported confidence in their ability to critically appraise internet information but were not confident enough in those skills to use the information to make decisions without consulting a healthcare provider. It appeared that the students were not yet fully immersed in their role of healthcare professional and seemed to move between the roles of healthcare provider and healthcare recipient as they reflected on the class' answers to the eHEALS assessment. Evaluation of eHealth literacy is complex and needs to consider the multiple roles assumed by those whose knowledge is being assessed.
20979218	Surveying the traumatic stress literature: the effective use of bibliographic databases in preparing reviews and meta-analyses.
J Trauma Stress  2010Dec
The value of a systematic review or meta-analysis depends upon the care with which it is designed and conducted. A major factor in this is the literature search that identifies the articles to be reviewed. Careful selection of bibliographic databases and the use of well-designed search strategies based on the controlled vocabularies contained in published thesauri are essential to ensure retrieval of all relevant articles. This commentary offers suggestions for identifying the literature to be searched, designing a suitable search strategy, using a controlled vocabulary, and describing the bibliographic methodology underlying a systematic review or meta-analysis.
21114253	Polymeric memory elements and logic circuits that store multiple bit states.
ACS Appl Mater Interfaces 20101129 2010Dec
The ever-increasing flow of information requires new approaches for high-density data storage (HDDS). Here, we present a novel solution that incorporates the easily accessible polymer poly(3,4-ethylenedioxythiophene) (PEDOT) with multistate memory. The electrical addressable polymer is able to store up to five different memory states, which are stable up to 20 min. The observed memory states are generated by the optical output signature of the PEDOT deposited on indium tin oxide (ITO) coated glass, upon applying specific electrical inputs. Moreover, the demonstrated platforms can be represented by a general logic circuit, which allows the construction of multistate memory, such as flip-flops and flip-flap-flop logic circuits.
21181349	A web-based platform for rice microarray annotation and data analysis.
Sci China Life Sci 20101223 2010Dec
Rice (Oryza sativa) feeds over half of the global population. A web-based integrated platform for rice microarray annotation and data analysis in various biological contexts is presented, which provides a convenient query for comprehensive annotation compared with similar databases. Coupled with existing rice microarray data, it provides online analysis methods from the perspective of bioinformatics. This comprehensive bioinformatics analysis platform is composed of five modules, including data retrieval, microarray annotation, sequence analysis, results visualization and data analysis. The BioChip module facilitates the retrieval of microarray data information via identifiers of "Probe Set ID", "Locus ID" and "Analysis Name". The BioAnno module is used to annotate the gene or probe set based on the gene function, the domain information, the KEGG biochemical and regulatory pathways and the potential microRNA which regulates the genes. The BioSeq module lists all of the related sequence information by a microarray probe set. The BioView module provides various visual results for the microarray data. The BioAnaly module is used to analyze the rice microarray's data set.
21210986	SADI, SHARE, and the in silico scientific method.
BMC Bioinformatics 20101221 2010
The emergence and uptake of Semantic Web technologies by the Life Sciences provides exciting opportunities for exploring novel ways to conduct in silico science. Web Service Workflows are already becoming first-class objects in "the new way", and serve as explicit, shareable, referenceable representations of how an experiment was done. In turn, Semantic Web Service projects aim to facilitate workflow construction by biological domain-experts such that workflows can be edited, re-purposed, and re-published by non-informaticians. However the aspects of the scientific method relating to explicit discourse, disagreement, and hypothesis generation have remained relatively impervious to new technologies. Here we present SADI and SHARE - a novel Semantic Web Service framework, and a reference implementation of its client libraries. Together, SADI and SHARE allow the semi- or fully-automatic discovery and pipelining of Semantic Web Services in response to ad hoc user queries. The semantic behaviours exhibited by SADI and SHARE extend the functionalities provided by Description Logic Reasoners such that novel assertions can be automatically added to a data-set without logical reasoning, but rather by analytical or annotative services. This behaviour might be applied to achieve the "semantification" of those aspects of the in silico scientific method that are not yet supported by Semantic Web technologies. We support this suggestion using an example in the clinical research space.
21156038	Initial steps towards a production platform for DNA sequence analysis on the grid.
BMC Bioinformatics 20101214 2010
Bioinformatics is confronted with a new data explosion due to the availability of high throughput DNA sequencers. Data storage and analysis becomes a problem on local servers, and therefore it is needed to switch to other IT infrastructures. Grid and workflow technology can help to handle the data more efficiently, as well as facilitate collaborations. However, interfaces to grids are often unfriendly to novice users. In this study we reused a platform that was developed in the VL-e project for the analysis of medical images. Data transfer, workflow execution and job monitoring are operated from one graphical interface. We developed workflows for two sequence alignment tools (BLAST and BLAT) as a proof of concept. The analysis time was significantly reduced. All workflows and executables are available for the members of the Dutch Life Science Grid and the VL-e Medical virtual organizations All components are open source and can be transported to other grid infrastructures. The availability of in-house expertise and tools facilitates the usage of grid resources by new users. Our first results indicate that this is a practical, powerful and scalable solution to address the capacity and collaboration issues raised by the deployment of next generation sequencers. We currently adopt this methodology on a daily basis for DNA sequencing and other applications. More information and source code is available via http://www.bioinformaticslaboratory.nl/
21103809	Getting to know journal bibliographic databases.
Singapore Med J  2010Oct
A bibliographic database is an organised digital collection of references to published literature. A bibliographic database may be general in scope or may cover a specific academic discipline. There are many types of medical and general bibliographic databases. They cover biomedical and scientific literature, morbidity and mortality statistics, therapeutic regimens, medical records, images and reviews of evidence-based medicine. Getting to know these databases will help researchers and authors to enhance their writing and publishing endeavours.
21113041	Development of a unified web-based national HIV/AIDS information system in China.
Int J Epidemiol  2010Dec
In the past, many data collection systems were in operation for different HIV/AIDS projects in China. We describe the creation of a unified, web-based national HIV/AIDS information system designed to streamline data collection and facilitate data use. Integration of separate HIV/AIDS data systems was carried out in three phases. Phase 1, from January 2006 to December 2007, involved creating a set of unified data collection forms that took into account existing program needs and the reporting requirements of various international organizations. Phase 2, from January to October 2007, involved creating a web-based platform to host the integrated HIV/AIDS data collection system. Phase 3, from November to December 2007, involved pilot testing the new, integrated system prior to nationwide application. Eight web-based data collection subsystems based on one platform began operation on 1 January 2008. These eight subsystems cover: (i) HIV/AIDS case reporting; (ii) HIV testing and counselling; (iii) antiretroviral treatment (ART) for adults; (iv) ART for children; (v) behavioural interventions for high-risk groups; (vi) methadone maintenance treatment; (vii) sentinel and behavioural surveillance; and (viii) local county background information. The system provides real-time data to monitor HIV testing, prevention and treatment programs across the country. China's new unified, web-based HIV/AIDS information system has improved the efficiency of data collection, reporting, analysis and use, as well as data quality and security. It is a powerful tool to support policy making, program evaluation and implementation of the national HIV/AIDS program and, thus, may serve a model for other countries.
21059227	miSolRNA: A tomato micro RNA relational database.
BMC Plant Biol. 20101108 2010
The economic importance of Solanaceae plant species is well documented and tomato has become a model for functional genomics studies. In plants, important processes are regulated by microRNAs (miRNA). We describe here a data base integrating genetic map positions of miRNA-targeted genes, their expression profiles and their relations with quantitative fruit metabolic loci and yield associated traits. miSolRNA provides a metadata source to facilitate the construction of hypothesis aimed at defining physiological modes of action of regulatory process underlying the metabolism of the tomato fruit. The MiSolRNA database allows the simple extraction of metadata for the proposal of new hypothesis concerning possible roles of miRNAs in the regulation of tomato fruit metabolism. It permits i) to map miRNAs and their predicted target sites both on expressed (SGN-UNIGENES) and newly annotated sequences (BAC sequences released), ii) to co-locate any predicted miRNA-target interaction with metabolic QTL found in tomato fruits, iii) to retrieve expression data of target genes in tomato fruit along their developmental period and iv) to design further experiments for unresolved questions in complex trait biology based on the use of genetic materials that have been proven to be a useful tools for map-based cloning experiments in Solanaceae plant species.
20970519	PubChem as a public resource for drug discovery.
Drug Discov. Today 20101021 2010Dec
PubChem is a public repository of small molecules and their biological properties. Currently, it contains more than 25 million unique chemical structures and 90 million bioactivity outcomes associated with several thousand macromolecular targets. To address the potential utility of this public resource for drug discovery, we systematically summarized the protein targets in PubChem by function, 3D structure and biological pathway. Moreover, we analyzed the potency, selectivity and promiscuity of the bioactive compounds identified for these biological targets, including the chemical probes generated by the NIH Molecular Libraries Program. As a public resource, PubChem lowers the barrier for researchers to advance the development of chemical tools for modulating biological processes and drug candidates for disease treatments.
21117268	Knowledge extraction from evolving spiking neural networks with rank order population coding.
Int J Neural Syst  2010Dec
This paper demonstrates how knowledge can be extracted from evolving spiking neural networks with rank order population coding. Knowledge discovery is a very important feature of intelligent systems. Yet, a disproportionally small amount of research is centered on the issue of knowledge extraction from spiking neural networks which are considered to be the third generation of artificial neural networks. The lack of knowledge representation compatibility is becoming a major detriment to end users of these networks. We show that a high-level knowledge can be obtained from evolving spiking neural networks. More specifically, we propose a method for fuzzy rule extraction from an evolving spiking network with rank order population coding. The proposed method was used for knowledge discovery on two benchmark taste recognition problems where the knowledge learnt by an evolving spiking neural network was extracted in the form of zero-order Takagi-Sugeno fuzzy IF-THEN rules.
21039701	Prestorm estimation of hurricane damage to electric power distribution systems.
Risk Anal. 20101006 2010Dec
Hurricanes frequently cause damage to electric power systems in the United States, leading to widespread and prolonged loss of electric service. Restoring service quickly requires the use of repair crews and materials that must be requested, at considerable cost, prior to the storm. U.S. utilities have struggled to strike a good balance between over- and underpreparation largely because of a lack of methods for rigorously estimating the impacts of an approaching hurricane on their systems. Previous work developed methods for estimating the risk of power outages and customer loss of power, with an outage defined as nontransitory activation of a protective device. In this article, we move beyond these previous approaches to directly estimate damage to the electric power system. Our approach is based on damage data from past storms together with regression and data mining techniques to estimate the number of utility poles that will need to be replaced. Because restoration times and resource needs are more closely tied to the number of poles and transformers that need to be replaced than to the number of outages, this pole-based assessment provides a much stronger basis for prestorm planning by utilities. Our results show that damage to poles during hurricanes can be assessed accurately, provided that adequate past damage data are available. However, the availability of data can, and currently often is, the limiting factor in developing these types of models in practice. Opportunities for further enhancing the damage data recorded during hurricanes are also discussed.
20973569	Compid: a new software tool to integrate and compare MS/MS based protein identification results from Mascot and Paragon.
J. Proteome Res. 20101111 2010Dec3
Tandem mass spectrometry-based proteomics experiments produce large amounts of raw data, and different database search engines are needed to reliably identify all the proteins from this data. Here, we present Compid, an easy-to-use software tool that can be used to integrate and compare protein identification results from two search engines, Mascot and Paragon. Additionally, Compid enables extraction of information from large Mascot result files that cannot be opened via the Web interface and calculation of general statistical information about peptide and protein identifications in a data set. To demonstrate the usefulness of this tool, we used Compid to compare Mascot and Paragon database search results for mitochondrial proteome sample of human keratinocytes. The reports generated by Compid can be exported and opened as Excel documents or as text files using configurable delimiters, allowing the analysis and further processing of Compid output with a multitude of programs. Compid is freely available and can be downloaded from http://users.utu.fi/lanatr/compid. It is released under an open source license (GPL), enabling modification of the source code. Its modular architecture allows for creation of supplementary software components e.g. to enable support for additional input formats and report categories.
21133042	bcnQL: a query language for biochemical networks.
Int J Data Min Bioinform  2010
This paper proposes a graph data model that can represent information present in Biochemical Networks. The study presented in this paper also proposes a query language, called bcnQL, which empowers users to query entities, interactions, processes and pathways with arbitrary conditions. We then discuss the query-processing techniques, more specifically, the translation of bcnQL queries into G-algebra and a set of algebraic operators on graph objects. Some query examples are presented to demonstrate the applicability of the language for this specific domain. Finally, we provide details of a prototype implementation for the query language.
21041159	IPADE: Iterative prototype adjustment for nearest neighbor classification.
IEEE Trans Neural Netw 20101028 2010Dec
Nearest prototype methods are a successful trend of many pattern classification tasks. However, they present several shortcomings such as time response, noise sensitivity, and storage requirements. Data reduction techniques are suitable to alleviate these drawbacks. Prototype generation is an appropriate process for data reduction, which allows the fitting of a dataset for nearest neighbor (NN) classification. This brief presents a methodology to learn iteratively the positioning of prototypes using real parameter optimization procedures. Concretely, we propose an iterative prototype adjustment technique based on differential evolution. The results obtained are contrasted with nonparametric statistical tests and show that our proposal consistently outperforms previously proposed methods, thus becoming a suitable tool in the task of enhancing the performance of the NN classifier.
20819111	Derivation and validation of a MEDLINE search strategy for research studies that use administrative data.
Health Serv Res 20100901 2010Dec
To derive and validate a search strategy that identifies administrative database research (ADR) in the MEDLINE database. Analytical survey. We downloaded all articles published between January 1, 2008 and October 7, 2009 in 20 top journals in internal medicine, cardiovascular medicine, public health, and health services research. These were reviewed to determine whether they were ADR (in which the study cohort, exposure, or outcome was defined using electronic data created for or during the processing of patients through their health care). We used chi-squared recursive partitioning to create a search strategy that maximized sensitivity based on publication type, MeSH headings, and text words. Sensitivity and positive predictive value of the search strategy for true ADR in three samples: derivation (n=5,513); internal validation (n=2,710); and external validation (n=1,500). The prevalence of ADR in the derivation, internal validation, and external validation samples was 2.6, 2.9, and 2.2 percent, respectively. The sensitivity of our search strategy in these samples was 90.9 percent (95 percent confidence interval [CI] 85.0-95.1), 88.5 percent (79.2-94.6), and 100 percent (99.3-100), respectively. The positive predictive value in these samples was 10.7 percent (9.0-12.6), 11.5 percent (9.1-14.4), and 3.3 percent (2.3-4.6), respectively. We derived and validated a search strategy that is highly sensitive for ADR in MEDLINE.
20954966	Conducting literature searches on Ayurveda in PubMed, Indian, and other databases.
J Altern Complement Med 20101018 2010Nov
Literature searches for articles on Ayurveda provide special challenges, since many of the Indian journals in which such articles appear are not indexed by current medical databases such as PubMed and Cochrane Central Register of Controlled Trials. The aim of this study was to develop a comprehensive search strategy on Ayurveda topics and to map the existing databases containing Ayurveda journal publications. We have developed a literature search procedure that can recover the great majority of articles on any given topic associated with Ayurveda. Our system is formulated in an easily reproducible fashion that all researchers can use. Using the keywords related to Ayurveda and vitiligo, we searched 41 databases that may contain complementary and alternative medicine publications. Only 11 databases yielded results; PubMed contained 9 articles. Each of 14 other databases named in our search procedure averaged 23 articles. International Bibliographic Information of Dietary Supplements, for example, gave 22, of which 1 satisfied our eligibility criteria. "Annotated Bibliography of Indian Medicine" gave 47, of which 7 satisfied eligibility criteria. This article proposes guidelines enabling comprehensive searches to locate all types of Ayurvedic articles, not necessarily only randomized controlled trials.
21063542	A comparison of two approaches to text processing: facilitating chart reviews of radiology reports in electronic medical records.
Perspect Health Inf Manag 20101001 2010
Chart review is central to health services research. Text processing, which analyzes free-text fields through automated methods, can facilitate this process. We compared precision and accuracy of NegEx and SQLServer 2008 Free-Text Search in identifying acute fractures in radiology reports.The term "fracture" was included in 23,595 radiology reports from the Veterans Aging Cohort Study. Four hundred reports were randomly selected and manually reviewed for acute fractures to establish a gold standard. Reports were then processed by SQLServer and NegEx. Results were compared to the gold standard to determine accuracy, precision, recall, and F-statistic.NegEx and the gold standard identified acute fractures in 13 reports. SQLServer identified 2 in a report-based analysis (precision: 1.00; accuracy: 0.97; recall: 0.15; F-statistic: 0.26), and 12 in a sentence-by-sentence analysis (precision: 1.00; recall: 0.92; accuracy: 0.92; F-statistic: 0.96).Text-processing tools utilizing basic database or programming skills are comparable, precise, and accurate in identifying reports for review.
21063547	Yes, Virginia, there is a paper record!
Perspect Health Inf Manag 20101001 2010
This tongue-in-cheek essay hopes to prompt discussion among health information management (HIM) professionals of all levels with regard to the fast-changing HIM landscape, particularly the electronic health record (EHR) and the financial and career options that healthcare systems and employees have had to accept, adapt to, and decipher. Many of us have preconceived notions about how we will work within the new electronic environment as we help the implementation process succeed. Perhaps we need to also look at the negative impact some of these changes have had on HIM personnel. Some may find their tasks outsourced, obsolete, or expendable once the new EHR product they helped to implement is up and running. Do we really want all the paper to go away?
21068463	Visualization and analysis of a cardio vascular disease- and MUPP1-related biological network combining text mining and data warehouse approaches.
J Integr Bioinform 20101111 2010
Detailed investigation of socially important diseases with modern experimental methods has resulted in the generation of large volume of valuable data. However, analysis and interpretation of this data needs application of efficient computational techniques and systems biology approaches. In particular, the techniques allowing the reconstruction of associative networks of various biological objects and events can be useful. In this publication, the combination of different techniques to create such a network associated with an abstract cell environment is discussed in order to gain insights into the functional as well as spatial interrelationships. It is shown that experimentally gained knowledge enriched with data warehouse content and text mining data can be used for the reconstruction and localization of a cardiovascular disease developing network beginning with MUPP1/MPDZ (multi-PDZ domain protein).
20969780	Information discovery on electronic health records using authority flow techniques.
BMC Med Inform Decis Mak 20101022 2010
As the use of electronic health records (EHRs) becomes more widespread, so does the need to search and provide effective information discovery within them. Querying by keyword has emerged as one of the most effective paradigms for searching. Most work in this area is based on traditional Information Retrieval (IR) techniques, where each document is compared individually against the query. We compare the effectiveness of two fundamentally different techniques for keyword search of EHRs. We built two ranking systems. The traditional BM25 system exploits the EHRs' content without regard to association among entities within. The Clinical ObjectRank (CO) system exploits the entities' associations in EHRs using an authority-flow algorithm to discover the most relevant entities. BM25 and CO were deployed on an EHR dataset of the cardiovascular division of Miami Children's Hospital. Using sequences of keywords as queries, sensitivity and specificity were measured by two physicians for a set of 11 queries related to congenital cardiac disease. Our pilot evaluation showed that CO outperforms BM25 in terms of sensitivity (65% vs. 38%) by 71% on average, while maintaining the specificity (64% vs. 61%). The evaluation was done by two physicians. Authority-flow techniques can greatly improve the detection of relevant information in EHRs and hence deserve further study.
20350844	Hybrid associative retrieval of three-dimensional models.
IEEE Trans Syst Man Cybern B Cybern 20100325 2010Dec
In this paper, we propose a novel 3-D model retrieval framework, which is referred to as hybrid 3-D model associative retrieval. Unlike the conventional 3-D model similarity retrieval approach, the query model and the models obtained by 3-D model hybrid associative retrieval have the following properties: They belong to different model classes and have different shape characteristics in general but are semantically related and preassembled in a certain associative group. For instance, given a furniture associative group { desk, chair, bed}, we may probably like to use a desk as a query model to search for a list of matching models, which belong to the chair or bed class. We consider the following possibilities: 1) there can be more than two classes in an association group and 2) different association groups might have different numbers of classes. The hybrid associative retrieval is performed in two stages: 1) to establish the relationship between different 3-D model categories with semantic associations, we propose three approaches based on neural network learning and 2) to address the aforementioned two conditions, we use a cyclic-shift scheme to partition different associative groups into two-class pairwise associative groups and then adopt two different strategies to combine the final retrieval results. Experiments by using different data sets demonstrate the effectiveness and efficiency of our proposed framework on the new hybrid associative retrieval task.
20977768	AGUIA: autonomous graphical user interface assembly for clinical trials semantic data services.
BMC Med Inform Decis Mak 20101026 2010
AGUIA is a front-end web application originally developed to manage clinical, demographic and biomolecular patient data collected during clinical trials at MD Anderson Cancer Center. The diversity of methods involved in patient screening and sample processing generates a variety of data types that require a resource-oriented architecture to capture the associations between the heterogeneous data elements. AGUIA uses a semantic web formalism, resource description framework (RDF), and a bottom-up design of knowledge bases that employ the S3DB tool as the starting point for the client's interface assembly. The data web service, S3DB, meets the necessary requirements of generating the RDF and of explicitly distinguishing the description of the domain from its instantiation, while allowing for continuous editing of both. Furthermore, it uses an HTTP-REST protocol, has a SPARQL endpoint, and has open source availability in the public domain, which facilitates the development and dissemination of this application. However, S3DB alone does not address the issue of representing content in a form that makes sense for domain experts. We identified an autonomous set of descriptors, the GBox, that provides user and domain specifications for the graphical user interface. This was achieved by identifying a formalism that makes use of an RDF schema to enable the automatic assembly of graphical user interfaces in a meaningful manner while using only resources native to the client web browser (JavaScript interpreter, document object model). We defined a generalized RDF model such that changes in the graphic descriptors are automatically and immediately (locally) reflected into the configuration of the client's interface application. The design patterns identified for the GBox benefit from and reflect the specific requirements of interacting with data generated by clinical trials, and they contain clues for a general purpose solution to the challenge of having interfaces automatically assembled for multiple and volatile views of a domain. By coding AGUIA in JavaScript, for which all browsers include a native interpreter, a solution was found that assembles interfaces that are meaningful to the particular user, and which are also ubiquitous and lightweight, allowing the computational load to be carried by the client's machine.
21089657	[A technology research for diagnosis of mammographic masses based on content-based image retrieval].
Sheng Wu Yi Xue Gong Cheng Xue Za Zhi  2010Oct
In order to assist doctors in making the diagnosis of mammographic masses, a method is proposed in this paper. Twenty-two features are extracted from each queried region of interest (ROI). A k-nearest neighbor (KNN) algorithm is used to retrieve similar images from database, and further calculate the mutual information (MI) between the queried image and the images which are in the retrieval results, so as to improve the retrieval performance. Finally, the scheme takes the first nine images with the highest MI scores as the final retrieval results. With the purpose of providing available decision-making information of diagnostic aids, we compare and analyze three calculating methods of decision index. The experiment results show that this method is better than the method using KNN only, and this method improves the accuracy of diagnosis effectively.
21044328	CaGrid Workflow Toolkit: a Taverna based workflow tool for cancer grid.
BMC Bioinformatics 20101102 2010
In biological and medical domain, the use of web services made the data and computation functionality accessible in a unified manner, which helped automate the data pipeline that was previously performed manually. Workflow technology is widely used in the orchestration of multiple services to facilitate in-silico research. Cancer Biomedical Informatics Grid (caBIG) is an information network enabling the sharing of cancer research related resources and caGrid is its underlying service-based computation infrastructure. CaBIG requires that services are composed and orchestrated in a given sequence to realize data pipelines, which are often called scientific workflows. CaGrid selected Taverna as its workflow execution system of choice due to its integration with web service technology and support for a wide range of web services, plug-in architecture to cater for easy integration of third party extensions, etc. The caGrid Workflow Toolkit (or the toolkit for short), an extension to the Taverna workflow system, is designed and implemented to ease building and running caGrid workflows. It provides users with support for various phases in using workflows: service discovery, composition and orchestration, data access, and secure service invocation, which have been identified by the caGrid community as challenging in a multi-institutional and cross-discipline domain. By extending the Taverna Workbench, caGrid Workflow Toolkit provided a comprehensive solution to compose and coordinate services in caGrid, which would otherwise remain isolated and disconnected from each other. Using it users can access more than 140 services and are offered with a rich set of features including discovery of data and analytical services, query and transfer of data, security protections for service invocations, state management in service interactions, and sharing of workflows, experiences and best practices. The proposed solution is general enough to be applicable and reusable within other service-computing infrastructures that leverage similar technology stack.
21095668	Low power wireless acquisition module for wearable health monitoring systems.
Conf Proc IEEE Eng Med Biol Soc  2010
This paper presents a low power wireless acquisition module for use within wearable health monitoring systems and Ambient Assisted Living applications. The acquisition module provides continuous monitoring of the user's electrocardiogram (ECG) and activity, as well as the local temperature at the module. The module is placed on the chest of the user, and its wearability is achieved due to its fabrication based on a flexible PCB, and by the complete absence of connecting wires, as a result of the integration of flexible and dry ECG monitoring electrodes on the acquisition module, which do not require preparation with electrolyte gel. The design of the acquisition module also aimed for the minimization of power consumption to enable long-term continuous monitoring, namely concerning the wireless link, for which a proprietary low power solution was adopted. A low power analog frontend was custom designed for single-lead ECG monitoring, achieving a current consumption of 220 εA. The wireless acquisition module has a current consumption down to 1.3 mA while processing the acquisition of sensor data, and 4 mA when the wireless transceiver is active.
21095679	Method to observe hemodynamic and metabolic changes during hemodiafiltration therapy with exercise.
Conf Proc IEEE Eng Med Biol Soc  2010
Intradyalitic exercise programas are important to improve patient's hemodynamic stability. Blood pressure and metabolic changes are correlated when heat accumulation is due to increment of the body core temperature (+1.0 °C). However, increase in temperature could be controlled by lowering dialysate's temperature using two main modalities techniques (isothermic and thermoneural) with different patient's thermal balance consequences, not yet well studied. In this work, a new method to observe the main physiological parameters (hearth rate variability (HRV), blood pressure, BTM dialysate temperature control and substrate utilization by indirect calorimtery) which are involved in hemodiafitration (HDF), are displayd. An experiment was carried out in a group of 5 patients waiting kidney transplant. In each patient, EE was assessed as well as the HRV during isothermic and thermoneutral modalities as a manner of cross and prospective study (a) at before therapy, (b) during therapy and (c) at the end of the HDF therapy. Power extraction was also measured by a BTM (Blood Temperature Monitor from Fresenius Inc), in order to determine how the dialysate temperature was controlled. The results showed important method's advantages which place the BTM performance as unstable control system with the possibility to produce undesirable HRV changes as the vagotonical response. However more patient cases are needed in order to identify the real advantage of this new method.
21095736	Content Based medical image retrieval based on BEMD: optimization of a similarity metric.
Conf Proc IEEE Eng Med Biol Soc  2010
Most medical images are now digitized and stored in patients files databases. The challenge is how to use them for acquiring knowledge or/and for aid to diagnosis. In this paper, we address the challenge of diagnosis aid by Content Based Image Retrieval (CBIR). We propose to characterize images by using the Bidimensional Empirical Mode Decomposition (BEMD). Images are decomposed into a set of functions named Bidimensional Intrinsic Mode Functions (BIMF). Two methods are used to characterize BIMFs information content: the Generalized Gaussian density functions (GGD) and the Huang-Hilbert transform (HHT). In order to enhance results, we introduce a similarity metric optimization process: weighted distances between BIMFs are adapted for each image in the database. Retrieval efficiency is given for different databases (DB), including a diabetic retinopathy DB, a mammography DB and a faces DB. Results are promising: the retrieval efficiency is higher than 95% for some cases.
21095766	A new XML-aware compression technique for improving performance of healthcare information systems over hospital networks.
Conf Proc IEEE Eng Med Biol Soc  2010
Most organizations exchange, collect, store and process data over the Internet. Many hospital networks deploy Web services to send and receive patient information. SOAP (Simple Object Access Protocol) is the most usable communication protocol for Web services. XML is the standard encoding language of SOAP messages. However, the major drawback of XML messages is the high network traffic caused by large overheads. In this paper, two XML-aware compressors are suggested to compress patient messages stemming from any data transactions between Web clients and servers. The proposed compression techniques are based on the XML structure concepts and use both fixed-length and Huffman encoding methods for translating the XML message tree. Experiments show that they outperform all the conventional compression methods and can save tremendous amount of network bandwidth.
21095909	A method for clinical and physiological event stream processing.
Conf Proc IEEE Eng Med Biol Soc  2010
This paper proposes a methodology for the event stream processing of synchronous (physiological) and asynchronous (clinical) health data streams. The purpose is to illustrate the feasibility of Artemis, our extension of IBM's InfoSphere Streams, to appropriately deliver notifications from an initial clinical hypothesis within the critical care environment. We demonstrate that an positive alert can be delivered that is indicative of an onset of instability in critically ill newborns. Artemis, is also tested for its potential to allow clinicians the ability to interact directly with the rule-based system to prove certain hypothesis. We begin this methodology with a model of the clinical case study, and then transform that model into Stream's SPADE code. Subsequently, it is compiled and executed within the Streams environment to deliver notifications in real-time of the newborns health state.
21096109	Unraveling the conundrum of seemingly discordant protein-protein interaction datasets.
Conf Proc IEEE Eng Med Biol Soc  2010
Most high-throughput experimental results of protein-protein interactions (PPIs) are seemingly inconsistent with each other. In this article, we re-evaluated these contradictions within the context of the underlying domain-domain interactions (DDIs) for two Escherichia coli and four Saccharomyces cerevisiae PPI datasets derived from high-throughput (yeast two-hybrid and tandem affinity purification) experimental platforms. For shared DDIs across pairs of compared datasets, we observed a remarkably high pair-wise correlation (Pearson correlation coefficient between 0.80 and 0.84) between datasets of the same organism derived from the same experimental platform. To a lesser degree, this concordance also held true for more general inter-platform and intra-species comparisons (Pearson correlation coefficient between 0.52 and 0.89). Thus, although varying experimental conditions can influence the ability of individual proteins to interact and, therefore, create apparent differences among PPIs, the physical nature of the underlying interactions, captured by DDIs, is the same and can be used to model and predict PPIs.
21096154	Ambient assisted living: a methodological approach.
Conf Proc IEEE Eng Med Biol Soc  2010
In this paper, the most important challenges and trends related to the application of Ambient Assisted Living (AAL) methods and techniques to the social/healthcare context are discussed. In order to find out technical solutions to these challenges, the main methodological issues concerning the design of open and distributed architectures are analyzed. The objective is to improve the efficiency/cost ratio in the provision of social and healthcare services to citizens with special needs, through the application of new paradigms in the context of AAL environments. Finally, some results and conclusions regarding the proposed open architecture are illustrated for the case of a distributed biomedical sensor network designed by the authors following this methodology.
21096155	On search guide phrase compilation for recommending home medical products.
Conf Proc IEEE Eng Med Biol Soc  2010
To help people find desired home medical products (HMPs), we developed an intelligent personal health record (iPHR) system that can automatically recommend HMPs based on users' health issues. Using nursing knowledge, we pre-compile a set of "search guide" phrases that provides semantic translation from words describing health issues to their underlying medical meanings. Then iPHR automatically generates queries from those phrases and uses them and a search engine to retrieve HMPs. To avoid missing relevant HMPs during retrieval, the compiled search guide phrases need to be comprehensive. Such compilation is a challenging task because nursing knowledge updates frequently and contains numerous details scattered in many sources. This paper presents a semi-automatic tool facilitating such compilation. Our idea is to formulate the phrase compilation task as a multi-label classification problem. For each newly obtained search guide phrase, we first use nursing knowledge and information retrieval techniques to identify a small set of potentially relevant classes with corresponding hints. Then a nurse makes the final decision on assigning this phrase to proper classes based on those hints. We demonstrate the effectiveness of our techniques by compiling search guide phrases from an occupational therapy textbook.
21096321	DiseaseAtlas: multi-facet visual analytics for online disease articles.
Conf Proc IEEE Eng Med Biol Soc  2010
Online health information portals provide valuable content to casual consumers. However, the page-oriented nature of these resources makes it difficult for users to understand the overall information space and navigate the complex relationships between various diseases. We have developed a visual analytic system named DiseaseAtlas that helps users navigate a large set of disease-related documents and understand multi-dimensional relationships for key semantic concepts such as symptoms and treatments. This paper describes several unique aspects of DiseaseAtlas and demonstrates its capabilities through a case study.
21096339	A single tri-axial accelerometer-based real-time personal life log system capable of activity classification and exercise information generation.
Conf Proc IEEE Eng Med Biol Soc  2010
Recording a personal life log (PLL) of daily activities is an emerging technology for u-lifecare and e-health services. In this paper, we present an accelerometer-based personal life log system capable of human activity classification and exercise information generation. In our system, we use a tri-axial accelerometer and a real-time activity recognition scheme in which a set of augmented features of accelerometer signals, processed with Linear Discriminant Analysis (LDA), is classified by our hierarchical artificial neural network classifier: in the lower level of the classifier, a state of an activity is recognized based on the statistical and spectral features; in the upper level, an activity is recognized with a set of augmented features including autoregressive (AR) coefficients, signal magnitude area (SMA), and tilt angles (TA). Upon the recognition of each activity, we further estimate exercise information such as energy expenditure based on Metabolic Equivalents (METS), step count, walking distance, walking speed, activity duration, etc. Our PLL system functions in real-time and all information generated from our system is archived in a daily-log database. By testing our system on seven different daily activities, we have obtained an average accuracy of 84.8% in activity recognition and generated their relative exercise information.
21096367	A simple yet effective data integration approach to tree-based microarray data classification.
Conf Proc IEEE Eng Med Biol Soc  2010
Different biological labs conduct similar experiments on same diseases. It is highly desirable to have a better model based on more experimental results than that on a single result. In this paper, we propose a method for integrating microarray data from multiple sources for building classification models. To test the method, we use three real world microarray data sets from different labs with different experimental devices and environments. Although microarray data is well known for its inconsistencies across labs, we demonstrate that it is possible to build consistent models using data sets from multiple labs. We report our method, experimental results and observations in the paper.
21096384	A low-power self-biased neural amplifier for implantable EEG recording system ICs.
Conf Proc IEEE Eng Med Biol Soc  2010
This paper presents a low-power and self-biased neural amplifier for implantable EEG recording system ICs with a high-density interface. To achieve low-power consumption, small die-area, high gain, and high CMRR, a fully differential Chappell OTA is employed along with a capacitive feedback loop. The amplifier operating at ± 1.2V has a gain of 65.6dB and consumes a power of 1.7 microW. The bandwidth extends from a low-frequency cutoff of below 1 Hz to a high-frequency cutoff of 300Hz which is suitable for EEG signals. This proposed amplifier has an input-referred noise of 9.76 mmicroV(RMS) and THD of 1.86% with respect to 1mV(PP) input at 100Hz. This low-power self-biased neural amplifier occupies an active die-area of 0.244 mm(2) and is under fabrication in 0.35 microm CMOS 4M2P Process.
21096629	An open data mining framework for the analysis of medical images: application on obstructive nephropathy microscopy images.
Conf Proc IEEE Eng Med Biol Soc  2010
This paper presents an open image-mining framework that provides access to tools and methods for the characterization of medical images. Several image processing and feature extraction operators have been implemented and exposed through Web Services. Rapid-Miner, an open source data mining system has been utilized for applying classification operators and creating the essential processing workflows. The proposed framework has been applied for the detection of salient objects in Obstructive Nephropathy microscopy images. Initial classification results are quite promising demonstrating the feasibility of automated characterization of kidney biopsy images.
21096742	Combining computer and human vision into a BCI: can the whole be greater than the sum of its parts?
Conf Proc IEEE Eng Med Biol Soc  2010
Our group has been investigating the development of BCI systems for improving information delivery to a user, specifically systems for triaging image content based on what captures a user's attention. One of the systems we have developed uses single-trial EEG scores as noisy labels for a computer vision image retrieval system. In this paper we investigate how the noisy nature of the EEG-derived labels affects the resulting accuracy of the computer vision system. Specifically, we consider how the precision of the EEG scores affects the resulting precision of images retrieved by a graph-based transductive learning model designed to propagate image class labels based on image feature similarity and sparse labels.
21097311	A robust volumetric feature extraction approach for 3D neuroimaging retrieval.
Conf Proc IEEE Eng Med Biol Soc  2010
The increased volume of 3D neuroimaging data has created a need for efficient data management and retrieval. We suggest that image retrieval via robust volumetric features could benefit managing these large image datasets. In this paper, we introduce a new feature extraction method, based on disorder-oriented masks, that uses the volumetric spatial distribution patterns in 3D physiological parametric neurological images. Our preliminary results indicate that the proposed volumetric feature extraction approach could support reliable 3D neuroimaging data retrieval and management.
20863752	Comparison of extent of use, information accuracy, and functions for manual and electronic patient status boards.
Int J Med Inform 20100921 2010Dec
Electronic software packages to support patient tracking and disposition decision making in emergency departments (EDs) are being considered for implementation in many hospitals. We compared extent of use, information accuracy, and functions of manual and electronic patient status boards at 2 EDs where both were continuously in use. Ethnographic observations were conducted at 2 Veterans Affairs Medical Center Emergency Departments using both manual and electronic patient status boards (100 h, 9 physicians at Site 1; 64 h, 14 physicians at Site 2). Data included board information collected at 20-min intervals, observable behavior while using boards, and interviews. Few physicians (3/9 [33%] Site 1; 0/14 [0%] Site 2) used the e-board, whereas all physicians used the whiteboards. Whiteboards had fewer inaccuracies (6/462 [1%] Site 1; 21/864 [3%] Site 2) than e-boards (62/462 [13%] Site 1; 107/864 [12%] Site 2). The primary functions of the whiteboard were to track real-time changes to patient identifiers, locations, nursing assignments, and pending activities; facilitate patient handoffs; inform physicians and nurses about newly arrived patients assigned to them; inform nurses of physicians' orders; and inform physicians of the status of ordered items. The primary functions of the e-board were to support electronic data entry (by clerks) of patient admitting and departure times; and highlight patients who had been in the ED for longer than 6 h. Whiteboards were more extensively used and had greater information accuracy than e-boards. Nevertheless, e-boards provided functionality not easily achievable with whiteboards.
20643225	Supporting retrieval of diverse biomedical data using evidence-aware queries.
J Biomed Inform 20100717 2010Dec
Though there have been many advances in providing access to linked and integrated biomedical data across repositories, developing methods which allow users to specify ambiguous and exploratory queries over disparate sources remains a challenge to extracting well-curated or diversely-supported biological information. In the following work, we discuss the concepts of data coverage and evidence in the context of integrated sources. We address diverse information retrieval via a simple framework for representing coverage and evidence that operates in parallel with an arbitrary schema, and a language upon which queries on the schema and framework may be executed. We show that this approach is capable of answering questions that require ranged levels of evidence or triangulation, and demonstrate that appropriately-formed queries can significantly improve the level of precision when retrieving well-supported biomedical data.
20670693	Automatically extracting information needs from complex clinical questions.
J Biomed Inform 20100727 2010Dec
Clinicians pose complex clinical questions when seeing patients, and identifying the answers to those questions in a timely manner helps improve the quality of patient care. We report here on two natural language processing models, namely, automatic topic assignment and keyword identification, that together automatically and effectively extract information needs from ad hoc clinical questions. Our study is motivated in the context of developing the larger clinical question answering system AskHERMES (Help clinicians to Extract and aRrticulate Multimedia information for answering clinical quEstionS). We developed supervised machine-learning systems to automatically assign predefined general categories (e.g. etiology, procedure, and diagnosis) to a question. We also explored both supervised and unsupervised systems to automatically identify keywords that capture the main content of the question. We evaluated our systems on 4654 annotated clinical questions that were collected in practice. We achieved an F1 score of 76.0% for the task of general topic classification and 58.0% for keyword extraction. Our systems have been implemented into the larger question answering system AskHERMES. Our error analyses suggested that inconsistent annotation in our training data have hurt both question analysis tasks. Our systems, available at http://www.askhermes.org, can automatically extract information needs from both short (the number of word tokens &lt;20) and long questions (the number of word tokens &gt;20), and from both well-structured and ill-formed questions. We speculate that the performance of general topic classification and keyword extraction can be further improved if consistently annotated data are made available.
20870033	A concept-driven biomedical knowledge extraction and visualization framework for conceptualization of text corpora.
J Biomed Inform 20100924 2010Dec
A number of techniques such as information extraction, document classification, document clustering and information visualization have been developed to ease extraction and understanding of information embedded within text documents. However, knowledge that is embedded in natural language texts is difficult to extract using simple pattern matching techniques and most of these methods do not help users directly understand key concepts and their semantic relationships in document corpora, which are critical for capturing their conceptual structures. The problem arises due to the fact that most of the information is embedded within unstructured or semi-structured texts that computers can not interpret very easily. In this paper, we have presented a novel Biomedical Knowledge Extraction and Visualization framework, BioKEVis to identify key information components from biomedical text documents. The information components are centered on key concepts. BioKEVis applies linguistic analysis and Latent Semantic Analysis (LSA) to identify key concepts. The information component extraction principle is based on natural language processing techniques and semantic-based analysis. The system is also integrated with a biomedical named entity recognizer, ABNER, to tag genes, proteins and other entity names in the text. We have also presented a method for collating information extracted from multiple sources to generate semantic network. The network provides distinct user perspectives and allows navigation over documents with similar information components and is also used to provide a comprehensive view of the collection. The system stores the extracted information components in a structured repository which is integrated with a query-processing module to handle biomedical queries over text documents. We have also proposed a document ranking mechanism to present retrieved documents in order of their relevance to the user query.
20887803	A new pivoting and iterative text detection algorithm for biomedical images.
J Biomed Inform 20100929 2010Dec
There is interest to expand the reach of literature mining to include the analysis of biomedical images, which often contain a paper's key findings. Examples include recent studies that use Optical Character Recognition (OCR) to extract image text, which is used to boost biomedical image retrieval and classification. Such studies rely on the robust identification of text elements in biomedical images, which is a non-trivial task. In this work, we introduce a new text detection algorithm for biomedical images based on iterative projection histograms. We study the effectiveness of our algorithm by evaluating the performance on a set of manually labeled random biomedical images, and compare the performance against other state-of-the-art text detection algorithms. We demonstrate that our projection histogram-based text detection approach is well suited for text detection in biomedical images, and that the iterative application of the algorithm boosts performance to an F score of .60. We provide a C++ implementation of our algorithm freely available for academic use.
20457733	Patient safety and systematic reviews: finding papers indexed in MEDLINE, EMBASE and CINAHL.
Qual Saf Health Care 20100510 2010Oct
To develop search strategies for identifying papers on patient safety in MEDLINE, EMBASE and CINAHL. Six journals were electronically searched for papers on patient safety published between 2000 and 2006. Identified papers were divided into two gold standards: one to build and the other to validate the search strategies. Candidate terms for strategy construction were identified using a word frequency analysis of titles, abstracts and keywords used to index the papers in the databases. Searches were run for each one of the selected terms independently in every database. Sensitivity, precision and specificity were calculated for each candidate term. Terms with sensitivity greater than 10% were combined to form the final strategies. The search strategies developed were run against the validation gold standard to assess their performance. A final step in the validation process was to compare the performance of each strategy to those of other strategies found in the literature. We developed strategies for all three databases that were highly sensitive (range 95%-100%), precise (range 40%-60%) and balanced (the product of sensitivity and precision being in the range of 30%-40%). The strategies were very specific and outperformed those found in the literature. The strategies we developed can meet the needs of users aiming to maximise either sensitivity or precision, or seeking a reasonable compromise between sensitivity and precision, when searching for papers on patient safety in MEDLINE, EMBASE or CINAHL.
20977994	Finding the best examples of healthcare quality improvement in Sub-Saharan Africa.
Qual Saf Health Care  2010Oct
The purpose of this study was to summarise the current state of healthcare quality improvement literature focusing on sub-Saharan Africa. Conventional methods of searching the literature were quickly found to be inadequate or inappropriate, given the different needs of practitioners in sub-Saharan Africa, and the inaccessibility of the literature. The group derived a core list of what were deemed exemplary quality improvement articles, based on consensus and a search into the "grey" literature of quality improvement. Quality improvement articles from sub-Saharan Africa are difficult to find, and suffer from a lack of centrality and organisation of literature. Efforts to address this are critical to fostering the growth of quality improvement literature in developing country settings.
20980714	PathJam: a new service for integrating biological pathway information.
J Integr Bioinform 20101028 2010
Biological pathways are crucial to much of the scientific research today including the study of specific biological processes related with human diseases. PathJam is a new comprehensive and freely accessible web-server application integrating scattered human pathway annotation from several public sources. The tool has been designed for both (i) being intuitive for wet-lab users providing statistical enrichment analysis of pathway annotations and (ii) giving support to the development of new integrative pathway applications. PathJam&amp;rsquo;s unique features and advantages include interactive graphs linking pathways and genes of interest, downloadable results in fully compatible formats, GSEA compatible output files and a standardized RESTful API.
20574988	Ultrashort echo time (UTE) MRI of the lung: assessment of tissue density in the lung parenchyma.
Magn Reson Med  2010Nov
Nonuniform disruption of lung architecture is usually assessed by CT, which carries potential radiation risk. Here we report our use of a three-dimensional ultrashort echo time MR method to image the lungs of normal mice at different positive end-expiratory pressures in a 3-T clinical MR system. The ultrashort echo time sequence in conjunction with a projection acquisition of the free induction decay could reduce the echo time to 100 μsec and provide a more inherent MR signal intensity from the lung parenchyma, which is usually invisible due to its short T*(2) in conventional MRI methods. The signal intensity and T*(2) was reduced as the positive end-expiratory pressure became higher. Further, these parameters were highly correlated to the changes in lung volume (% lung expansion). The results indicated that the MR signal acquired at ultrashort echo time in the lung parenchyma represents interstitial tissue density including blood. The capability of acquiring sufficient MR signal would have implications for the direct assessment of parenchymal architecture in the lung. Therefore, ultrashort echo time imaging may have the potential to assist detection of early and localized pathological destruction of lung tissue architecture observed in various pulmonary disorders such as emphysema without incurring the risks of radiation exposure.
20848635	Reconstruction of MRI data encoded with arbitrarily shaped, curvilinear, nonbijective magnetic fields.
Magn Reson Med 20100916 2010Nov
A basic framework for image reconstruction from spatial encoding by curvilinear, nonbijective magnetic encoding fields in combination with multiple receivers is presented. The theory was developed in the context of the recently introduced parallel imaging technique using localized gradients (PatLoc) approach. In this new imaging modality, the linear gradient fields are generalized to arbitrarily shaped, nonbijective spatial encoding magnetic fields, which lead to ambiguous encoding. Ambiguities are resolved by adaptation of concepts developed for parallel imaging. Based on theoretical considerations, a practical algorithm for Cartesian trajectories is derived in the case that the conventional gradient coils are replaced by coils for PatLoc. The reconstruction method extends Cartesian sensitivity encoding (SENSE) reconstruction with an additional voxelwise intensity-correction step. Spatially varying resolution, signal-to-noise ratio, and truncation artifacts are described and analyzed. Theoretical considerations are validated by two-dimensional simulations based on multipolar encoding fields and they are confirmed by applying the reconstruction algorithm to initial experimental data.
20937152	Boolean versus ranked querying for biomedical systematic reviews.
BMC Med Inform Decis Mak 20101012 2010
The process of constructing a systematic review, a document that compiles the published evidence pertaining to a specified medical topic, is intensely time-consuming, often taking a team of researchers over a year, with the identification of relevant published research comprising a substantial portion of the effort. The standard paradigm for this information-seeking task is to use Boolean search; however, this leaves the user(s) the requirement of examining every returned result. Further, our experience is that effective Boolean queries for this specific task are extremely difficult to formulate and typically require multiple iterations of refinement before being finalized. We explore the effectiveness of using ranked retrieval as compared to Boolean querying for the purpose of constructing a systematic review. We conduct a series of experiments involving ranked retrieval, using queries defined methodologically, in an effort to understand the practicalities of incorporating ranked retrieval into the systematic search task. Our results show that ranked retrieval by itself is not viable for this search task requiring high recall. However, we describe a refinement of the standard Boolean search process and show that ranking within a Boolean result set can improve the overall search performance by providing early indication of the quality of the results, thereby speeding up the iterative query-refinement process. Outcomes of experiments suggest that an interactive query-development process using a hybrid ranked and Boolean retrieval system has the potential for significant time-savings over the current search process in the systematic reviewing.
20942966	Booly: a new data integration platform.
BMC Bioinformatics 20101013 2010
Data integration is an escalating problem in bioinformatics. We have developed a web tool and warehousing system, Booly, that features a simple yet flexible data model coupled with the ability to perform powerful comparative analysis, including the use of Boolean logic to merge datasets together, and an integrated aliasing system to decipher differing names of the same gene or protein. Furthermore, Booly features a collaborative sharing system and a public repository so that users can retrieve new datasets while contributors can easily disseminate new content. We illustrate the uses of Booly with several examples including: the versatile creation of homebrew datasets, the integration of heterogeneous data to identify genes useful for comparing avian and mammalian brain architecture, and generation of a list of Food and Drug Administration (FDA) approved drugs with possible alternative disease targets. The Booly paradigm for data storage and analysis should facilitate integration between disparate biological and medical fields and result in novel discoveries that can then be validated experimentally. Booly can be accessed at http://booly.ucsd.edu.
20946670	Data-driven approach for creating synthetic electronic medical records.
BMC Med Inform Decis Mak 20101014 2010
New algorithms for disease outbreak detection are being developed to take advantage of full electronic medical records (EMRs) that contain a wealth of patient information. However, due to privacy concerns, even anonymized EMRs cannot be shared among researchers, resulting in great difficulty in comparing the effectiveness of these algorithms. To bridge the gap between novel bio-surveillance algorithms operating on full EMRs and the lack of non-identifiable EMR data, a method for generating complete and synthetic EMRs was developed. This paper describes a novel methodology for generating complete synthetic EMRs both for an outbreak illness of interest (tularemia) and for background records. The method developed has three major steps: 1) synthetic patient identity and basic information generation; 2) identification of care patterns that the synthetic patients would receive based on the information present in real EMR data for similar health problems; 3) adaptation of these care patterns to the synthetic patient population. We generated EMRs, including visit records, clinical activity, laboratory orders/results and radiology orders/results for 203 synthetic tularemia outbreak patients. Validation of the records by a medical expert revealed problems in 19% of the records; these were subsequently corrected. We also generated background EMRs for over 3000 patients in the 4-11 yr age group. Validation of those records by a medical expert revealed problems in fewer than 3% of these background patient EMRs and the errors were subsequently rectified. A data-driven method was developed for generating fully synthetic EMRs. The method is general and can be applied to any data set that has similar data elements (such as laboratory and radiology orders and results, clinical activity, prescription orders). The pilot synthetic outbreak records were for tularemia but our approach may be adapted to other infectious diseases. The pilot synthetic background records were in the 4-11 year old age group. The adaptations that must be made to the algorithms to produce synthetic background EMRs for other age groups are indicated.
20714013	Visually inspecting specular surfaces: A generalized image capture and image description approach.
IEEE Trans Pattern Anal Mach Intell  2010Nov
Image capturing and image content description can be regarded as the two major steps of a computer vision process. This paper focuses on both within the field of specular surface inspection, by generalizing a previously defined stripebased inspection method to free-form surfaces on the basis of a specific stripe illumination technique and by outlining a general feature-based stripe image characterization approach by means of new theoretical concepts. One major purpose of this paper is to propose a general stripe image interpretation approach on the basis of a three-step procedure: 1) comparison of different image content description techniques, 2) fusion of the most appropriate ones, and 3) selection of the optimal features. It is shown that this approach leads to an increase in the classification rates of more than 2 percent between the initial fused set and the selected one. The new contributions encompass 1) the generalization of a cylindrical specular surface enhancement technique to more complex specular geometries, 2) the generalization of the previously defined stripe image description by using the same number of features for the bright and the dark stripes, and 3) the definition of an optimal, in terms of classification rates and computational costs, stripe feature set.
21058175	The quest for full text: an in-depth examination of Pubget for medical searchers.
Med Ref Serv Q  2010Oct
This article examines Pubget, a free Web-based search engine for life sciences researchers for conducting searches of the medical literature and retrieving full-text PDFs. Its search functionality and add-on features are evaluated to determine potential for library instruction and promotion. With many libraries relying on OpenURL link resolvers to connect searchers with institutional subscriptions, Pubget offers an alternative by combining search, article-level link resolving, and authentication in a single platform. The authors determine advantages and disadvantages for using Pubget based on product testing and make recommendations for institutions interested in "activating" subscriptions in Pubget.
21058178	Electronic resources at the University of Sharjah medical library: an investigation of students' information-seeking behavior.
Med Ref Serv Q  2010Oct
Electronic information is becoming prevalent worldwide, and its use is growing exponentially as more and more users are recognizing the potential that it offers in terms of access and delivery. However, with the introduction of new tools for e-information searching and retrieval, users have to readjust their information-seeking behavior to cope with the corresponding changes. The University of Sharjah library is steadily increasing its investment in e-resources to offer ubiquitous access to the growing body of literature in areas that interest the community it serves. This study reports the findings of a survey conducted to investigate the information-seeking behavior of medical students at the medical library. Results showed evidence of use of e-resources, but they did not explicitly establish that some of the major problems mentioned by participants did hinder the information searches of the respondents. An extensive literature review sets the background for the study.
21058179	Information literacy skills of occupational therapy graduates: promoting evidence-based practice in the MOT curriculum.
Med Ref Serv Q  2010Oct
Are Master of Occupational Therapy (MOT) graduates more successful than BS graduates in accessing and analyzing research literature? This retrospective cohort study used a survey sent to Ohio State University MOT graduates, asking why they need information for their practice, what types of information they seek, and how they search for and use it. Results suggest that the MOT program has fostered higher-level skills than did the BS program in independent writing, a greater focus on evidence-based practice, and the use of bibliographic databases. The MOT graduates report high confidence in their ability to apply research to practice and high satisfaction with the lifelong learning skills they learned. The survey findings support the importance of collaboration between Occupational Therapy faculty and medical librarians in developing MOT educational programs.
21058181	Reference and PDF-manager software: complexities, support and workflow.
Med Ref Serv Q  2010Oct
In the past, librarians taught reference management by training library users to use established software programs such as RefWorks or EndNote. In today's environment, there is a proliferation of Web-based programs that are being used by library clientele that offer a new twist on the well-known reference management programs. Basically, these new programs are PDF-manager software (e.g., Mendeley or Papers). Librarians are faced with new questions, issues, and concerns, given the new workflows and pathways that these PDF-manager programs present. This article takes a look at some of those.
20940177	iRefWeb: interactive analysis of consolidated protein interaction data and their supporting evidence.
Database (Oxford) 20101012 2010
We present iRefWeb, a web interface to protein interaction data consolidated from 10 public databases: BIND, BioGRID, CORUM, DIP, IntAct, HPRD, MINT, MPact, MPPI and OPHID. iRefWeb enables users to examine aggregated interactions for a protein of interest, and presents various statistical summaries of the data across databases, such as the number of organism-specific interactions, proteins and cited publications. Through links to source databases and supporting evidence, researchers may gauge the reliability of an interaction using simple criteria, such as the detection methods, the scale of the study (high- or low-throughput) or the number of cited publications. Furthermore, iRefWeb compares the information extracted from the same publication by different databases, and offers means to follow-up possible inconsistencies. We provide an overview of the consolidated protein-protein interaction landscape and show how it can be automatically cropped to aid the generation of meaningful organism-specific interactomes. iRefWeb can be accessed at: http://wodaklab.org/iRefWeb. Database URL: http://wodaklab.org/iRefWeb/
20940178	EuroDia: a beta-cell gene expression resource.
Database (Oxford) 20101012 2010
Type 2 diabetes mellitus (T2DM) is a major disease affecting nearly 280 million people worldwide. Whilst the pathophysiological mechanisms leading to disease are poorly understood, dysfunction of the insulin-producing pancreatic beta-cells is key event for disease development. Monitoring the gene expression profiles of pancreatic beta-cells under several genetic or chemical perturbations has shed light on genes and pathways involved in T2DM. The EuroDia database has been established to build a unique collection of gene expression measurements performed on beta-cells of three organisms, namely human, mouse and rat. The Gene Expression Data Analysis Interface (GEDAI) has been developed to support this database. The quality of each dataset is assessed by a series of quality control procedures to detect putative hybridization outliers. The system integrates a web interface to several standard analysis functions from R/Bioconductor to identify differentially expressed genes and pathways. It also allows the combination of multiple experiments performed on different array platforms of the same technology. The design of this system enables each user to rapidly design a custom analysis pipeline and thus produce their own list of genes and pathways. Raw and normalized data can be downloaded for each experiment. The flexible engine of this database (GEDAI) is currently used to handle gene expression data from several laboratory-run projects dealing with different organisms and platforms. Database URL: http://eurodia.vital-it.ch.
20624013	Design and development of an automatic data acquisition system for a balance study using a smartcard system.
J Med Eng Technol 20100712 2010 Oct-Nov
For measurement value logging of board angle values during balance training, it is necessary to develop a measurement system. This study will provide data for a balance study using the smartcard. The data acquisition comes automatically. An individually training plan for each proband is necessary. To store the proband identification a smartcard with an I2C data bus protocol and an E2PROM memory system is used. For reading the smartcard data a smartcard reader is connected via universal serial bus (USB) to a notebook. The data acquisition and smartcard read programme is designed with Microsoft® Visual C#. A training plan file contains the individual training plan for each proband. The data of the test persons are saved in a proband directory. Each event is automatically saved as a log-file for the exact documentation. This system makes study development easy and time-saving.
20955140	Using what we gather--harnessing information for improved care.
Med. J. Aust.  2010Oct18
Currently available data can be used to focus clinical quality, patient centredness and safety of care in hospitals.
20962128	An analysis of computer-related patient safety incidents to inform the development of a classification.
J Am Med Inform Assoc  2010 Nov-Dec
To analyze patient safety incidents associated with computer use to develop the basis for a classification of problems reported by health professionals. Incidents submitted to a voluntary incident reporting database across one Australian state were retrieved and a subset (25%) was analyzed to identify 'natural categories' for classification. Two coders independently classified the remaining incidents into one or more categories. Free text descriptions were analyzed to identify contributing factors. Where available medical specialty, time of day and consequences were examined. Descriptive statistics; inter-rater reliability. A search of 42,616 incidents from 2003 to 2005 yielded 123 computer related incidents. After removing duplicate and unrelated incidents, 99 incidents describing 117 problems remained. A classification with 32 types of computer use problems was developed. Problems were grouped into information input (31%), transfer (20%), output (20%) and general technical (24%). Overall, 55% of problems were machine related and 45% were attributed to human-computer interaction. Delays in initiating and completing clinical tasks were a major consequence of machine related problems (70%) whereas rework was a major consequence of human-computer interaction problems (78%). While 38% (n=26) of the incidents were reported to have a noticeable consequence but no harm, 34% (n=23) had no noticeable consequence. Only 0.2% of all incidents reported were computer related. Further work is required to expand our classification using incident reports and other sources of information about healthcare IT problems. Evidence based user interface design must focus on the safe entry and retrieval of clinical information and support users in detecting and correcting errors and malfunctions.
20962132	Using global unique identifiers to link autism collections.
J Am Med Inform Assoc  2010 Nov-Dec
To propose a centralized method for generating global unique identifiers to link collections of research data and specimens. The work is a collaboration between the Simons Foundation Autism Research Initiative and the National Database for Autism Research. The system is implemented as a web service: an investigator inputs identifying information about a participant into a client application and sends encrypted information to a server application, which returns a generated global unique identifier. The authors evaluated the system using a volume test of one million simulated individuals and a field test on 2000 families (over 8000 individual participants) in an autism study. Inverse probability of hash codes; rate of false identity of two individuals; rate of false split of single individual; percentage of subjects for which identifying information could be collected; percentage of hash codes generated successfully. Large-volume simulation generated no false splits or false identity. Field testing in the Simons Foundation Autism Research Initiative Simplex Collection produced identifiers for 96% of children in the study and 77% of parents. On average, four out of five hash codes per subject were generated perfectly (only one perfect hash is required for subsequent matching). The system must achieve balance among the competing goals of distinguishing individuals, collecting accurate information for matching, and protecting confidentiality. Considerable effort is required to obtain approval from institutional review boards, obtain consent from participants, and to achieve compliance from sites during a multicenter study. Generic unique identifiers have the potential to link collections of research data, augment the amount and types of data available for individuals, support detection of overlap between collections, and facilitate replication of research findings.
20920264	The structural and content aspects of abstracts versus bodies of full text journal articles are different.
BMC Bioinformatics 20100929 2010
An increase in work on the full text of journal articles and the growth of PubMedCentral have the opportunity to create a major paradigm shift in how biomedical text mining is done. However, until now there has been no comprehensive characterization of how the bodies of full text journal articles differ from the abstracts that until now have been the subject of most biomedical text mining research. We examined the structural and linguistic aspects of abstracts and bodies of full text articles, the performance of text mining tools on both, and the distribution of a variety of semantic classes of named entities between them. We found marked structural differences, with longer sentences in the article bodies and much heavier use of parenthesized material in the bodies than in the abstracts. We found content differences with respect to linguistic features. Three out of four of the linguistic features that we examined were statistically significantly differently distributed between the two genres. We also found content differences with respect to the distribution of semantic features. There were significantly different densities per thousand words for three out of four semantic classes, and clear differences in the extent to which they appeared in the two genres. With respect to the performance of text mining tools, we found that a mutation finder performed equally well in both genres, but that a wide variety of gene mention systems performed much worse on article bodies than they did on abstracts. POS tagging was also more accurate in abstracts than in article bodies. Aspects of structure and content differ markedly between article abstracts and article bodies. A number of these differences may pose problems as the text mining field moves more into the area of processing full-text articles. However, these differences also present a number of opportunities for the extraction of data types, particularly that found in parenthesized text, that is present in article bodies but not in article abstracts.
20829443	SBML2TikZ: supporting the SBML render extension in LaTeX.
Bioinformatics 20100909 2010Nov1
The SBML Render Extension enables coloring and shape information of biochemical models to be stored in the Systems Biology Markup Language (SBML). Rendering of this stored graphical information in a portable and well supported system such as TeX would be useful for researchers preparing documentation and presentations. In addition, since the Render Extension is not yet supported by many applications, it is helpful for such rendering functionality be extended to the more popular CellDesigner annotation as well. SBML2TikZ supports automatic generation of graphics for biochemical models in the popular TeX typesetting system. The library generates a script of TeX macro commands for the vector graphics languages PGF/TikZ that can be compiled into scalable vector graphics described in a model. Source code, documentation and compiled binaries for the SBML2TikZ library can be found at http://www.sbml2tikz.org. In addition, a web application is available at http://www.sys-bio.org/layout
20659868	[The Web: a help or a labyrinth for doctors?].
Bull Cancer  2010Oct
Internet, unknown there fifteen years, has become commonly used today to communicate and exchange all types of information. Concerning medical points, this network is full of data and has become indispensable to physicians. The biomedical knowledge base is changing so fast that it is impossible for a doctor to know everything about everything. Specialists are increasingly specialized but must keep aware of developments in medical knowledge and above all must be able to search quickly and effectively in a given time. To stay informed, they have several means of which the most common are reading the press, Continuing Medical Education and now Internet. Unfortunately, the richness of the Web is also its main fault so we must have the knowledges for searching, analyzing, among the flood of documents available if we do not want to be caught by the screen. Internet usage is done by two approaches: the pull: dynamic approach to information retrieval and the push: passive process of receiving documents. Most doctors have not been trained to use this tool although it became their main source of knowledge and research. This article aims to outline the main ways to learn about a specific topic by giving some tips for saving time.
20880892	Developing registries of volunteers: key principles to manage issues regarding personal information protection.
J Med Ethics 20100929 2010Nov
Much biomedical research cannot be performed without recruiting human subjects. Increasingly, volunteer registries are being developed to assist researchers with this challenging task. Yet, volunteer registries raise confidentiality issues. Having recently developed a registry of volunteers, the authors searched for normative guidance on how to implement the principle of confidentiality. The authors found that the protection of confidentiality in registries are based on the 10 key elements which are elaborated in detail in the Canadian Standards Association Model Code. This paper describes how these 10 detailed key principles can be used during the developmental stages of volunteer registries.
20923585	Information on ethical issues in health technology assessment: how and where to find them.
Int J Technol Assess Health Care 20101006 2010Oct
Comprehensive health technology assessments (HTAs) include thorough reflections on ethical issues associated with health technologies, their use, and value-based decisions in the assessment process. As methods of information retrieval for effectiveness assessments are not applicable to information retrieval on ethical issues, a specific methodological approach is necessary. In the absence of existing adapted methods, our objective was to develop a methodological approach for the systematic retrieval of information on ethical issues related to health technologies. A literature search was conducted to verify the non-existence of published comprehensive methodological approaches for the information retrieval on ethical issues for HTAs, and resulted in no hits. We, therefore, developed a step-by-step workflow following the workflow of information retrieval for effectiveness assessments: Step 1: Translation of the search question using the PICO scheme and additional components. Step 2: Concept building by modeling and linking search components. Step 3: Identification of synonyms in all relevant languages. Step 4: Selection of relevant information sources. Step 5: Design of search strategies for bibliographic databases. Step 6: Execution of search strategies and information seeking, including hand-searching. Step 7: Saving of retrieval results and standardized reporting of the process and results. Step 8: Final quality check and calculation of precision and recall. Systematic searching for information on ethical issues related to health technologies can be performed following the common retrieval workflow for effectiveness assessments, but should be performed separately applying adapted procedures and search terms on ethical issues relevant to the research question.
20923586	How much searching is enough? Comprehensive versus optimal retrieval for technology assessments.
Int J Technol Assess Health Care 20101006 2010Oct
The aim of this study is to review briefly different methods for determining the optimal retrieval of studies for inclusion in a health technology assessment (HTA) report. This study reviews the methodology literature related to specific methods for evaluating yield from literature searching strategies and for deciding whether to continue or desist in the searching process. Eight different methods were identified. These include using the Capture-recapture technique; obtaining Feedback from the commissioner of the HTA report; seeking the Disconfirming case; undertaking comparison against a known Gold standard; evaluating retrieval of Known items; recognizing the Law of diminishing returns, specifying a priori Stopping rules, and identifying a point of Theoretical saturation. While this study identified a variety of possible methods, there has been very little formal evaluation of the specific strengths and weaknesses of the different techniques. The author proposes an evaluation agenda drawing on an examination of existing data together with exploration of the specific impact of missing relevant studies.
20923588	Classification of evidence in decision-analytic models of cost-effectiveness: a content analysis of published reports.
Int J Technol Assess Health Care 20101006 2010Oct
The aim of this study was to assess systematically the scope of evidence and purposes for which evidence is used in decision-analytic models of cost-effectiveness and to assess the implications for search methods. A content analysis of published reports of models was undertaken. Details of cited sources were extracted and categorized according to three dimensions; type of information provided by the evidence, type of source from which the evidence was drawn and type of modeling activity supported by the evidence. The analysis was used to generate a classification of evidence. Relationships within and between the categories within the classification were sought and the implications for searching considered. The classification generated fourteen types of information, seven types of sources of evidence and five modeling activities supported by evidence. A broad range of evidence was identified drawn from a diverse range of sources including both research-based and non-research-based sources. The use of evidence was not restricted to the population of model parameters but was used to inform the development of the modeling framework and to justify the analytical and methodological approach. Decision-analytic models use evidence to support all aspects of model development. The classification of evidence defines in depth the role of evidence in modeling. It can be used to inform the systematic identification of evidence.
20942989	Reporting and presenting information retrieval processes: the need for optimizing common practice in health technology assessment.
Int J Technol Assess Health Care 20101013 2010Oct
Information retrieval (IR) in health technology assessment (HTA) calls for transparency and reproducibility, but common practice in the documentation and presentation of this process is inadequate in fulfilling this demand. Our objective is to promote good IR practice by presenting the conceptualization of retrieval and transcription readable to non-information specialists, and reporting of effectively processed search strategies. We performed a comprehensive database search (04/2010) to synthesize the current state-of-the-art. We then developed graphical and tabular presentation methods and tested their feasibility on existing research questions and defined recommendations. No generally accepted standard of reporting of IR in HTA exists. We, therefore, developed templates for presenting the retrieval conceptualization, database selection, and additional hand-searching as well as for presenting search histories of complex and lengthy search strategies. No single template fits all conceptualizations, but some can be applied to most processes. Database interface providers report queries as entered, not as they are actually processed. In PubMed, the huge difference between entered and processed query is shown in "Details." Quality control and evaluation of search strategies using a validated tool such as the PRESS checklist is suboptimal when only entry-query based search histories are applied. Moving toward an internationally accepted IR reporting standard calls for advances in common reporting practices. Comprehensive, process-based reporting and presentation would make IR more understandable to others than information specialists and facilitate quality control.
20975957	Novel primate-specific genes, RMEL 1, 2 and 3, with highly restricted expression in melanoma, assessed by new data mining tool.
PLoS ONE 20101020 2010
Melanoma is a highly aggressive and therapy resistant tumor for which the identification of specific markers and therapeutic targets is highly desirable. We describe here the development and use of a bioinformatic pipeline tool, made publicly available under the name of EST2TSE, for the in silico detection of candidate genes with tissue-specific expression. Using this tool we mined the human EST (Expressed Sequence Tag) database for sequences derived exclusively from melanoma. We found 29 UniGene clusters of multiple ESTs with the potential to predict novel genes with melanoma-specific expression. Using a diverse panel of human tissues and cell lines, we validated the expression of a subset of three previously uncharacterized genes (clusters Hs.295012, Hs.518391, and Hs.559350) to be highly restricted to melanoma/melanocytes and named them RMEL1, 2 and 3, respectively. Expression analysis in nevi, primary melanomas, and metastatic melanomas revealed RMEL1 as a novel melanocytic lineage-specific gene up-regulated during melanoma development. RMEL2 expression was restricted to melanoma tissues and glioblastoma. RMEL3 showed strong up-regulation in nevi and was lost in metastatic tumors. Interestingly, we found correlations of RMEL2 and RMEL3 expression with improved patient outcome, suggesting tumor and/or metastasis suppressor functions for these genes. The three genes are composed of multiple exons and map to 2q12.2, 1q25.3, and 5q11.2, respectively. They are well conserved throughout primates, but not other genomes, and were predicted as having no coding potential, although primate-conserved and human-specific short ORFs could be found. Hairpin RNA secondary structures were also predicted. Concluding, this work offers new melanoma-specific genes for future validation as prognostic markers or as targets for the development of therapeutic strategies to treat melanoma.
20849585	MetNetGE: interactive views of biological networks and ontologies.
BMC Bioinformatics 20100917 2010
Linking high-throughput experimental data with biological networks is a key step for understanding complex biological systems. Currently, visualization tools for large metabolic networks often result in a dense web of connections that is difficult to interpret biologically. The MetNetGE application organizes and visualizes biological networks in a meaningful way to improve performance and biological interpretability. MetNetGE is an interactive visualization tool based on the Google Earth platform. MetNetGE features novel visualization techniques for pathway and ontology information display. Instead of simply showing hundreds of pathways in a complex graph, MetNetGE gives an overview of the network using the hierarchical pathway ontology using a novel layout, called the Enhanced Radial Space-Filling (ERSF) approach that allows the network to be summarized compactly. The non-tree edges in the pathway or gene ontology, which represent pathways or genes that belong to multiple categories, are linked using orbital connections in a third dimension. Biologists can easily identify highly activated pathways or gene ontology categories by mapping of summary experiment statistics such as coefficient of variation and overrepresentation values onto the visualization. After identifying such pathways, biologists can focus on the corresponding region to explore detailed pathway structure and experimental data in an aligned 3D tiered layout. In this paper, the use of MetNetGE is illustrated with pathway diagrams and data from E. coli and Arabidopsis. MetNetGE is a visualization tool that organizes biological networks according to a hierarchical ontology structure. The ERSF technique assigns attributes in 3D space, such as color, height, and transparency, to any ontological structure. For hierarchical data, the novel ERSF layout enables the user to identify pathways or categories that are differentially regulated in particular experiments. MetNetGE also displays complex biological pathway in an aligned 3D tiered layout for exploration.
20807877	An international bioinformatics infrastructure to underpin the Arabidopsis community.
Plant Cell 20100831 2010Aug
The future bioinformatics needs of the Arabidopsis community as well as those of other scientific communities that depend on Arabidopsis resources were discussed at a pair of recent meetings held by the Multinational Arabidopsis Steering Committee and the North American Arabidopsis Steering Committee. There are extensive tools and resources for information storage, curation, and retrieval of Arabidopsis data that have been developed over recent years primarily through the activities of The Arabidopsis Information Resource, the Nottingham Arabidopsis Stock Centre, and the Arabidopsis Biological Resource Center, among others. However, the rapid expansion in many data types, the international basis of the Arabidopsis community, and changing priorities of the funding agencies all suggest the need for changes in the way informatics infrastructure is developed and maintained. We propose that there is a need for a single core resource that is integrated into a larger international consortium of investigators. We envision this to consist of a distributed system of data, tools, and resources, accessed via a single information portal and funded by a variety of sources, under shared international management of an International Arabidopsis Informatics Consortium (IAIC). This article outlines the proposal for the development, management, operations, and continued funding for the IAIC.
20879243	Reconstructing geometrically consistent tree structures from noisy images.
Med Image Comput Comput Assist Interv  2010
We present a novel approach to fully automated reconstruction of tree structures in noisy 2D images. Unlike in earlier approaches, we explicitly handle crossovers and bifurcation points, and impose geometric constraints while optimizing a global cost function. We use manually annotated retinal scans to evaluate our method and demonstrate that it brings about a very substantial improvement.
20879334	Inter-subject connectivity-based parcellation of a patch of cerebral cortex.
Med Image Comput Comput Assist Interv  2010
This paper presents a connectivity-based parcellation of the human post-central gyrus, at the level of the group of subjects. The dimension of the clustering problem is reduced using a set of cortical regions of interest determined at the inter-subject level using a surface-based coordinate system, and representing the regions with a strong connection to the post-central gyrus. This process allows a clustering based on criteria which are more reproducible across subjects than in an intra-subject approach. We obtained parcels relatively stable in localisation across subjects as well as homogenous and well-separated to each other in terms of connectivity profiles. To address the parcellation at the inter-subject level provides a direct matching between parcels across subjects. In addition, this method allows the identification of subject-specific parcels. This property could be useful for the study of pathologies.
20879350	An image retrieval approach to setup difficulty levels in training systems for endomicroscopy diagnosis.
Med Image Comput Comput Assist Interv  2010
Learning medical image interpretation is an evolutive process that requires modular training systems, from non-expert to expert users. Our study aims at developing such a system for endomicroscopy diagnosis. It uses a difficulty predictor to try and shorten the physician learning curve. As the understanding of video diagnosis is driven by visual similarities, we propose a content-based video retrieval approach to estimate the level of interpretation difficulty. The performance of our retrieval method is compared with several state of the art methods, and its genericity is demonstrated with two different clinical databases, on the Barrett's Esophagus and on colonic polyps. From our retrieval results, we learn a difficulty predictor against a ground truth given by the percentage of false diagnoses among several physicians. Our experiments show that, although our datasets are not large enough to test for statistical significance, there is a noticeable relationship between our retrieval-based difficulty estimation and the difficulty experienced by the physicians.
20879379	Incorporating priors on expert performance parameters for segmentation validation and label fusion: a maximum a posteriori STAPLE.
Med Image Comput Comput Assist Interv  2010
In order to evaluate the quality of segmentations of an image and assess intra- and inter-expert variability in segmentation performance, an Expectation Maximization (EM) algorithm for Simultaneous Truth And Performance Level Estimation (STAPLE) was recently developed. This algorithm, originally presented for segmentation validation, has since been used for many applications, such as atlas construction and decision fusion. However, the manual delineation of structures of interest is a very time consuming and burdensome task. Further, as the time required and burden of manual delineation increase, the accuracy of the delineation is decreased. Therefore, it may be desirable to ask the experts to delineate only a reduced number of structures or the segmentation of all structures by all experts may simply not be achieved. Fusion from data with some structures not segmented by each expert should be carried out in a manner that accounts for the missing information. In other applications, locally inconsistent segmentations may drive the STAPLE algorithm into an undesirable local optimum, leading to misclassifications or misleading experts performance parameters. We present a new algorithm that allows fusion with partial delineation and which can avoid convergence to undesirable local optima in the presence of strongly inconsistent segmentations. The algorithm extends STAPLE by incorporating prior probabilities for the expert performance parameters. This is achieved through a Maximum A Posteriori formulation, where the prior probabilities for the performance parameters are modeled by a beta distribution. We demonstrate that this new algorithm enables dramatically improved fusion from data with partial delineation by each expert in comparison to fusion with STAPLE.
20879388	A semi-automatic method for segmentation of the carotid bifurcation and bifurcation angle quantification on black blood MRA.
Med Image Comput Comput Assist Interv  2010
Quantitative information about the geometry of the carotid artery bifurcation may help in predicting the development of atherosclerosis. A geodesic active contours based segmentation method combining both gradient and intensity information was developed for semi-automatic, accurate and robust quantification of the carotid bifurcation angle in Black Blood MRA data. The segmentation method was evaluated by comparing its accuracy to inter and intra observer variability on a large dataset that has been acquired as part of a longitudinal population study which investigates the natural progression of carotid atherosclerosis. Furthermore, the method is shown to be robust to initialization differences. The bifurcation angle obtained from the segmented lumen corresponds well with the angle derived from the manual lumen segmentation, which demonstrates that the method has large potential to replace manual segmentations for extracting the carotid bifurcation angle from Black Blood MRA data.
20879462	Concept of computer-assisted clinical diagnostic documentation systems for the practice with the option of later scientific evaluations.
Int J Comput Dent  2010
Treatment data from practices and specialization centers, especially in the increasingly specialized areas which university clinics do not cover, are very important for evaluating the effectiveness and efficiency of dental examination and treatment methods. In the case of paper-based documentation, the evaluation of these data usually fails because of the cost it entails. With the use of electronic medical records, this expense can be markedly lower, provided the data acquisition and storage is structured accordingly. Since access to sensitive person-related data is simplified considerably by this method, such health data are protected, especially on the European level. Other than generally assumed, this protection is not restricted solely to the confidentiality principle, but also comprises the power of disposition over the data (data protection). The result is that from a legal point of view, the treatment data cannot be readily used for scientific studies, not even by dentists and physicians who have collected the data legally during the course of their therapeutic work. The technical separation of treatment data from the personal data offers a legally acceptable solution to this problem. It must ensure that a later assignment to individual persons will not be feasible at a realistic expense ("effective anonymization"). This article describes the legal and information technology principles and their practical implementation, as illustrated by the concept of a respective compliant IT architecture for the dentaConcept CMD fact diagnostic software. Here, a special export function automatically separates the anonymized treatment data and thus facilitates multicentric studies within an institution and among dental practices.
20879602	Adaptive learning for relevance feedback: application to digital mammography.
Med Phys  2010Aug
With the rapid growing volume of images in medical databases, development of efficient image retrieval systems to retrieve relevant or similar images to a query image has become an active research area. Despite many efforts to improve the performance of techniques for accurate image retrieval, its success in biomedicine thus far has been quite limited. This article presents an adaptive content-based image retrieval (CBIR) system for improving the performance of image retrieval in mammographic databases. In this work, the authors propose a new relevance feedback approach based on incremental learning with support vector machine (SVM) regression. Also, the authors present a new local perturbation method to further improve the performance of the proposed relevance feedback system. The approaches enable efficient online learning by adapting the current trained model to changes prompted by the user's relevance feedback, avoiding the burden of retraining the CBIR system. To demonstrate the proposed image retrieval system, the authors used two mammogram data sets: A set of 76 mammograms scored based on geometrical similarity and a larger set of 200 mammograms scored by expert radiologists based on pathological findings. The experimental results show that the proposed relevance feedback strategy improves the retrieval precision for both data sets while achieving high efficiency compared to offline SVM. For the data set of 200 mammograms, the authors obtained an average precision of 0.48 and an area under the precision-recall curve of 0.79. In addition, using the same database, the authors achieved a high pathology matching rate greater than 80% between the query and the top retrieved images after relevance feedback. Using mammographic databases, the results demonstrate that the proposed approach is more accurate than the model without using relevance feedback not only in image retrieval but also in pathology matching while maintaining its effectiveness for online relevance feedback applications.
20881801	PubMed searching for home care clinicians: a guide for success in identifying articles for a literature review.
Home Healthc Nurse  2010Oct
Home care clinicians frequently lack access to a health sciences library. Nevertheless, they require ongoing access to current evidence-based nursing literature. Learning to use PubMed efficiently will enable home care clinicians striving to provide high-quality care to identify and, in some cases, access the full text of peer-reviewed articles that support evidence-based practice.
20411864	Integrating evidence-based practice into RN-to-BSN clinical nursing education.
J Nurs Educ 20100331 2010Jul
This study examines the effects of integrating evidence-based practice (EBP) into clinical practicum on EBP efficacy and barriers to research utilization among Korean RN-to-BSN students. A one-group pretest-posttest design was used. Eighty-one students were recruited from a school of nursing in Korea. Evidence-based practice clinical practicum was composed of two consecutive programs during one semester. Lectures, individual mentoring on EBP practicum, small group, and wrap-up conferences were provided. Outcomes of EBP efficacy and barriers to research utilization were analyzed using paired t tests for 74 final participants. Evidence-based practice efficacy scores increased significantly (p &lt; 0.05), and the barriers to research utilization scores decreased significantly after the EBP clinical practicum. The results highlight the effectiveness of EBP education among RN-to-BSN students. These results may help health educators develop effective educational strategies to integrate EBP concepts into a clinical practicum.
20615486	Proteomics data repositories: providing a safe haven for your data and acting as a springboard for further research.
J Proteomics 20100706 2010Oct10
Despite the fact that data deposition is not a generalised fact yet in the field of proteomics, several mass spectrometry (MS) based proteomics repositories are publicly available for the scientific community. The main existing resources are: the Global Proteome Machine Database (GPMDB), PeptideAtlas, the PRoteomics IDEntifications database (PRIDE), Tranche, and NCBI Peptidome. In this review the capabilities of each of these will be described, paying special attention to four key properties: data types stored, applicable data submission strategies, supported formats, and available data mining and visualization tools. Additionally, the data contents from model organisms will be enumerated for each resource. There are other valuable smaller and/or more specialized repositories but they will not be covered in this review. Finally, the concept behind the ProteomeXchange consortium, a collaborative effort among the main resources in the field, will be introduced.
20849574	ClustalXeed: a GUI-based grid computation version for high performance and terabyte size multiple sequence alignment.
BMC Bioinformatics 20100917 2010
There is an increasing demand to assemble and align large-scale biological sequence data sets. The commonly used multiple sequence alignment programs are still limited in their ability to handle very large amounts of sequences because the system lacks a scalable high-performance computing (HPC) environment with a greatly extended data storage capacity. We designed ClustalXeed, a software system for multiple sequence alignment with incremental improvements over previous versions of the ClustalX and ClustalW-MPI software. The primary advantage of ClustalXeed over other multiple sequence alignment software is its ability to align a large family of protein or nucleic acid sequences. To solve the conventional memory-dependency problem, ClustalXeed uses both physical random access memory (RAM) and a distributed file-allocation system for distance matrix construction and pair-align computation. The computation efficiency of disk-storage system was markedly improved by implementing an efficient load-balancing algorithm, called "idle node-seeking task algorithm" (INSTA). The new editing option and the graphical user interface (GUI) provide ready access to a parallel-computing environment for users who seek fast and easy alignment of large DNA and protein sequence sets. ClustalXeed can now compute a large volume of biological sequence data sets, which were not tractable in any other parallel or single MSA program. The main developments include: 1) the ability to tackle larger sequence alignment problems than possible with previous systems through markedly improved storage-handling capabilities. 2) Implementing an efficient task load-balancing algorithm, INSTA, which improves overall processing times for multiple sequence alignment with input sequences of non-uniform length. 3) Support for both single PC and distributed cluster systems.
20846442	Does the mind map learning strategy facilitate information retrieval and critical thinking in medical students?
BMC Med Educ 20100916 2010
A learning strategy underutilized in medical education is mind mapping. Mind maps are multi-sensory tools that may help medical students organize, integrate, and retain information. Recent work suggests that using mind mapping as a note-taking strategy facilitates critical thinking. The purpose of this study was to investigate whether a relationship existed between mind mapping and critical thinking, as measured by the Health Sciences Reasoning Test (HSRT), and whether a relationship existed between mind mapping and recall of domain-based information. In this quasi-experimental study, 131 first-year medical students were randomly assigned to a standard note-taking (SNT) group or mind map (MM) group during orientation. Subjects were given a demographic survey and pre-HSRT. They were then given an unfamiliar text passage, a pre-quiz based upon the passage, and a 30-minute break, during which time subjects in the MM group were given a presentation on mind mapping. After the break, subjects were given the same passage and wrote notes based on their group (SNT or MM) assignment. A post-quiz based upon the passage was administered, followed by a post-HSRT. Differences in mean pre- and post-quiz scores between groups were analyzed using independent samples t-tests, whereas differences in mean pre- and post-HSRT total scores and subscores between groups were analyzed using ANOVA. Mind map depth was assessed using the Mind Map Assessment Rubric (MMAR). There were no significant differences in mean scores on both the pre- and post-quizzes between note-taking groups. And, no significant differences were found between pre- and post-HSRT mean total scores and subscores. Although mind mapping was not found to increase short-term recall of domain-based information or critical thinking compared to SNT, a brief introduction to mind mapping allowed novice MM subjects to perform similarly to SNT subjects. This demonstrates that medical students using mind maps can successfully retrieve information in the short term, and does not put them at a disadvantage compared to SNT students. Future studies should explore longitudinal effects of mind-map proficiency training on both short- and long-term information retrieval and critical thinking.
20923755	Biomedical informatics techniques for processing and analyzing web blogs of military service members.
J. Med. Internet Res. 20101005 2010
Web logs ("blogs") have become a popular mechanism for people to express their daily thoughts, feelings, and emotions. Many of these expressions contain health care-related themes, both physical and mental, similar to information discussed during a clinical interview or medical consultation. Thus, some of the information contained in blogs might be important for health care research, especially in mental health where stress-related conditions may be difficult and expensive to diagnose and where early recognition is often key to successful treatment. In the field of biomedical informatics, techniques such as information retrieval (IR) and natural language processing (NLP) are often used to unlock information contained in free-text notes. These methods might assist the clinical research community to better understand feelings and emotions post deployment and the burden of symptoms of stress among US military service members. In total, 90 military blog posts describing deployment situations and 60 control posts of Operation Enduring Freedom/Operation Iraqi Freedom (OEF/OIF) were collected. After "stop" word exclusion and stemming, a "bag-of-words" representation and term weighting was performed, and the most relevant words were manually selected out of the high-weight words. A pilot ontology was created using Collaborative Protégé, a knowledge management application. The word lists and the ontology were then used within General Architecture for Text Engineering (GATE), an NLP framework, to create an automated pipeline for recognition and analysis of blogs related to combat exposure. An independent expert opinion was used to create a reference standard and evaluate the results of the GATE pipeline. The 2 dimensions of combat exposure descriptors identified were: words dealing with physical exposure and the soldiers' emotional reactions to it. GATE pipeline was able to retrieve blog texts describing combat exposure with precision 0.9, recall 0.75, and F-score 0.82. Natural language processing and automated information retrieval might potentially provide valuable tools for retrieving and analyzing military blog posts and uncovering military service members' emotions and experiences of combat exposure.
20927012	Open access to scientific research: where are we and where are we going? Facts and figures on the occasion of the 2010 Open Access Week (October 18-24).
Eur J Phys Rehabil Med  2010Sep
This contribution is aimed at presenting a sort of "state of the art" of Open Access on the occasion of the 2010 international Open Access Week, to be held from October 18 to October 24. We shall see facts and figures about open archives and the mandates to deposit; about Open Access journals; about impact and citation advantages for the researchers, and about economic sustainability.
20798169	METAREP: JCVI metagenomics reports--an open source tool for high-performance comparative metagenomics.
Bioinformatics 20100826 2010Oct15
JCVI Metagenomics Reports (METAREP) is a Web 2.0 application designed to help scientists analyze and compare annotated metagenomics datasets. It utilizes Solr/Lucene, a high-performance scalable search engine, to quickly query large data collections. Furthermore, users can use its SQL-like query syntax to filter and refine datasets. METAREP provides graphical summaries for top taxonomic and functional classifications as well as a GO, NCBI Taxonomy and KEGG Pathway Browser. Users can compare absolute and relative counts of multiple datasets at various functional and taxonomic levels. Advanced comparative features comprise statistical tests as well as multidimensional scaling, heatmap and hierarchical clustering plots. Summaries can be exported as tab-delimited files, publication quality plots in PDF format. A data management layer allows collaborative data analysis and result sharing. Web site http://www.jcvi.org/metarep; source code http://github.com/jcvi/METAREP CONTACT: syooseph@jcvi.org Supplementary data are available at Bioinformatics online.
20650667	Computer-aided detection; the effect of training databases on detection of subtle breast masses.
Acad Radiol 20100722 2010Nov
Lesion conspicuity is typically highly correlated with visual difficulty for lesion detection, and computer-aided detection (CAD) has been widely used as a "second reader" in mammography. Hence, increasing CAD sensitivity in detecting subtle cancers without increasing false-positive rates is important. The aim of this study was to investigate the effect of training database case selection on CAD performance in detecting low-conspicuity breast masses. A full-field digital mammographic image database that included 525 cases depicting malignant masses was randomly partitioned into three subsets. A CAD scheme was applied to detect all initially suspected mass regions and compute region conspicuity. Training samples were iteratively selected from two of the subsets. Four types of training data sets-(1) one including all available true-positive mass regions in the two subsets ("all"), (2) one including 350 randomly selected mass regions ("diverse"), (3) one including 350 high-conspicuity mass regions ("easy"), and (4) one including 350 low-conspicuity mass regions ("difficult")-were assembled. In each training data set, the same number of randomly selected false-positive regions as the true-positives were also included. Two classifiers, an artificial neural network (ANN) and a k-nearest neighbor (KNN) algorithm, were trained using each of the four training data sets and tested on all suspected regions in the remaining data set. Using a threefold cross-validation method, the performance changes of the CAD schemes trained using one of the four training data sets were computed and compared. CAD initially detected 1025 true-positive mass regions depicted on 507 cases (97% case-based sensitivity) and 9569 false-positive regions (3.5 per image) in the entire database. Using the all training data set, CAD achieved the highest overall performance on the entire testing database. However, CAD detected the highest number of low-conspicuity masses when the difficult training data set was used for training. Results did agree for both ANN-based and KNN-based classifiers in all tests. Compared to the use of the all training data set, the sensitivity of the schemes trained using the difficult data set decreased by 8.6% and 8.4% for the ANN and KNN algorithm on the entire database, respectively, but the detection of low-conspicuity masses increased by 7.1% and 15.1% for the ANN and KNN algorithm at a false-positive rate of 0.3 per image. CAD performance depends on the size, diversity, and difficulty level of the training database. To increase CAD sensitivity in detecting subtle cancer, one should increase the fraction of difficult cases in the training database rather than simply increasing the training data set size.
20801696	Fully automatic registration and segmentation of first-pass myocardial perfusion MR image sequences.
Acad Radiol  2010Nov
Derivation of diagnostically relevant parameters from first-pass myocardial perfusion magnetic resonance images involves the tedious and time-consuming manual segmentation of the myocardium in a large number of images. To reduce the manual interaction and expedite the perfusion analysis, we propose an automatic registration and segmentation method for the derivation of perfusion linked parameters. A complete automation was accomplished by first registering misaligned images using a method based on independent component analysis, and then using the registered data to automatically segment the myocardium with active appearance models. We used 18 perfusion studies (100 images per study) for validation in which the automatically obtained (AO) contours were compared with expert drawn contours on the basis of point-to-curve error, Dice index, and relative perfusion upslope in the myocardium. Visual inspection revealed successful segmentation in 15 out of 18 studies. Comparison of the AO contours with expert drawn contours yielded 2.23 ± 0.53 mm and 0.91 ± 0.02 as point-to-curve error and Dice index, respectively. The average difference between manually and automatically obtained relative upslope parameters was found to be statistically insignificant (P = .37). Moreover, the analysis time per slice was reduced from 20 minutes (manual) to 1.5 minutes (automatic). We proposed an automatic method that significantly reduced the time required for analysis of first-pass cardiac magnetic resonance perfusion images. The robustness and accuracy of the proposed method were demonstrated by the high spatial correspondence and statistically insignificant difference in perfusion parameters, when AO contours were compared with expert drawn contours.
20920176	ExaCT: automatic extraction of clinical trial characteristics from journal publications.
BMC Med Inform Decis Mak 20100928 2010
Clinical trials are one of the most important sources of evidence for guiding evidence-based practice and the design of new trials. However, most of this information is available only in free text - e.g., in journal publications - which is labour intensive to process for systematic reviews, meta-analyses, and other evidence synthesis studies. This paper presents an automatic information extraction system, called ExaCT, that assists users with locating and extracting key trial characteristics (e.g., eligibility criteria, sample size, drug dosage, primary outcomes) from full-text journal articles reporting on randomized controlled trials (RCTs). ExaCT consists of two parts: an information extraction (IE) engine that searches the article for text fragments that best describe the trial characteristics, and a web browser-based user interface that allows human reviewers to assess and modify the suggested selections. The IE engine uses a statistical text classifier to locate those sentences that have the highest probability of describing a trial characteristic. Then, the IE engine's second stage applies simple rules to these sentences to extract text fragments containing the target answer. The same approach is used for all 21 trial characteristics selected for this study. We evaluated ExaCT using 50 previously unseen articles describing RCTs. The text classifier (first stage) was able to recover 88% of relevant sentences among its top five candidates (top5 recall) with the topmost candidate being relevant in 80% of cases (top1 precision). Precision and recall of the extraction rules (second stage) were 93% and 91%, respectively. Together, the two stages of the extraction engine were able to provide (partially) correct solutions in 992 out of 1050 test tasks (94%), with a majority of these (696) representing fully correct and complete answers. Our experiments confirmed the applicability and efficacy of ExaCT. Furthermore, they demonstrated that combining a statistical method with 'weak' extraction rules can identify a variety of study characteristics. The system is flexible and can be extended to handle other characteristics and document types (e.g., study protocols).
20936066	Basic list of veterinary medical serials, third edition: using a decision matrix to update the core list of veterinary journals.
J Med Libr Assoc  2010Oct
This paper presents the methods and results of a study designed to produce the third edition of the "Basic List of Veterinary Medical Serials," which was established by the Veterinary Medical Libraries Section in 1976 and last updated in 1986. A set of 238 titles were evaluated using a decision matrix in order to systematically assign points for both objective and subjective criteria and determine an overall score for each journal. Criteria included: coverage in four major indexes, scholarly impact rank as tracked in two sources, identification as a recommended journal in preparing for specialty board examinations, and a veterinary librarian survey rating. Of the 238 titles considered, a minimum scoring threshold determined the 123 (52%) journals that constituted the final list. The 36 subject categories represented on the list include general and specialty disciplines in veterinary medicine. A ranked list of journals and a list by subject category were produced. Serials appearing on the third edition of the "Basic List of Veterinary Medical Serials" met expanded objective measures of quality and impact as well as subjective perceptions of value by both librarians and veterinary practitioners.
20936067	Mapping the literature of health education: 2006-2008.
J Med Libr Assoc  2010Oct
The study updates Schloman's 1997 study, "Mapping the Literature of Health Education." The authors identify an updated list of core health education journals and determine the coverage of these journals by electronic indexes. Citations from four source journals for the years 2006 to 2008 were analyzed using the established methodology of the "Mapping the Literature of Allied Health Project." The cited journals were divided into three zones of productivity by using Bradford's Law of Scattering. There were 19,907 citations in 602 source articles. Journal articles were the most commonly cited format type. Of the 1,896 journal titles cited, 20 (1.1%) made up the core journals. Together, the fields of medicine, health education, and psychology accounted for 85.0% of the journals in the core. Self-citation was found to be a common practice in the source journals. Scopus had the broadest journal coverage of the indexes examined. The results of this study provide a new picture of the health education literature: The volume has grown significantly, cites older materials, and relies less on sexual health journals and more on psychology journals.
20854690	Surfing the web during pandemic flu: availability of World Health Organization recommendations on prevention.
BMC Public Health 20100920 2010
People often search for information on influenza A(H1N1)v prevention on the web. The extent to which information found on the Internet is consistent with recommendations issued by the World Health Organization is unknown. We conducted a search for "swine flu" accessing 3 of the most popular search engines through different proxy servers located in 4 English-speaking countries (Australia, Canada, UK, USA). We explored each site resulting from the searches, up to 4 clicks starting from the search engine page, analyzing availability of World Health Organization recommendations for swine flu prevention. Information on hand cleaning was reported on 79% of the 147 websites analyzed; staying home when sick was reported on 77.5% of the websites; disposing tissues after sneezing on 75.5% of the websites. Availability of other recommendations was lower. The probability of finding preventative recommendations consistent with World Health Organization varied by country, type of website, and search engine. Despite media coverage on H1N1 influenza, relevant information for prevention is not easily found on the web. Strategies to improve information delivery to the general public through this channel should be improved.
20682010	Proximity to the US-Mexico border: a key to explaining geographic variation in US methamphetamine, cocaine and heroin purity.
Addiction  2010Oct
Although illicit drug purity is a widely discussed health risk, research explaining its geographic variation within a country is rare. This study examines whether proximity to the US-Mexico border, the United States' primary drug import portal, is associated with geographic variation in US methamphetamine, heroin and cocaine purity. Distances (proximity) between the US-Mexico border and locations of methamphetamine, cocaine and heroin seizures/acquisitions (n = 239,070) recorded in STRIDE (System to Retrieve Information from Drug Evidence) were calculated for the period of 1990-2004. The association of drug purity with these distances and other variables, including time and seizure/acquisition size, was examined using hierarchical multivariate linear modeling (HMLM). Coterminous United States. Methamphetamine, cocaine and heroin purity generally decreased with distance from the US-Mexico border. Heroin purity, however, after initially declining with distance, turned upwards-a U-shaped association. During 2000-04, methamphetamine purity also had a U-shaped association with distance. For each of the three drugs, temporal changes in the purity of small acquisitions (&lt;10 g) were typically more dynamic in areas closer to the US-Mexico border. Geographic variance in methamphetamine, cocaine and heroin purity throughout the coterminous United States was associated with US-Mexico border proximity. The U-shaped associations between border-distance and purity for heroin and methamphetamine may be due to imports of those drugs via the eastern United States and southeast Canada, respectively. That said, areas closer to the US-Mexico border generally had relatively high illicit drug purity, as well as more dynamic change in the purity of small ('retail level') drug amounts.
20083458	An analysis of random projection for changeable and privacy-preserving biometric verification.
IEEE Trans Syst Man Cybern B Cybern 20100115 2010Oct
Changeability and privacy protection are important factors for widespread deployment of biometrics-based verification systems. This paper presents a systematic analysis of a random-projection (RP)-based method for addressing these problems. The employed method transforms biometric data using a random matrix with each entry an independent and identically distributed Gaussian random variable. The similarity- and privacy-preserving properties, as well as the changeability of the biometric information in the transformed domain, are analyzed in detail. Specifically, RP on both high-dimensional image vectors and dimensionality-reduced feature vectors is discussed and compared. A vector translation method is proposed to improve the changeability of the generated templates. The feasibility of the introduced solution is well supported by detailed theoretical analyses. Extensive experimentation on a face-based biometric verification problem shows the effectiveness of the proposed method.
20683645	Chemical space: missing pieces in cheminformatics.
Pharm. Res. 20100804 2010Oct
Cheminformatics is at a turning point, the pharmaceutical industry benefits from using the various methods developed over the last twenty years, but in our opinion we need to see greater development of novel approaches that non-experts can use. This will be achieved by more collaborations between software companies, academics and the evolving pharmaceutical industry. We suggest that cheminformatics should also be looking to other industries that use high performance computing technologies for inspiration. We describe the needs and opportunities which may benefit from the development of open cheminformatics technologies, mobile computing, the movement of software to the cloud and precompetitive initiatives.
20841656	A scheme for assuring lifelong readability in computer based medical records.
Stud Health Technol Inform  2010
Medical records must be kept over an extended period of time, meanwhile computer based medical records are renewed every 5-6 years. Readability of medical records must be assured even though the systems are renewed by different vendors. To achieve this, we proposed a method called DACS, in which a medical record is considered as an aggregation of documents. A Document generated by a system is transformed to a format read by free software such as PDF, which is transferred with the document meta-information and important data written on the XML to the Document Deliverer. It stores these data into the Document Archiver, the Document Sharing Server and the Data Warehouse (DWH). We developed the Matrix View which shows documents in chronological order, and the Tree View showing documents in class tree structure. By this method all the documents can be integrated and be viewed by a single viewer. This helps users figure out patient history and find a document being sought. In addition, documents' data can be shared among systems and analyzed by DWH. Most importantly DACS can assure the lifelong readability of medical records.
20841657	Experience implementing a point-of-care electronic medical record system for primary care in Malawi.
Stud Health Technol Inform  2010
Due to the fact that health care professionals in Malawi are often overstretched, the use and quality of health data can be compromised. The Malawi Health Management Information System (HMIS) has streamlined data collection and reporting and increased the use of data to improve care. Obstacles remain, including incomplete reporting and low staff morale. With the Baobab Health Trust and the Malawi Ministry of Health, Partners In Health piloted an innovative point-of-care data system for primary care that functions alongside OpenMRS, an open source medical record platform. The system has given access to a patient-level primary care dataset in real time. Initial results highlight some of the benefits of a point-of-care system such as improved data quality, emphasize the importance of sharing data with clinical practitioners, and shed light on how this approach could strengthen HMIS.
20841659	Process-aware EHR BPM systems: two prototypes and a conceptual framework.
Stud Health Technol Inform  2010
Systematic methods to improve the effectiveness and efficiency of electronic health record-mediated processes will be key to EHRs playing an important role in the positive transformation of healthcare. Business process management (BPM) systematically optimizes process effectiveness, efficiency, and flexibility. Therefore BPM offers relevant ideas and technologies. We provide a conceptual model based on EHR productivity and negative feedback control that links EHR and BPM domains, describe two EHR BPM prototype modules, and close with the argument that typical EHRs must become more process-aware if they are to take full advantage of BPM ideas and technology. A prediction: Future extensible clinical groupware will coordinate delivery of EHR functionality to teams of users by combining modular components with executable process models whose usability (effectiveness, efficiency, and user satisfaction) will be systematically improved using business process management techniques.
20841663	Integration of healthcare information: from enterprise PACS to patient centered multimedia health record.
Stud Health Technol Inform  2010
Every single piece of healthcare information should be fully integrated and transparent within the electronic health record. The Italian Hospital of Buenos Aires initiated the project Multimedia Health Record with the goal to achieve this integration while maintaining a holistic view of current structure of the systems of the Hospital, where the axis remains are the patient and longitudinal history, commencing with section Computed Tomography. Was implemented DICOM standard for communication and image storage and bought a PACS. It was necessary adapt our generic reporting system for live up to the commercial RIS. The Computerized Tomography (CT) Scanners of our hospital were easily integrated into the DICOM network and all the CT Scans generated by our radiology service were stored in the PACS, reported using the Structured Reporting System (we installed diagnostic terminals equipped with 3 monitors) and displayed in the EHR at any point of HIBA's healthcare network.
20841667	Towards automating the initial screening phase of a systematic review.
Stud Health Technol Inform  2010
Systematic review authors synthesize research to guide clinicians in their practice of evidence-based medicine. Teammates independently identify provisionally eligible studies by reading the same set of hundreds and sometimes thousands of citations during an initial screening phase. We investigated whether supervised machine learning methods can potentially reduce their workload. We also extended earlier research by including observational studies of a rare condition. To build training and test sets, we used annotated citations from a search conducted for an in-progress Cochrane systematic review. We extracted features from titles, abstracts, and metadata, then trained, optimized, and tested several classifiers with respect to mean performance based on 10-fold cross-validations. In the training condition, the evolutionary support vector machine (EvoSVM) with an Epanechnikov or radial kernel is the best classifier: mean recall=100%; mean precision=48% and 41%, respectively. In the test condition, EvoSVM performance degrades: mean recall=77%, mean precision ranges from 26% to 37%. Because near-perfect recall is essential in this context, we conclude that supervised machine learning methods may be useful for reducing workload under certain conditions.
20841668	Balancing centralised and decentralised EHR approaches to manage standardisation.
Stud Health Technol Inform  2010
Balancing regional and national electronic health record (EHR) approaches requires cooperation between clinical and technical experts at different organisational levels. Bridging is necessary to achieve interoperability between regional EHR systems, without neglecting the clinical usefulness. This study has investigated the approaches chosen in modelling the clinical content of EHRs in two out of five regions in Denmark. Based on the knowledge obtained in these studies a 'clinical content format' was developed to facilitate the work of the regions, where the clinical content of EHR systems is modelled. The objective of the clinical content format is to enable share and reuse across organisations, furthermore an objective is to gradually introduce standards. The results of the first iteration of a 'clinical content format' are presented and future adjustments are discussed based on the results.
20841669	Towards iconic language for patient records, drug monographs, guidelines and medical search engines.
Stud Health Technol Inform  2010
Practicing physicians have limited time for consulting medical knowledge and records. We have previously shown that using icons instead of text to present drug monographs may allow contraindications and adverse effects to be identified more rapidly and more accurately. These findings were based on the use of an iconic language designed for drug knowledge, providing icons for many medical concepts, including diseases, antecedents, drug classes and tests. In this paper, we describe a new project aimed at extending this iconic language, and exploring the possible applications of these icons in medicine. Based on evaluators' comments, focus groups of physicians and opinions of academic, industrial and associative partners, we propose iconic applications related to patient records, for example summarizing patient conditions, searching for specific clinical documents and helping to code structured data. Other applications involve the presentation of clinical practice guidelines and improving the interface of medical search engines. These new applications could use the same iconic language that was designed for drug knowledge, with a few additional items that respect the logic of the language.
20841675	Steps towards single source--collecting data about quality of life within clinical information systems.
Stud Health Technol Inform  2010
Information about the quality of life from patients being treated in routine medical care is important for the attending physician. This data is also needed in research for example to evaluate the therapy and the course of the disease respectively. Especially skin diseases often negatively affect the quality of life. Therefore we aimed to design a concept to collect such data during treatment and use it for both medical care and research in the setting of dermatology. We performed a workflow analysis and implemented a designated form using the tools of the local clinical information system. Quality of life data is now collected within the clinical information system during treatment and is used for discharge letters, progress overviews as well as research about the treatment and course of disease. This concept which contributes to the single source approach was feasible within dermatology and is ready to be expanded into other domains.
20841676	Methodology of integration of a clinical data warehouse with a clinical information system: the HEGP case.
Stud Health Technol Inform  2010
Clinical Data Warehouses (CDW) can complement current Clinical Information Systems (CIS) with functions that are not easily implemented by traditional operational database systems. Here, we describe the design and deployment strategy used at the Pompidou University Hospital in southwest Paris. Four realms are described: technological realm, data realm, restitution realm, and administration realm. The corresponding UML use cases and the mapping rules from the shared integrated electronic health records to the five axes of the i2b2 CDW star model are presented. Priority is given to the anonymization and security principles used for the 1.2 million patient records currently stored in the CDW. Exploitation of a CDW by clinicians and investigators can facilitate clinical research, quality evaluations and outcome studies. These indirect benefits are among the reasons for the continuous use of an integrated CIS.
20841677	TEDIS: an information system dedicated to patients with pervasive developmental disorders.
Stud Health Technol Inform  2010
Pervasive Development Disorders (PDD) represent a life disorder which significantly affects individuals and families. It requires long term specialized institutions health care, education and social accompaniment. In France, 350,000 to 600,000 patients are estimated to be affected and 5,000 to 8,000 newborns will develop the disorder every year. In 2005, Autism Resource Centres were created in each of the 23 regions in France, to support the PDD hospital reference centres in providing formal clinical assessment for each patient. Such assessments will support the prescription of health care measures, educative and intuitional orientation and accompaniment. An information system called TEDIS was designed to assist the psychiatrists and multidisciplinary medical experts at Necker child-psychiatry hospital, in organizing PDD patient's information and providing ground for improving knowledge about the disorder, its epidemiology and underlying biological mechanisms. The professionals' involvement from the beginning in the development process facilitated TEDIS design and implementation. The results of first experimentations are encouraging. They are described as well as the short term and mid-term deployment planning.
20841678	Developing a user-centered voluntary medical incident reporting system.
Stud Health Technol Inform  2010
Medical errors are one of leading causes of death among adults in the United States. According to the Institute of Medicine, reporting of medical incidents could be a cornerstone to learn from errors and to improve patient safety, if incident data are collected in a properly structured format which is useful for the detection of patterns, discovery of underlying factors, and generation of solutions. Globally, a number of medical incident reporting systems were deployed for collecting observable incident data in care delivery organizations (CDO) over the past several years. However, few researches delved into design of user-centered reporting system for improving completeness and accuracy of medical incident collection, let alone design models created for other institutes to follow. In this paper, we introduce the problems identified in a current using voluntary reporting system and our effort is being made towards complete, accurate and useful user-centered new reporting system through a usability engineering process.
20841679	CEDRIC: a computerized chronic disease management system for urban, safety net clinics.
Stud Health Technol Inform  2010
To meet the challenge of improving health care quality in urban, medically underserved areas of the US that have a predominance of chronic diseases such as diabetes, we have developed a new information system called CEDRIC for managing chronic diseases. CEDRIC was developed in collaboration with clinicians at an urban safety net clinic, using a community-participatory partnered research approach, with a view to addressing the particular needs of urban clinics with a high physician turnover and large uninsured/underinsured patient population. The pilot implementation focuses on diabetes management. In this paper, we describe the system's architecture and features.
20841685	Implementation, monitoring and utilization of an integrated Hospital Information System--lessons from a case study.
Stud Health Technol Inform  2010
In most hospitals several heterogeneous Information Systems (IS) store parts of a still scattered patient record. Virtual Patient Records (VPR) are systems that aggregate known data elements about the patient from different IS in real-time. This papers aims to present the main lessons learned from the implementation and the usage during 6 years of a VPR system. Ten major lessons were divided in recommendations for software developers, information managers and institutional policy makers. Implementing and using a VPR is a difficult journey but can generate great value for the institution if most of these recommendations are taken in consideration.
20841689	A Lab-EMR interoperability profile as an eHealth architecture component for resource-constrained settings.
Stud Health Technol Inform  2010
Implementation of computerized systems in resource-constrained settings have been gaining traction as a means of improving the delivery of health care, the use and reuse of information, and providing a standards-based capacity for assessing the process and impact of health care. In a resource-constrained environment, systems are often implemented as stand-alone entities focused on specific care activities (for example, delivering antiretroviral therapy). As such, in many countries, taking a generalized approach to linking electronic medical record systems with laboratory information systems (EMR-LIS) is an important area in which to achieve interoperability. In this paper we describe a scenario of use and information interaction interoperability profile based on our experience implementing EMR-LIS integration in two resource-constrained settings. Of significance, the profile emphasizes queued matching in order to avoid mutual dependence while achieving interoperability between systems.
20841710	Interoperability prototype between hospitals and general practitioners in Switzerland.
Stud Health Technol Inform  2010
Interoperability in data exchange has the potential to improve the care processes and decrease costs of the health care system. Many countries have related eHealth initiatives in preparation or already implemented. In this area, Switzerland has yet to catch up. Its health system is fragmented, because of the federated nature of cantons. It is thus more difficult to coordinate efforts between the existing healthcare actors. In the Medicoordination project a pragmatic approach was selected: integrating several partners in healthcare on a regional scale in French speaking Switzerland. In parallel with the Swiss eHealth strategy, currently being elaborated by the Swiss confederation, particularly medium-sized hospitals and general practitioners were targeted in Medicoordination to implement concrete scenarios of information exchange between hospitals and general practitioners with a high added value. In this paper we focus our attention on a prototype implementation of one chosen scenario: the discharge summary. Although simple in concept, exchanging release letters shows small, hidden difficulties due to the multi-partner nature of the project. The added value of such a prototype is potentially high and it is now important to show that interoperability can work in practice.
20841718	A socio-technical approach to continuity of care and electronic records in the South African context.
Stud Health Technol Inform  2010
Paper-based techniques of record keeping are contributing greatly to the discontinuity of patient care among healthcare providers. To achieve continuity, access to the information contained in medical records collected by various healthcare providers is necessary. To improve the sharing of information contained in these medical records the use of electronic methods of record keeping as opposed to paper-based records becomes very important. Even though the benefits of using electronic methods of record keeping are widely documented, the majority of South African healthcare practitioners still use paper-based methods. This paper describes an explorative study to determine barriers to the adoption of electronic records in the private primary care sector of South Africa. An interpretive approach using a socio-technical systems theory perspective was used to conduct the study. Based on the analysis of the socio-technical subsystems in the South African context it was revealed that there is not sufficient information available on the barriers to adoption of electronic records and further research will be necessary to identify the barriers to the adoption of electronic records.
20841719	Implementing OpenMRS for patient monitoring in an HIV/AIDS care and treatment program in rural Mozambique.
Stud Health Technol Inform  2010
We have adopted the Open Medical Record System (OpenMRS) framework to implement an electronic patient monitoring system for an HIV care and treatment program in Mozambique. The program provides technical assistance to the Ministry of Health supporting the scale up of integrated HIV care and support services in health facilities in rural resource limited settings. The implementation is in use for adult and pediatric programs, with ongoing roll-out to cover all supported sites. We describe early experiences in adapting the system to the program needs, addressing infrastructure challenges, creating a regional support team, training data entry staff, migrating a legacy database, deployment, and current use. We find that OpenMRS offers excellent prospects for in-country development of health information systems, even in severely resource limited settings. However, it also requires considerable organizational infrastructure investment and technical capacity building to ensure continued local support.
20841723	Electronic surveillance of healthcare-associated infections with MONI-ICU--a clinical breakthrough compared to conventional surveillance systems.
Stud Health Technol Inform  2010
Surveillance of clinical entities such as healthcare-associated infections (HCAI) by conventional techniques is a time-consuming task for highly trained experts. Such are neither available nor affordable in sufficient numbers on a permanent basis. Nevertheless, expert surveillance is a key parameter for good clinical practice, especially in intensive care medicine. MONI-ICU (monitoring of nosocomial infections in intensive care units) has been developed methodically and practically in a stepwise manner over the last 20 years and is now a reliable tool for clinical experts. It provides an almost real-time view of clinical indicators for HCAI--at the cost of almost no additional time on the part of surveillance staff or clinicians. We describe the use of this system in clinical routine and compare the results generated automatically by MONI-ICU with those generated in parallel by trained surveillance staff using patient chart reviews and other available information ("gold standard"). A total of 99 ICU patient admissions representing 1007 patient days were analyzed. MONI-ICU identified correctly the presence of an HCAI condition in 28/31 cases (sensitivity, 90.3%) and their absence in 68/68 of the non-HCAI cases (specificity, 100%), the latter meaning that MONI-ICU produced no "false alarms". The time taken for conventional surveillance at the 52 ward visits was 82.5 hours. MONI-ICU analysis of the same patient cases, including careful review of the generated results required only 12.5 hours (15.2%).
20841733	CEMARA an information system for rare diseases.
Stud Health Technol Inform  2010
Rare diseases cover a group of conditions characterized by a low prevalence, affecting less than 1 in 2,000 people; 5000 to 7000 rare diseases have been currently identified in Europe. Most diseases do not have any curative treatment. They represent thus an important public health concern. CEMARA is based on a n-tier architecture. Its main objective is to collect continuous and complete records of patients with rare diseases, and their follow-up through a web-based Information System, and to analyse the epidemiological patterns. In France, 41 out of 131 labelled Reference Centres (RC) are sharing CEMARA. Presently 56,593 cases have been registered by more than 850 health care professionals belonging to 171 clinical sites. The national demand of care was explored in relation with the offer of care in order to reach an improved match. Within 2 years, CEMARA stimulated sharing a common platform, a common ontology with Orphanet and initiating new cohorts of rare diseases for improving patient care and research.
20841821	Bridging the HL7 template - 13606 archetype gap with detailed clinical models.
Stud Health Technol Inform  2010
The idea of two level modeling has been taken up in healthcare information systems development. There is ongoing debate which approach should be taken. From the premise that there is a lack of clinician's time available, and the need for semantic interoperability, harmonization efforts are important. The question this paper addresses is whether Detailed Clinical Models (DCM) can bridge the gap between existing approaches. As methodology, a bottom up approach in multilevel comparison of existing content and modeling is used. Results indicate that it is feasible to compare and reuse DCM with clinical content from one approach to the other, when specific limitations are taken into account and precise analysis of each data-item is carried out. In particular the HL7 templates, the ISO/CEN 13606 and OpenEHR archetypes reveal more commonalties than differences. The linkage of DCM to terminologies suggests that data-items can be linked to concepts present in multiple terminologies. This work concludes that it is feasible to model a multitude of precise items of clinical information in the format of DCM and that transformations between different approaches are possible without loss of meaning. However, a set of single or combined clinical items and assessment scales have been tested. Larger groupings of clinical information might bring up more challenges.
20841823	Automatically detecting medications and the reason for their prescription in clinical narrative text documents.
Stud Health Technol Inform  2010
An important proportion of the information about the medications a patient is taking is mentioned only in narrative text in the electronic health record. Automated information extraction can make this information accessible for decision support, research, or any other automated processing. In the context of the "i2b2 medication extraction challenge," we have developed a new NLP application called Textractor to automatically extract medications and details about them (e.g., dosage, frequency, reason for their prescription). This application and its evaluation with part of the reference standard for this "challenge" are presented here, along with an analysis of the development of this reference standard. During this evaluation, Textractor reached a system-level overall F&lt;inf&gt;1&lt;/inf&gt;-measure, the reference metric for this challenge, of about 77% for exact matches. The best performance was measured with medication routes (F&lt;inf&gt;1&lt;/inf&gt;-measure 86.4%), and the worst with prescription reasons (F&lt;inf&gt;1&lt;/inf&gt;-measure 29%). These results are consistent with the agreement observed between human annotators when developing the reference standard, and with other published research.
20841824	Extracting medication information from French clinical texts.
Stud Health Technol Inform  2010
Much more Natural Language Processing (NLP) work has been performed on the English language than on any other. This general observation is also true of medical NLP, although clinical language processing needs are as strong in other languages as they are in English. In specific subdomains, such as drug prescription, the expression of information can be closely related across different languages, which should help transfer systems from English to other languages. We report here the implementation of a medication extraction system which extracts drugs and related information from French clinical texts, on the basis of an approach initially designed for English within the framework of the i2b2 2009 challenge. The system relies on specialized lexicons and a set of extraction rules. A first evaluation on 50 annotated texts obtains 86.7% F-measure, a level higher than the original English system and close to related work. This shows that the same rule-based approach can be applied to English and French languages, with a similar level of performance. We further discuss directions for improving both systems.
20841826	Performance analysis of a POS tagger applied to discharge summaries in Portuguese.
Stud Health Technol Inform  2010
Part of speech taggers need a considerable amount of data to train their models. Such data is not readily available for medical texts in Portuguese. We evaluated the accuracy of a morphological tagger against a gold standard when trained with corpora of different sizes and domains. Accuracy was the highest with a medical corpus during the complete training process, achieving 91.5%. Training on a newswire corpus achieved 75.3% only. Furthermore, an active learning technique has been adapted to the POS tagging task. The algorithm uses a POS tagger committee to isolate the sentences with the highest disagreement indexes for manual correction. However, the method was not able to reduce training and tagging times when compared to a random selection strategy. We encourage that future works employ some effort in order to annotate a small amount of random data in the domain of study, which should be enough for higher accuracy rates.
20841834	Bridging the semantics gap between terminologies, ontologies, and information models.
Stud Health Technol Inform  2010
SNOMED CT and other biomedical vocabularies provide semantic identifiers for all kinds of linguistic expressions, many of which cannot be considered terms in a strict sense. We analyzed such "non-terms" in SNOMED CT and concluded that many of them cannot be interpreted as directly referring to objects or processes, but rather to information entities. Discussing two approaches to represent information entities, viz. the OBO Information artifact ontology (IAO) and the HL7 v3 Reference Information Model (RIM), we propose an integrative solution for representing information entities in SNOMED CT, in a way that is still compatible with RIM and the IAO and uses moderately enhanced description logics.
20841845	Enhancing a taxonomy for health information technology: an exploratory study of user input towards folksonomy.
Stud Health Technol Inform  2010
The U.S. Agency for Healthcare Research and Quality has created a public website to disseminate critical information regarding its health information technology initiative. The website is maintained by AHRQ's Natiomal Resource Center (NRC) for Health Information Technology. In the latest continuous quality improvement project, the NRC used the site's search logs to extract user-generated search phrases. The phrases were then compared to the site's controlled vocabulary with respect to language, grammar, and search precision. Results of the comparison demonstrate that search log data can be a cost-effective way to improve controlled vocabularies as well as information retrieval. User-entered search phrases were found to also share many similarities with folksonomy tags.
20841846	The DebugIT core ontology: semantic integration of antibiotics resistance patterns.
Stud Health Technol Inform  2010
Antibiotics resistance development poses a significant problem in today's hospital care. Massive amounts of clinical data are being collected and stored in proprietary and unconnected systems in heterogeneous format. The DebugIT EU project promises to make this data geographically and semantically interoperable for case-based knowledge analysis approaches aiming at the discovery of patterns that help to align antibiotics treatment schemes. The semantic glue for this endeavor is DCO, an application ontology that enables data miners to query distributed clinical information systems in a semantically rich and content driven manner. DCO will hence serve as the core component of the interoperability platform for the DebugIT project. Here we present DCO and an approach thet uses the semantic web query language SPARQL to bind and ontologically query hospital database content using DCO and information model mediators. We provide a query example that indicates that ontological querying over heterogeneous information models is feasible via SPARQL construct- and resource mapping queries.
20841860	Towards an implicit treatment of periodically-repeated medical data.
Stud Health Technol Inform  2010
Temporal information plays a crucial role in medicine, so that in Medical Informatics there is an increasing awareness that suitable database approaches are needed to store and support it. Specifically, a great amount of clinical data (e.g., therapeutic data) are periodically repeated. Although an explicit treatment is possible in most cases, it causes severe storage and disk I/O problems. In this paper, we propose an innovative approach to cope with periodic medical data in an implicit way. We propose a new data model, representing periodic data in a compact (implicit) way, which is a consistent extension of TSQL2 consensus approach. Then, we identify some important types of temporal queries, and present query answering algorithms to answer them. We also sketch a temporal relational algebra for our approach. Finally, we show experimentally that our approach outperforms current explicit approaches.
20841867	A model-driven approach for biomedical data integration.
Stud Health Technol Inform  2010
A core challenge in biomedical data integration is to enable semantic interoperability between its various stakeholders as well as other interested parties. Promoting the adoption of worldwide accepted information standards along with common controlled terminologies is the right path to achieve this. Our paper describes a solution to this fundamental problem by proposing an approach to semantic data integration based on information models serving as a common language to represent health data coupled with technology that is able to represent the data semantics. We used the HL7 v3 Reference Information Model (RIM) [1] to derive a specific data model for the integrated data, the Web Ontology Language (OWL) [2] to build an ontology that harmonizes the metadata from the disparate data sources, the Unified Modeling Language (UML) [3] to model the data representation, and the Object Constraint Language (OCL) [4] to specify UML model constraints. To illustrate the approach, we use the Essential Hypertension Summary CDA document and related models from Hypergenes, a European Commission funded project [5] exploring the Essential Hypertension disease model.
20841869	Clinical task-specific query expansion for the retrieval of scientifically rigorous research documents.
Stud Health Technol Inform  2010
To support the practice of evidence-based medicine (EBM), clinically relevant and scientifically sound articles should be easily accessible. Due to the huge volume of medical literature and the low performance of present retrieval models, clinicians could only get relevant documents in the order of publication time. This study propose a new clinical task-specific retrieval technique that improves retrieval accuracy by exploiting clinical task-specific EBM terms to query expansion using co-occurrence analysis technique. The idea is aimed at selecting query expansion terms that are relevant to a specific clinical-task using task-specific EBM terms. Focusing on treatment and diagnosis tasks, the new method which was performed on the OHSUMED collection showed a further improved result than the existing method.
20841871	Pediatric pain management knowledge linkages: mapping experiential knowledge to explicit knowledge.
Stud Health Technol Inform  2010
The goal of this project is to augment clinician communication by connecting it to evidence-based research, providing explicit knowledge to corroborate the experiential knowledge shared between health care practitioners. The source of tacit knowledge sharing is the Pediatric Pain Mailing List (PPML), a forum for practicing clinicians to contact peers on the subject of pain in children. The messages, dating back to 1993, are processed for pertinent information and gathered together into threads. They are then parsed and connected to a set of MeSH keywords, which is used to search Pubmed and return a set of papers that correspond to the subject being discussed. The results are presented in an online forum, providing clinicians with an arena in which they can browse the archives of the PPML and connect those conversations to pertinent medical literature.
20841872	Retrieving similar cases from the medical literature - the ImageCLEF experience.
Stud Health Technol Inform  2010
An increasing number of clinicians, researchers, educators and patients routinely search for relevant medical images using search engines on the internet as well as in image archives and PACS systems. However, image retrieval is far less understood and developed compared to text-based searching. The ImageCLEF medical image retrieval task is an international challenge evaluation that enables researchers to assess and compare techniques for medical image retrieval using test collections. In this paper, we describe the development of the ImageCLEF medical image test collection, consisting of a database of images and their associated annotations, as well as a set of realistic search topics and relevance judgments obtained using a set of experts. 2009 was the sixth year for the ImageCLEF medical retrieval task and had strong participation from research groups across the globe. We will provide results from this year's evaluation and discuss the successes that we have had as well as challenges going forward.
20841889	A web service for enabling medical image retrieval integrated into a social medical image sharing platform.
Stud Health Technol Inform  2010
Content-based visual image access is in the process from a research domain towards real applications. So far, most image retrieval applications have been in one specialized domain such as lung CTs as diagnosis aid or for classification of general images based on anatomic region, modality, and view. This article describes the use of a content-based image retrieval system in connection with the medical image sharing platform MEDTING, so a data set with a very large variety. Similarity retrieval is possible for all cases of the social image sharing platform, so cases can be linked by either visual similarity or similarity in keywords. The visual retrieval search is based on the GIFT (GNU Image Finding Tool). The technology for updating the index with new images added by users employs RSS (Really Simple Syndication) feeds. The ARC (Advanced Resource Connector) middleware is used for the implementation of a web service for similarity retrieval, simplifying the integration of this service. Novelty of this article is the application/integration and image updating strategy. Retrieval methods themselves employ existing techniques that are all open source and can easily be reproduced.
20841890	Indexing the medical open access literature for textual and content-based visual retrieval.
Stud Health Technol Inform  2010
Over the past few years an increasing amount of scientific journals have been created in an open access format. Particularly in the medical field the number of openly accessible journals is enormous making a wide body of knowledge available for analysis and retrieval. Part of the trend towards open access publications can be linked to funding bodies such as the NIH&lt;sup&gt;1&lt;/sup&gt; (National Institutes of Health) and the Swiss National Science Foundation (SNF&lt;sup&gt;2&lt;/sup&gt;) requiring funded projects to make all articles of funded research available publicly. This article describes an approach to make part of the knowledge of open access journals available for retrieval including the textual information but also the images contained in the articles. For this goal all articles of 24 journals related to medical informatics and medical imaging were crawled from the web pages of BioMed Central. Text and images of the PDF (Portable Document Format) files were indexed separately and a web-based retrieval interface allows for searching via keyword queries or by visual similarity queries. Starting point for a visual similarity query can be an image on the local hard disk that is uploaded or any image found via the textual search. Search for similar documents is also possible.
20842840	[Research on information extraction of electronic medical records in Chinese].
Sheng Wu Yi Xue Gong Cheng Xue Za Zhi  2010Aug
This is a research to enhance the application of natural language understanding and ontology in the Chinese medical text semantic annotation and content analysis, and so to provide technology support for the computer-readable electronic medical records (EMR). The Chinese EMR information extraction and statistical analysis of related subjects in accordance to the user's demands were performed through building the named entity rules, the classified word list and field ontology by using GATE platform on the basis of EMR text set's construction and pre-processing. The automatic and artificial semantic annotation of EMR text set was implemented. The situation of drugs used in medicinal treatment and the distribution of patients' age and sex were obtained. The ontology-based semantic information extraction can improve the function of computer for text understanding, and the discovery of knowledge in EMR through field ontology is feasible.
20842867	[A preliminary study on data mining techniques for utilizing the breast ultrasound database].
Sheng Wu Yi Xue Gong Cheng Xue Za Zhi  2010Aug
Based on the breast ultrasound database of West China Hospital from January 1, 2002 to December 31, 2007, a study of data mining techniques for utilizing the diagnostic information of breast ultrasound and breast pathology was carried out. An innovative computerized retrieval system was invented. With the visual user interface of the system, the data of benignancy or malignancy diagnosed by ultrasound and pathologic examination, and the data on the diagnostic correlation of ultrasound and pathology were obtained, respectively. The qualities of data mining were 99. 98%-100%. By means of the retrieval system, the users can secure numerous data from the breast ultrasound database rapidly and accurately; so it contributes to the rational utilization of information from medical database for serving various medical studies. This method may also be helpful for doctors to utilize ultrasound database in other fields.
20847386	A dynamic texture-based approach to recognition of facial actions and their temporal models.
IEEE Trans Pattern Anal Mach Intell  2010Nov
In this work, we propose a dynamic texture-based approach to the recognition of facial Action Units (AUs, atomic facial gestures) and their temporal models (i.e., sequences of temporal segments: neutral, onset, apex, and offset) in near-frontal-view face videos. Two approaches to modeling the dynamics and the appearance in the face region of an input video are compared: an extended version of Motion History Images and a novel method based on Nonrigid Registration using Free-Form Deformations (FFDs). The extracted motion representation is used to derive motion orientation histogram descriptors in both the spatial and temporal domain. Per AU, a combination of discriminative, frame-based GentleBoost ensemble learners and dynamic, generative Hidden Markov Models detects the presence of the AU in question and its temporal segments in an input image sequence. When tested for recognition of all 27 lower and upper face AUs, occurring alone or in combination in 264 sequences from the MMI facial expression database, the proposed method achieved an average event recognition accuracy of 89.2 percent for the MHI method and 94.3 percent for the FFD method. The generalization performance of the FFD method has been tested using the Cohn-Kanade database. Finally, we also explored the performance on spontaneous expressions in the Sensitive Artificial Listener data set.
20847389	Computing accurate correspondences across groups of images.
IEEE Trans Pattern Anal Mach Intell  2010Nov
Groupwise image registration algorithms seek to establish dense correspondences between sets of images. Typically, they involve iteratively improving the registration between each image and an evolving mean. A variety of methods have been proposed, which differ in their choice of objective function, representation of deformation field, and optimization methods. Given the complexity of the task, the final accuracy is significantly affected by the choices made for each component. Here, we present a groupwise registration algorithm which can take advantage of the statistics of both the image intensities and the range of shapes across the group to achieve accurate matching. By testing on large sets of images (in both 2D and 3D), we explore the effects of using different image representations and different statistical shape constraints. We demonstrate that careful choice of such representations can lead to significant improvements in overall performance.
20847394	Stereo matching with Mumford-Shah regularization and occlusion handling.
IEEE Trans Pattern Anal Mach Intell  2010Nov
This paper addresses the problem of correspondence establishment in binocular stereo vision. We suggest a novel spatially continuous approach for stereo matching based on the variational framework. The proposed method suggests a unique regularization term based on Mumford-Shah functional for discontinuity preserving, combined with a new energy functional for occlusion handling. The evaluation process is based on concurrent minimization of two coupled energy functionals, one for domain segmentation (occluded versus visible) and the other for disparity evaluation. In addition to a dense disparity map, our method also provides an estimation for the half-occlusion domain and a discontinuity function allocating the disparity/depth boundaries. Two new constraints are introduced improving the revealed discontinuity map. The experimental tests include a wide range of real data sets from the Middlebury stereo database. The results demonstrate the capability of our method in calculating an accurate disparity function with sharp discontinuities and occlusion map recovery. Significant improvements are shown compared to a recently published variational stereo approach. A comparison on the Middlebury stereo benchmark with subpixel accuracies shows that our method is currently among the top-ranked stereo matching algorithms.
20727819	A systematic review of interventions promoting clinical information retrieval technology (CIRT) adoption by healthcare professionals.
Int J Med Inform 20100819 2010Oct
This paper presents the evidence on the effectiveness of interventions promoting the use of clinical information retrieval technologies (CIRTs) by healthcare professionals. We electronically searched articles published between January 1990 and March 2008 using following inclusion criteria: (1) participants were healthcare professionals; (2) specific intervention promoted CIRT adoption; (3) studies were randomised controlled trials, controlled clinical trials, controlled before and after studies or interrupted time series analyses; and (4) they objectively reporting measured outcomes on CIRT use. We found nine studies focusing on CIRT use. Main outcomes measured were searching skills and/or frequency of use of electronic databases by healthcare professionals. Three studies reported a positive effect of the intervention on CIRT use, one showed a positive impact post-intervention, and four studies failed to demonstrate significant intervention effect. The ninth study examined financial disincentives, and found a significant negative effect of introducing user fees for searching MEDLINE in clinical settings. A meta-analysis showed that educational meetings were the only type of interventions reporting consistent positive effects on CIRT adoption. CIRT is an information and communication technology commonly used in healthcare settings. Interventions promoting CIRT adoption by healthcare professionals have shown some success in improving searching skills and use of electronic databases. However, the effectiveness of these interventions remains uncertain and more rigorous studies are needed.
20598000	Consumers' disease information-seeking behaviour on the Internet in Korea.
J Clin Nurs  2010Oct
This study was conducted to explain the relationships of the factors affecting consumers' disease information-seeking behaviour on the Internet in Korea. Similar to other countries, Korea is facing an increasing use of Internet as a resource of health information. With the paradigm shifts towards consumer-centred health service, it is expected that more health care consumers will use the Internet actively in the future. A survey was conducted using a self-selected convenience sample. A conceptual model was derived by extending technology acceptance model and tested via structural equation modelling. The overall goodness of fit of the conceptual model was acceptable. Consumers' health consciousness, perceived health risk and Internet health information use efficacy were found to influence consumers' beliefs, attitude and intention of use disease information on the Internet. But Internet health information use efficacy did not significantly influence perceived usefulness. It was also identified that consumers' perceived credibility of the information in the websites was the main determinant in forming of attitude towards disease information on the Internet. Technology acceptance model has been extended and examined successfully in explaining consumers' disease information-seeking behaviour on the Internet. It was found that consumers' cognitive and affective characteristics, determined as initiators in health-related behaviour, also impacted consumers' disease information-seeking behaviour on the Internet. These findings may be used to help nurses to predict and identify the factors affecting individual's use of disease information on the Internet. Based on this knowledge, nurses will be able to develop nursing intervention programmes for the patients' health management.
20727159	Gene Expression Browser: large-scale and cross-experiment microarray data integration, management, search &amp; visualization.
BMC Bioinformatics 20100820 2010
In the last decade, a large amount of microarray gene expression data has been accumulated in public repositories. Integrating and analyzing high-throughput gene expression data have become key activities for exploring gene functions, gene networks and biological pathways. Effectively utilizing these invaluable microarray data remains challenging due to a lack of powerful tools to integrate large-scale gene-expression information across diverse experiments and to search and visualize a large number of gene-expression data points. Gene Expression Browser is a microarray data integration, management and processing system with web-based search and visualization functions. An innovative method has been developed to define a treatment over a control for every microarray experiment to standardize and make microarray data from different experiments homogeneous. In the browser, data are pre-processed offline and the resulting data points are visualized online with a 2-layer dynamic web display. Users can view all treatments over control that affect the expression of a selected gene via Gene View, and view all genes that change in a selected treatment over control via treatment over control View. Users can also check the changes of expression profiles of a set of either the treatments over control or genes via Slide View. In addition, the relationships between genes and treatments over control are computed according to gene expression ratio and are shown as co-responsive genes and co-regulation treatments over control. Gene Expression Browser is composed of a set of software tools, including a data extraction tool, a microarray data-management system, a data-annotation tool, a microarray data-processing pipeline, and a data search &amp; visualization tool. The browser is deployed as a free public web service (http://www.ExpressionBrowser.com) that integrates 301 ATH1 gene microarray experiments from public data repositories (viz. the Gene Expression Omnibus repository at the National Center for Biotechnology Information and Nottingham Arabidopsis Stock Center). The set of Gene Expression Browser software tools can be easily applied to the large-scale expression data generated by other platforms and in other species.
20807438	Relations as patterns: bridging the gap between OBO and OWL.
BMC Bioinformatics 20100831 2010
most biomedical ontologies are represented in the OBO Flatfile Format, which is an easy-to-use graph-based ontology language. The semantics of the OBO Flatfile Format 1.2 enforces a strict predetermined interpretation of relationship statements between classes. It does not allow flexible specifications that provide better approximations of the intuitive understanding of the considered relations. If relations cannot be accurately expressed then ontologies built upon them may contain false assertions and hence lead to false inferences. Ontologies in the OBO Foundry must formalize the semantics of relations according to the OBO Relationship Ontology (RO). Therefore, being able to accurately express the intended meaning of relations is of crucial importance. Since the Web Ontology Language (OWL) is an expressive language with a formal semantics, it is suitable to de ne the meaning of relations accurately. we developed a method to provide definition patterns for relations between classes using OWL and describe a novel implementation of the RO based on this method. We implemented our extension in software that converts ontologies in the OBO Flatfile Format to OWL, and also provide a prototype to extract relational patterns from OWL ontologies using automated reasoning. The conversion software is freely available at http://bioonto.de/obo2owl, and can be accessed via a web interface. explicitly defining relations permits their use in reasoning software and leads to a more flexible and powerful way of representing biomedical ontologies. Using the extended langua0067e and semantics avoids several mistakes commonly made in formalizing biomedical ontologies, and can be used to automatically detect inconsistencies. The use of our method enables the use of graph-based ontologies in OWL, and makes complex OWL ontologies accessible in a graph-based form. Thereby, our method provides the means to gradually move the representation of biomedical ontologies into formal knowledge representation languages that incorporates an explicit semantics. Our method facilitates the use of OWL-based software in the back-end while ontology curators may continue to develop ontologies with an OBO-style front-end.
20865540	Using Web and social media for influenza surveillance.
Adv. Exp. Med. Biol.  2010
Analysis of Google influenza-like-illness (ILI) search queries has shown a strongly correlated pattern with Centers for Disease Control (CDC) and Prevention seasonal ILI reporting data. Web and social media provide another resource to detect increases in ILI. This paper evaluates trends in blog posts that discuss influenza. Our key finding is that from 5th October 2008 to 31st January 2009, a high correlation exists between the frequency of posts, containing influenza keywords, per week and CDC influenza-like-illness surveillance data.
20865550	Toward automating an inference model on unstructured terminologies: OXMIS case study.
Adv. Exp. Med. Biol.  2010
Most modern biomedical vocabularies employ some hierarchical representation that provides a "broader/narrower" meaning relationship among the "codes" or "concepts" found within them. Often, however, we may find within the clinical setting the creation and curation of unstructured custom vocabularies used in the everyday practice of classifying and categorizing clinical data and findings.A significant and widely used example of this lies in the General Practice Research Database which makes use of the Oxford Medical Information Systems (OXMIS) coding scheme to represent drugs and medical conditions. This scheme is intrinsically unstructured, is generally regarded as disorganized, and is not amenable to comparison with other hierarchically structured medical coding schemes. To improve processes of data analysis and extraction, we define a semantically meaningful representation of the OXMIS codes by way of the Unified Medical Language System (UMLS) Metathesaurus. A structure-imposing ontology mapping is created, and this process provides a complete illustration of a general semantic mapping technique applicable to unstructured biomedical terminologies.
20865551	Semantic content-based recommendations using semantic graphs.
Adv. Exp. Med. Biol.  2010
Recommender systems (RSs) can be useful for suggesting items that might be of interest to specific users. Most existing content-based recommendation (CBR) systems are designed to recommend items based on text content, and the items in these systems are usually described with keywords. However, similarity evaluations based on keywords suffer from the ambiguity of natural languages. We present a semantic CBR method that uses Semantic Web technologies to recommend items that are more similar semantically with the items that the user prefers. We use semantic graphs to represent the items and we calculate the similarity scores for each pair of semantic graphs using an inverse graph frequency algorithm. The items having higher similarity scores to the items that are known to be preferred by the user are recommended.
20702400	Identifying informative subsets of the Gene Ontology with information bottleneck methods.
Bioinformatics 20100811 2010Oct1
The Gene Ontology (GO) is a controlled vocabulary designed to represent the biological concepts pertaining to gene products. This study investigates the methods for identifying informative subsets of GO terms in an automatic and objective fashion. This task in turn requires addressing the following issues: how to represent the semantic context of GO terms, what metrics are suitable for measuring the semantic differences between terms, how to identify an informative subset that retains as much as possible of the original semantic information of GO. We represented the semantic context of a GO term using the word-usage-profile associated with the term, which enables one to measure the semantic differences between terms based on the differences in their semantic contexts. We further employed the information bottleneck methods to automatically identify subsets of GO terms that retain as much as possible of the semantic information in an annotation database. The automatically retrieved informative subsets align well with an expert-picked GO slim subset, cover important concepts and proteins, and enhance literature-based GO annotation. http://carcweb.musc.edu/TextminingProjects/.
20562044	Two-dimensional intraventricular flow mapping by digital processing conventional color-Doppler echocardiography images.
IEEE Trans Med Imaging 20100617 2010Oct
Doppler echocardiography remains the most extended clinical modality for the evaluation of left ventricular (LV) function. Current Doppler ultrasound methods, however, are limited to the representation of a single flow velocity component. We thus developed a novel technique to construct 2D time-resolved (2D+t) LV velocity fields from conventional transthoracic clinical acquisitions. Combining color-Doppler velocities with LV wall positions, the cross-beam blood velocities were calculated using the continuity equation under a planar flow assumption. To validate the algorithm, 2D Doppler flow mapping and laser particle image velocimetry (PIV) measurements were carried out in an atrio-ventricular duplicator. Phase-contrast magnetic resonance (MR) acquisitions were used to measure in vivo the error due to the 2D flow assumption and to potential scan-plane misalignment. Finally, the applicability of the Doppler technique was tested in the clinical setting. In vitro experiments demonstrated that the new method yields an accurate quantitative description of the main vortex that forms during the cardiac cycle (mean error for vortex radius, position and circulation). MR image analysis evidenced that the error due to the planar flow assumption is close to 15% and does not preclude the characterization of major vortex properties neither in the normal nor in the dilated LV. These results are yet to be confirmed by a head-to-head clinical validation study. Clinical Doppler studies showed that the method is readily applicable and that a single large anterograde vortex develops in the healthy ventricle while supplementary retrograde swirling structures may appear in the diseased heart. The proposed echocardiographic method based on the continuity equation is fast, clinically-compliant and does not require complex training. This technique will potentially enable investigators to study of additional quantitative aspects of intraventricular flow dynamics in the clinical setting by high-throughput processing conventional color-Doppler images.
20416344	Data storage based on photochromic and photoconvertible fluorescent proteins.
J. Biotechnol. 20100421 2010Sep15
The recent discovery of photoconvertible and photoswitchable fluorescent proteins (PCFPs and RSFPs, respectively) that can undergo photoinduced changes of their absorption/emission spectra opened new research possibilities in subdiffraction microscopy and optical data storage. Here we demonstrate the proof-of-principle for read only and rewritable data storage both in 2D and 3D, using PCFPs and RSFPs. The irreversible burning of information was achieved by photoconverting from green to red defined areas in a layer of the PCFP Kaede. Data were also written and erased several times in layers of the photochromic fluorescent protein Dronpa. Using IrisFP, which combines the properties of PCFPs and RSFPs, we performed the first encoding of data in four colours using only one type of fluorescent protein. Finally, three-dimensional optical data storage was demonstrated using three mutants of EosFP (d1EosFP, mEosFP and IrisFP) in their crystalline form. Two-photon excitation allowed the precise addressing of regions of interest (ROIs) within the three-dimensional crystalline matrix without excitation of out-of-focus optical planes. Hence, this contribution highlights several data storage schemes based on the remarkable properties of PCFPs/RSFPs.
19954132	Partnering with your library to strengthen nursing research.
J Nurs Educ 20100305 2010Mar
Nurses must know how to locate and evaluate health information to optimize their professional practice. However, much of the information available online and in print lacks credibility, and navigating professional research databases can prove complex. As more nursing students and nurse educators move from centralized, brick-and-mortar campuses to satellite locations and online courses, the need for adaptable library services becomes pronounced. This article describes a program that serves decentralized nursing students and nurse educators. Relationship building, one-to-one contact, and flexibility are program hallmarks. Services provided by the program and evaluation methods are discussed, and ways of building collaboration are recommended.
20651709	A cross-sectional survey to determine the ages of emergence of permanent teeth of Caucasian children of the Colchester area of the UK.
Br Dent J 20100723 2010Sep25
To determine the ages of emergence of permanent teeth of Caucasian children of the Colchester area of the UK. Emergence data for all permanent teeth except third molars was collected from 12,395 children between four and 15 years of age, in the Colchester area of the UK between April 1998 and July 2001. A simple, robust, easy-to-follow experimental protocol was devised to provide reliable data collection. The ages of emergence of the permanent teeth in this study which covered the period 1998-2001 in this Colchester population are later than earlier studies conducted throughout the twentieth century. If confirmed, the results of this study would suggest that contemporary children's dental development is retarded which may have implications for their general health. The type of study reported here may have long-term value in rapidly identifying trends in children's development of public health importance.
20872122	Therapeutic evaluation on complex interventions of integrative medicine and the potential role of data mining.
Chin J Integr Med 20100925 2010Oct
It is a common view that the integration of Chinese medicine (CM) and modern Western medicine is an efficient way to facilitate the development of CM. Integrative medicine is a kind of complex interventions. Scientific therapeutic evaluation plays a crucial role in making integrative medicine universally acknowledged. However, the modern method of clinical study, which is based on the concept of evidence-based medicine, mostly focuses on the population characteristics and single interventional factor. As a result, it is difficult for this method to totally adapt to the clinical features of CM and integrative medicine as complex interventions. One possible way to solve this issue is to improve and integrate with the existing method and to utilize the evaluation model on complex interventions from abroad. As an interdisciplinary technique, data mining involves database technology, artificial intelligence, machine learning, statistics, neural network and some other latest technologies, and has been widely used in the field of CM. Therefore, the application of data mining in the therapeutic evaluation of integrative medicine has broad prospects.
20796305	A standard variation file format for human genome sequences.
Genome Biol. 20100826 2010
Here we describe the Genome Variation Format (GVF) and the 10Gen dataset. GVF, an extension of Generic Feature Format version 3 (GFF3), is a simple tab-delimited format for DNA variant files, which uses Sequence Ontology to describe genome variation data. The 10Gen dataset, ten human genomes in GVF format, is freely available for community analysis from the Sequence Ontology website and from an Amazon elastic block storage (EBS) snapshot for use in Amazon's EC2 cloud computing environment.
20805069	Summarizing green practices in U.S. hospitals.
Hosp Top  2010 Jul-Sep
The author used an Internet search to locate hospitals presently practicing green policies. She also included results from a Practice Greenhealth survey. Governmental antecedents and benefits of the green movement were also included. With limited documentation of the green movement in hospitals, the primary goal was to provide a compilation of policies and procedures that may be useful to hospitals considering the implementation of green practices. To that end, approximately 10 case hospitals were highlighted, along with a team of hospitals that collaborated on environmental and patient initiatives. The Practice Greenhealth survey of over 700 member hospitals highlighted additional progress that hospitals have achieved in the green movement.
20812460	Electronic medical records and cost efficiency in hospital medical-surgical units.
Inquiry  2010Summer
This study examines the impact of electronic medical records (EMRs) on cost efficiency in hospital medical-surgical units. Using panel data on California hospitals from 1998 to 2007, we employed stochastic frontier analysis (SFA) to estimate the relationships between EMR implementation and the cost inefficiency of medical-surgical units. We categorized EMR implementation into three stages based on the level of sophistication. We also examined the effects of specific EMR systems on cost inefficiency. Our SFA models addressed potential bias from unobserved heterogeneity and heteroskedasticity. EMR Stages 1 and 2, nursing documentation, electronic medication administration records, and clinical decision support were associated with significantly higher inefficiency.
20615852	Heterogeneity in search strategies among Cochrane acupuncture reviews: is there room for improvement?
Acupunct Med 20100628 2010Sep
Given the international focus and rigorous literature searches employed in Cochrane systematic reviews, this study was undertaken to evaluate strategies employed in Cochrane reviews and protocols assessing acupuncture as a primary or secondary intervention. The Cochrane Collaboration of systematic reviews was searched in February 2009 for all reviews and protocols including information on acupuncture. Information was abstracted from all retrieved articles on review status, type and number of English and Chinese language databases searched, participation of at least one Chinese speaking author and language restriction. Frequencies were calculated and bivariate analyses were performed stratifying on interventions of interest to assess differences in search strategy techniques, language restrictions and results. The search retrieved 68 titles, including 48 completed reviews, 17 protocols and three previously withdrawn titles. Acupuncture was the primary intervention of interest in 44/65 (67.7%) of the retrieved reviews and protocols. While all articles searched at least one English language database, only 26/65 (40.0%) articles searched Chinese language databases. Significantly more articles where acupuncture was the primary intervention of interest searched Chinese language databases (53% vs 9%, p&lt;0.01). Inconclusive findings as to the effectiveness of acupuncture were found in 28/48 (58.3%) of all completed reviews; this type of finding was more common in reviews which did not search any Chinese language databases. It is important for reviews assessing the effectiveness of acupuncture to search Chinese language databases. The Cochrane Collaboration should develop specific criteria for Chinese language search strategies to ensure the continued publication of high-quality reviews.
20813626	Medical case retrieval from a committee of decision trees.
IEEE Trans Inf Technol Biomed  2010Sep
A novel content-based information retrieval framework, designed to cover several medical applications, is presented in this paper. The presented framework allows the retrieval of possibly incomplete medical cases consisting of several images together with semantic information. It relies on a committee of decision trees, decision support tools well suited to process this type of information. In our proposed framework, images are characterized by their digital content. It was applied to two heterogeneous medical datasets for computer-aided diagnoses: a diabetic retinopathy follow-up dataset (DRD) and a mammography-screening dataset (DDSM). Measure of precision among the top five retrieved results of 0.788 + or - 0.137 and 0.869 + or - 0.161 was obtained on DRD and DDSM, respectively. On DRD, for instance, it increases by half the retrieval of single images.
20382265	Reflective random indexing for semi-automatic indexing of the biomedical literature.
J Biomed Inform 20100409 2010Oct
The rapid growth of biomedical literature is evident in the increasing size of the MEDLINE research database. Medical Subject Headings (MeSH), a controlled set of keywords, are used to index all the citations contained in the database to facilitate search and retrieval. This volume of citations calls for efficient tools to assist indexers at the US National Library of Medicine (NLM). Currently, the Medical Text Indexer (MTI) system provides assistance by recommending MeSH terms based on the title and abstract of an article using a combination of distributional and vocabulary-based methods. In this paper, we evaluate a novel approach toward indexer assistance by using nearest neighbor classification in combination with Reflective Random Indexing (RRI), a scalable alternative to the established methods of distributional semantics. On a test set provided by the NLM, our approach significantly outperforms the MTI system, suggesting that the RRI approach would make a useful addition to the current methodologies.
20394839	On the query reformulation technique for effective MEDLINE document retrieval.
J Biomed Inform 20100413 2010Oct
Improving the retrieval accuracy of MEDLINE documents is still a challenging issue due to low retrieval precision. Focusing on a query expansion technique based on pseudo-relevance feedback (PRF), this paper addresses the problem by systematically examining the effects of expansion term selection and adjustment of the term weights of the expanded query using a set of MEDLINE test documents called OHSUMED. Implementing a baseline information retrieval system based on the Okapi BM25 retrieval model, we compared six well-known term ranking algorithms for useful expansion term selection and then compared traditional term reweighting algorithms with our new variant of the standard Rocchio's feedback formula, which adopts a group-based weighting scheme. Our experimental results on the OHSUMED test collection showed a maximum improvement of 20.2% and 20.4% for mean average precision and recall measures over unexpanded queries when terms were expanded using a co-occurrence analysis-based term ranking algorithm in conjunction with our term reweighting algorithm (p-value&lt;0.05). Our study shows the behaviors of different query reformulation techniques that can be utilized for more effective MEDLINE document retrieval.
20435161	DSGeo: software tools for cross-platform analysis of gene expression data in GEO.
J Biomed Inform 20100507 2010Oct
The Gene Expression Omnibus (GEO) is the largest resource of public gene expression data. While GEO enables data browsing, query and retrieval, additional tools can help realize its potential for aggregating and comparing data across multiple studies and platforms. This paper describes DSGeo-a collection of valuable tools that were developed for annotating, aggregating, integrating, and analyzing data deposited in GEO. The core set of tools include a Relational Database, a Data Loader, a Data Browser, and an Expression Combiner and Analyzer. The application enables querying for specific sample characteristics and identifying studies containing samples that match the query. The Expression Combiner application enables normalization and aggregation of data from these samples and returns these data to the user after filtering, according to the user's preferences. The Expression Analyzer allows simple statistical comparisons between groups of data. This seamless integration makes annotated cross-platform data directly available for analysis.
20561912	An approach for the semantic interoperability of ISO EN 13606 and OpenEHR archetypes.
J Biomed Inform 20100601 2010Oct
The communication between health information systems of hospitals and primary care organizations is currently an important challenge to improve the quality of clinical practice and patient safety. However, clinical information is usually distributed among several independent systems that may be syntactically or semantically incompatible. This fact prevents healthcare professionals from accessing clinical information of patients in an understandable and normalized way. In this work, we address the semantic interoperability of two EHR standards: OpenEHR and ISO EN 13606. Both standards follow the dual model approach which distinguishes information and knowledge, this being represented through archetypes. The solution presented here is capable of transforming OpenEHR archetypes into ISO EN 13606 and vice versa by combining Semantic Web and Model-driven Engineering technologies. The resulting software implementation has been tested using publicly available collections of archetypes for both standards.
20637898	Unsupervised grammar induction and similarity retrieval in medical language processing using the Deterministic Dynamic Associative Memory (DDAM) model.
J Biomed Inform 20100715 2010Oct
This paper is an overview of unsupervised grammar induction and similarity retrieval, two fundamental information processing functions of importance to medical language processing applications and to the construction of intelligent medical information systems. Existing literature with a focus on text segmentation tasks is reviewed. The review includes a comparison of existing approaches and reveals the longstanding interest in these traditionally distinct topics despite the significant computational challenges that characterizes them. Further, a unifying approach to unsupervised representation and processing of sequential data, the Deterministic Dynamic Associative Memory (DDAM) model, is introduced and described theoretically from both structural and functional perspectives. The theoretical descriptions of the model are complemented by a selection and discussion of interesting experimental results in the tasks of unsupervised grammar induction and similarity retrieval with applications to medical language processing. Notwithstanding the challenges associated with the evaluation of unsupervised information-processing models, it is concluded that the DDAM model demonstrates interesting properties that encourage further investigations in both theoretical and applied contexts.
20819853	Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications.
J Am Med Inform Assoc  2010 Sep-Oct
We aim to build and evaluate an open-source natural language processing system for information extraction from electronic medical record clinical free-text. We describe and evaluate our system, the clinical Text Analysis and Knowledge Extraction System (cTAKES), released open-source at http://www.ohnlp.org. The cTAKES builds on existing open-source technologies-the Unstructured Information Management Architecture framework and OpenNLP natural language processing toolkit. Its components, specifically trained for the clinical domain, create rich linguistic and semantic annotations. Performance of individual components: sentence boundary detector accuracy=0.949; tokenizer accuracy=0.949; part-of-speech tagger accuracy=0.936; shallow parser F-score=0.924; named entity recognizer and system-level evaluation F-score=0.715 for exact and 0.824 for overlapping spans, and accuracy for concept mapping, negation, and status attributes for exact and overlapping spans of 0.957, 0.943, 0.859, and 0.580, 0.939, and 0.839, respectively. Overall performance is discussed against five applications. The cTAKES annotations are the foundation for methods and modules for higher-level semantic processing of clinical free-text.
20819854	Extracting medication information from clinical text.
J Am Med Inform Assoc  2010 Sep-Oct
The Third i2b2 Workshop on Natural Language Processing Challenges for Clinical Records focused on the identification of medications, their dosages, modes (routes) of administration, frequencies, durations, and reasons for administration in discharge summaries. This challenge is referred to as the medication challenge. For the medication challenge, i2b2 released detailed annotation guidelines along with a set of annotated discharge summaries. Twenty teams representing 23 organizations and nine countries participated in the medication challenge. The teams produced rule-based, machine learning, and hybrid systems targeted to the task. Although rule-based systems dominated the top 10, the best performing system was a hybrid. Of all medication-related fields, durations and reasons were the most difficult for all systems to detect. While medications themselves were identified with better than 0.75 F-measure by all of the top 10 systems, the best F-measure for durations and reasons were 0.525 and 0.459, respectively. State-of-the-art natural language processing systems go a long way toward extracting medication names, dosages, modes, and frequencies. However, they are limited in recognizing duration and reason fields and would benefit from future research.
20819855	Community annotation experiment for ground truth generation for the i2b2 medication challenge.
J Am Med Inform Assoc  2010 Sep-Oct
Within the context of the Third i2b2 Workshop on Natural Language Processing Challenges for Clinical Records, the authors (also referred to as 'the i2b2 medication challenge team' or 'the i2b2 team' for short) organized a community annotation experiment. For this experiment, the authors released annotation guidelines and a small set of annotated discharge summaries. They asked the participants of the Third i2b2 Workshop to annotate 10 discharge summaries per person; each discharge summary was annotated by two annotators from two different teams, and a third annotator from a third team resolved disagreements. In order to evaluate the reliability of the annotations thus produced, the authors measured community inter-annotator agreement and compared it with the inter-annotator agreement of expert annotators when both the community and the expert annotators generated ground truth based on pooled system outputs. For this purpose, the pool consisted of the three most densely populated automatic annotations of each record. The authors also compared the community inter-annotator agreement with expert inter-annotator agreement when the experts annotated raw records without using the pool. Finally, they measured the quality of the community ground truth by comparing it with the expert ground truth. The authors found that the community annotators achieved comparable inter-annotator agreement to expert annotators, regardless of whether the experts annotated from the pool. Furthermore, the ground truth generated by the community obtained F-measures above 0.90 against the ground truth of the experts, indicating the value of the community as a source of high-quality ground truth even on intricate and domain-specific annotation tasks.
20819856	High accuracy information extraction of medication information from clinical notes: 2009 i2b2 medication extraction challenge.
J Am Med Inform Assoc  2010 Sep-Oct
Medication information comprises a most valuable source of data in clinical records. This paper describes use of a cascade of machine learners that automatically extract medication information from clinical records. Authors developed a novel supervised learning model that incorporates two machine learning algorithms and several rule-based engines. Evaluation of each step included precision, recall and F-measure metrics. The final outputs of the system were scored using the i2b2 workshop evaluation metrics, including strict and relaxed matching with a gold standard. Evaluation results showed greater than 90% accuracy on five out of seven entities in the name entity recognition task, and an F-measure greater than 95% on the relationship classification task. The strict micro averaged F-measure for the system output achieved best submitted performance of the competition, at 85.65%. Clinical staff will only use practical processing systems if they have confidence in their reliability. Authors estimate that an acceptable accuracy for a such a working system should be approximately 95%. This leaves a significant performance gap of 5 to 10% from the current processing capabilities. A multistage method with mixed computational strategies using a combination of rule-based classifiers and statistical classifiers seems to provide a near-optimal strategy for automated extraction of medication information from clinical records.
20819857	Integrating existing natural language processing tools for medication extraction from discharge summaries.
J Am Med Inform Assoc  2010 Sep-Oct
To develop an automated system to extract medications and related information from discharge summaries as part of the 2009 i2b2 natural language processing (NLP) challenge. This task required accurate recognition of medication name, dosage, mode, frequency, duration, and reason for drug administration. We developed an integrated system using several existing NLP components developed at Vanderbilt University Medical Center, which included MedEx (to extract medication information), SecTag (a section identification system for clinical notes), a sentence splitter, and a spell checker for drug names. Our goal was to achieve good performance with minimal to no specific training for this document corpus; thus, evaluating the portability of those NLP tools beyond their home institution. The integrated system was developed using 17 notes that were annotated by the organizers and evaluated using 251 notes that were annotated by participating teams. The i2b2 challenge used standard measures, including precision, recall, and F-measure, to evaluate the performance of participating systems. There were two ways to determine whether an extracted textual finding is correct or not: exact matching or inexact matching. The overall performance for all six types of medication-related findings across 251 annotated notes was considered as the primary metric in the challenge. Our system achieved an overall F-measure of 0.821 for exact matching (0.839 precision; 0.803 recall) and 0.822 for inexact matching (0.866 precision; 0.782 recall). The system ranked second out of 20 participating teams on overall performance at extracting medications and related information. The results show that the existing MedEx system, together with other NLP components, can extract medication information in clinical text from institutions other than the site of algorithm development with reasonable performance.
20819858	Medication information extraction with linguistic pattern matching and semantic rules.
J Am Med Inform Assoc  2010 Sep-Oct
This study presents a system developed for the 2009 i2b2 Challenge in Natural Language Processing for Clinical Data, whose aim was to automatically extract certain information about medications used by a patient from his/her medical report. The aim was to extract the following information for each medication: name, dosage, mode/route, frequency, duration and reason. The system implements a rule-based methodology, which exploits typical morphological, lexical, syntactic and semantic features of the targeted information. These features were acquired from the training dataset and public resources such as the UMLS and relevant web pages. Information extracted by pattern matching was combined together using context-sensitive heuristic rules. The system was applied to a set of 547 previously unseen discharge summaries, and the extracted information was evaluated against a manually prepared gold standard consisting of 251 documents. The overall ranking of the participating teams was obtained using the micro-averaged F-measure as the primary evaluation metric. The implemented method achieved the micro-averaged F-measure of 81% (with 86% precision and 77% recall), which ranked this system third in the challenge. The significance tests revealed the system's performance to be not significantly different from that of the second ranked system. Relative to other systems, this system achieved the best F-measure for the extraction of duration (53%) and reason (46%). Based on the F-measure, the performance achieved (81%) was in line with the initial agreement between human annotators (82%), indicating that such a system may greatly facilitate the process of extracting relevant information from medical records by providing a solid basis for a manual review process.
20819859	Extracting Rx information from clinical narrative.
J Am Med Inform Assoc  2010 Sep-Oct
The authors used the i2b2 Medication Extraction Challenge to evaluate their entity extraction methods, contribute to the generation of a publicly available collection of annotated clinical notes, and start developing methods for ontology-based reasoning using structured information generated from the unstructured clinical narrative. Extraction of salient features of medication orders from the text of de-identified hospital discharge summaries was addressed with a knowledge-based approach using simple rules and lookup lists. The entity recognition tool, MetaMap, was combined with dose, frequency, and duration modules specifically developed for the Challenge as well as a prototype module for reason identification. Evaluation metrics and corresponding results were provided by the Challenge organizers. The results indicate that robust rule-based tools achieve satisfactory results in extraction of simple elements of medication orders, but more sophisticated methods are needed for identification of reasons for the orders and durations. Owing to the time constraints and nature of the Challenge, some obvious follow-on analysis has not been completed yet. The authors plan to integrate the new modules with MetaMap to enhance its accuracy. This integration effort will provide guidance in retargeting existing tools for better processing of clinical text.
20819860	Improving textual medication extraction using combined conditional random fields and rule-based systems.
J Am Med Inform Assoc  2010 Sep-Oct
In the i2b2 Medication Extraction Challenge, medication names together with details of their administration were to be extracted from medical discharge summaries. The task of the challenge was decomposed into three pipelined components: named entity identification, context-aware filtering and relation extraction. For named entity identification, first a rule-based (RB) method that was used in our overall fifth place-ranked solution at the challenge was investigated. Second, a conditional random fields (CRF) approach is presented for named entity identification (NEI) developed after the completion of the challenge. The CRF models are trained on the 17 ground truth documents, the output of the rule-based NEI component on all documents, a larger but potentially inaccurate training dataset. For both NEI approaches their effect on relation extraction performance was investigated. The filtering and relation extraction components are both rule-based. In addition to the official entry level evaluation of the challenge, entity level analysis is also provided. On the test data an entry level F(1)-score of 80% was achieved for exact matching and 81% for inexact matching with the RB-NEI component. The CRF produces a significantly weaker result, but CRF outperforms the rule-based model with 81% exact and 82% inexact F(1)-score (p&lt;0.02). This study shows that a simple rule-based method is on a par with more complicated machine learners; CRF models can benefit from the addition of the potentially inaccurate training data, when only very few training documents are available. Such training data could be generated using the outputs of rule-based methods.
20819861	Automatic extraction of medication information from medical discharge summaries.
J Am Med Inform Assoc  2010 Sep-Oct
This article describes a system developed for the 2009 i2b2 Medication Extraction Challenge. The purpose of this challenge is to extract medication information from hospital discharge summaries. The system explored several linguistic natural language processing techniques (eg, term-based and token-based rule matching) to identify medication-related information in the narrative text. A number of lexical resources was constructed to profile lexical or morphological features for different categories of medication constituents. Performance was evaluated in terms of the micro-averaged F-measure at the horizontal system level. The automated system performed well, and achieved an F-micro of 80% for the term-level results and 81% for the token-level results, placing it sixth in exact matches and fourth in inexact matches in the i2b2 competition. The overall results show that this relatively simple rule-based approach is capable of tackling multiple entity identification tasks such as medication extraction under situations in which few training documents are annotated for machine learning approaches, and the entity information can be characterized with a set of feature tokens.
20819862	Linguistic approach for identification of medication names and related information in clinical narratives.
J Am Med Inform Assoc  2010 Sep-Oct
Pharmacotherapy is an integral part of any medical care process and plays an important role in the medical history of most patients. Information on medication is crucial for several tasks such as pharmacovigilance, medical decision or biomedical research. Within a narrative text, medication-related information can be buried within other non-relevant data. Specific methods, such as those provided by text mining, must be designed for accessing them, and this is the objective of this study. The authors designed a system for analyzing narrative clinical documents to extract from them medication occurrences and medication-related information. The system also attempts to deduce medications not covered by the dictionaries used. Results provided by the system were evaluated within the framework of the I2B2 NLP challenge held in 2009. The system achieved an F-measure of 0.78 and ranked 7th out of 20 participating teams (the highest F-measure was 0.86). The system provided good results for the annotation and extraction of medication names, their frequency, dosage and mode of administration (F-measure over 0.81), while information on duration and reasons is poorly annotated and extracted (F-measure 0.36 and 0.29, respectively). The performance of the system was stable between the training and test sets.
20819863	Extracting medical information from narrative patient records: the case of medication-related information.
J Am Med Inform Assoc  2010 Sep-Oct
While essential for patient care, information related to medication is often written as free text in clinical records and, therefore, difficult to use in computerized systems. This paper describes an approach to automatically extract medication information from clinical records, which was developed to participate in the i2b2 2009 challenge, as well as different strategies to improve the extraction. Our approach relies on a semantic lexicon and extraction rules as a two-phase strategy: first, drug names are recognized and, then, the context of these names is explored to extract drug-related information (mode, dosage, etc) according to rules capturing the document structure and the syntax of each kind of information. Different configurations are tested to improve this baseline system along several dimensions, particularly drug name recognition-this step being a determining factor to extract drug-related information. Changes were tested at the level of the lexicons and of the extraction rules. The initial system participating in i2b2 achieved good results (global F-measure of 77%). Further testing of different configurations substantially improved the system (global F-measure of 81%), performing well for all types of information (eg, 84% for drug names and 88% for modes), except for durations and reasons, which remain problematic. This study demonstrates that a simple rule-based system can achieve good performance on the medication extraction task. We also showed that controlled modifications (lexicon filtering and rule refinement) were the improvements that best raised the performance.
20819864	Textractor: a hybrid system for medications and reason for their prescription extraction from clinical text documents.
J Am Med Inform Assoc  2010 Sep-Oct
OBJECTIVE To describe a new medication information extraction system-Textractor-developed for the 'i2b2 medication extraction challenge'. The development, functionalities, and official evaluation of the system are detailed. Textractor is based on the Apache Unstructured Information Management Architecture (UMIA) framework, and uses methods that are a hybrid between machine learning and pattern matching. Two modules in the system are based on machine learning algorithms, while other modules use regular expressions, rules, and dictionaries, and one module embeds MetaMap Transfer. The official evaluation was based on a reference standard of 251 discharge summaries annotated by all teams participating in the challenge. The metrics used were recall, precision, and the F(1)-measure. They were calculated with exact and inexact matches, and were averaged at the level of systems and documents. The reference metric for this challenge, the system-level overall F(1)-measure, reached about 77% for exact matches, with a recall of 72% and a precision of 83%. Performance was the best with route information (F(1)-measure about 86%), and was good for dosage and frequency information, with F(1)-measures of about 82-85%. Results were not as good for durations, with F(1)-measures of 36-39%, and for reasons, with F(1)-measures of 24-27%. The official evaluation of Textractor for the i2b2 medication extraction challenge demonstrated satisfactory performance. This system was among the 10 best performing systems in this challenge.
20819865	Lancet: a high precision medication event extraction system for clinical text.
J Am Med Inform Assoc  2010 Sep-Oct
This paper presents Lancet, a supervised machine-learning system that automatically extracts medication events consisting of medication names and information pertaining to their prescribed use (dosage, mode, frequency, duration and reason) from lists or narrative text in medical discharge summaries. Lancet incorporates three supervised machine-learning models: a conditional random fields model for tagging individual medication names and associated fields, an AdaBoost model with decision stump algorithm for determining which medication names and fields belong to a single medication event, and a support vector machines disambiguation model for identifying the context style (narrative or list). The authors, from the University of Wisconsin-Milwaukee, participated in the third i2b2 shared-task for challenges in natural language processing for clinical data: medication extraction challenge. With the performance metrics provided by the i2b2 challenge, the micro F1 (precision/recall) scores are reported for both the horizontal and vertical level. Among the top 10 teams, Lancet achieved the highest precision at 90.4% with an overall F1 score of 76.4% (horizontal system level with exact match), a gain of 11.2% and 12%, respectively, compared with the rule-based baseline system jMerki. By combining the two systems, the hybrid system further increased the F1 score by 3.4% from 76.4% to 79.0%. Supervised machine-learning systems with minimal external knowledge resources can achieve a high precision with a competitive overall F1 score.Lancet based on this learning framework does not rely on expensive manually curated rules. The system is available online at http://code.google.com/p/lancet/.
20701772	Ranked retrieval of Computational Biology models.
BMC Bioinformatics 20100811 2010
The study of biological systems demands computational support. If targeting a biological problem, the reuse of existing computational models can save time and effort. Deciding for potentially suitable models, however, becomes more challenging with the increasing number of computational models available, and even more when considering the models' growing complexity. Firstly, among a set of potential model candidates it is difficult to decide for the model that best suits ones needs. Secondly, it is hard to grasp the nature of an unknown model listed in a search result set, and to judge how well it fits for the particular problem one has in mind. Here we present an improved search approach for computational models of biological processes. It is based on existing retrieval and ranking methods from Information Retrieval. The approach incorporates annotations suggested by MIRIAM, and additional meta-information. It is now part of the search engine of BioModels Database, a standard repository for computational models. The introduced concept and implementation are, to our knowledge, the first application of Information Retrieval techniques on model search in Computational Systems Biology. Using the example of BioModels Database, it was shown that the approach is feasible and extends the current possibilities to search for relevant models. The advantages of our system over existing solutions are that we incorporate a rich set of meta-information, and that we provide the user with a relevance ranking of the models found for a query. Better search capabilities in model databases are expected to have a positive effect on the reuse of existing models.
20624778	Interactive and fuzzy search: a dynamic way to explore MEDLINE.
Bioinformatics 20100712 2010Sep15
The MEDLINE database, consisting of over 19 million publication records, is the primary source of information for biomedicine and health questions. Although the database itself has been growing rapidly, the search paradigm of MEDLINE has remained largely unchanged. Here, we propose a new system for exploring the entire MEDLINE collection, represented by two unique features: (i) interactive: providing instant feedback to users' query letter by letter, and (ii) fuzzy: allowing approximate search. We develop novel index structures and search algorithms to make such a search model possible. We also develop incremental-update techniques to keep the data up to date. Interactive and fuzzy searching algorithms for exploring MEDLINE are implemented in a system called iPubMed, freely accessible over the web at http://ipubmed.ics.uci.edu/ and http://tastier.cs.tsinghua.edu.cn/ipubmed.
20823319	BioXSD: the common data-exchange format for everyday bioinformatics web services.
Bioinformatics  2010Sep15
The world-wide community of life scientists has access to a large number of public bioinformatics databases and tools, which are developed and deployed using diverse technologies and designs. More and more of the resources offer programmatic web-service interface. However, efficient use of the resources is hampered by the lack of widely used, standard data-exchange formats for the basic, everyday bioinformatics data types. BioXSD has been developed as a candidate for standard, canonical exchange format for basic bioinformatics data. BioXSD is represented by a dedicated XML Schema and defines syntax for biological sequences, sequence annotations, alignments and references to resources. We have adapted a set of web services to use BioXSD as the input and output format, and implemented a test-case workflow. This demonstrates that the approach is feasible and provides smooth interoperability. Semantics for BioXSD is provided by annotation with the EDAM ontology. We discuss in a separate section how BioXSD relates to other initiatives and approaches, including existing standards and the Semantic Web. The BioXSD 1.0 XML Schema is freely available at http://www.bioxsd.org/BioXSD-1.0.xsd under the Creative Commons BY-ND 3.0 license. The http://bioxsd.org web page offers documentation, examples of data in BioXSD format, example workflows with source codes in common programming languages, an updated list of compatible web services and tools and a repository of feature requests from the community.
20539892	An in silico analysis of microRNAs: mining the miRNAome.
Mol Biosyst 20100611 2010Oct
Systematic analysis of literature- and experimentally-derived datasets using text mining with ontological enrichment and network modeling revealed global trends in the microRNA (miRNA) interactome. A total of 756 unique miRNAs were resolved from PubMed abstracts and 1165 direct relationships between 270 miRNAs and 581 genes were identified as phrase groups using semantic search techniques. These miRNA:gene interactions were built into a bipartite network (the miRNAome) which displays scale-free degree distribution. Functional classification of miRNA-target genes using PANTHER revealed 189 distinct molecular functions, with significant enrichment of nucleic acid binding, transcription and protein phosphorylation. Pathway analysis revealed a network of 176 miRNAs linked to 368 OMIM disorders via their target genes, which are enriched (p = 0.0047) for disease-associated SNP variations. Reference to a database of drug targets revealed that 24.8% of all published miRNA-targets are targets for drug development programs, while a sub-set (18.2%) are targets for FDA-approved drugs. Consistent with topological analysis of the miRNA-disease network, the most prevalent class of FDA-approved drugs is anti-neoplastic agents against published miRNA-target genes. Linking miRNAs to biological process and diseases reveals distinct co-regulation of phenotypes that could aid in understanding the role miRNA-based gene regulation plays in biological phenomena.
20685344	Development of search filters for retrieval of literature on the molecular epidemiology of cancer.
Mutat. Res. 20100602 2010Aug30
The available tools for searching literature in the area of molecular epidemiology of cancer, a relatively novel discipline, are largely unsatisfactory. The aim of this project was to develop two search filters in MEDLINE that would facilitate the retrieval of studies on cancer molecular epidemiology and to compare their performance. Citations of all articles published in 2007 in three top journals in this area were hand-screened to establish a gold standard reference set of relevant papers. Two filters were created with this set: the first was based on the frequency of descriptors listed in the National Library of Medicine (NLM) Medical Subject Headings Thesaurus (MeSH terms) only, the second was based on the frequency of words in titles and abstracts (text-words), besides MeSH terms. The sensitivity, specificity, precision and accuracy rates of the two filters were calculated as performance measures. Out of all 1602 published articles, 344 were found to be pertinent to cancer molecular epidemiology; these formed the gold standard set. The first filter was the most specific (99%) and showed a good precision (93.2%), the second showed a higher sensitivity (78.2%) and accuracy (91.6%). The two filters proposed here represent a balance between sensitivity and specificity. Redefinition and elaboration of more appropriate NLM MeSH terms could greatly facilitate the retrieval of studies on the molecular epidemiology of cancer. Authors themselves could facilitate this by inserting adequate words in title or abstract of their articles.
20824212	Reporting of methodologic information on trial registries for quality assessment: a study of trial records retrieved from the WHO search portal.
PLoS ONE 20100831 2010
Although randomized clinical trials (RCTs) are considered the gold standard of evidence, their reporting is often suboptimal. Trial registries have the potential to contribute important methodologic information for critical appraisal of study results. The objective of the study was to evaluate the reporting of key methodologic study characteristics in trial registries. We identified a random sample (n = 265) of actively recruiting RCTs using the World Health Organization International Clinical Trials Registry Platform (ICTRP) search portal in 2008. We assessed the reporting of relevant domains from the Cochrane Collaboration's 'Risk of bias' tool and other key methodological aspects. Our primary outcomes were the proportion of registry records with adequate reporting of random sequence generation, allocation concealment, blinding, and trial outcomes. Two reviewers independently assessed each record. Weighted overall proportions in the ICTRP search portal for adequate reporting of sequence generation, allocation concealment, blinding (including and excluding open label RCT) and primary outcomes were 5.7% (95% CI 3.0-8.4%), 1.4% (0-2.8%), 41% (35-47%), 8.4% (4.1-13%), and 66% (60-72%), respectively. The proportion of adequately reported RCTs was higher for registries that used specific methodological fields for describing methods of randomization and allocation concealment compared to registries that did not. Concerning other key methodological aspects, weighted overall proportions of RCTs with adequately reported items were as follows: eligibility criteria (81%), secondary outcomes (46%), harm (5%) follow-up duration (62%), description of the interventions (53%) and sample size calculation (1%). Trial registries currently contain limited methodologic information about registered RCTs. In order to permit adequate critical appraisal of trial results reported in journals and registries, trial registries should consider requesting details on key RCT methods to complement journal publications. Full protocols remain the most comprehensive source of methodologic information and should be made publicly available.
20731858	Identifying nurse staffing research in Medline: development and testing of empirically derived search strategies with the PubMed interface.
BMC Med Res Methodol 20100823 2010
The identification of health services research in databases such as PubMed/Medline is a cumbersome task. This task becomes even more difficult if the field of interest involves the use of diverse methods and data sources, as is the case with nurse staffing research. This type of research investigates the association between nurse staffing parameters and nursing and patient outcomes. A comprehensively developed search strategy may help identify nurse staffing research in PubMed/Medline. A set of relevant references in PubMed/Medline was identified by means of three systematic reviews. This development set was used to detect candidate free-text and MeSH terms. The frequency of these terms was compared to a random sample from PubMed/Medline in order to identify terms specific to nurse staffing research, which were then used to develop a sensitive, precise and balanced search strategy. To determine their precision, the newly developed search strategies were tested against a) the pool of relevant references extracted from the systematic reviews, b) a reference set identified from an electronic journal screening, and c) a sample from PubMed/Medline. Finally, all newly developed strategies were compared to PubMed's Health Services Research Queries (PubMed's HSR Queries). The sensitivities of the newly developed search strategies were almost 100% in all of the three test sets applied; precision ranged from 6.1% to 32.0%. PubMed's HSR queries were less sensitive (83.3% to 88.2%) than the new search strategies. Only minor differences in precision were found (5.0% to 32.0%). As with other literature on health services research, nurse staffing studies are difficult to identify in PubMed/Medline. Depending on the purpose of the search, researchers can choose between high sensitivity and retrieval of a large number of references or high precision, i.e. and an increased risk of missing relevant references, respectively. More standardized terminology (e.g. by consistent use of the term "nurse staffing") could improve the precision of future searches in this field. Empirically selected search terms can help to develop effective search strategies. The high consistency between all test sets confirmed the validity of our approach.
20738850	Measuring the impact of health policies using Internet search patterns: the case of abortion.
BMC Public Health 20100825 2010
Internet search patterns have emerged as a novel data source for monitoring infectious disease trends. We propose that these data can also be used more broadly to study the impact of health policies across different regions in a more efficient and timely manner. As a test use case, we studied the relationships between abortion-related search volume, local abortion rates, and local abortion policies available for study. Our initial integrative analysis found that, both in the US and internationally, the volume of Internet searches for abortion is inversely proportional to local abortion rates and directly proportional to local restrictions on abortion. These findings are consistent with published evidence that local restrictions on abortion lead individuals to seek abortion services outside of their area. Further validation of these methods has the potential to produce a timely, complementary data source for studying the effects of health policies.
20558318	Clinical and biological data integration for biomarker discovery.
Drug Discov. Today 20100615 2010Sep
Biomarkers hold promise for increasing success rates of clinical trials. Biomarker discovery requires searching for associations across a spectrum of data. The field of biomedical data integration has made strides in developing management and analysis tools for structured biological data, but best practices are still evolving for the integration of high-throughput data with less structured clinical data. Integrated repositories are needed to support data analysis, storage and access. We describe a data integration strategy that implements a clinical and biological database and a wiki interface. We integrated parameters across clinical trials and associated genetic, gene expression and protein data. We provide examples to illustrate the utility of data integration to explore disease heterogeneity and develop predictive biomarkers.
20601095	Drug profiling: knowing where it hits.
Drug Discov. Today 20100618 2010Sep
Off-target hits of drugs can lead to serious adverse effects or, conversely, to unforeseen alternative medical utility. Selectivity profiling against large panels of potential targets is essential for the drug discovery process to minimize attrition and maximize therapeutic utility. Lately, it has become apparent that drug promiscuity (polypharmacology) goes well beyond target families; therefore, lowering the profiling costs and expanding the coverage of targets is an industry-wide challenge to improve predictions. Here, we review current and promising drug profiling alternatives and commercial solutions in these exciting emerging fields.
20712711	Key words and their role in information retrieval.
Health Info Libr J  2010Sep
As any good library or information worker knows the accurate and consistent application of keywords can serve to enhance the content representation and retrieval of literature. Research has demonstrated that this aspect of the library and information science evidence base is particularly well represented. Drawing on the thesauri of the Library &amp; Science Abstracts, Library, Information Science &amp; Technology Abstracts and medline databases, the Health Information and Libraries Journal (HILJ) has recently updated and expanded the HILJ keyword list. Based on the content of reviews and original articles published in HILJ over the past 4 years, the keyword list will be used by submitting authors to represent the content of the manuscripts and enable more accurate matching of manuscript to HILJ referees.
20712713	The impact of information skills training on independent literature searching activity and requests for mediated literature searches.
Health Info Libr J  2010Sep
Most NHS library services routinely offer both mediated searches and information skills training sessions to their users. We analyse the impact of these two services on the amount of literature searching demonstrated by users of hospital- based library services in the north-west of England. Data for (1) mediated literature searches, (2) number of library users attending information skills training sessions, (3) amount of library staff time devoted to information skills training, and (4) number of Athens-authenticated log-ins to databases were obtained from statistical returns for 2007, and analysed for significant correlations. There was evidence of quite strong correlations between the two measures of training activity and the number of mediated literature searches performed by library staff. There was weaker evidence of correlation between training activity and total literature searching activity. Attending training sessions may make some library users aware of the difficulty of complex literature searches and actually reduce their confidence to perform their own complex searches independently. The relationships between information skills training, mediated literature searches, and independent literature searching activity remain complex.
20712717	Developing and testing of search filters for the new European Union Member States' research.
Health Info Libr J  2010Sep
To develop and apply search filters retrieving the scientific output (SO) after 2000 focusing on Public Health (PH) of the new European Union (EU) Member States after the 2004 and 2007 enlargements. Twelve geographical filters (GFs) were designed and applied to retrieve references added since 2001 in Medline (accessed through PubMed) and originated in the new EU countries. The PH area was accessed using Medical Subject Heading terms. The filters were evaluated through a manual check and the agreement/non-agreement percentages were calculated. A number of 99 912 articles revealing the total SO and 6502 articles focusing on PH were retrieved. More than 66% were published abroad and more than 80% in English. The evaluation revealed an average agreement percentage of 98.97%. The results were compared with those obtained by using simple search strategies. Twelve GFs applied to medline retrieved references belonging to twelve countries for a specific period of time. The evaluation of the GFs through the manual check demonstrated effectiveness of these filters. Complementary studies would be advisable to focus on the development of search filters to retrieve complete and accurate information.
20715467	[How to use Pubmed for beginners].
Kyobu Geka  2010Aug
PubMed is the world's largest biomedical journal literature database operated by the National Library of Medicine (NLM), USA. This resource is accessible via the Internet free of charge. The database covers literature in English and more than 50 other languages including Japanese. It includes the fields of medicine, health care system, and others. PubMed is a very handy, quick, and easy-to-use database, and remains an optimal tool for clinicians and researchers.
20691054	RoBuST: an integrated genomics resource for the root and bulb crop families Apiaceae and Alliaceae.
BMC Plant Biol. 20100806 2010
Root and bulb vegetables (RBV) include carrots, celeriac (root celery), parsnips (Apiaceae), onions, garlic, and leek (Alliaceae)--food crops grown globally and consumed worldwide. Few data analysis platforms are currently available where data collection, annotation and integration initiatives are focused on RBV plant groups. Scientists working on RBV include breeders, geneticists, taxonomists, plant pathologists, and plant physiologists who use genomic data for a wide range of activities including the development of molecular genetic maps, delineation of taxonomic relationships, and investigation of molecular aspects of gene expression in biochemical pathways and disease responses. With genomic data coming from such diverse areas of plant science, availability of a community resource focused on these RBV data types would be of great interest to this scientific community. The RoBuST database has been developed to initiate a platform for collecting and organizing genomic information useful for RBV researchers. The current release of RoBuST contains genomics data for 294 Alliaceae and 816 Apiaceae plant species and has the following features: (1) comprehensive sequence annotations of 3663 genes 5959 RNAs, 22,723 ESTs and 11,438 regulatory sequence elements from Apiaceae and Alliaceae plant families; (2) graphical tools for visualization and analysis of sequence data; (3) access to traits, biosynthetic pathways, genetic linkage maps and molecular taxonomy data associated with Alliaceae and Apiaceae plants; and (4) comprehensive plant splice signal repository of 659,369 splice signals collected from 6015 plant species for comparative analysis of plant splicing patterns. RoBuST, available at http://robust.genome.com, provides an integrated platform for researchers to effortlessly explore and analyze genomic data associated with root and bulb vegetables.
20721232	Laser-induced phase transitions of Ge2Sb2Te5 thin films used in optical and electronic data storage and in thermal lithography.
Opt Express  2010Aug16
Amorphous thin films of Ge(2)Sb(2)Te(5), sputter-deposited on a ZnS-SiO(2) dielectric layer, are investigated for the purpose of understanding the structural phase-transitions that occur under the influence of tightly-focused laser beams. Selective chemical etching of recorded marks in conjunction with optical, atomic force, and electron microscopy as well as local electron diffraction analysis are used to discern the complex structural features created under a broad range of laser powers and pulse durations. Clarifying the nature of phase transitions associated with laser-recorded marks in chalcogenide Ge(2)Sb(2)Te(5) thin films provides useful information for reversible optical and electronic data storage, as well as for phase-change (thermal) lithography.
20698959	IDOMAL: an ontology for malaria.
Malar. J. 20100810 2010
Ontologies are rapidly becoming a necessity for the design of efficient information technology tools, especially databases, because they permit the organization of stored data using logical rules and defined terms that are understood by both humans and machines. This has as consequence both an enhanced usage and interoperability of databases and related resources. It is hoped that IDOMAL, the ontology of malaria will prove a valuable instrument when implemented in both malaria research and control measures. The OBOEdit2 software was used for the construction of the ontology. IDOMAL is based on the Basic Formal Ontology (BFO) and follows the rules set by the OBO Foundry consortium. The first version of the malaria ontology covers both clinical and epidemiological aspects of the disease, as well as disease and vector biology. IDOMAL is meant to later become the nucleation site for a much larger ontology of vector borne diseases, which will itself be an extension of a large ontology of infectious diseases (IDO). The latter is currently being developed in the frame of a large international collaborative effort. IDOMAL, already freely available in its first version, will form part of a suite of ontologies that will be used to drive IT tools and databases specifically constructed to help control malaria and, later, other vector-borne diseases. This suite already consists of the ontology described here as well as the one on insecticide resistance that has been available for some time. Additional components are being developed and introduced into IDOMAL.
20799824	Shack-Hartmann wavefront-sensor-based adaptive optics system for multiphoton microscopy.
J Biomed Opt  2010 Jul-Aug
The imaging depth of two-photon excitation fluorescence microscopy is partly limited by the inhomogeneity of the refractive index in biological specimens. This inhomogeneity results in a distortion of the wavefront of the excitation light. This wavefront distortion results in image resolution degradation and lower signal level. Using an adaptive optics system consisting of a Shack-Hartmann wavefront sensor and a deformable mirror, wavefront distortion can be measured and corrected. With adaptive optics compensation, we demonstrate that the resolution and signal level can be better preserved at greater imaging depth in a variety of ex-vivo tissue specimens including mouse tongue muscle, heart muscle, and brain. However, for these highly scattering tissues, we find signal degradation due to scattering to be a more dominant factor than aberration.
20624324	YeastWeb: a workset-centric web resource for gene family analysis in yeast.
BMC Genomics 20100713 2010
Currently, a number of yeast genomes with different physiological features have been sequenced and annotated, which provides invaluable information to investigate yeast genetics, evolutionary mechanism, structure and function of gene families. YeastWeb is a novel database created to provide access to gene families derived from the available yeast genomes by assigning the genes into putative families. It has many useful features that complement existing databases, such as SGD, CYGD and Génolevures: 1) Detailed computational annotation was conducted with each entry with InterProScan, EMBOSS and functional/pathway databases, such as GO, COG and KEGG; 2) A well established user-friendly environment was created to allow users to retrieve the annotated genes and gene families using functional classification browser, keyword search or similarity-based search; 3) Workset offers users many powerful functions to manage the retrieved data efficiently, associate the individual items easily and save the intermediate results conveniently; 4) A series of comparative genomics and molecular evolution analysis tools are neatly implemented to allow users to view multiple sequence alignments and phylogenetic tree of gene families. At present, YeastWeb holds the gene families clustered from various MCL inflation values from a total of 13 available yeast genomes. Given the great interest in yeast research, YeastWeb has the potential to become a useful resource for the scientific community of yeast biologists and related researchers investigating the evolutionary relationship of yeast gene families. YeastWeb is available at http://centre.bioinformatics.zj.cn/Yeast/.
20400331	Quantitative biomechanical workplace exposure measures: distribution centers.
J Electromyogr Kinesiol 20100418 2010Oct
Physical work exposure characteristics assessed in most previous epidemiologic studies have been described mostly in gross categorical terms (e.g. heavy work, lifting and forceful movements, etc.) and have resulted in relatively moderate associations with low back pain risk. We hypothesized that it was necessary to characterize work demands in a much more quantitative fashion so that the precise biomechanically meaningful measures of exposure were available for risk analysis. In this study, we used sophisticated instrumentation to continuously document 390 physical exposures during lifting (in four types of distribution centers) throughout work. This study profiles these exposures and shows how these exposures vary as a function of the type of distribution center and compares the exposures to (previously documented) manufacturing exposures. Static load and load moment measures were found to greatly under-represent true (dynamic) load and load moment exposures to workers. Lift durations averaged 11-12% of the cycle time in distribution environments. This study indicates that distribution workers are commonly exposed to greater extreme loads and move much more rapidly than manufacturing employees. The information provided here can serve as a basis for low back pain risk assessments.
20677056	Online resources for persons recently diagnosed with HIV/AIDS: an analysis of HIV-related webpages.
J Health Commun  2010Jul
The Internet is a major source of HIV-related information and resources for persons recently diagnosed with HIV/AIDS (PRDHA). This study examined the types of HIV-related websites that appear as a result of HIV-related keyword searches and the extent to which website information targets PRDHA. The first page of HIV-related webpages from 18 keyword searches was coded. Among 137 webpages meeting inclusion criteria, 63% represented HIV-informational websites, 31% targeted HIV-positive individuals, and over half contained or provided access to HIV prevention, treatment, and transmission information. Thirty-three percent of webpages contained or provided access to PRDHA-targeted information, with a greater percentage of those webpages having mobile, non-English, and "Ask the Expert" features compared with non-PRDHA targeted webpages. Implications for PRDHA include the following: (1) they should explore HIV-related websites to gain insight into the credibility of the information contained on those sites; (2) PRDHA must be aware that HIV-related websites have the potential to elicit dated, emotionally distressing, or irrelevant information; and (3) to obtain information that relates to their demographic and situational profile, they may wish to use specific key terms (e.g., "HIV women") rather than attempting to navigate webpages that arise from general search terms (e.g., "HIV"). Recommendations for future development of online resources for PRDHA include providing HIV-relevant information in a stepwise fashion, providing demographically targeted HIV information, and greater utilization of mobile technology.
20677061	Collaborating and delivering literature search results to clinical teams using web 2.0 tools.
Med Ref Serv Q  2010Jul
This article describes the experiences of librarians at the Research Medical Library embedded within clinical teams at The University of Texas MD Anderson Cancer Center and their efforts to enhance communication within their teams using Web 2.0 tools. Pros and cons of EndNote Web, Delicious, Connotea, PBWorks, and SharePoint are discussed.
20373102	APPC--a new standardised coding system for trans-organisational PACS retrieval.
Eur Radiol 20100408 2010Sep
As part of a general strategy to integrate the health care enterprise, Austria plans to connect the Picture Archiving and Communication Systems (PACS) of all radiological institutions into a nationwide network. To facilitate the search for relevant correlative imaging data in the PACS of different organisations, a coding system was compiled for all radiological procedures and necessary anatomical details. This code, called the Austrian PACS Procedure Code (APPC), was granted the status of a standard under HL7. Examples are provided of effective coding and filtering when searching for relevant imaging material using the APPC, as well as the planned process for future adjustments of the APPC. The implementation and how the APPC will fit into the future electronic environment, which will include an electronic health act for all citizens in Austria, are discussed. A comparison to other nationwide electronic health record projects and coding systems is given. Limitations and possible use in physical storage media are contemplated.
20605490	Mild traumatic brain injury: tissue texture analysis correlated to neuropsychological and DTI findings.
Acad Radiol 20100603 2010Sep
The aim of this study was to evaluate whether texture analysis (TA) can detect subtle changes in cerebral tissue caused by mild traumatic brain injury (MTBI) and to determine whether these changes correlate with neuropsychological and diffusion tensor imaging (DTI) findings. Forty-two patients with MTBIs were imaged using 1.5T magnetic resonance imaging within 3 weeks after head injury. TA was performed for the regions corresponding to the mesencephalon, centrum semiovale, and corpus callosum. Using DTI, the fractional anisotropic and apparent diffusion coefficient values for the same regions were evaluated. The same analyses were performed on a group of 10 healthy volunteers. Patients also underwent a battery of neurocognitive tests within 6 weeks after injury. TA revealed textural differences between the right and left hemispheres in patients with MTBIs, whereas differences were minimal in healthy controls. A significant correlation was found between scores on memory tests and texture parameters (sum of squares, sum entropy, inverse difference moment, and sum average) in patients in the area of the mesencephalon and the genu of the corpus callosum. Significant correlations were also found between texture parameters for the left mesencephalon and both fractional anisotropic and apparent diffusion coefficient values. The data suggest that heterogeneous texture and abnormal DTI patterns in the area of the mesencephalon may be linked with verbal memory deficits among patients with MTBIs. Therefore, TA combined with DTI in patients with MTBIs may increase the ability to detect early and subtle neuropathologic changes.
20573463	Assessment of spatial BOLD sensitivity variations in fMRI using gradient-echo field maps.
Magn Reson Imaging 20100622 2010Sep
Clinical blood oxygenation level-dependent (BOLD) functional magnetic resonance imaging (fMRI) is becoming increasingly valuable in, e.g., presurgical planning, but the commonly used gradient-echo echo-planar imaging (GE-EPI) technique is sometimes hampered by macroscopic field inhomogeneities. This can affect the degree of signal change that will occur in the GE-EPI images as a response to neural activation and the subsequent blood oxygenation changes, i.e., the BOLD sensitivity (BS). In this study, quantitative BS maps were calculated directly from gradient-echo field maps obtainable on most clinical scanners. In order to validate the accuracy of the calculated BS-maps, known shim gradients were applied and field maps and GE-EPI images of a phantom were acquired. Measured GE-EPI image intensity was then compared with the calculated (predicted) image intensity (pII) which was obtained from the field maps using theoretical expressions for image-intensity loss. The validated expressions for pII were used to calculate the corresponding predicted BOLD sensitivity (pBS) maps in healthy volunteers. Since the field map is assumed to be valid throughout an entire fMRI experiment, the influence of subject motion on the pBS maps was also assessed. To demonstrate the usefulness of such maps, pBS was investigated for clinically important functional areas including hippocampus, Broca's area and primary motor cortex. A systematic left/right pBS difference was observed in Broca's area and in the hippocampus, most likely due to magnetic field inhomogeneity of the particular MRI-system used in this study. For all subjects, the hippocampus showed pBS values above unity with a clear anterior-posterior gradient and with an abrupt drop to zero pBS in the anterior parts of hippocampus. It is concluded that GE field maps can be used to accurately predict BOLD sensitivity and that this parameter is useful to assess spatial variations which will influence fMRI experiments.
20211251	Microbiological Common Language (MCL): a standard for electronic information exchange in the Microbial Commons.
Res. Microbiol. 20100306 2010 Jul-Aug
Although Biological Resource Centers (BRCs) traditionally have open catalogs of their holdings, it is quite cumbersome to access meta-information about microorganisms electronically due to the variety of access methods used by those catalogs. Therefore, we propose Microbiological Common Language (MCL), aimed at standardizing the electronic exchange of meta-information about microorganisms. Its application ranges from representing the online catalog of a single collection to accessing the results of StrainInfo integration and ad hoc use in other contexts. The abstract model of the standard precisely defines the elements of the standard, which enables implementation using a variety of representation technologies. Currently, XML and RDF/XML implementations are readily available. MCL is an open standard, and therefore greatly encourages input from the microbiological community.
20663121	GeneBrowser 2: an application to explore and identify common biological traits in a set of genes.
BMC Bioinformatics 20100721 2010
The development of high-throughput laboratory techniques created a demand for computer-assisted result analysis tools. Many of these techniques return lists of genes whose interpretation requires finding relevant biological roles for the problem at hand. The required information is typically available in public databases, and usually, this information must be manually retrieved to complement the analysis. This process is a very time-consuming task that should be automated as much as possible. GeneBrowser is a web-based tool that, for a given list of genes, combines data from several public databases with visualisation and analysis methods to help identify the most relevant and common biological characteristics. The functionalities provided include the following: a central point with the most relevant biological information for each inserted gene; a list of the most related papers in PubMed and gene expression studies in ArrayExpress; and an extended approach to functional analysis applied to Gene Ontology, homologies, gene chromosomal localisation and pathways. GeneBrowser provides a unique entry point to several visualisation and analysis methods, providing fast and easy analysis of a set of genes. GeneBrowser fills the gap between Web portals that analyse one gene at a time and functional analysis tools that are limited in scope and usually desktop-based.
20207441	MammoSys: A content-based image retrieval system using breast density patterns.
Comput Methods Programs Biomed 20100307 2010Sep
In this paper, we present a content-based image retrieval system designed to retrieve mammographies from large medical image database. The system is developed based on breast density, according to the four categories defined by the American College of Radiology, and is integrated to the database of the Image Retrieval in Medical Applications (IRMA) project, that provides images with classification ground truth. Two-dimensional principal component analysis is used in breast density texture characterization, in order to effectively represent texture and allow for dimensionality reduction. A support vector machine is used to perform the retrieval process. Average precision rates are in the range from 83% to 97% considering a data set of 5024 images. The results indicate the potential of the system as the first stage of a computer-aided diagnosis framework.
20531223	The association of depression with adherence to antihypertensive medications: a systematic review.
J. Hypertens.  2010Sep
To examine the strength and consistency of the evidence on the relationship between depression and adherence to antihypertensive medications. The MEDLINE, CINAHL, PsycINFO, Embase, SCOPUS, and ISI databases were searched from inception until 11 December 2009 for published studies of original research that assessed adherence to antihypertensive medications and used a standardized interview, validated questionnaire, or International Classification of Diseases Ninth Revision code to assess depression or symptoms of depression in patients with hypertension. Manual searching was conducted on 22 selected journals. Citations of included articles were tracked using Web of Science and Google Scholar. Two investigators independently extracted data from the selected articles and discrepancies were resolved by consensus. Eight studies were identified that included a total of 42,790 patients. Ninety-five percent of these patients were from one study. Only four of the studies had the assessment of this relationship as a primary objective. Adherence rates varied from 29 to 91%. There were widely varying results within and across studies. All eight studies reported at least one significant bivariate or multivariate negative relationship between depression and adherence to antihypertensive medications. Insignificant findings in bivariate or multivariate analyses were reported in six of eight studies. All studies reported statistically significant relationships between depression and poor adherence to antihypertensive medications, but definitive conclusions cannot be drawn because of substantial heterogeneity between studies with respect to the assessment of depression and adherence, as well as inconsistencies in results both within and between studies. Additional studies would help clarify this relationship.
20479501	Multistage gene normalization and SVM-based ranking for protein interactor extraction in full-text articles.
IEEE/ACM Trans Comput Biol Bioinform  2010 Jul-Sep
The interactor normalization task (INT) is to identify genes that play the interactor role in protein-protein interactions (PPIs), to map these genes to unique IDs, and to rank them according to their normalized confidence. INT has two subtasks: gene normalization (GN) and interactor ranking. The main difficulties of INT GN are identifying genes across species and using full papers instead of abstracts. To tackle these problems, we developed a multistage GN algorithm and a ranking method, which exploit information in different parts of a paper. Our system achieved a promising AUC of 0.43471. Using the multistage GN algorithm, we have been able to improve system performance (AUC) by 1.719 percent compared to a one-stage GN algorithm. Our experimental results also show that with full text, versus abstract only, INT AUC performance was 22.6 percent higher.
20701052	Using google scholar to conduct a literature search.
Nurs Stand  2010 Jul 14-20
This article provides information about conducting a literature search on the Google Scholar website. The article briefly describes how to narrow or expand a search and how to find non-journal literature. Although Google Scholar is not without limitations, it offers a practical starting point for a literature search.
20682075	Extracting the abstraction pyramid from complex networks.
BMC Bioinformatics 20100803 2010
At present, the organization of system modules is typically limited to either a multilevel hierarchy that describes the "vertical" relationships between modules at different levels (e.g., module A at level two is included in module B at level one), or a single-level graph that represents the "horizontal" relationships among modules (e.g., genetic interactions between module A and module B). Both types of organizations fail to provide a broader and deeper view of the complex systems that arise from an integration of vertical and horizontal relationships. We propose a complex network analysis tool, Pyramabs, which was developed to integrate vertical and horizontal relationships and extract information at various granularities to create a pyramid from a complex system of interacting objects. The pyramid depicts the nested structure implied in a complex system, and shows the vertical relationships between abstract networks at different levels. In addition, at each level the abstract network of modules, which are connected by weighted links, represents the modules' horizontal relationships. We first tested Pyramabs on hierarchical random networks to verify its ability to find the module organization pre-embedded in the networks. We later tested it on a protein-protein interaction (PPI) network and a metabolic network. According to Gene Ontology (GO) and the Kyoto Encyclopedia of Genes and Genomes (KEGG), the vertical relationships identified from the PPI and metabolic pathways correctly characterized the inclusion (i.e., part-of) relationship, and the horizontal relationships provided a good indication of the functional closeness between modules. Our experiments with Pyramabs demonstrated its ability to perform knowledge mining in complex systems. Networks are a flexible and convenient method of representing interactions in a complex system, and an increasing amount of information in real-world situations is described by complex networks. We considered the analysis of a complex network as an iterative process for extracting meaningful information at multiple granularities from a system of interacting objects. The quality of the interpretation of the networks depends on the completeness and expressiveness of the extracted knowledge representations. Pyramabs was designed to interpret a complex network through a disclosure of a pyramid of abstractions. The abstraction pyramid is a new knowledge representation that combines vertical and horizontal viewpoints at different degrees of abstraction. Interpretations in this form are more accurate and more meaningful than multilevel dendrograms or single-level graphs. Pyramabs can be accessed at http://140.113.166.165/pyramabs.php/.
20132491	Sensing performance of a new wireless implantable loop recorder: a 12-month follow up study.
Pacing Clin Electrophysiol 20100128 2010Jul
The implantable loop recorder (ILR) is a cost-effective tool with a high diagnostic yield in the evaluation of unexplained recurrent syncope. The Sleuth ILR (Transoma Medical, St. Paul MN, USA) is a new-generation ILR with wireless transmission capability approved by the Food and Drug Administration. We report the feasibility of achieving appropriate sensing over 1-year follow-up at the traditional midclavicular and alternative inframammary implantation sites without preimplant electrocardiogram (ECG) mapping. We studied 32 patients with unexplained syncope, aged 58.4+/-18.44 years, with an ILR implanted at the left midclavicular location (n = 17) or the left inframammary site (n = 15) over 1-year post implant. No preimplant electrocardiogram (ECG) mapping was performed. The highest R-wave amplitudes were observed at the inframammary site, but over the entire follow-up period, amplitudes were not significantly different from those at the midclavicular site. At both sites, R-wave amplitudes at the 6-month follow-up were significantly higher than those measured at 1 week. P-waves were visible in 80-90% of the patients. There was no discernible difference in P-waves (amplitude) relative to implant location. Body mass index, left ventricular ejection fraction, and age did not significantly influence the R-wave amplitude or the ability to discern P-waves. Our findings show that the Sleuth ILR implanted at both the midclavicular and inframammary locations without preimplant ECG mapping achieves acceptable "R" waves. This may simplify implantation procedures and improve patient satisfaction.
20646088	Identifying nursing concepts: are we similar?
Int J Nurs Terminol Classif  2010 Jul-Sep
The purpose of this article was to define and describe the fundamental aspects of similarity with application to the use of nursing terminologies. Data were obtained from Google, Cumulative Index to Nursing and Allied Health Literature (CINAHL), PsychINFO, and PubMed using the keywords "similarity views,""similarity,""concepts and categorization," and other published sources. Three prominent similarity views were compared, contrasted, and applied to the use of nursing diagnoses. Each view has intentions (requirements) that guide the categorization of information to concepts and influence naming of nursing concepts. By understanding similarity, nurse educators and technology designers can influence how nursing concepts are represented.
20651174	Renal cortical thickness measured at ultrasound: is it better than renal length as an indicator of renal function in chronic kidney disease?
AJR Am J Roentgenol  2010Aug
The purpose of our study was to determine whether there is a relationship between renal cortical thickness or length measured on ultrasound and the degree of renal impairment in chronic kidney disease (CKD). From October to December 2007, 25 patients (13 men and 12 women, mean age 73 years) were identified who had CKD but were not on dialysis. The patients were from a single institution and had undergone renal ultrasound and at least three serum creatinines within 90 days. The lowest creatinine was used for estimated glomerular filtration rate (eGFR) calculation using both the Cockcroft-Gault (CG) and the Modification of Diet in Renal Disease Study (MDRD) equations. Ultrasounds were consensus reviewed by three radiologists (2 attendings and a resident) blinded to specific renal function. Cortical thickness was measured in the sagittal plane over a medullary pyramid, perpendicular to the capsule. Length was measured pole-to-pole. Linear regression was used for statistical analysis. Mean cortical thickness was 5.9 mm (range, 3.2-11.0 mm). Mean length was 10 cm (7.2-12.4 cm). Mean minimum serum creatinine was 2.1 mg/dL (1.1-6.1 mg/dL). Mean eGFR using CG was 34.8 mL/min (10.6-99.4 mL/min) and 36 mL/min (8-66 mL/min) using MDRD. There was a statistically significant relationship between eGFR and cortical thickness using both CG (p &lt; 0.0001) and MDRD (p = 0.005). There was a statistically significant relationship between CG and length (p = 0.003) but not between MDRD and length (p = 0.08). Cortical thickness measured on ultrasound appears to be more closely related to eGFR than renal length. Reporting cortical thickness in patients with CKD who are not on dialysis should be considered.
20586744	National representation in the anaesthesia literature: a bibliometric analysis of highly cited anaesthesia journals.
Anaesthesia 20100625 2010Aug
While previous studies have investigated the country of origin of anaesthetic publications, they have generally used a medline computer search to identify original articles and have often excluded non-English language articles. We undertook a hand-search of journals in the Journal Citation Reports using the subject category of Anesthesiology. We quantified the number of original articles, editorials, review articles, case reports and correspondence attributed to each country. We also calculated the proportion of articles of each type from countries of each national income category. We analysed 9684 articles published in 2007 and 2008. The United States published more original articles than any other country. High-income countries published 89.2% of original articles, middle-income countries 10.5%, and low-income countries just 0.3%. There were more articles published by middle-income countries during the study period than a decade earlier, notably from Turkey, China and India. We discuss barriers to publications from low-income countries.
20650795	[Clinical effects of modified ultrafiltration during pediatric cardiac surgery: a systematic review].
Nan Fang Yi Ke Da Xue Xue Bao  2010Jul
To assess the clinical effects and safety of modified ultrafiltration during pediatric cardiac surgery. The clinical trials were located through electronic searches of the Cochrane Library (Issue 2, 2009), PubMed (1991 to April 2009), EMBASE (1991 to April 2009), China National Knowledge Infrastructure (CNKI, 1994 to April 2009), VIP (1991 to April 2009) and China Biomedicine Database (CBM, 1991 to April 2009), with the languages limited in English and Chinese. In strict accordance with the inclusion and exclusion criteria of the studies, two authors independently evaluated the quality of the included studies. Meta analysis of the studies was conducted using RevMan5.0 software, and the studies that could not be combined was analyzed descriptively. A total of 9 trials involving 587 patients were included. The results showed that compared with the group without ultrafiltration, the modified ultrafiltration group was superior in duration of postoperative mechanical ventilation [MD=-3.66, 95%CI (-6.02, -1.29), P=0.002] and showed no significant differences from the conventional ultrafiltration group [MD=-3.21, 95%CI (-6.90, 0.49), P=0.09]. Compared with balanced ultrafiltration group, the mechanical ventilation time, intensive care unit (ICU) monitoring time and the results of chest drainage in children were similar. Compared with the group receiving conventional or balanced ultrafiltration alone, the combined group of modified ultrafiltration had similar ventilation time [MD=-2.34, 95%CI (-6.74, 2.07), P=0.30] and ICU time [MD=-0.12, 95%CI (-0.31, 0.06), P=0.19]. The included studies reported no ultrafiltration-related complications. Modified ultrafiltration improves the clinical outcomes of patients undergoing cardiopulmonary bypass during pediatric cardiac surgery, but the current evidence has not been sufficient to support the notion that the modified ultrafiltration achieves better clinical results than conventional or balanced ultrafiltration.
20573251	TabSQL: a MySQL tool to facilitate mapping user data to public databases.
BMC Bioinformatics 20100623 2010
With advances in high-throughput genomics and proteomics, it is challenging for biologists to deal with large data files and to map their data to annotations in public databases. We developed TabSQL, a MySQL-based application tool, for viewing, filtering and querying data files with large numbers of rows. TabSQL provides functions for downloading and installing table files from public databases including the Gene Ontology database (GO), the Ensembl databases, and genome databases from the UCSC genome bioinformatics site. Any other database that provides tab-delimited flat files can also be imported. The downloaded gene annotation tables can be queried together with users' data in TabSQL using either a graphic interface or command line. TabSQL allows queries across the user's data and public databases without programming. It is a convenient tool for biologists to annotate and enrich their data.
20657978	[An information system to integrate outpatient cancer care data in the Unified National Health System].
Cad Saude Publica  2010Jun
This study focuses on the development of a system to integrate monthly outpatient cancer care data in the Unified National Health System (SUS). The system was modeled to retrace the treatment evolution in each cancer case and services output, with the following advantage: the system focuses on the cancer case, does not require knowledge by the user in order to handle the database, and allows updating the base as new data emerge. The results of direct queries in the system were identical to those obtained from direct inspection of the original database and those from another integration approach. The use of a tool with these characteristics by public administrators can help improve the quality of outpatient cancer care provided by the National Health System.
20579843	Expectations, validity, and reality in gene expression profiling.
J Clin Epidemiol 20100625 2010Sep
To provide a critical overview of gene expression profiling methodology and discuss areas of future development. Gene expression profiling has been used extensively in biological research and has resulted in significant advances in the understanding of the molecular mechanisms of complex disorders, including cancer, heart disease, and metabolic disorders. However, translating this technology into genomic medicine for use in diagnosis and prognosis faces many challenges. In addition, gene expression profile analysis is frequently controversial, because its conclusions often lack reproducibility and claims of effective dissemination into translational medicine have, in some cases, been remarkably unjustified. In the last decade, a large number of methodological and technical solutions have been offered to overcome the challenges. We consider the strengths, limitations, and appropriate applications of gene expression profiling techniques, with particular reference to the clinical relevance. Some studies have demonstrated the ability and clinical utility of gene expression profiling for use as diagnostic, prognostic, and predictive molecular markers. The challenges of gene expression profiling lie with the standardization of analytic approaches and the evaluation of the clinical merit in broader heterogeneous populations by prospective clinical trials.
20472078	A least angle regression method for fMRI activation detection in phase-encoded experimental designs.
Neuroimage 20100525 2010Oct1
This paper presents a new regression method for functional magnetic resonance imaging (fMRI) activation detection. Unlike general linear models (GLM), this method is based on selecting models for activation detection adaptively which overcomes the limitation of requiring a predefined design matrix in GLM. This limitation is because GLM designs assume that the response of the neuron populations will be the same for the same stimuli, which is often not the case. In this work, the fMRI hemodynamic response model is selected from a series of models constructed online by the least angle regression (LARS) method. The slow drift terms in the design matrix for the activation detection are determined adaptively according to the fMRI response in order to achieve the best fit for each fMRI response. The LARS method is then applied along with the Moore-Penrose pseudoinverse (PINV) and fast orthogonal search (FOS) algorithm for implementation of the selected model to include the drift effects in the design matrix. Comparisons with GLM were made using 11 normal subjects to test method superiority. This paper found that GLM with fixed design matrix was inferior compared to the described LARS method for fMRI activation detection in a phased-encoded experimental design. In addition, the proposed method has the advantage of increasing the degrees of freedom in the regression analysis. We conclude that the method described provides a new and novel approach to the detection of fMRI activation which is better than GLM based analyses.
20656845	Characterization of focal liver lesions by means of assessment of hepatic transit time with contrast-enhanced US.
Radiology  2010Aug
To assess whether hepatic transit times (HTTs), as measured with contrast material-enhanced ultrasonography (US), can help predict the nature of focal liver lesions. The study was approved by the local institutional ethics committee, with written informed patient consent. A total of 402 patients were enrolled in the study. HTT, the time between the appearance of the microbubble contrast agent in the hepatic artery and its appearance in the hepatic vein, was measured in the contrast pulse sequencing mode after injection of a sulphur hexafluoride microbubble US contrast agent. Logistic regression was used to identify factors indicative of the malignant or nonmalignant status of focal liver lesions. Receiver operating characteristic (ROC) analysis was performed to determine the predictive value of the HTT. Observed HTTs for malignant focal liver lesions (mean, 6.2 seconds; range, 2-10 seconds) were significantly lower than those for nonmalignant lesions (mean, 9.5 seconds; range, 4-25 seconds; P &lt; .001). ROC analysis revealed cutoff values of 7 seconds for HTT and 0.879 for area under the ROC curve. For HTTs of 7 seconds or shorter, hepatic malignancies were detected with a sensitivity of 79%, a specificity of 80%, a positive predictive value of 53%, and a negative predictive value of 93%. No malignant lesions had an HTT longer than 10 seconds. HTT alone could be a good predictor for nonmalignancy of focal liver lesions.
20654208	[Recognizing correctly of SCI].
Zhonghua Yan Ke Za Zhi  2010May
The actual effect will be investigated by realizing the origin, background, founding purpose and the function of SCI. Thereby, the source of SCI and its irrationality, harmfulness are analyzed. And it will remind the scientific circle to recognize correctly and clearly of SCI.
20661035	Tracking data in the office environment.
Clin Obstet Gynecol  2010Sep
Data tracking in the office setting focuses on a narrow spectrum of the entire patient safety arena; however, when properly executed, data tracking increases staff members' awareness of the importance of patient safety. Data tracking is also a high-volume event and thereby continues to loop back on the consciousness of providers in all aspects of their practice. Improvement in date tracking will improve the collateral areas of patient safety such as proper medication usage, legibility of written communication, effective delegation of patient safety initiatives, and a collegial effort at developing teams for safety design processes.
20659846	Electronic data-capturing technology for clinical trials: experience with a global postmarketing study.
IEEE Eng Med Biol Mag  2010 Mar-Apr
The purpose of this article was to address three questions: What were the electronic data-capturing (EDC) technologies employed in a typical industry-sponsored clinical study? How is the developed system meeting the clinical research need? What would we want more from this EDC technology? This article is prepared from industry perspectives to present and analyze the advantages, benefits, and challenges in applying EDC technologies to address industry's clinical trial operational needs based on a systematic overview.
20071261	Intelligible support vector machines for diagnosis of diabetes mellitus.
IEEE Trans Inf Technol Biomed 20100112 2010Jul
Diabetes mellitus is a chronic disease and a major public health challenge worldwide. According to the International Diabetes Federation, there are currently 246 million diabetic people worldwide, and this number is expected to rise to 380 million by 2025. Furthermore, 3.8 million deaths are attributable to diabetes complications each year. It has been shown that 80% of type 2 diabetes complications can be prevented or delayed by early identification of people at risk. In this context, several data mining and machine learning methods have been used for the diagnosis, prognosis, and management of diabetes. In this paper, we propose utilizing support vector machines (SVMs) for the diagnosis of diabetes. In particular, we use an additional explanation module, which turns the "black box" model of an SVM into an intelligible representation of the SVM's diagnostic (classification) decision. Results on a real-life diabetes dataset show that intelligible SVMs provide a promising tool for the prediction of diabetes, where a comprehensible ruleset have been generated, with prediction accuracy of 94%, sensitivity of 93%, and specificity of 94%. Furthermore, the extracted rules are medically sound and agree with the outcome of relevant medical studies.
20304729	General retinal vessel segmentation using regularization-based multiconcavity modeling.
IEEE Trans Med Imaging 20100318 2010Jul
Detecting blood vessels in retinal images with the presence of bright and dark lesions is a challenging unsolved problem. In this paper, a novel multiconcavity modeling approach is proposed to handle both healthy and unhealthy retinas simultaneously. The differentiable concavity measure is proposed to handle bright lesions in a perceptive space. The line-shape concavity measure is proposed to remove dark lesions which have an intensity structure different from the line-shaped vessels in a retina. The locally normalized concavity measure is designed to deal with unevenly distributed noise due to the spherical intensity variation in a retinal image. These concavity measures are combined together according to their statistical distributions to detect vessels in general retinal images. Very encouraging experimental results demonstrate that the proposed method consistently yields the best performance over existing state-of-the-art methods on the abnormal retinas and its accuracy outperforms the human observer, which has not been achieved by any of the state-of-the-art benchmark methods. Most importantly, unlike existing methods, the proposed method shows very attractive performances not only on healthy retinas but also on a mixture of healthy and pathological retinas.
20529736	Learning task-optimal registration cost functions for localizing cytoarchitecture and function in the cerebral cortex.
IEEE Trans Med Imaging 20100607 2010Jul
Image registration is typically formulated as an optimization problem with multiple tunable, manually set parameters. We present a principled framework for learning thousands of parameters of registration cost functions, such as a spatially-varying tradeoff between the image dissimilarity and regularization terms. Our approach belongs to the classic machine learning framework of model selection by optimization of cross-validation error. This second layer of optimization of cross-validation error over and above registration selects parameters in the registration cost function that result in good registration as measured by the performance of the specific application in a training data set. Much research effort has been devoted to developing generic registration algorithms, which are then specialized to particular imaging modalities, particular imaging targets and particular postregistration analyses. Our framework allows for a systematic adaptation of generic registration cost functions to specific applications by learning the "free" parameters in the cost functions. Here, we consider the application of localizing underlying cytoarchitecture and functional regions in the cerebral cortex by alignment of cortical folding. Most previous work assumes that perfectly registering the macro-anatomy also perfectly aligns the underlying cortical function even though macro-anatomy does not completely predict brain function. In contrast, we learn 1) optimal weights on different cortical folds or 2) optimal cortical folding template in the generic weighted sum of squared differences dissimilarity measure for the localization task. We demonstrate state-of-the-art localization results in both histological and functional magnetic resonance imaging data sets.
20172822	Large-scale pattern storage and retrieval using generalized brain-state-in-a-box neural networks.
IEEE Trans Neural Netw 20100217 2010Apr
In this paper, a generalized Brain-State-in-a-Box (gBSB)-based hybrid neural network is proposed for storing and retrieving pattern sequences. The hybrid network consists of autoassociative and heteroassociative parts. Then, a large-scale image storage and retrieval neural system is constructed using the gBSB-based hybrid neural network and the pattern decomposition concept. The notion of the deadbeat stability is employed to describe the stability property of the vertices of the hypercube to which the trajectories of the gBSB neural system are constrained. Extensive simulations of large scale pattern and image storing and retrieval are presented to illustrate the results obtained.
20227975	Feature selection with redundancy-constrained class separability.
IEEE Trans Neural Netw 20100311 2010May
Scatter-matrix-based class separability is a simple and efficient feature selection criterion in the literature. However, the conventional trace-based formulation does not take feature redundancy into account and is prone to selecting a set of discriminative but mutually redundant features. In this brief, we first theoretically prove that in the context of this trace-based criterion the existence of sufficiently correlated features can always prevent selecting the optimal feature set. Then, on top of this criterion, we propose the redundancy-constrained feature selection (RCFS). To ensure the algorithm's efficiency and scalability, we study the characteristic of the constraints with which the resulted constrained 0-1 optimization can be efficiently and globally solved. By using the totally unimodular (TUM) concept in integer programming, a necessary condition for such constraints is derived. This condition reveals an interesting special case in which qualified redundancy constraints can be conveniently generated via a clustering of features. We study this special case and develop an efficient feature selection approach based on Dinkelbach's algorithm. Experiments on benchmark data sets demonstrate the superior performance of our approach to those without redundancy constraints.
20350849	A convolutional learning system for object classification in 3-D Lidar data.
IEEE Trans Neural Netw 20100329 2010May
In this brief, a convolutional learning system for classification of segmented objects represented in 3-D as point clouds of laser reflections is proposed. Several novelties are discussed: (1) extension of the existing convolutional neural network (CNN) framework to direct processing of 3-D data in a multiview setting which may be helpful for rotation-invariant consideration, (2) improvement of CNN training effectiveness by employing a stochastic meta-descent (SMD) method, and (3) combination of unsupervised and supervised training for enhanced performance of CNN. CNN performance is illustrated on a two-class data set of objects in a segmented outdoor environment.
20421179	Discriminant analysis for fast multiclass data classification through regularized kernel function approximation.
IEEE Trans Neural Netw 20100422 2010Jun
In this brief we have proposed the multiclass data classification by computationally inexpensive discriminant analysis through vector-valued regularized kernel function approximation (VVRKFA). VVRKFA being an extension of fast regularized kernel function approximation (FRKFA), provides the vector-valued response at single step. The VVRKFA finds a linear operator and a bias vector by using a reduced kernel that maps a pattern from feature space into the low dimensional label space. The classification of patterns is carried out in this low dimensional label subspace. A test pattern is classified depending on its proximity to class centroids. The effectiveness of the proposed method is experimentally verified and compared with multiclass support vector machine (SVM) on several benchmark data sets as well as on gene microarray data for multi-category cancer classification. The results indicate the significant improvement in both training and testing time compared to that of multiclass SVM with comparable testing accuracy principally in large data sets. Experiments in this brief also serve as comparison of performance of VVRKFA with stratified random sampling and sub-sampling.
20442047	The infinite hidden Markov random field model.
IEEE Trans Neural Netw 20100503 2010Jun
Hidden Markov random field (HMRF) models are widely used for image segmentation, as they appear naturally in problems where a spatially constrained clustering scheme is asked for. A major limitation of HMRF models concerns the automatic selection of the proper number of their states, i.e., the number of region clusters derived by the image segmentation procedure. Existing methods, including likelihood- or entropy-based criteria, and reversible Markov chain Monte Carlo methods, usually tend to yield noisy model size estimates while imposing heavy computational requirements. Recently, Dirichlet process (DP, infinite) mixture models have emerged in the cornerstone of nonparametric Bayesian statistics as promising candidates for clustering applications where the number of clusters is unknown a priori; infinite mixture models based on the original DP or spatially constrained variants of it have been applied in unsupervised image segmentation applications showing promising results. Under this motivation, to resolve the aforementioned issues of HMRF models, in this paper, we introduce a nonparametric Bayesian formulation for the HMRF model, the infinite HMRF model, formulated on the basis of a joint Dirichlet process mixture (DPM) and Markov random field (MRF) construction. We derive an efficient variational Bayesian inference algorithm for the proposed model, and we experimentally demonstrate its advantages over competing methodologies.
20550988	Biologically inspired means for rank-order encoding images: a quantitative analysis.
IEEE Trans Neural Netw 20100614 2010Jul
In this paper, we present biologically inspired means to enhance perceptually important information retrieval from rank-order encoded images. Validating a retinal model proposed by VanRullen and Thorpe, we observe that on average only up to 70% of the available information can be retrieved from rank-order encoded images. We propose a biologically inspired treatment to reduce losses due to a high correlation of adjacent basis vectors and introduce a filter-overlap correction algorithm (FoCal) based on the lateral inhibition technique used by sensory neurons to deal with data redundancy. We observe a more than 10% increase in perceptually important information recovery. Subsequently, we present a model of the primate retinal ganglion cell layout corresponding to the foveal-pit. We observe that information recovery using the foveal-pit model is possible only if FoCal is used in tandem. Furthermore, information recovery is similar for both the foveal-pit model and VanRullen and Thorpe's retinal model when used with FoCal. This is in spite of the fact that the foveal-pit model has four ganglion cell layers as in biology while VanRullen and Thorpe's retinal model has a 16-layer structure.
20550990	Multiple incremental decremental learning of support vector machines.
IEEE Trans Neural Netw 20100614 2010Jul
We propose a multiple incremental decremental algorithm of support vector machines (SVM). In online learning, we need to update the trained model when some new observations arrive and/or some observations become obsolete. If we want to add or remove single data point, conventional single incremental decremental algorithm can be used to update the model efficiently. However, to add and/or remove multiple data points, the computational cost of current update algorithm becomes inhibitive because we need to repeatedly apply it for each data point. In this paper, we develop an extension of incremental decremental algorithm which efficiently works for simultaneous update of multiple data points. Some analyses and experimental results show that the proposed algorithm can substantially reduce the computational cost. Our approach is especially useful for online SVM learning in which we need to remove old data points and add new data points in a short amount of time.
20562047	An augmented LKF approach involving derivative information of both state and delay.
IEEE Trans Neural Netw 20100617 2010Jul
An augmented Lyapunov-Krasovskii functional (LKF) approach is presented to derive sufficient conditions for the existence, uniqueness, and globally exponential stability of the equilibrium point of a class of cellular neural networks with time-varying delays. By dividing the variation interval of the time delay into several subintervals with equal length, a novel vector LKF is introduced and new conditions are obtained based on the homeomorphism mapping principle, free-weighting matrix method, and linear matrix inequality techniques. Since the criteria are involving derivative information of both state and delay, the obtained results are less conservative than some previous ones. Two examples are also given to show the effectiveness of the presented criteria.
20662241	[Ichushi, Japanese medical bibliography].
Kyobu Geka  2010Jul
"Ichushi" is a Japanese medical bibliographic database containing 7 million citations originated in Japan, with the aim to contribute to medicine in our country. It was initiated by one Japanese medical practitioner in 1903, in the style of a collection of abstracts from the domestic medical literature at that time. The form was changed from booklet to CD-ROM, and then through the internet. Recently, the database covers medicine, nursing, dentistry, and veterinary medicine.
20665362	[Systematic reviews and metaanalyses. Basic knowledge, strengths and weaknesses of an important tool for healthcare professionals].
Anasthesiol Intensivmed Notfallmed Schmerzther 20100721 2010Jul
Once rather a niche business, evidence-based medicine (EBM) as an approach to clinical decision-making requiring the integration of the best available research evidence with individual clinical expertise and patient values, has gained widespread acceptance and support within the healthcare community. This review article covers the basic principles of systematic reviews and meta-analyses, and their role in the process of evidence-based decision-making. The strengths and weaknesses of traditional narrative reviews are discussed, as well as the way systematic reviews limit bias associated with the assembly, critical appraisal and synthesis of studies addressing specific clinical questions. The authors provide a step-by-step introduction for the most relevant issues in writing a systematic review from the initial formulation of a research question according to the PICO-rule to sensitivity analyses in conjunction with the combined analysis of the pooled data. Special emphasis is put on important issues that need to be considered when appraising a systematic review or meta-analysis, thus providing a description on how to use systematic reviews for clinical decision-making. Some of the terms that are frequently used in the reporting of systematic reviews and meta-analyses, such as relative risk, confidence interval, Forest plot or L'Abbé plot, will be introduced and explained.
20665783	Reproducibility of quantitative magnetization-transfer imaging parameters from repeated measurements.
Magn Reson Med  2010Aug
Quantitative magnetization-transfer imaging methods provide in vivo estimates of parameters of the two-pool model for magnetization-transfer in tissue. The goal of this study was to evaluate the reproducibility of quantitative magnetization-transfer imaging parameter estimates in healthy subjects. Magnetization-transfer-weighted and T(1) relaxometry data were acquired in five healthy subjects at multiple time points, and the variability of the resulting fitted magnetization-transfer parameters was evaluated. The impact of subsampling the magnetization-transfer data and correcting field inhomogeneities was also evaluated. The key parameters measured in this study had an average variability, across time points, of 4.7% for the relative size of the restricted pool (F), 7.3% for the forward exchange constant (k(f)), 1.9% for the free pool spin-lattice relaxation constant (R(1f)), 4.5% for the T(2) of the free pool (T(2f)), and 2.3% for the T(2) of the restricted pool (T(2r)). Our findings show that serial quantitative magnetization-transfer imaging experiments can be performed reliably, with good reproducibility of the model parameter estimates, and demonstrate the reproducibility of acquisition schemes with fewer magnetization-transfer contrasts. This establishes the feasibility of this technique for monitoring patients affected by degenerative white matter diseases while providing critical data to estimate the statistical power of such studies.
20665787	Vessel-encoded dynamic magnetic resonance angiography using arterial spin labeling.
Magn Reson Med  2010Aug
A new noninvasive MRI method for vessel-selective angiography is presented. The technique combines vessel-encoded pseudocontinuous arterial spin labeling with a two-dimensional dynamic angiographic readout and was used to image the cerebral arteries in healthy volunteers. Time-of-flight angiograms were also acquired prior to vessel-selective dynamic angiography acquisitions in axial, coronal, and/or sagittal planes, using a 3-T MRI scanner. The latter consisted of a vessel-encoded pseudocontinuous arterial spin labeling pulse train of 300 or 1000 ms followed by a two-dimensional thick-slab flow-compensated fast low-angle shot readout combined with a segmented Look-Locker sampling strategy (temporal resolution = 55 ms). Selective labeling was performed at the level of the neck to generate individual angiograms for both right and left internal carotid and vertebral arteries. Individual vessel angiograms were reconstructed using a bayesian inference method. The vessel-selective dynamic angiograms obtained were consistent with the time-of-flight images, and the longer of the two vessel-encoded pseudocontinuous arterial spin labeling pulse train durations tested (1000 ms) was found to give better distal vessel visibility. This technique provides highly selective angiograms quickly and noninvasively that could potentially be used in place of intra-arterial x-ray angiography for larger vessels.
20667303	Monitoring influenza activity in Europe with Google Flu Trends: comparison with the findings of sentinel physician networks - results for 2009-10.
Euro Surveill. 20100722 2010
The number of Internet searches has recently been used by Google to estimate the influenza incidence in the United States. We examined the correlation between the Google Flu Trends tool and sentinel networks estimates in several European countries during the 2009 influenza A(H1N1) pandemic and found a good correlation between estimates and peak incidence timing, with the highest peaks in countries where Internet is most frequently used for health-related searching. Although somehow limited, Google could be a valuable tool for syndromic surveillance.
20591175	MetaBar - a tool for consistent contextual data acquisition and standards compliant submission.
BMC Bioinformatics 20100630 2010
Environmental sequence datasets are increasing at an exponential rate; however, the vast majority of them lack appropriate descriptors like sampling location, time and depth/altitude: generally referred to as metadata or contextual data. The consistent capture and structured submission of these data is crucial for integrated data analysis and ecosystems modeling. The application MetaBar has been developed, to support consistent contextual data acquisition. MetaBar is a spreadsheet and web-based software tool designed to assist users in the consistent acquisition, electronic storage, and submission of contextual data associated to their samples. A preconfigured Microsoft Excel spreadsheet is used to initiate structured contextual data storage in the field or laboratory. Each sample is given a unique identifier and at any stage the sheets can be uploaded to the MetaBar database server. To label samples, identifiers can be printed as barcodes. An intuitive web interface provides quick access to the contextual data in the MetaBar database as well as user and project management capabilities. Export functions facilitate contextual and sequence data submission to the International Nucleotide Sequence Database Collaboration (INSDC), comprising of the DNA DataBase of Japan (DDBJ), the European Molecular Biology Laboratory database (EMBL) and GenBank. MetaBar requests and stores contextual data in compliance to the Genomic Standards Consortium specifications. The MetaBar open source code base for local installation is available under the GNU General Public License version 3 (GNU GPL3). The MetaBar software supports the typical workflow from data acquisition and field-sampling to contextual data enriched sequence submission to an INSDC database. The integration with the megx.net marine Ecological Genomics database and portal facilitates georeferenced data integration and metadata-based comparisons of sampling sites as well as interactive data visualization. The ample export functionalities and the INSDC submission support enable exchange of data across disciplines and safeguarding contextual data.
20671316	Extracting protein interactions from text with the unified AkaneRE event extraction system.
IEEE/ACM Trans Comput Biol Bioinform  2010 Jul-Sep
Currently, relation extraction (RE) and event extraction (EE) are the two main streams of biological information extraction. In 2009, the majority of these RE and EE research efforts were centered around the BioCreative II.5 Protein-Protein Interaction (PPI) challenge and the "BioNLP event extraction shared task." Although these challenges took somewhat different approaches, they share the same ultimate goal of extracting bio-knowledge from the literature. This paper compares the two challenge task definitions, and presents a unified system that was successfully applied in both these and several other PPI extraction task settings. The AkaneRE system has three parts: A core engine for RE, a pool of modules for specific solutions, and a configuration language to adapt the system to different tasks. The core engine is based on machine learning, using either Support Vector Machines or Statistical Classifiers and features extracted from given training data. The specific modules solve tasks like sentence boundary detection, tokenization, stemming, part-of-speech tagging, parsing, named entity recognition, generation of potential relations, generation of machine learning features for each relation, and finally, assignment of confidence scores and ranking of candidate relations. With these components, the AkaneRE system produces state-of-the-art results, and the system is freely available for academic purposes at http://www-tsujii.is.s.u-tokyo.ac.jp/satre/akane/.
20671317	An IR-aided machine learning framework for the BioCreative II.5 Challenge.
IEEE/ACM Trans Comput Biol Bioinform  2010 Jul-Sep
The team at the University of Wisconsin-Milwaukee developed an information retrieval and machine learning framework. Our framework requires only the standardized training data and depends upon minimal external knowledge resources and minimal parsing. Within the framework, we built our text mining systems and participated for the first time in all three BioCreative II.5 Challenge tasks. The results show that our systems performed among the top five teams for raw F1 scores in all three tasks and came in third place for the homonym ortholog F1 scores for the INT task. The results demonstrated that our IR-based framework is efficient, robust, and potentially scalable.
20400863	Therapeutic antibodies, vaccines and antibodyomes.
MAbs 20100514 2010 May-Jun
The potential for antibodies to act as "magic bullets" for treatment of human disease was recognized a century ago, but its full realization has began to occur only during the last decade. A key to their current success is the ability to make libraries of antibodies/B cells, isolate a single species, and engineer it to be safe, efficacious and of high quality. Despite this progress, major challenges to the effective prevention, diagnosis and treatment of a vast majority of diseases remain. Limited success in the development of effective vaccines against diseases such as AIDS and cancer reflects our incomplete understanding of how antibodies are generated and function. Only a miniscule number of antibodies are characterized out of the universe of antibodies generated by the immune system. Knowledge of antibodyomes-the complete sets of antibodies-could help solve these and other challenges.
20363371	Detective work in drug-induced liver injury: sometimes it is all about interviewing the right witness.
Clin. Gastroenterol. Hepatol. 20100402 2010Jul
Diagnosing drug-induced liver injury (DILI) relies primarily on history taking. We report 4 cases in which DILI was missed or the drug was misidentified when physicians relied solely on patient history. We reviewed 27 cases referred with possible DILI from August 1, 2009, to February 1, 2010. Four patients seemed to be reliable historians, but their cases were greatly clarified by a call to their pharmacist. One subject, who forgot a new medication, underwent an unfruitful evaluation including surgery. Another patient had acetaminophen toxicity that was missed because she grossly underreported her pain medication use. The third and fourth patients forgot taking amoxicillin/clavulanate, so other agents mistakenly were implicated. Roussel Uclaf Causality Assessment Method scores were 8 (probable) or 9 (highly probable) for all 4 cases. Without pharmacy input, DILI was missed in 2 cases and the wrong agent was implicated twice. Reviewing pharmacy records can be crucial for patients and DILI research. We recommend calling the pharmacist directly for increased liver enzyme levels of unclear source or suspected DILI regardless of patient history.
20494504	A rapid method for assessing social versus independent interest in health issues: a case study of 'bird flu' and 'swine flu'.
Soc Sci Med 20100424 2010Aug
Effective communication strategies regarding health issues are affected by the way in which the public obtain their knowledge, particularly whether people become interested independently, or through their social networks. This is often investigated through localized ethnography or surveys. In rapidly-evolving situations, however, there may also be a need for swift, case-specific assessment as a guide to initial strategy development. With this aim, we analyze real-time online data, provided by the new 'Google Trends' tool, concerning Internet search frequency for health-related issues. To these data we apply a simple model to characterise the effective degree of social transmission versus decisions made individually. As case examples, we explore two rapidly-evolved issues, namely the world-wide interest in avian influenza, or 'bird flu', in 2005, and in H1N1, or 'swine flu', from late April to early May 2009. The 2005 'bird flu' scare demonstrated almost pure imitation for two months initially, followed by a spike of independent decision that corresponded with an announcement by US president George Bush. For 'swine flu' in 2009, imitation was the more prevalent throughout. Overall, the results show how interest in health scares can spread primarily by social means, and that engaging more independent decisions at the population scale may require a dramatic announcement to push a populace over the 'tipping point'.
20616993	Text and structural data mining of influenza mentions in Web and social media.
Int J Environ Res Public Health 20100222 2010Feb
Text and structural data mining of web and social media (WSM) provides a novel disease surveillance resource and can identify online communities for targeted public health communications (PHC) to assure wide dissemination of pertinent information. WSM that mention influenza are harvested over a 24-week period, 5 October 2008 to 21 March 2009. Link analysis reveals communities for targeted PHC. Text mining is shown to identify trends in flu posts that correlate to real-world influenza-like illness patient report data. We also bring to bear a graph-based data mining technique to detect anomalies among flu blogs connected by publisher type, links, and user-tags.
20231047	Optimal search filters for renal information in EMBASE.
Am. J. Kidney Dis. 20100315 2010Jul
EMBASE is a popular database used to retrieve biomedical information. Our objective was to develop and test search filters to help clinicians and researchers efficiently retrieve articles with renal information in EMBASE. We used a diagnostic test assessment framework because filters operate similarly to screening tests. We divided a sample of 5,302 articles from 39 journals into development and validation sets of articles. Information retrieval properties were assessed by treating each search filter as a "diagnostic test" or screening procedure for the detection of relevant articles. We tested the performance of 1,936,799 search filters made of unique renal terms and their combinations. REFERENCE STANDARD &amp; OUTCOME: The reference standard was manual review of each article. We calculated the sensitivity and specificity of each filter to identify articles with renal information. The best renal filters consisted of multiple search terms, such as "renal replacement therapy," "renal," "kidney disease," and "proteinuria," and the truncated terms "kidney," "dialy," "neph," "glomerul," and "hemodial." These filters achieved peak sensitivities of 98.7% (95% CI, 97.9-99.6) and specificities of 98.5% (95% CI, 98.0-99.0). The retrieval performance of these filters remained excellent in the validation set of independent articles. The retrieval performance of any search will vary depending on the quality of all search concepts used, not just renal terms. We empirically developed and validated high-performance renal search filters for EMBASE. These filters can be programmed into the search engine or used on their own to improve the efficiency of searching.
20472411	Document classification for mining host pathogen protein-protein interactions.
Artif Intell Med 20100515 2010Jul
Scientific findings regarding human pathogens and their host responses are buried in the growing volume of biomedical literature and there is an urgent need to mine information pertaining to pathogenesis-related proteins especially host pathogen protein-protein interactions (HP-PPIs) from literature. In this paper, we report our exploration of developing an automated system to identify MEDLINE abstracts referring to HP-PPIs. An annotated corpus consisting of 1360 MEDLINE abstracts was generated. With this corpus, we developed and evaluated document classification systems using support vector machines (SVMs). We also investigated the effects of three feature selection methods:information gain (IG), chi(2) test, and specific mutual information (SI). The performance was measured using normalized discounted cumulative gain (NDCG) and positive predictive value (PPV) and all measures were obtained through 10-fold cross validation. NDCG measures for classification systems using all features or a subset of features selected using IG and chi(2) test range from 0.83 to 0.89 while classification systems built based on features selected using SI had relatively lower NDCG measures. The classification system achieved a PPV of 50.7% for the top 10% ranked documents comparing to a baseline PPV of 10.0%. Our results indicate that document classification systems can be constructed to efficiently retrieve HP-PPI related documents. Feature selection was effective in reducing the dimensionality of features to build a compact system.
20515448	EnvMine: a text-mining system for the automatic extraction of contextual information.
BMC Bioinformatics 20100601 2010
For ecological studies, it is crucial to count on adequate descriptions of the environments and samples being studied. Such a description must be done in terms of their physicochemical characteristics, allowing a direct comparison between different environments that would be difficult to do otherwise. Also the characterization must include the precise geographical location, to make possible the study of geographical distributions and biogeographical patterns. Currently, there is no schema for annotating these environmental features, and these data have to be extracted from textual sources (published articles). So far, this had to be performed by manual inspection of the corresponding documents. To facilitate this task, we have developed EnvMine, a set of text-mining tools devoted to retrieve contextual information (physicochemical variables and geographical locations) from textual sources of any kind. EnvMine is capable of retrieving the physicochemical variables cited in the text, by means of the accurate identification of their associated units of measurement. In this task, the system achieves a recall (percentage of items retrieved) of 92% with less than 1% error. Also a Bayesian classifier was tested for distinguishing parts of the text describing environmental characteristics from others dealing with, for instance, experimental settings.Regarding the identification of geographical locations, the system takes advantage of existing databases such as GeoNames to achieve 86% recall with 92% precision. The identification of a location includes also the determination of its exact coordinates (latitude and longitude), thus allowing the calculation of distance between the individual locations. EnvMine is a very efficient method for extracting contextual information from different text sources, like published articles or web pages. This tool can help in determining the precise location and physicochemical variables of sampling sites, thus facilitating the performance of ecological analyses. EnvMine can also help in the development of standards for the annotation of environmental features.
20623018	Using a relational database to index infectious disease information.
Int J Environ Res Public Health 20100504 2010May
Mapping medical knowledge into a relational database became possible with the availability of personal computers and user-friendly database software in the early 1990s. To create a database of medical knowledge, the domain expert works like a mapmaker to first outline the domain and then add the details, starting with the most prominent features. The resulting "intelligent database" can support the decisions of healthcare professionals. The intelligent database described in this article contains profiles of 275 infectious diseases. Users can query the database for all diseases matching one or more specific criteria (symptom, endemic region of the world, or epidemiological factor). Epidemiological factors include sources (patients, water, soil, or animals), routes of entry, and insect vectors. Medical and public health professionals could use such a database as a decision-support software tool.
20144734	A network-theoretic approach for decompositional translation across Open Biological Ontologies.
J Biomed Inform 20100206 2010Aug
Biological ontologies are now being widely used for annotation, sharing and retrieval of the biological data. Many of these ontologies are hosted under the umbrella of the Open Biological Ontologies Foundry. In order to support interterminology mapping, composite terms in these ontologies need to be translated into atomic or primitive terms in other, orthogonal ontologies, for example, gluconeogenesis (biological process term) to glucose (chemical ontology term). Identifying such decompositional ontology translations is a challenging problem. In this paper, we propose a network-theoretic approach based on the structure of the integrated OBO relationship graph. We use a network-theoretic measure, called the clustering coefficient, to find relevant atomic terms in the neighborhood of a composite term. By eliminating the existing GO to ChEBI Ontology mappings from OBO, we evaluate whether the proposed approach can re-identify the corresponding relationships. The results indicate that the network structure provides strong cues for decompositional ontology translation and the existing relationships can be used to identify new translations.
20152935	UMLS content views appropriate for NLP processing of the biomedical literature vs. clinical text.
J Biomed Inform 20100210 2010Aug
Identification of medical terms in free text is a first step in such Natural Language Processing (NLP) tasks as automatic indexing of biomedical literature and extraction of patients' problem lists from the text of clinical notes. Many tools developed to perform these tasks use biomedical knowledge encoded in the Unified Medical Language System (UMLS) Metathesaurus. We continue our exploration of automatic approaches to creation of subsets (UMLS content views) which can support NLP processing of either the biomedical literature or clinical text. We found that suppression of highly ambiguous terms in the conservative AutoFilter content view can partially replace manual filtering for literature applications, and suppression of two character mappings in the same content view achieves 89.5% precision at 78.6% recall for clinical applications.
20362071	Selecting information in electronic health records for knowledge acquisition.
J Biomed Inform 20100331 2010Aug
Knowledge acquisition of relations between biomedical entities is critical for many automated biomedical applications, including pharmacovigilance and decision support. Automated acquisition of statistical associations from biomedical and clinical documents has shown some promise. However, acquisition of clinically meaningful relations (i.e. specific associations) remains challenging because textual information is noisy and co-occurrence does not typically determine specific relations. In this work, we focus on acquisition of two types of relations from clinical reports: disease-manifestation related symptom (MRS) and drug-adverse drug event (ADE), and explore the use of filtering by sections of the reports to improve performance. Evaluation indicated that applying the filters improved recall (disease-MRS: from 0.85 to 0.90; drug-ADE: from 0.43 to 0.75) and precision (disease-MRS: from 0.82 to 0.92; drug-ADE: from 0.16 to 0.31). This preliminary study demonstrates that selecting information in narrative electronic reports based on the sections improves the detection of disease-MRS and drug-ADE types of relations. Further investigation of complementary methods, such as more sophisticated statistical methods, more complex temporal models and use of information from other knowledge sources, is needed.
20628666	Enabling research in general practice--increasing functionality of electronic medical records.
Aust Fam Physician  2010Jul
With an estimated 80% of Australians visiting a general practitioner at least once a year, the data generated by GPs is a rich source of the overall health profile of patients. However, this data is rarely used to report on health outcomes. This article reports on the use of remote access of electronic medical records (EMRs) for the purpose of collecting data during a collaborative research project involving the staff of three general practices and an external research team. Throughout the project numerous benefits to remotely accessing general practice EMRs were identified. However, there remain some difficulties which need to be addressed. An increased functionality of the software programs used in general practice is required, along with improvements in the utilisation of the software capabilities. Collaboration between clinicians, researchers and clinical software developers will be vital to advance this process.
20632571	A fast, angle-dependent, analytical model of CsI detector response for optimization of 3D x-ray breast imaging systems.
Med Phys  2010Jun
Accurate models of detector blur are crucial for performing meaningful optimizations of three-dimensional (3D) x-ray breast imaging systems as well as for developing reconstruction algorithms that faithfully reproduce the imaged object anatomy. So far, x-ray detector blur has either been ignored or modeled as a shift-invariant symmetric function for these applications. The recent development of a Monte Carlo simulation package called MANTIS has allowed detailed modeling of these detector blur functions and demonstrated the magnitude of the anisotropy for both tomosynthesis and breast CT imaging systems. Despite the detailed results that MANTIS produces, the long simulation times required make inclusion of these results impractical in rigorous optimization and reconstruction algorithms. As a result, there is a need for detector blur models that can be rapidly generated. In this study, the authors have derived an analytical model for deterministic detector blur functions, referred to here as point response functions (PRFs), of columnar CsI phosphor screens. The analytical model is x-ray energy and incidence angle dependent and draws on results from MANTIS to indirectly include complicated interactions that are not explicitly included in the mathematical model. Once the mathematical expression is derived, values of the coefficients are determined by a two-dimensional (2D) fit to MANTIS-generated results based on a figure-of-merit (FOM) that measures the normalized differences between the MANTIS and analytical model results averaged over a region of interest. A smaller FOM indicates a better fit. This analysis was performed for a monochromatic x-ray energy of 25 keV, a CsI scintillator thickness of 150 microm, and four incidence angles (0 degrees, 15 degrees, 30 degrees, and 45 degrees). The FOMs comparing the analytical model to MANTIS for these parameters were 0.1951 +/- 0.0011, 0.1915 +/- 0.0014, 0.2266 +/- 0.0021, and 0.2416 +/- 0.0074 for 0 degrees, 15 degrees, 30 degrees, and 45 degrees, respectively. As a comparison, the same FOMs comparing MANTIS to 2D symmetric Gaussian fits to the zero-angle PRF were 0.6234 +/- 0.0020, 0.9058 +/- 0.0029, 1.491 +/- 0.012, and 2.757 +/- 0.039 for the same set of incidence angles. Therefore, the analytical model matches MANTIS results much better than a 2D symmetric Gaussian function. A comparison was also made against experimental data for a 170 microm thick CsI screen and an x-ray energy of 25.6 keV. The corresponding FOMs were 0.3457 +/- 0.0036, 0.3281 +/- 0.0057, 0.3422 +/- 0.0023, and 0.3677 +/- 0.0041 for 0 degrees, 15 degrees, 30 degrees, and 45 degrees, respectively. In a previous study, FOMs comparing the same experimental data to MANTIS PRFs were found to be 0.2944 +/- 0.0027, 0.2387 +/- 0.0039, 0.2816 +/- 0.0025, and 0.2665 +/- 0.0032 for the same set of incidence angles. The two sets of derived FOMs, comparing MANTIS-generated PRFs and experimental data to the analytical model, demonstrate that the analytical model is able to reproduce experimental data with a FOM of less than two times that comparing MANTIs and experimental data. This performance is achieved in less than one millionth the computation time required to generate a comparable PRF with MANTIS. Such small computation times will allow for the inclusion of detailed detector physics in rigorous optimization and reconstruction algorithms for 3D x-ray breast imaging systems.
20632607	Extraction of tube current values from DICOM CT images for patient dose estimation.
Med Phys  2010Jun
When CT examinations are conducted with automatic mA modulation (z-axis modulation), the tube current (mA) may vary from one CT slice to next, i.e., mA is no longer a fixed constant. Hence, patient dose calculation is no longer a straightforward process. The purpose of this article is to show how the mA information may be extracted from DICOM CT images without having to manually read off "mA values" from the displayed images one at a time. A statistical programming language called "R," which is capable of reading DICOM files, is employed to extract the mA values from a series of DICOM CT images. This task is carried out with a "script" designed to read the mA values from the DICOM CT images and generate a file in "delimited ASCII" format. This file can be imported into a spreadsheet program such as Excel (a Microsoft spreadsheet program) for further processing, calculation, and chart production. A CT examination of the chest was selected to carry out this operation for demonstration purposes. The "script" generated a delimited ASCII "file" which is a two column data sheet with the slice location and its corresponding mA values. After the file is imported into Excel for calculation, and with other pertinent scan parameters, the average mA can now be plugged into a CT dosimetry calculation program such as ImPACT for calculation of CTDI*w, CTDIvol, dose-length-product, and critical organ doses. Furthermore, a graphical presentation of the "mA" vs slice location can be produced with Excel. The chart generated by Excel shows the variation in tube current as a function of slice location. The area under the mA curve is equal to a rectangle of "average mA" x "distance". Here, the distance is the range covered by the CT scan. The script that the authors have written is able to extract mA values from DICOM CT images, generating a delimited ASCII file for further processing with Microsoft Excel spreadsheet program. After the DICOM images are imported into a personal computer, this semiautomated process of extracting mA values enabled the authors to perform dose calculation for patients undergoing CT examination scanned with automatic mA modulation control.
20634551	Color to gray: visual cue preservation.
IEEE Trans Pattern Anal Mach Intell  2010Sep
Both commercial and scientific applications often need to transform color images into gray-scale images, e.g., to reduce the publication cost in printing color images or to help color blind people see visual cues of color images. However, conventional color to gray algorithms are not ready for practical applications because they encounter the following problems: 1) Visual cues are not well defined so it is unclear how to preserve important cues in the transformed gray-scale images; 2) some algorithms have extremely high time cost for computation; and 3) some require human-computer interactions to have a reasonable transformation. To solve or at least reduce these problems, we propose a new algorithm based on a probabilistic graphical model with the assumption that the image is defined over a Markov random field. Thus, color to gray procedure can be regarded as a labeling process to preserve the newly well--defined visual cues of a color image in the transformed gray-scale image. Visual cues are measurements that can be extracted from a color image by a perceiver. They indicate the state of some properties of the image that the perceiver is interested in perceiving. Different people may perceive different cues from the same color image and three cues are defined in this paper, namely, color spatial consistency, image structure information, and color channel perception priority. We cast color to gray as a visual cue preservation procedure based on a probabilistic graphical model and optimize the model based on an integral minimization problem. We apply the new algorithm to both natural color images and artificial pictures, and demonstrate that the proposed approach outperforms representative conventional algorithms in terms of effectiveness and efficiency. In addition, it requires no human-computer interactions.
20150321	Fast and efficient searching of biological data resources--using EB-eye.
Brief. Bioinformatics 20100211 2010Jul
The EB-eye is a fast and efficient search engine that provides easy and uniform access to the biological data resources hosted at the EMBL-EBI. Currently, users can access information from more than 62 distinct datasets covering some 400 million entries. The data resources represented in the EB-eye include: nucleotide and protein sequences at both the genomic and proteomic levels, structures ranging from chemicals to macro-molecular complexes, gene-expression experiments, binary level molecular interactions as well as reaction maps and pathway models, functional classifications, biological ontologies, and comprehensive literature libraries covering the biomedical sciences and related intellectual property. The EB-eye can be accessed over the web or programmatically using a SOAP Web Services interface. This allows its search and retrieval capabilities to be exploited in workflows and analytical pipe-lines. The EB-eye is a novel alternative to existing biological search and retrieval engines. In this article we describe in detail how to exploit its powerful capabilities.
20636174	The preparation of technologically literate graduates for professional practice.
Contemp Nurse  2010 Apr-May
The impact of information and communication technology has been felt globally and the healthcare sector is not immune to the changes brought about by the introduction of new technologies. In contemporary clinical practice environments, information and communication technology skills are advantageous, not only to nurses, but also to the patients for whom they care. There is good evidence that these skills, appropriately utilised, can have a significant impact on patient outcomes. This scholarly paper presents the background to a project that explores graduate nurses' experiences of using information and communication technology in clinical contexts. A broad historical overview of the implementation of information and communication technology in higher education and healthcare in Australia is provided before discussing the extent to which the technology skills learnt at university are relevant or transferable to contemporary practice environments. The current levels and use of information and communication technology among new graduate nurses, the apparent dichotomy between technological versus humanised healthcare, and the need for national information and communication technology competency standards are discussed.
20565551	OTseeker helps library and allied health professionals to find quality evidence efficiently.
Health Info Libr J  2010Jun
Research is essential for evidence-based practice yet many health professionals do not have enough time to find research. Studies relevant to occupational therapists can be particularly difficult to find. Most search engines are broad and return a large number of irrelevant articles. Occupational Therapy Systematic Evaluation of Evidence (OTseeker) is an occupational therapy database available at http://www.otseeker.com. Developed by Australian occupational therapists, the resource aims to increase access to research and support clinical decision making. This discipline-specific database contains pre-appraised information from a variety of sources and decreases the time required to locate best evidence. The aims of this paper are to: (i) describe how health librarians can use OTseeker to help allied health students, researchers and practitioners, particularly in occupational therapy, to find quality evidence; (ii) provide a teaching resource for health librarians based around the OTseeker evidence database; and (iii) highlight new features contained on the OTseeker database. A case study is provided which focuses on searching for evidence on the effectiveness of upper limb rehabilitation after stroke using OTseeker. This paper may increase the knowledge, skills and competencies of health librarians, helping them to access evidence-based databases, and educate other professionals.
20565552	Literature searching for social science systematic reviews: consideration of a range of search techniques.
Health Info Libr J  2010Jun
Literature for a systematic review on the student experience of e-learning is located across a range of subject areas including health, education, social science, library and information science. To assess the merits and shortcomings of using different search techniques in retrieval of evidence in the social science literature. A conventional subject search was undertaken as the principal method of identifying the literature for the review. Four supplementary search methods were used including citation searching, reference list checking, contact with experts and pearl growing. The conventional subject search identified 30 of 41 included references; retrieved from 10 different databases. References were missed by this method and a further 11 references were identified via citation searching, reference list checking and contact with experts. Pearl growing was suspended as the nominated pearls were dispersed across numerous databases, with no single database indexing more than four pearls. Searching within the social sciences literature requires careful consideration. Conventional subject searching identified the majority of references, but additional search techniques were essential and located further high quality references.
20565555	Romanian psychiatric literature: analysis of accessibility and nature of Romanian psychiatric articles.
Health Info Libr J  2010Jun
Romania is a low-income country of 22 million people and, currently, information regarding mental health research is limited. Romania is one of the last countries in eastern Europe not to have its own bibliographic biomedical database. To assess the content and quality of Romanian psychiatric research activity over time. EMBASE (1980 to April 2008), MEDLINE (1950 to April 2008) and PsycINFO (1806 to April 2008) were systematically searched for psychiatric articles originating from Romania. The sample from PsycINFO was described. PsycINFO was by far the best source of Romanian mental health literature with a considerable increase in the publication activity since 2000 (PsycINFO identified 3236 hits, MEDLINE 549, EMBASE 139). Most papers are in English, but a sizeable minority are in Romanian (30%), French (4%) or Hungarian (4%). The main topics of interest are cognitive processes, creativity, schizophrenia and cognitive development and stress and are, according to PsycINFO's indexing, 'empirical studies'. Seventeen randomised trials were identified with all studies after 2000 being sponsored by industry. Surprisingly, and not in keeping with other studies of the literature of neighbouring countries, PsycINFO is the major source of psychiatric bibliographic records of this region. There are signs of a resurgence of research activity in Romania and as the number of local mental health workers increases we can expect more output. Industry is now funding evaluative studies in Romania. As everywhere, but perhaps more acutely in situations of severely limited research support, there is a difficult balance to be struck between benefiting support and losing independence.
20565556	Characteristics and publication patterns of theses from a Peruvian medical school.
Health Info Libr J  2010Jun
Many medical schools require a student thesis before graduation. Publishing results in a peer-reviewed journal could be an indicator of scientific value and acceptability by the scientific community. The publication pattern of theses published by medical students in Peru is unknown. The aim of this study was to assess the characteristics and publication pattern of theses in biomedical-indexed journals conducted by medical students in a university with the highest research output in Peru. Data from registered theses between 2000 and 2003 were obtained from the university library. Publication of theses in biomedical journals was assessed in 2008 by a search strategy using PubMed, Google Scholar, LILACS, LIPECS and SciELO. Four hundred and eighty-two medical theses were registered between 2000 and 2003; 85 (17.6%) were published in biomedical-indexed journals. Of the published theses, 28 (5.8%) were published in MEDLINE-indexed journals, 55 (11.4%) in SciELO-indexed journals, 61 (12.6%) in LILACS-indexed journals and 68 (14.1%) in LIPECS-indexed journals. Most of the published theses (80%) were in Spanish and published in Peruvian journals; and 17 theses (20%) were published in foreign journals (all of them indexed in MEDLINE). In addition, 37 (43.5%) belong primarily to internal medicine, and 24 (28.2%) belong primarily to infectious diseases. Medical students were first authors in 71 (83.5%) of the articles. In this study, most of the published theses were in Spanish, published in local journals and indexed in LIPECS. The percentage of published theses in biomedical journals at this university is comparable with others coming from developed countries.
20566495	Wildlife tracking data management: a new vision.
Philos. Trans. R. Soc. Lond., B, Biol. Sci.  2010Jul27
To date, the processing of wildlife location data has relied on a diversity of software and file formats. Data management and the following spatial and statistical analyses were undertaken in multiple steps, involving many time-consuming importing/exporting phases. Recent technological advancements in tracking systems have made large, continuous, high-frequency datasets of wildlife behavioural data available, such as those derived from the global positioning system (GPS) and other animal-attached sensor devices. These data can be further complemented by a wide range of other information about the animals' environment. Management of these large and diverse datasets for modelling animal behaviour and ecology can prove challenging, slowing down analysis and increasing the probability of mistakes in data handling. We address these issues by critically evaluating the requirements for good management of GPS data for wildlife biology. We highlight that dedicated data management tools and expertise are needed. We explore current research in wildlife data management. We suggest a general direction of development, based on a modular software architecture with a spatial database at its core, where interoperability, data model design and integration with remote-sensing data sources play an important role in successful GPS data handling.
20568579	[New drugs of abuse on the Web: the role of the Psychonaut Web Mapping Project].
Riv Psichiatr  2010 Mar-Apr
In the rapid change of drug scenarios, as the powerful development in the drug market, particularly in the number and the kind of the compound available, Internet plays a dominant role to become one of the major "drug market". The European Commission funded the Psychonaut Web Mapping Project (carried out in the time-frame January 2008-December 2009), with the aim to start/implement an Early Warning System (through the data/information collected from the Web virtual market), to identify and categorise novel recreational drugs/psychoactive compounds (synthetical/herbal drugs), and new trends in drug use to provide information for immediate and prevention intervention. The Psychonaut is a multi-site research project involving 8 research centres (De Sleutel, Belgium; University of Hertfordshire School of Pharmacy, St George's University of London, England; A-klinikkasäätiö, Finlandia; Klinik für Psychiatrie und Psychotherapie, Germany; Assessorato Salute Regione Marche, Italy; Drug Abuse Unit, Spain; Centre of Competence Bergen Clinics Foundation, Norway) based in 7 European Countries (England, Italy, Belgium, Finland, Germany, Spain, Norway).
20571374	The outcomes of anxiety, confidence, and self-efficacy with Internet health information retrieval in older adults: a pilot study.
Comput Inform Nurs  2010 Jul-Aug
Technology has a great impact on nursing practice. With the increasing numbers of older Americans using computers and the Internet in recent years, nurses have the capability to deliver effective and efficient health education to their patients and the community. Based on the theoretical framework of Bandura's self-efficacy theory, the pilot project reported findings from a 5-week computer course on Internet health searches in older adults, 65 years or older, at a senior activity learning center. Twelve participants were recruited and randomized to either the intervention or the control group. Measures of computer anxiety, computer confidence, and computer self-efficacy scores were analyzed at baseline, at the end of the program, and 6 weeks after the completion of the program. Analysis was conducted with repeated-measures analysis of variance. Findings showed participants who attended a structured computer course on Internet health information retrieval reported lowered anxiety and increased confidence and self-efficacy at the end of the 5-week program and 6 weeks after the completion of the program as compared with participants who were not in the program. The study demonstrated that a computer course can help reduce anxiety and increase confidence and self-efficacy in online health searches in older adults.
20482806	HEALTH GeoJunction: place-time-concept browsing of health publications.
Int J Health Geogr 20100518 2010
The volume of health science publications is escalating rapidly. Thus, keeping up with developments is becoming harder as is the task of finding important cross-domain connections. When geographic location is a relevant component of research reported in publications, these tasks are more difficult because standard search and indexing facilities have limited or no ability to identify geographic foci in documents. This paper introduces HEALTH GeoJunction, a web application that supports researchers in the task of quickly finding scientific publications that are relevant geographically and temporally as well as thematically. HEALTH GeoJunction is a geovisual analytics-enabled web application providing: (a) web services using computational reasoning methods to extract place-time-concept information from bibliographic data for documents and (b) visually-enabled place-time-concept query, filtering, and contextualizing tools that apply to both the documents and their extracted content. This paper focuses specifically on strategies for visually-enabled, iterative, facet-like, place-time-concept filtering that allows analysts to quickly drill down to scientific findings of interest in PubMed abstracts and to explore relations among abstracts and extracted concepts in place and time. The approach enables analysts to: find publications without knowing all relevant query parameters, recognize unanticipated geographic relations within and among documents in multiple health domains, identify the thematic emphasis of research targeting particular places, notice changes in concepts over time, and notice changes in places where concepts are emphasized. PubMed is a database of over 19 million biomedical abstracts and citations maintained by the National Center for Biotechnology Information; achieving quick filtering is an important contribution due to the database size. Including geography in filters is important due to rapidly escalating attention to geographic factors in public health. The implementation of mechanisms for iterative place-time-concept filtering makes it possible to narrow searches efficiently and quickly from thousands of documents to a small subset that meet place-time-concept constraints. Support for a more-like-this query creates the potential to identify unexpected connections across diverse areas of research. Multi-view visualization methods support understanding of the place, time, and concept components of document collections and enable comparison of filtered query results to the full set of publications.
20574204	Evidence-based practice step by step: Critical appraisal of the evidence: part I.
Am J Nurs  2010Jul
This is the fifth article in a series from the Arizona State University College of Nursing and Health Innovation's Center for the Advancement of Evidence-Based Practice. Evidence-based practice (EBP) is a problem-solving approach to the delivery of health care that integrates the best evidence from studies and patient care data with clinician expertise and patient preferences and values. When delivered in a context of caring and in a supportive organizational culture, the highest quality of care and best patient outcomes can be achieved.The purpose of this series is to give nurses the knowledge and skills they need to implement EBP consistently, one step at a time. Articles will appear every two months to allow you time to incorporate information as you work toward implementing EBP at your institution. Also, we've scheduled "Chat with the Authors" calls every few months to provide a direct line to the experts to help you resolve questions. Details about how to participate in the next call will be published with September's Evidence-Based Practice, Step by Step.
20509921	Classifying genes to the correct Gene Ontology Slim term in Saccharomyces cerevisiae using neighbouring genes with classification learning.
BMC Genomics 20100528 2010
There is increasing evidence that gene location and surrounding genes influence the functionality of genes in the eukaryotic genome. Knowing the Gene Ontology Slim terms associated with a gene gives us insight into a gene's functionality by informing us how its gene product behaves in a cellular context using three different ontologies: molecular function, biological process, and cellular component. In this study, we analyzed if we could classify a gene in Saccharomyces cerevisiae to its correct Gene Ontology Slim term using information about its location in the genome and information from its nearest-neighbouring genes using classification learning. We performed experiments to establish that the MultiBoostAB algorithm using the J48 classifier could correctly classify Gene Ontology Slim terms of a gene given information regarding the gene's location and information from its nearest-neighbouring genes for training. Different neighbourhood sizes were examined to determine how many nearest neighbours should be included around each gene to provide better classification rules. Our results show that by just incorporating neighbour information from each gene's two-nearest neighbours, the percentage of correctly classified genes to their correct Gene Ontology Slim term for each ontology reaches over 80% with high accuracy (reflected in F-measures over 0.80) of the classification rules produced. We confirmed that in classifying genes to their correct Gene Ontology Slim term, the inclusion of neighbour information from those genes is beneficial. Knowing the location of a gene and the Gene Ontology Slim information from neighbouring genes gives us insight into that gene's functionality. This benefit is seen by just including information from a gene's two-nearest neighbouring genes.
20509878	Spatio-structural granularity of biological material entities.
BMC Bioinformatics 20100528 2010
With the continuously increasing demands on knowledge- and data-management that databases have to meet, ontologies and the theories of granularity they use become more and more important. Unfortunately, currently used theories and schemes of granularity unnecessarily limit the performance of ontologies due to two shortcomings: (i) they do not allow the integration of multiple granularity perspectives into one granularity framework; (ii) they are not applicable to cumulative-constitutively organized material entities, which cover most of the biomedical material entities. The above mentioned shortcomings are responsible for the major inconsistencies in currently used spatio-structural granularity schemes. By using the Basic Formal Ontology (BFO) as a top-level ontology and Keet's general theory of granularity, a granularity framework is presented that is applicable to cumulative-constitutively organized material entities. It provides a scheme for granulating complex material entities into their constitutive and regional parts by integrating various compositional and spatial granularity perspectives. Within a scale dependent resolution perspective, it even allows distinguishing different types of representations of the same material entity. Within other scale dependent perspectives, which are based on specific types of measurements (e.g. weight, volume, etc.), the possibility of organizing instances of material entities independent of their parthood relations and only according to increasing measures is provided as well. All granularity perspectives are connected to one another through overcrossing granularity levels, together forming an integrated whole that uses the compositional object perspective as an integrating backbone. This granularity framework allows to consistently assign structural granularity values to all different types of material entities. The here presented framework provides a spatio-structural granularity framework for all domain reference ontologies that model cumulative-constitutively organized material entities. With its multi-perspectives approach it allows querying an ontology stored in a database at one's own desired different levels of detail: The contents of a database can be organized according to diverse granularity perspectives, which in their turn provide different views on its content (i.e. data, knowledge), each organized into different levels of detail.
20470429	Combining classifiers for robust PICO element detection.
BMC Med Inform Decis Mak 20100515 2010
Formulating a clinical information need in terms of the four atomic parts which are Population/Problem, Intervention, Comparison and Outcome (known as PICO elements) facilitates searching for a precise answer within a large medical citation database. However, using PICO defined items in the information retrieval process requires a search engine to be able to detect and index PICO elements in the collection in order for the system to retrieve relevant documents. In this study, we tested multiple supervised classification algorithms and their combinations for detecting PICO elements within medical abstracts. Using the structural descriptors that are embedded in some medical abstracts, we have automatically gathered large training/testing data sets for each PICO element. Combining multiple classifiers using a weighted linear combination of their prediction scores achieves promising results with an f-measure score of 86.3% for P, 67% for I and 56.6% for O. Our experiments on the identification of PICO elements showed that the task is very challenging. Nevertheless, the performance achieved by our identification method is competitive with previously published results and shows that this task can be achieved with a high accuracy for the P element but lower ones for I and O elements.
20585653	Open access to the scientific journal literature: situation 2009.
PLoS ONE 20100623 2010
The Internet has recently made possible the free global availability of scientific journal articles. Open Access (OA) can occur either via OA scientific journals, or via authors posting manuscripts of articles published in subscription journals in open web repositories. So far there have been few systematic studies showing how big the extent of OA is, in particular studies covering all fields of science. The proportion of peer reviewed scholarly journal articles, which are available openly in full text on the web, was studied using a random sample of 1837 titles and a web search engine. Of articles published in 2008, 8.5% were freely available at the publishers' sites. For an additional 11.9% free manuscript versions could be found using search engines, making the overall OA percentage 20.4%. Chemistry (13%) had the lowest overall share of OA, Earth Sciences (33%) the highest. In medicine, biochemistry and chemistry publishing in OA journals was more common. In all other fields author-posted manuscript copies dominated the picture. The results show that OA already has a significant positive impact on the availability of the scientific journal literature and that there are big differences between scientific disciplines in the uptake. Due to the lack of awareness of OA-publishing among scientists in most fields outside physics, the results should be of general interest to all scholars. The results should also interest academic publishers, who need to take into account OA in their business strategies and copyright policies, as well as research funders, who like the NIH are starting to require OA availability of results from research projects they fund. The method and search tools developed also offer a good basis for more in-depth studies as well as longitudinal studies.
20588838	Time domain spectral phase encoding/DPSK data modulation using single phase modulator for OCDMA application.
Opt Express  2010May10
A novel scheme using single phase modulator for simultaneous time domain spectral phase encoding (SPE) signal generation and DPSK data modulation is proposed and experimentally demonstrated. Array- Waveguide-Grating and Variable-Bandwidth-Spectrum-Shaper based devices can be used for decoding the signal directly in spectral domain. The effects of fiber dispersion, light pulse width and timing error on the coding performance have been investigated by simulation and verified in experiment. In the experiment, SPE signal with 8-chip, 20GHz/chip optical code patterns has been generated and modulated with 2.5 Gbps DPSK data using single modulator. Transmission of the 2.5 Gbps data over 34km fiber with BER&lt;10(-9) has been demonstrated successfully. The proposed scheme has simple configuration and improved flexibility that can significantly improve the data confidentiality for optical code division multiple access (OCDMA) and secure optical communication applications.
20588846	Sonification of optical coherence tomography data and images.
Opt Express  2010May10
Sonification is the process of representing data as non-speech audio signals. In this manuscript, we describe the auditory presentation of OCT data and images. OCT acquisition rates frequently exceed our ability to visually analyze image-based data, and multi-sensory input may therefore facilitate rapid interpretation. This conversion will be especially valuable in time-sensitive surgical or diagnostic procedures. In these scenarios, auditory feedback can complement visual data without requiring the surgeon to constantly monitor the screen, or provide additional feedback in non-imaging procedures such as guided needle biopsies which use only axial-scan data. In this paper we present techniques to translate OCT data and images into sound based on the spatial and spatial frequency properties of the OCT data. Results obtained from parameter-mapped sonification of human adipose and tumor tissues are presented, indicating that audio feedback of OCT data may be useful for the interpretation of OCT images.
20588876	Large phase-stepping approach for high-resolution hard X-ray grating-based multiple-information imaging.
Opt Express  2010May10
High-resolution hard X-ray grating-based imaging method with conventional X-ray sources provides attenuation, refraction and scattering information synchronously, and it is regarded as the next-generation X-ray imaging technology for medical and industrial applications. In this letter, a large phase-stepping approach with at least one order of magnitude lower resolution of the movement is presented to equivalently substitute the current high-positioning-resolution phase-stepping approach. Both the theoretical deduction and actual experiment prove that the new approach is available to relax the requirement of high positioning resolution and strict circumstances so as to benefit the future commercial applications of the grating-based multiple-information imaging technology.
20589065	Double image encryption by using iterative random binary encoding in gyrator domains.
Opt Express  2010May24
We propose a double image encryption by using random binary encoding and gyrator transform. Two secret images are first regarded as the real part and imaginary part of complex function. Chaotic map is used for obtaining random binary matrix. The real part and imaginary part of complex function are exchanged under the control of random binary data. An iterative structure composed of the random binary encoding method is designed and employed for enhancing the security of encryption algorithm. The parameters in chaotic map and gyrator transform serve as the keys of this encryption scheme. Some numerical simulations have been made, to demonstrate the performance this algorithm.
20505002	Threshold Average Precision (TAP-k): a measure of retrieval designed for bioinformatics.
Bioinformatics 20100526 2010Jul15
Since database retrieval is a fundamental operation, the measurement of retrieval efficacy is critical to progress in bioinformatics. This article points out some issues with current methods of measuring retrieval efficacy and suggests some improvements. In particular, many studies have used the pooled receiver operating characteristic for n irrelevant records (ROC(n)) score, the area under the ROC curve (AUC) of a 'pooled' ROC curve, truncated at n irrelevant records. Unfortunately, the pooled ROC(n) score does not faithfully reflect actual usage of retrieval algorithms. Additionally, a pooled ROC(n) score can be very sensitive to retrieval results from as little as a single query. To replace the pooled ROC(n) score, we propose the Threshold Average Precision (TAP-k), a measure closely related to the well-known average precision in information retrieval, but reflecting the usage of E-values in bioinformatics. Furthermore, in addition to conditions previously given in the literature, we introduce three new criteria that an ideal measure of retrieval efficacy should satisfy. PSI-BLAST, GLOBAL, HMMER and RPS-BLAST provided examples of using the TAP-k and pooled ROC(n) scores to evaluate sequence retrieval algorithms. In particular, compelling examples using real data highlight the drawbacks of the pooled ROC(n) score, showing that it can produce evaluations skewing far from intuitive expectations. In contrast, the TAP-k satisfies most of the criteria desired in an ideal measure of retrieval efficacy. The TAP-k web server and downloadable Perl script are freely available at http://www.ncbi.nlm.nih.gov/CBBresearch/Spouge/html.ncbi/tap/
20551243	Enhancing search efficiency by means of a search filter for finding all studies on animal experimentation in PubMed.
Lab. Anim. 20100615 2010Jul
Collecting and analysing all available literature before starting an animal experiment is important and it is indispensable when writing a systematic review (SR) of animal research. Writing such review prevents unnecessary duplication of animal studies and thus unnecessary animal use (Reduction). One of the factors currently impeding the production of 'high-quality' SRs in laboratory animal science is the fact that searching for all available literature concerning animal experimentation is rather difficult. In order to diminish these difficulties, we developed a search filter for PubMed to detect all publications concerning animal studies. This filter was compared with the method most frequently used, the PubMed Limit: Animals, and validated further by performing two PubMed topic searches. Our filter performs much better than the PubMed limit: it retrieves, on average, 7% more records. Other important advantages of our filter are that it also finds the most recent records and that it is easy to use. All in all, by using our search filter in PubMed, all available literature concerning animal studies on a specific topic can easily be found and assessed, which will help in increasing the scientific quality and thereby the ethical validity of animal experiments.
20529261	In-vivo high resolution imaging of optic nerve head drusen using spectral-domain Optical Coherence Tomography.
BMC Med Imaging 20100607 2010
Optic nerve head drusen (ONHD) are white calcareous deposits, seen either superficially on the optic nerve head or buried within it. Diagnosis of ONHD is made by one or more ways: clinical exam, autofluorescence, ultrasound of the optic nerve, CT scan and/or visual field examination. The present study describes features of ONHD based on another diagnostic modality, the spectral-domain OCT (Spectralis). This is a retrospective case series of 5 patients with bilateral ONHD with a best-corrected visual acuity of 20/20 and no other posterior segment pathology. All the patients underwent fundus photography, fundus autofluorescence, B-scan ultrasonography, Spectralis OCT and Humphrey 30-2 threshold visual fields. All 5 patients had surface ONHD which were autofluorescent and echodense on B-scan ultrasonography. Spectralis OCT findings in the corresponding areas include 'scattered spots with high reflectivity' casting a shadow underneath. The reflectivity can be distinctly differentiated from the blood vessels on the optic nerve. Two patients had an arcuate scotoma on the Humphrey visual fields. No correlation was found between the changes on Spectralis OCT with that of visual field. Spectralis OCT is another useful ancillary investigation in the diagnosis of ONHD and we describe the features in the present study.
20595299	The population health record: concepts, definition, design, and implementation.
J Am Med Inform Assoc  2010 Jul-Aug
In 1997, the American Medical Informatics Association proposed a US information strategy that included a population health record (PopHR). Despite subsequent progress on the conceptualization, development, and implementation of electronic health records and personal health records, minimal progress has occurred on the PopHR. Adapting International Organization for Standarization electronic health records standards, we define the PopHR as a repository of statistics, measures, and indicators regarding the state of and influences on the health of a defined population, in computer processable form, stored and transmitted securely, and accessible by multiple authorized users. The PopHR is based upon an explicit population health framework and a standardized logical information model. PopHR purpose and uses, content and content sources, functionalities, business objectives, information architecture, and system architecture are described. Barriers to implementation and enabling factors and a three-stage implementation strategy are delineated.
20595313	A new algorithm for reducing the workload of experts in performing systematic reviews.
J Am Med Inform Assoc  2010 Jul-Aug
To determine whether a factorized version of the complement naïve Bayes (FCNB) classifier can reduce the time spent by experts reviewing journal articles for inclusion in systematic reviews of drug class efficacy for disease treatment. The proposed classifier was evaluated on a test collection built from 15 systematic drug class reviews used in previous work. The FCNB classifier was constructed to classify each article as containing high-quality, drug class-specific evidence or not. Weight engineering (WE) techniques were added to reduce underestimation for Medical Subject Headings (MeSH)-based and Publication Type (PubType)-based features. Cross-validation experiments were performed to evaluate the classifier's parameters and performance. Work saved over sampling (WSS) at no less than a 95% recall was used as the main measure of performance. The minimum workload reduction for a systematic review for one topic, achieved with a FCNB/WE classifier, was 8.5%; the maximum was 62.2% and the average over the 15 topics was 33.5%. This is 15.0% higher than the average workload reduction obtained using a voting perceptron-based automated citation classification system. The FCNB/WE classifier is simple, easy to implement, and produces significantly better results in reducing the workload than previously achieved. The results support it being a useful algorithm for machine-learning-based automation of systematic reviews of drug class efficacy for disease treatment.
20596938	Computational RNomics: structure identification and functional prediction of non-coding RNAs in silico.
Sci China Life Sci 20100523 2010May
The eukaryotic genome contains varying numbers of non-coding RNA (ncRNA) genes. "Computational RNomics" takes a multidisciplinary approach, like information science, to resolve the structure and function of ncRNAs. Here, we review the main issues in "Computational RNomics" of data storage and management, ncRNA gene identification and characterization, ncRNA target identification and functional prediction, and we summarize the main methods and current content of "computational RNomics".
20601351	Clinician search behaviors may be influenced by search engine design.
J. Med. Internet Res. 20100630 2010
Searching the Web for documents using information retrieval systems plays an important part in clinicians' practice of evidence-based medicine. While much research focuses on the design of methods to retrieve documents, there has been little examination of the way different search engine capabilities influence clinician search behaviors. Previous studies have shown that use of task-based search engines allows for faster searches with no loss of decision accuracy compared with resource-based engines. We hypothesized that changes in search behaviors may explain these differences. In all, 75 clinicians (44 doctors and 31 clinical nurse consultants) were randomized to use either a resource-based or a task-based version of a clinical information retrieval system to answer questions about 8 clinical scenarios in a controlled setting in a university computer laboratory. Clinicians using the resource-based system could select 1 of 6 resources, such as PubMed; clinicians using the task-based system could select 1 of 6 clinical tasks, such as diagnosis. Clinicians in both systems could reformulate search queries. System logs unobtrusively capturing clinicians' interactions with the systems were coded and analyzed for clinicians' search actions and query reformulation strategies. The most frequent search action of clinicians using the resource-based system was to explore a new resource with the same query, that is, these clinicians exhibited a "breadth-first" search behaviour. Of 1398 search actions, clinicians using the resource-based system conducted 401 (28.7%, 95% confidence interval [CI] 26.37-31.11) in this way. In contrast, the majority of clinicians using the task-based system exhibited a "depth-first" search behavior in which they reformulated query keywords while keeping to the same task profiles. Of 585 search actions conducted by clinicians using the task-based system, 379 (64.8%, 95% CI 60.83-68.55) were conducted in this way. This study provides evidence that different search engine designs are associated with different user search behaviors.
20557232	Not published, not indexed: issues in generating and finding hospice and palliative care literature.
J Palliat Med  2010Jun
Accessing new knowledge as the evidence base for hospice and palliative care grows has specific challenges for the discipline. This study aimed to describe conversion rates of palliative and hospice care conference abstracts to journal articles and to highlight that some palliative care literature may not be retrievable because it is not indexed on bibliographic databases. Substudy A tracked the journal publication of conference abstracts selected for inclusion in a gray literature database on www.caresearch.com.au . Abstracts were included in the gray literature database following handsearching of proceedings of over 100 Australian conferences likely to have some hospice or palliative care content that were held between 1980 and 1999. Substudy B looked at indexing from first publication until 2001 of three international hospice and palliative care journals in four widely available bibliographic databases through systematic tracing of all original papers in the journals. Substudy A showed that for the 1338 abstracts identified only 15.9% were published (compared to an average in health of 45%). Published abstracts were found in 78 different journals. Multiauthor abstracts and oral presentations had higher rates of conversion. Substudy B demonstrated lag time between first publication and bibliographic indexing. Even after listing, idiosyncratic noninclusions were identified. There are limitations to retrieval of all possible literature through electronic searching of bibliographic databases. Encouraging publication in indexed journals of studies presented at conferences, promoting selection of palliative care journals for database indexing, and searching more than one bibliographic database will improve the accessibility of existing and new knowledge in hospice and palliative care.
20529940	Using semantic web rules to reason on an ontology of pseudogenes.
Bioinformatics  2010Jun15
Recent years have seen the development of a wide range of biomedical ontologies. Notable among these is Sequence Ontology (SO) which offers a rich hierarchy of terms and relationships that can be used to annotate genomic data. Well-designed formal ontologies allow data to be reasoned upon in a consistent and logically sound way and can lead to the discovery of new relationships. The Semantic Web Rules Language (SWRL) augments the capabilities of a reasoner by allowing the creation of conditional rules. To date, however, formal reasoning, especially the use of SWRL rules, has not been widely used in biomedicine. We have built a knowledge base of human pseudogenes, extending the existing SO framework to incorporate additional attributes. In particular, we have defined the relationships between pseudogenes and segmental duplications. We then created a series of logical rules using SWRL to answer research questions and to annotate our pseudogenes appropriately. Finally, we were left with a knowledge base which could be queried to discover information about human pseudogene evolution. The fully populated knowledge base described in this document is available for download from http://ontology.pseudogene.org. A SPARQL endpoint from which to query the dataset is also available at this location.
20529942	Semi-automated ontology generation within OBO-Edit.
Bioinformatics  2010Jun15
Ontologies and taxonomies have proven highly beneficial for biocuration. The Open Biomedical Ontology (OBO) Foundry alone lists over 90 ontologies mainly built with OBO-Edit. Creating and maintaining such ontologies is a labour-intensive, difficult, manual process. Automating parts of it is of great importance for the further development of ontologies and for biocuration. We have developed the Dresden Ontology Generator for Directed Acyclic Graphs (DOG4DAG), a system which supports the creation and extension of OBO ontologies by semi-automatically generating terms, definitions and parent-child relations from text in PubMed, the web and PDF repositories. DOG4DAG is seamlessly integrated into OBO-Edit. It generates terms by identifying statistically significant noun phrases in text. For definitions and parent-child relations it employs pattern-based web searches. We systematically evaluate each generation step using manually validated benchmarks. The term generation leads to high-quality terms also found in manually created ontologies. Up to 78% of definitions are valid and up to 54% of child-ancestor relations can be retrieved. There is no other validated system that achieves comparable results. By combining the prediction of high-quality terms, definitions and parent-child relations with the ontology editor OBO-Edit we contribute a thoroughly validated tool for all OBO ontology engineers. DOG4DAG is available within OBO-Edit 2.1 at http://www.oboedit.org. Supplementary data are available at Bioinformatics online.
20441537	Control and data acquisition software for high-density CMOS-based microprobe arrays implementing electronic depth control.
Biomed Tech (Berl)  2010Jun
This paper presents the NeuroSelect software for managing the electronic depth control of cerebral CMOS-based microprobes for extracellular in vivo recordings. These microprobes contain up to 500 electronically switchable electrodes which can be appropriately selected with regard to specific neuron locations in the course of a recording experiment. NeuroSelect makes it possible to scan the electrodes electronically and to (re)select those electrodes of best signal quality resulting in a closed-loop design of a neural acquisition system. The signal quality is calculated by the relative power of the spikes compared with the background noise. The spikes are detected by an adaptive threshold using a robust estimator of the standard deviation. Electrodes can be selected in a manual or semi-automatic mode based on the signal quality. This electronic depth control constitutes a significant improvement for multielectrode probes, given that so far the only alternative has been the fine positioning by mechanical probe translation. In addition to managing communication with the hardware controller of the probe array, the software also controls acquisition, processing, display and storage of the neural signals for further analysis.
20379792	Workflow management of content-based image retrieval for CAD support in PACS environments based on IHE.
Int J Comput Assist Radiol Surg 20100409 2010Jul
Content-based image retrieval (CBIR) bears great potential for computer-aided diagnosis (CAD). However, current CBIR systems are not able to integrate with clinical workflow and PACS generally. One essential factor in this setting is scheduling. Applied and proved with modalities and the acquisition of images for a long time, we now establish scheduling with CBIR. Our workflow is based on the IHE integration profile 'Post-Processing Workflow' (PPW) and the use of a DICOM work list. We configured dcm4chee PACS and its including IHE actors for the application of CBIR. In order to achieve a convenient interface for integrating arbitrary CBIR systems, we realized an adapter between the CBIR system and PACS. Our system architecture constitutes modular components communicating over standard protocols. The proposed workflow management system offers the possibility to embed CBIR conveniently into PACS environments. We achieve a chain of references that fills the information gap between acquisition and post-processing. Our approach takes into account the tight and solid organization of scheduled and performed tasks in clinical settings.
20531955	Validation of coevolving residue algorithms via pipeline sensitivity analysis: ELSC and OMES and ZNMI, oh my!
PLoS ONE 20100601 2010
Correlated amino acid substitution algorithms attempt to discover groups of residues that co-fluctuate due to either structural or functional constraints. Although these algorithms could inform both ab initio protein folding calculations and evolutionary studies, their utility for these purposes has been hindered by a lack of confidence in their predictions due to hard to control sources of error. To complicate matters further, naive users are confronted with a multitude of methods to choose from, in addition to the mechanics of assembling and pruning a dataset. We first introduce a new pair scoring method, called ZNMI (Z-scored-product Normalized Mutual Information), which drastically improves the performance of mutual information for co-fluctuating residue prediction. Second and more important, we recast the process of finding coevolving residues in proteins as a data-processing pipeline inspired by the medical imaging literature. We construct an ensemble of alignment partitions that can be used in a cross-validation scheme to assess the effects of choices made during the procedure on the resulting predictions. This pipeline sensitivity study gives a measure of reproducibility (how similar are the predictions given perturbations to the pipeline?) and accuracy (are residue pairs with large couplings on average close in tertiary structure?). We choose a handful of published methods, along with ZNMI, and compare their reproducibility and accuracy on three diverse protein families. We find that (i) of the algorithms tested, while none appear to be both highly reproducible and accurate, ZNMI is one of the most accurate by far and (ii) while users should be wary of predictions drawn from a single alignment, considering an ensemble of sub-alignments can help to determine both highly accurate and reproducible couplings. Our cross-validation approach should be of interest both to developers and end users of algorithms that try to detect correlated amino acid substitutions.
20529623	"Ten-point" 3D cephalometric analysis using low-dosage cone beam computed tomography.
Prog Orthod 20100511 2010
The aim of this study was to combine the huge amount of information of low dose Cone Beam CT with a cephalometric simplified protocol thanks to the latest informatics aids. Lateral cephalograms are two-dimensional (2-D) radiographs that are used to represent three-dimensional (3-D) structures. Cephalograms have inherent limitations as a result of distortion, super imposition and differential magnification of the craniofacial complex. This may lead to errors of identification and reduced measurement accuracy. The advantages of CBCT over conventional CT include low radiation exposure, imaging quality improvement, potentially better access, high spatial resolution and lower cost. This study assessed cephalometric 2D and 3D measurements and the analysis of CBCT cephalograms of the volume and centroid of the maxilla and mandible, in 10 clinical cases. With a few exceptions the linear and angular cephalometric measurements obtained from CBCT and from conventional cephalograms did not differ statistically (p&gt;0.01). There was a correlation between the variation in the skeletal malocclusion and growth direction of the jaws, and the variation in the spatial position (x, y, z) of the centroids and their volumes (p&lt;0.01). The 3D cephalometric analysis is easier to interpret than 2D cephalometric analysis. In contrast to those made on projective radiographies, the angular and linear measurements detected on 3D become real, moreover the fewest points to select and the automatic measurements made by the computer drastically reduced human error, for a much more reliable reproducible and repeatable diagnosis.
20470392	XML-BSPM: an XML format for storing Body Surface Potential Map recordings.
BMC Med Inform Decis Mak 20100514 2010
The Body Surface Potential Map (BSPM) is an electrocardiographic method, for recording and displaying the electrical activity of the heart, from a spatial perspective. The BSPM has been deemed more accurate for assessing certain cardiac pathologies when compared to the 12-lead ECG. Nevertheless, the 12-lead ECG remains the most popular ECG acquisition method for non-invasively assessing the electrical activity of the heart. Although data from the 12-lead ECG can be stored and shared using open formats such as SCP-ECG, no open formats currently exist for storing and sharing the BSPM. As a result, an innovative format for storing BSPM datasets has been developed within this study. The XML vocabulary was chosen for implementation, as opposed to binary for the purpose of human readability. There are currently no standards to dictate the number of electrodes and electrode positions for recording a BSPM. In fact, there are at least 11 different BSPM electrode configurations in use today. Therefore, in order to support these BSPM variants, the XML-BSPM format was made versatile. Hence, the format supports the storage of custom torso diagrams using SVG graphics. This diagram can then be used in a 2D coordinate system for retaining electrode positions. This XML-BSPM format has been successfully used to store the Kornreich-117 BSPM dataset and the Lux-192 BSPM dataset. The resulting file sizes were in the region of 277 kilobytes for each BSPM recording and can be deemed suitable for example, for use with any telemonitoring application. Moreover, there is potential for file sizes to be further reduced using basic compression algorithms, i.e. the deflate algorithm. Finally, these BSPM files have been parsed and visualised within a convenient time period using a web based BSPM viewer. This format, if widely adopted could promote BSPM interoperability, knowledge sharing and data mining. This work could also be used to provide conceptual solutions and inspire existing formats such as DICOM, SCP-ECG and aECG to support the storage of BSPMs. In summary, this research provides initial ground work for creating a complete BSPM management system.
20542857	Definition of Health 2.0 and Medicine 2.0: a systematic review.
J. Med. Internet Res. 20100611 2010
During the last decade, the Internet has become increasingly popular and is now an important part of our daily life. When new "Web 2.0" technologies are used in health care, the terms "Health 2.0" or "Medicine 2.0" may be used. The objective was to identify unique definitions of Health 2.0/Medicine 2.0 and recurrent topics within the definitions. A systematic literature review of electronic databases (PubMed, Scopus, CINAHL) and gray literature on the Internet using the search engines Google, Bing, and Yahoo was performed to find unique definitions of Health 2.0/Medicine 2.0. We assessed all literature, extracted unique definitions, and selected recurrent topics by using the constant comparison method. We found a total of 1937 articles, 533 in scientific databases and 1404 in the gray literature. We selected 46 unique definitions for further analysis and identified 7 main topics. Health 2.0/Medicine 2.0 are still developing areas. Many articles concerning this subject were found, primarily on the Internet. However, there is still no general consensus regarding the definition of Health 2.0/Medicine 2.0. We hope that this study will contribute to building the concept of Health 2.0/Medicine 2.0 and facilitate discussion and further research.
20543282	The moderating role of need for cognition on excessive searching bias: a case of finding romantic partners online.
Stud Health Technol Inform  2010
Using online-dating websites to expand social networks and form close relationships is popular for people in information technology era. Wu and Chiou (2009) demonstrated that more options triggered excessive searching, leading to poorer decision-making and reduced selectivity. They proposed that the more-means-worse effect refers to more searching leads to worse choices by reducing users' cognitive resources, distracting them with irrelevant information and reducing their ability to screen out inferior options. A 2 by 2 experimental study was conducted to investigate the moderating effect of individual differences in need for cognition (NFC) and number of available options on excessive searching and decision quality. A total of 120 undergraduates with experiences of online romantic relationships participated in the experiment. After participants were administrated their need for cognition, they were assigned to review either a small or a large number of options to search for their most desirable romantic partners via a popular online-dating website in Taiwan. Results indicated that high-NFC participants showed more excessive searching than did low-NFC participants. Moreover, the more-means-worse effect was more salient for high-NFC participants than high-NFC participants. The findings suggest that users with high NFC may be more vulnerable to the negative effect of excessive searching.
20543347	Management of knowledge gaps: concept representation of things we don't know.
Stud Health Technol Inform  2010
Suva (Swiss National Accident Insurance Fund) is the most important carrier of obligatory accident insurance in Switzerland. Its medical division supports doctors working in inpatient and outpatient care with comprehensive case management and with conciliar advice. The Suva hospitals provide inpatient rehabilitation. In 2002, Suva started the InWiM project. InWiM is an acronym and stands for "Integrierte Wissensbasen der Medizin", which can be translated as "Integrated Knowledge Bases in Medicine". Information retrieval within InWiM is achieved by means of the MeSH Index (Medical Subject Headings), the thesaurus of the United States National Library of Medicine (NLM). InWiM has now been extended towards the management not only of publications but also of areas where sound knowledge is missing, so called "knowledge gaps": Knowledge gaps are indexed with MeSH terms in a similar way to publications. This improves knowledge management: In particular it is possible to search and find knowledge gaps and solutions covering the same or a similar topic, thus allowing adequate collating and it prevents duplication of work. Furthermore, literature search strategies for the NML are predefined and do not need every time to be reinvented from scratch.
20543446	Use of NLM medical subject headings with the MeSH2010 thesaurus in the PORTAL-DOORS system.
Stud Health Technol Inform  2010
The NLM MeSH Thesaurus has been incorporated for use in the PORTAL-DOORS System (PDS) for resource metadata management on the semantic web. All 25588 descriptor records from the NLM 2010 MeSH Thesaurus have been exposed as web accessible resources by the PDS MeSH2010 Thesaurus implemented as a PDS PORTAL Registry operating as a RESTful web service. Examples of records from the PDS MeSH2010 PORTAL are demonstrated along with their use by records in other PDS PORTAL Registries that reference the concepts from the MeSH2010 Thesaurus. Use of this important biomedical terminology will greatly enhance the quality of metadata content of other PDS records thus improving cross-domain searches between different problem oriented domains and amongst different clinical specialty fields.
20466756	Difficulties of tracing health research funded by the European Union.
J Health Serv Res Policy 20100513 2010Jul
European Union (EU) information from research projects, including key findings, should be available on the European Commission's Community Research and Development Information Service (CORDIS) database. We describe the results of the Health Research for Europe (HR4E) project which aimed to synthesize results of health research from the EU's Fifth and Sixth Framework Programmes (FP5 and FP6) of research. Screening of titles and abstracts of all projects funded within FP5 and FP6 to identify health-related projects followed by allocation of such projects to one of the 47 themes of the European Union's Health Portal. Extraction of key findings relevant for policy and practice from data on the CORDIS database and, in a subset of 182 projects selected from five themes, attempted contact with project co-ordinators to obtain missing information. The information held on CORDIS was inadequate, with many fields not completed. Data were rarely updated after the project had been funded. Of the 182 attempts to contact co-ordinators, useful information was obtained in only 17% of cases, with many contact details missing or unverifiable. CORDIS does not meet its stated objectives of facilitating and disseminating EU research. There is a clear need to review the systems designed to manage the CORDIS platform.
20491004	[Routine data from general practitioner's software systems - Export, analysis and preparation for research].
Gesundheitswesen 20100520 2010Jun
An advanced and integrative information technology (IT)-landscape is needed for optimal support of future processes in health-care, including health services research. Most researches in the primary care sector are based on data collected for reimbursement. The aim of this study is to show the limits and options of secondary analysis based on data that was exported via the "Behandlungsdatentransfer" (treatment data transport) BDT-interface in the software systems of German general practitioners and afterwards prepared for further research in SPSS. From the middle of 2005 to the end of 2007 all 168 teaching practices of the Hannover Medical School (MHH) were invited to join the study. Finally routine data from 28 practices could be collected successfully. The data from 139 other practices which had been collected for the project "Health Care in Practice" ("Medizinische Versorgung in der Praxis" - MedViP) was also added to the pool. The process of data preparation included a complete cycle from data collection, merging the data in a relational database system, via statistics and analysis to publishing and generating a feedback report for the participating practices. During the whole study the limits and options of this method were systematically identified. Of the 168 practices, 68 (40.5%) were interested to participate. From 28 (16.7%) physicians the data could be exported from their software systems. In 15 (8.9%) cases no collection was possible due to technical and in 26 (15.5%) to administrative reasons. The method of data extraction varied, as the BDT-interface was differently implemented by the software companies. Together with the MedViP data, the database at the MHH now consists of 167 practices with 974 304 patients and 12 555 943 treatments. For 44.1% of the 11 497 899 prescription entries an anatomic therapeutic chemical (ATC) code could be applied, by matching the entries to the master data from the Scientific Institute of Local Health-Care Funds ("Wissenschaftliches Instituts der Ortskrankenkassen" - WIdO). Periodically consistent sets of SPSS files could successfully be created for further research and feedback reports for the participating practices were generated as portable document format (PDF) files. The BDT-interface seems quite out of date, but can still reveal interesting information, especially on data about medical treatments and findings. Much of the data is contained in fields based on free text, which makes analysis difficult. Coded information, like agents, as ATC, could partially be extracted from the data, which afterwards was easy to prepare for further research. Quality and content of the data depend mainly on the data enterer, the physicians and their practice staff. Future research could be improved by more classified and coded data, which would better be transported through an interface more advanced than BDT.
20493291	A novel storage method for near infrared spectroscopy chemometric models.
Anal. Chim. Acta 20100424 2010Jun4
Chemometric Modeling Markup Language (CMML) is developed by us for containing chemometrics models within one document through converting binary data into strings by base64 encode/decode algorithms to solve the interoperability issue in sharing chemometrics models. It provides a base functionality for storage of sampling, variable selection, pretreating, outlier and modeling parameters and data. With the help of base64 algorithm, the usability of CMML is in equilibrium with size by transforming the binary data into base64 encoded string. Due to the advantages of Extensible Markup Language (XML), models stored in CMML can be easily reused in various other software and programming languages as long as the programming language has XML parsing library. One can also use the XML Path Language (XPath) query language to select desired data from the CMML file effectively. The application of this language in near infrared spectroscopy model storage is implemented as a class in C++ language and available as open source software (http://code.google.com/p/cmml), and the implementations in other languages, such as MATLAB and R are in progress.
20405959	A semantic web ontology for small molecules and their biological targets.
J Chem Inf Model  2010May24
A wide range of data on sequences, structures, pathways, and networks of genes and gene products is available for hypothesis testing and discovery in biological and biomedical research. However, data describing the physical, chemical, and biological properties of small molecules have not been well-integrated with these resources. Semantically rich representations of chemical data, combined with Semantic Web technologies, have the potential to enable the integration of small molecule and biomolecular data resources, expanding the scope and power of biomedical and pharmacological research. We employed the Semantic Web technologies Resource Description Framework (RDF) and Web Ontology Language (OWL) to generate a Small Molecule Ontology (SMO) that represents concepts and provides unique identifiers for biologically relevant properties of small molecules and their interactions with biomolecules, such as proteins. We instanced SMO using data from three public data sources, i.e., DrugBank, PubChem and UniProt, and converted to RDF triples. Evaluation of SMO by use of predetermined competency questions implemented as SPARQL queries demonstrated that data from chemical and biomolecular data sources were effectively represented and that useful knowledge can be extracted. These results illustrate the potential of Semantic Web technologies in chemical, biological, and pharmacological research and in drug discovery.
20458135	Wireless recording of the calls of Rousettus aegyptiacus and their reproduction using electrostatic transducers.
Bioinspir Biomim 20100511 2010Jun
Bats are capable of imaging their surroundings in great detail using echolocation. To apply similar methods to human engineering systems requires the capability to measure and recreate the signals used, and to understand the processing applied to returning echoes. In this work, the emitted and reflected echolocation signals of Rousettus aegyptiacus are recorded while the bat is in flight, using a wireless sensor mounted on the bat. The sensor is designed to replicate the acoustic gain control which bats are known to use, applying a gain to returning echoes that is dependent on the incurred time delay. Employing this technique allows emitted and reflected echolocation calls, which have a wide dynamic range, to be recorded. The recorded echoes demonstrate the complexity of environment reconstruction using echolocation. The sensor is also used to make accurate recordings of the emitted calls, and these calls are recreated in the laboratory using custom-built wideband electrostatic transducers, allied with a spectral equalization technique. This technique is further demonstrated by recreating multi-harmonic bioinspired FM chirps. The ability to record and accurately synthesize echolocation calls enables the exploitation of biological signals in human engineering systems for sonar, materials characterization and imaging.
20502165	Developing a patient safety surveillance system to identify adverse events in the intensive care unit.
Crit. Care Med.  2010Jun
Aggregation of adverse drug event data has evolved in the last decade. Several approaches are available to augment the standard voluntary incident reporting system. Most of these methods are applicable to nonmedication adverse events as well. To identify appropriately system trends as well as process failures, intensive care units should participate in various collection methods. Several different methods are available for robust adverse drug event data collection, such as target chart review, nontargeted chart review, and direct observation. As the various methods usually capture different types of events, employing more than one technique will improve the assessment of intensive care unit care. Some of these surveillance methods offer real-time or near real-time identification of adverse drug events and potentially afford the practitioner time for intervention. Continued development of adverse drug event detection will allow for further quality improvement efforts and preventive strategies to be utilized.
20506061	The need for a biological registration system.
IDrugs  2010Jun
A biological registration system is capable of determining whether two complex biological molecules are the same or different, and can assign identifiers based on this determination. Although such systems are frequently employed by chemists, they are rarely used by biological scientists in the pharmaceutical industry. However, a biological registration system would have several enterprise-wide benefits, from R&amp;D to IP to laboratory safety. Beyond these evident benefits, a biological registration system that integrates appropriately with other systems such as electronic laboratory notebooks and inventory databases could provide critical links to allow the integration of otherwise-siloed data and knowledge generated across global pharmaceutical companies and other large research institutions. Data and knowledge integration are widely recognized as critical yet elusive components of effective translational science and systems biology programs that would create greater efficiencies for drug discovery. However, determining the optimal construction of such systems remains a challenge. This feature review describes how a special interest group comprising several pharmaceutical companies and a software company was used to create a commercially viable and supportable system.
20205657	Development of tools and database for analysis of metal binding sites in protein.
Protein Pept. Lett.  2010Jun
In this study, we have developed a standalone tool called as ANAMBS (Analysis of Metal Binding Site) to derive metal neighbourhood information using PERL as the programming language. The tool accepts the structures in the pdb format. The cut off distance to define the metal binding region can be specified. The metal binding site composition, orientation of various amino acids and atoms along with the Hydropathy index within the metal binding site region can be measured. Its speed and efficiency makes it a beneficial tool for various structural biology projects, especially when the characterization of the metal binding domain is needed. Additionally, the database MEDB (Metal Environment Database) was developed which presents quantitative information on metal-binding sites in protein structures. It can be used for identification of trends or patterns in the metal-binding sites. The information obtained can be used to generate structural templates from metal binding sites of known enzymes and to develop constraints for computational modeling of metalloproteins. The tool and database are available at http://www.uohyd.ernet.in/anambs/
20505913	Effectiveness of the use of internet search by third year medical students to establish a clinical diagnosis.
Singapore Med J  2010Apr
Internet search has been the main source for information and data mining in medical research. Its use by medical students has immensely contributed to learning activities. The main aim of the study was to determine the effectiveness of internet use by medical students during their initial years of clinical instruction in order to establish a diagnosis after being provided with the history and physical findings of a clinical problem. A total of 47 cases derived from the New England Journal of Medicine (NEJM) were utilised. The Google search engine was utilised to establish a reasonable diagnosis. A congruency rate of 44.7% was obtained. This was considered commendable in view of the complexities of the cases published in the NEJM and the fact that the medical students were only in the third year of their Bachelor of Medicine and Bachelor of Surgery program. The study illustrates that common search engines could complement the traditionally used medical education methods.
20508300	Bioinformatics strategies in life sciences: from data processing and data warehousing to biological knowledge extraction.
J Integr Bioinform 20100527 2010
With the large variety of Proteomics workflows, as well as the large variety of instruments and data-analysis software available, researchers today face major challenges validating and comparing their Proteomics data. Here we present a new generation of the ProteinScape bioinformatics platform, now enabling researchers to manage Proteomics data from the generation and data warehousing to a central data repository with a strong focus on the improved accuracy, reproducibility and comparability demanded by many researchers in the field. It addresses scientists; current needs in proteomics identification, quantification and validation. But producing large protein lists is not the end point in Proteomics, where one ultimately aims to answer specific questions about the biological condition or disease model of the analyzed sample. In this context, a new tool has been developed at the Spanish Centro Nacional de Biotecnologia Proteomics Facility termed PIKE (Protein information and Knowledge Extractor) that allows researchers to control, filter and access specific information from genomics and proteomic databases, to understand the role and relationships of the proteins identified in the experiments. Additionally, an EU funded project, ProDac, has coordinated systematic data collection in public standards-compliant repositories like PRIDE. This will cover all aspects from generating MS data in the laboratory, assembling the whole annotation information and storing it together with identifications in a standardised format.
20507843	The touro 12-step: a systematic guide to optimizing survey research with online discussion boards.
J. Med. Internet Res. 20100527 2010
The Internet, in particular discussion boards, can provide a unique opportunity for recruiting participants in online research surveys. Despite its outreach potential, there are significant barriers which can limit its success. Trust, participation, and visibility issues can all hinder the recruitment process; the Touro 12-Step was developed to address these potential hurdles. By following this step-by-step approach, researchers will be able to minimize these pitfalls and maximize their recruitment potential via online discussion boards.
20232414	KMeyeDB: a graphical database of mutations in genes that cause eye diseases.
Hum. Mutat.  2010Jun
KMeyeDB (http://mutview.dmb.med.keio.ac.jp/) is a database of human gene mutations that cause eye diseases. We have substantially enriched the amount of data in the database, which now contains information about the mutations of 167 human genes causing eye-related diseases including retinitis pigmentosa, cone-rod dystrophy, night blindness, Oguchi disease, Stargardt disease, macular degeneration, Leber congenital amaurosis, corneal dystrophy, cataract, glaucoma, retinoblastoma, Bardet-Biedl syndrome, and Usher syndrome. KMeyeDB is operated using the database software MutationView, which deals with various characters of mutations, gene structure, protein functional domains, and polymerase chain reaction (PCR) primers, as well as clinical data for each case. Users can access the database using an ordinary Internet browser with smooth user-interface, without user registration. The results are displayed on the graphical windows together with statistical calculations. All mutations and associated data have been collected from published articles. Careful data analysis with KMeyeDB revealed many interesting features regarding the mutations in 167 genes that cause 326 different types of eye diseases. Some genes are involved in multiple types of eye diseases, whereas several eye diseases are caused by different mutations in one gene.
20513138	An interactive web-tool for molecular analyses links naturally occurring mutation data with three-dimensional structures of the rhodopsin-like glycoprotein hormone receptors.
Hum. Mutat.  2010Jun
The collection, description and molecular analysis of naturally occurring (pathogenic) mutations are important for understanding the functional mechanisms and malfunctions of biological units such as proteins. Numerous databases collate a huge amount of functional data or descriptions of mutations, but tools to analyse the molecular effects of genetic variations are as yet poorly provided. The goal of this work was therefore to develop a translational web-application that facilitates the interactive linkage of functional and structural data and which helps improve our understanding of the molecular basis of naturally occurring gain- or loss- of function mutations. Here we focus on the human glycoprotein hormone receptors (GPHRs), for which a huge number of mutations are known to cause diseases. We describe new options for interactive data analyses within three-dimensional structures, which enable the assignment of molecular relationships between structure and function. Strikingly, as the functional data are converted into relational percentage values, the system allows the comparison and classification of data from different GPHR subtypes and different experimental approaches. Our new application has been incorporated into a freely available database and website for the GPHRs (http://www.ssfa-gphr.de), but the principle development would also be applicable to other macromolecules.
20511179	Patient and parent views on a Web 2.0 Diabetes Portal--the management tool, the generator, and the gatekeeper: qualitative study.
J. Med. Internet Res. 20100528 2010
The Internet has undergone rapid development, with significant impact on social life and on modes of communication. Modern management of type 1 diabetes requires that patients have access to continuous support and learning opportunities. Although Web 2.0 resources can provide this support, few pediatric clinics offer it as part of routine diabetes care. We aimed to explore patients' and parents' attitudes toward a local Web 2.0 portal tailored to young patients with type 1 diabetes and their parents, with social networking tools such as message boards and blogs, locally produced self-care and treatment information, and interactive pedagogic devices. Opportunities and obstacles to the implementation of Web 2.0 applications in clinical practice were sought. Participants were 16 mothers, 3 fathers, and 5 young patients (ages 11-18 years; median 14 years) who each wrote an essay on their experience using the portal, irrespective of frequency and/or their success in using it. Two main guiding questions were asked. A qualitative content analysis was conducted of the essays as a whole. Three main categories of portal users' attitudes were found; we named them "the management tool," "the generator," and "the gatekeeper." One category was related to the management tool functionality of the portal, and a wide range of concrete examples was found regarding useful facts and updates. Being enabled to search when necessary and find reliable information provided by local clinicians was regarded as a great advantage, facilitating a feeling of security and being in control. Finding answers to difficult-to-ask questions, questions portal users did not know they had before, and questions focusing on sensitive areas such as anxiety and fear, was also an important feature. A second category was related to the generator function in that visiting the portal could generate more information than expected, which could lead to increased use. Active message boards and chat rooms were found to have great value for enhancing mediation of third party peer-to-peer information. A certain level of active users from peer families and visible signs of their activity were considered necessary to attract returning users. A third category was related to the gatekeeper function of the password requirement, which created various access problems. This and other unsuccessful experiences caused users to drop the portal. A largely open portal was suggested to enhance use by those associated with the child with diabetes, such as school personnel, relatives, friends and others, and also by young users somewhat unwilling to self-identify with the disease. Web 2.0 services have great potential for supporting parents and patients with type 1 diabetes by enhancing their information retrieval and disease management. Well-developed services, such as this one, may generate continued use and should, therefore, be carefully maintained and updated by health care professionals who are alert and active on the site with new information and updates. Login procedures should be simple and minimized as much as possible. The education of clinical practitioners regarding the use of Web 2.0 resources needs more attention.
20517399	Rapid spectrally encoded fluorescence imaging using a wavelength-swept source.
Opt Lett  2010Jun1
We present rapid imaging of fluorescent samples using spectral encoding (SE). A near-IR wavelength-swept source in used to preserve the SE of the position, despite Stokes shifts. To validate this approach, we imaged fluorescent PbS quantum dot solutions at concentrations down to 0.5+/-0.1micromol/L. This simple configuration allowed acquisition rates of up to 9920 lines of 1024 pixels per second to create high-resolution images. This spectrally encoded setup could be easily miniaturized for endoscopy, thus combining high-resolution fluorescence with confocal reflectance imaging at unmatched acquisition speed.
20520037	Data repositories for medical education research: issues and recommendations.
Acad Med  2010May
The authors explore issues surrounding digital repositories with the twofold intention of clarifying their creation, structure, content, and use, and considering the implementation of a global digital repository for medical education research data sets-an online site where medical education researchers would be encouraged to deposit their data in order to facilitate the reuse and reanalysis of the data by other researchers. By motivating data sharing and reuse, investigators, medical schools, and other stakeholders might see substantial benefits to their own endeavors and to the progress of the field of medical education.The authors review digital repositories in medicine, social sciences, and education, describe the contents and scope of repositories, and present extant examples. The authors describe the potential benefits of a medical education data repository and report results of a survey of the Society for Directors of Research in Medicine Education, in which participants responded to questions about data sharing and a potential data repository. Respondents strongly endorsed data sharing, with the caveat that principal investigators should choose whether or not to share data they collect. A large majority believed that a repository would benefit their unit and the field of medical education. Few reported using existing repositories. Finally, the authors consider challenges to the establishment of such a repository, including taxonomic organization, intellectual property concerns, human subjects protection, technological infrastructure, and evaluation standards. The authors conclude with recommendations for how a medical education data repository could be successfully developed.
20524550	Searching for information on the World Wide Web with a search engine: a pilot study on cognitive flexibility in younger and older users.
Psychol Rep  2010Apr
This pilot study investigated the age-related differences in searching for information on the World Wide Web with a search engine. 11 older adults (6 men, 5 women; M age=59 yr., SD=2.76, range=55-65 yr.) and 12 younger adults (2 men, 10 women; M=23.7 yr., SD=1.07, range=22-25 yr.) had to conduct six searches differing in complexity, and for which a search method was or was not induced. The results showed that the younger and older participants provided with an induced search method were less flexible than the others and produced fewer new keywords. Moreover, older participants took longer than the younger adults, especially in the complex searches. The younger participants were flexible in the first request and spontaneously produced new keywords (spontaneous flexibility), whereas the older participants only produced new keywords when confronted by impasses (reactive flexibility). Aging may influence web searches, especially the nature of keywords used.
20418167	An efficient similarity search based on indexing in large DNA databases.
Comput Biol Chem 20100404 2010Apr
Index-based search algorithms are an important part of a genomic search, and how to construct indices is the key to an index-based search algorithm to compute similarities between two DNA sequences. In this paper, we propose an efficient query processing method that uses special transformations to construct an index. It uses small storage and it rapidly finds the similarity between two sequences in a DNA sequence database. At first, a sequence is partitioned into equal length windows. We select the likely subsequences by computing Hamming distance to query sequence. The algorithm then transforms the subsequences in each window into a multidimensional vector space by indexing the frequencies of the characters, including the positional information of the characters in the subsequences. The result of our experiments shows that the algorithm has faster run time than other heuristic algorithms based on index structure. Also, the algorithm is as accurate as those heuristic algorithms.
20093269	Simulation of lung nodules in chest tomosynthesis.
Radiat Prot Dosimetry 20100121 2010 Apr-May
The aim of the present work was to develop an adequate method for simulating lung nodules in clinical chest tomosynthesis images. Based on the visual appearance of real nodules, artificial, three-dimensional nodules with irregular shape and surface structure were created using an approach of combining spheres of different sizes and central points. The nodules were virtually positioned at the desired locations inside the patient and by using the known geometry of the tomosynthesis acquisition, the radiation emitted from the focal spot, passing through the nodule and reaching the detector could be simulated. The created nodules were thereby projected into raw-data tomosynthesis projection images before reconstruction of the tomosynthesis section images. The focal spot size, signal spread in the detector, scattered radiation, patient motion and existing anatomy at the location of the nodule were taken into account in the simulations. It was found that the blurring caused by the modulation transfer function and the patient motion overshadows the effects of a finite focal spot and aliasing and also obscures the surface structure of the nodules, which provides an opportunity to simplify the simulations and decrease the simulation times. Also, the limited in-depth resolution of the reconstructed tomosynthesis section images reduces the necessity to take details of the anatomical structures at the location of the inserted nodule into account.
20223848	Optimisation of tube voltage for conventional urography using a Gd2O2S:Tb flat panel detector.
Radiat Prot Dosimetry 20100311 2010 Apr-May
With the increasing use of computed tomography (CT) for urography examinations, the indications for 'conventional' projection urography have changed and are more focused on high-contrast details. The purpose of the present study was to optimise the beam quality for urography examinations performed with a Gd(2)O(2)S:Tb flat-panel detector for the new conditions. Images of an anthropomorphic phantom were collected at different tube voltages with a CXDI-40G detector (Canon Inc., Tokyo, Japan). The images were analysed by radiologists and residents in a visual grading characteristics (VGCs) study. The tube voltage resulting in the best image quality was 55 kV, which therefore was selected for a clinical study. Images from 62 patients exposed with either 55 or 73 kV (original tube voltage) at constant effective doses were included. The 55-kV images underwent simulated dose reduction to represent images collected at 80, 64, 50, 40 and 32 % of the original dose level. All images were included in a VGC study where the observers rated the visibility of important anatomical landmarks. For images collected at 55 kV, an effective dose of approximately 85 % resulted in the same image quality as for images collected at 73 kV at 100 % dose. In conclusion, a low tube voltage should be used for conventional urography focused on high-contrast details. The study indicates that using a tube voltage of 55 kV instead of 73 kV for a Gd(2)O(2)S:Tb flat-panel detector, the effective dose can be reduced by approximately 10-20 % for normal-sized patients while maintaining image quality.
20459222	Additional correction for energy transfer efficiency calculation in filter-based Forster resonance energy transfer microscopy for more accurate results.
J Biomed Opt  2010 Mar-Apr
Forster resonance energy transfer (FRET) microscopy is commonly used to monitor protein interactions with filter-based imaging systems, which require spectral bleedthrough (or cross talk) correction to accurately measure energy transfer efficiency (E). The double-label (donor+acceptor) specimen is excited with the donor wavelength, the acceptor emission provided the uncorrected FRET signal and the donor emission (the donor channel) represents the quenched donor (qD), the basis for the E calculation. Our results indicate this is not the most accurate determination of the quenched donor signal as it fails to consider the donor spectral bleedthrough (DSBT) signals in the qD for the E calculation, which our new model addresses, leading to a more accurate E result. This refinement improves E comparisons made with lifetime and spectral FRET imaging microscopy as shown here using several genetic (FRET standard) constructs, where cerulean and venus fluorescent proteins are tethered by different amino acid linkers.
20459264	Quantitative characterization of developing collagen gels using optical coherence tomography.
J Biomed Opt  2010 Mar-Apr
Nondestructive optical imaging methods such as optical coherence tomography (OCT) have been proposed for characterizing engineered tissues such as collagen gels. In our study, OCT was used to image collagen gels with different seeding densities of smooth muscle cells (SMCs), including acellular gels, over a five-day period during which the gels contracted and became turbid with increased optical scattering. The gels were characterized quantitatively by their optical properties, specified by analysis of OCT data using a theoretical model. At 6 h, seeded cell density and scattering coefficient (mu(s)) were correlated, with mu(s) equal to 10.8 cm(-1)(10(6) cells mL). Seeded cell density and the scattering anisotropy (g) were uncorrelated. Over five days, the reflectivity in SMC gels gradually doubled with little change in optical attenuation, which indicated a decrease in g that increased backscatter, but only a small drop in mu(s). At five days, a subpopulation of sites on the gel showed substantially higher reflectivity (approximately a tenfold increase from the first 24 h). In summary, the increased turbidity of SMC gels that develops over time is due to a change in the structure of collagen, which affects g, and not simply due to a change in number density of collagen fibers due to contraction.
20464800	Data is the currency of R&amp;D, and that currency is now generated and traded globally.
Curr Opin Drug Discov Devel  2010May
Changes in the understanding of biological science, translational research and corporate business models require a corresponding change in the approach to chemical and biological information management. The concept of operations being partitioned into discrete departments for drug discovery is beginning to be replaced by a translational approach to this process. Pharmaceutical business and organizational models are also constantly evolving. Traditional approaches to transactional systems, transferring data up to a departmental data warehouse, are no longer meeting the needs of pharmaceutical scientists and, thus, IT departments are not considered as relevant to the business. These changes and their impact on information systems, as well as some solutions to the challenges faced, are discussed in this editorial.
20467059	Visual integration of quantitative proteomic data, pathways, and protein interactions.
IEEE Trans Vis Comput Graph  2010 Jul-Aug
We introduce several novel visualization and interaction paradigms for visual analysis of published protein-protein interaction networks, canonical signaling pathway models, and quantitative proteomic data. We evaluate them anecdotally with domain scientists to demonstrate their ability to accelerate the proteomic analysis process. Our results suggest that structuring protein interaction networks around canonical signaling pathway models, exploring pathways globally and locally at the same time, and driving the analysis primarily by the experimental data, all accelerate the understanding of protein pathways. Concrete proteomic discoveries within T-cells, mast cells, and the insulin signaling pathway validate the findings. The aim of the paper is to introduce novel protein network visualization paradigms and anecdotally assess the opportunity of incorporating them into established proteomic applications. We also make available a prototype implementation of our methods, to be used and evaluated by the proteomic community.
20399012	Structural modeling and analysis of an effluent treatment process for electroplating--a graph theoretic approach.
J. Hazard. Mater. 20100323 2010Jul15
An attempt is made to address a few ecological and environment issues by developing different structural models for effluent treatment system for electroplating. The effluent treatment system is defined with the help of different subsystems contributing to waste minimization. Hierarchical tree and block diagram showing all possible interactions among subsystems are proposed. These non-mathematical diagrams are converted into mathematical models for design improvement, analysis, comparison, storage retrieval and commercially off-the-shelf purchases of different subsystems. This is achieved by developing graph theoretic model, matrix models and variable permanent function model. Analysis is carried out by permanent function, hierarchical tree and block diagram methods. Storage and retrieval is done using matrix models. The methodology is illustrated with the help of an example. Benefits to the electroplaters/end user are identified.
20426836	Concept-based query expansion for retrieving gene related publications from MEDLINE.
BMC Bioinformatics 20100428 2010
Advances in biotechnology and in high-throughput methods for gene analysis have contributed to an exponential increase in the number of scientific publications in these fields of study. While much of the data and results described in these articles are entered and annotated in the various existing biomedical databases, the scientific literature is still the major source of information. There is, therefore, a growing need for text mining and information retrieval tools to help researchers find the relevant articles for their study. To tackle this, several tools have been proposed to provide alternative solutions for specific user requests. This paper presents QuExT, a new PubMed-based document retrieval and prioritization tool that, from a given list of genes, searches for the most relevant results from the literature. QuExT follows a concept-oriented query expansion methodology to find documents containing concepts related to the genes in the user input, such as protein and pathway names. The retrieved documents are ranked according to user-definable weights assigned to each concept class. By changing these weights, users can modify the ranking of the results in order to focus on documents dealing with a specific concept. The method's performance was evaluated using data from the 2004 TREC genomics track, producing a mean average precision of 0.425, with an average of 4.8 and 31.3 relevant documents within the top 10 and 100 retrieved abstracts, respectively. QuExT implements a concept-based query expansion scheme that leverages gene-related information available on a variety of biological resources. The main advantage of the system is to give the user control over the ranking of the results by means of a simple weighting scheme. Using this approach, researchers can effortlessly explore the literature regarding a group of genes and focus on the different aspects relating to these genes.
20374657	Identifying work related injuries: comparison of methods for interrogating text fields.
BMC Med Inform Decis Mak 20100407 2010
Work-related injuries in Australia are estimated to cost around $57.5 billion annually, however there are currently insufficient surveillance data available to support an evidence-based public health response. Emergency departments (ED) in Australia are a potential source of information on work-related injuries though most ED's do not have an 'Activity Code' to identify work-related cases with information about the presenting problem recorded in a short free text field. This study compared methods for interrogating text fields for identifying work-related injuries presenting at emergency departments to inform approaches to surveillance of work-related injury. Three approaches were used to interrogate an injury description text field to classify cases as work-related: keyword search, index search, and content analytic text mining. Sensitivity and specificity were examined by comparing cases flagged by each approach to cases coded with an Activity code during triage. Methods to improve the sensitivity and/or specificity of each approach were explored by adjusting the classification techniques within each broad approach. The basic keyword search detected 58% of cases (Specificity 0.99), an index search detected 62% of cases (Specificity 0.87), and the content analytic text mining (using adjusted probabilities) approach detected 77% of cases (Specificity 0.95). The findings of this study provide strong support for continued development of text searching methods to obtain information from routine emergency department data, to improve the capacity for comprehensive injury surveillance.
20470454	[Bibliographic research of efficiency tests: analysis of the validity of the meta-database].
Prof Inferm  2010 Jan-Mar
The use of a meta-database as a first approach to bibliographic research can be just as efficient as interrogating single data-bases of the literature. The advantages and drawbacks of the two strategies are compared . A comparison of the results obtained using an identical interrogation made using the TRIP meta-database and different single databases (15 of guide-lines, 4 of systematic reviews, 3 prevalently consisting of primary studies) made it possible to analyse these methods as well as to study 4 meta-databases and identify the most efficient one. Using the same MeSH terms in both strategies, the following results were obtained: 204 publications using TRIP and 475 using different databases. Evaluation demonstrated the pertinence of 142 (69,6%) of the 204 found using TRIP compared to 185 (38,9%) of those elicited by single data-bases. The TRIP meta-database yields a lower number of documents but with a higher degree of pertinency, meaning that the researcher employs less time finding pertinent documents. With respect to the traditional approach, beginning research by testing the efficiency of the TRIP meta-database proved advantageous.
20053732	Real-time classification of datasets with hardware embedded neuromorphic neural networks.
Brief. Bioinformatics 20100106 2010May
Neuromorphic artificial neural networks attempt to understand the essential computations that take place in the dense networks of interconnected neurons making up the central nervous systems in living creatures. This article demonstrates that artificial spiking neural networks--built to resemble the biological model--encoding information in the timing of single spikes, are capable of computing and learning clusters from realistic data. It shows how a spiking neural network based on spike-time coding can successfully perform unsupervised and supervised clustering on real-world data. A temporal encoding procedure of continuously valued data is developed, together with a hardware implementation oriented new learning rule set. Solutions that make use of embedded soft-core microcontrollers are investigated, to implement some of the most resource-consuming components of the artificial neural network. Details of the implementations are given, with benchmark application evaluation and test bench description. Measurement results are presented, showing real-time and adaptive data processing capabilities, comparing these to related findings in the specific literature.
20481329	[The compression and storage of enhanced external counterpulsation waveform based on DICOM standard].
Sheng Wu Yi Xue Gong Cheng Xue Za Zhi  2010Apr
The development of external counterpulsation (ECP) local area network system and extensible markup language (XML)-based remote ECP medical information system conformable to digital imaging and communications in medicine (DICOM) standard has been improving the digital interchangeablity and sharability of ECP data. However, the therapy process of ECP is a continuous and longtime supervision which builds a mass of waveform data. In order to reduce the storage space and improve the transmission efficiency, the waveform data with the normative format of ECP data files have to be compressed. In this article, we introduced the compression arithmetic of template matching and improved quick fitting of linear approximation distance thresholding (LADT) in combimation with the characters of enhanced external counterpulsation (EECP) waveform signal. The DICOM standard is used as the storage and transmission standard to make our system compatible with hospital information system. According to the rules of transfer syntaxes, we defined private transfer syntax for one-dimensional compressed waveform data and stored EECP data into a DICOM file. Testing result indicates that the compressed and normative data can be correctly transmitted and displayed between EECP workstations in our EECP laboratory.
20485232	Everything in its place. Social bookmarking and reference manager tools to collect, manage and cite information sources.
Eur J Phys Rehabil Med  2010Jun
Aim of this contribution was to present some free reference manager software and social bookmarking tools. They help scholars and authors in recording, managing and re-using Web pages, scientific articles and bibliographic citations. Most of them support integration within the commonly used browsers or word processors, in order to easily create or import a full bibliography or a single reference.
20418942	Full text and figure display improves bioscience literature search.
PLoS ONE 20100414 2010
When reading bioscience journal articles, many researchers focus attention on the figures and their captions. This observation led to the development of the BioText literature search engine, a freely available Web-based application that allows biologists to search over the contents of Open Access Journals, and see figures from the articles displayed directly in the search results. This article presents a qualitative assessment of this system in the form of a usability study with 20 biologist participants using and commenting on the system. 19 out of 20 participants expressed a desire to use a bioscience literature search engine that displays articles' figures alongside the full text search results. 15 out of 20 participants said they would use a caption search and figure display interface either frequently or sometimes, while 4 said rarely and 1 said undecided. 10 out of 20 participants said they would use a tool for searching the text of tables and their captions either frequently or sometimes, while 7 said they would use it rarely if at all, 2 said they would never use it, and 1 was undecided. This study found evidence, supporting results of an earlier study, that bioscience literature search systems such as PubMed should show figures from articles alongside search results. It also found evidence that full text and captions should be searched along with the article title, metadata, and abstract. Finally, for a subset of users and information needs, allowing for explicit search within captions for figures and tables is a useful function, but it is not entirely clear how to cleanly integrate this within a more general literature search interface. Such a facility supports Open Access publishing efforts, as it requires access to full text of documents and the lifting of restrictions in order to show figures in the search interface.
20418944	BioTorrents: a file sharing service for scientific data.
PLoS ONE 20100414 2010
The transfer of scientific data has emerged as a significant challenge, as datasets continue to grow in size and demand for open access sharing increases. Current methods for file transfer do not scale well for large files and can cause long transfer times. In this study we present BioTorrents, a website that allows open access sharing of scientific data and uses the popular BitTorrent peer-to-peer file sharing technology. BioTorrents allows files to be transferred rapidly due to the sharing of bandwidth across multiple institutions and provides more reliable file transfers due to the built-in error checking of the file sharing technology. BioTorrents contains multiple features, including keyword searching, category browsing, RSS feeds, torrent comments, and a discussion forum. BioTorrents is available at http://www.biotorrents.net.
20428278	The effectiveness of the practice of correction and republication in the biomedical literature.
J Med Libr Assoc  2010Apr
This research measures the effectiveness of the practice of correction and republication of invalidated articles in the biomedical literature by analyzing the rate of citation of the flawed and corrected versions of scholarly articles over time. If the practice of correction and republication is effective, then the incidence of citation of flawed versions should diminish over time and increased incidence of citation of the republication should be observed. This is a bibliometric study using citation analysis and statistical analysis of pairs of flawed and corrected articles in MEDLINE and Web of Science. The difference between citation levels of flawed originals and corrected republications does not approach statistical significance until eight to twelve years post-republication. Results showed substantial variability among bibliographic sources in their provision of authoritative bibliographic information. Correction and republication is a marginally effective biblioremediative practice. The data suggest that inappropriate citation behavior may be partly attributable to author ignorance.
20432136	Foundations of database searching: integrating evidence-based medicine into the medical curriculum.
Med Ref Serv Q  2010Apr
Library integration into the medical school curriculum is a crucial aspect of meeting Liaison Committee on Medical Education (LCME) Accreditation Standards and the Association of American Medical Colleges (AAMC) Medical School Objectives Project (MSOP) guidelines. To accomplish this, academic health sciences libraries seek to develop evidence-based medicine (EBM) literature searching classes within the medical school curriculum. Establishing a basic understanding of the fundamental concepts behind health sciences database searching among medical students is a prerequisite for a more demanding evidence-based literature searching curriculum. The George T. Harrell Health Sciences Library, Penn State College of Medicine, sought to incorporate an evidence-based medicine literature searching structure by working within the existing problem-based learning system during the preclinical years. Students in the clinical years will participate in evidence-based assignments during their rotations. A fourth-year EBM elective will be created to reinforce and round out students' exposure to these concepts.
20432138	Automatic Export of PubMed Citations to EndNote.
Med Ref Serv Q  2010Apr
The export of MEDLINE references to EndNote can be accomplished in various ways. Unlike Ovid MEDLINE, PubMed does not have a direct export feature to EndNote. Until recently, PubMed references had to be saved as a text file to import into EndNote. Now, the automatic export of PubMed references can be done using Internet Explorer (IE) or Mozilla Firefox Web browsers. The development and teaching of seamless citation management is a value-added service to health professionals.
20432139	Natural Standard database.
Med Ref Serv Q  2010Apr
The Natural Standard database contains systematic reviews on foods, functional foods, diets, supplements, vitamins, minerals, and Complementary and Alternative Medicine (CAM) modalities. It is designed to serve as a clinical decision support tool. This column will provide an overview of Natural Standard and its content and scope, as well as provide some basics for searching this resource.
20433037	Carbon monoxide-related hospitalizations in the U.S.: evaluation of a web-based query system for public health surveillance.
Public Health Rep  2010 May-Jun
Carbon monoxide (CO) poisoning is preventable, yet it remains one of the most common causes of poisoning in the U.S. In the absence of a national data reporting system for CO-poisoning surveillance, the burden of CO-related hospitalizations is unknown. Our objective was to generate the first national estimates of CO-related hospitalizations and to evaluate the use of a Web-based query system for public health surveillance. The Healthcare Cost and Utilization Project's (HCUP's) 2005 Nationwide Inpatient Sample (NIS) data were used for CO-related hospitalization estimates. Data for confirmed, probable, and suspected cases were generated using the HCUPnet Web-based query system. We used data from 1993 through 2005 NIS to describe trends in CO-related hospitalizations. We used the Centers for Disease Control and Prevention's surveillance evaluation guidelines to evaluate the system. In 2005, there were 24,891 CO-related hospitalizations nationwide: 16.9% (n=4,216) were confirmed, 1.1% (n=279) were probable, and 81.9% (n=20,396) were suspected CO-poisoning cases. Of the confirmed cases (1.42/100,000 population), the highest hospitalization rates occurred among males, older adults (aged &gt; or = 85 years), and Midwestern residents. CO-related hospitalization rates declined from 1993 through 2000 and plateaued from 2001 through 2005. The simplicity, acceptability, sensitivity, and representativeness of the HCUPnet surveillance system were excellent. However, HCUPnet showed limited flexibility and specificity. Nationwide, the burden of CO exposure resulting in hospitalization is substantial. HCUPnet is a useful surveillance tool that efficiently characterized CO-related hospitalizations for the first time. Public health practitioners can utilize this data source for state-level surveillance.
20433059	Intelligent technique for knowledge reuse of dental medical records based on case-based reasoning.
J Med Syst  2010Apr
With the rapid development of both information technology and the management of modern medical regulation, the generation of medical records tends to be increasingly intelligent. In this paper, Case-Based Reasoning is applied to the process of generating records of dental cases. Based on the analysis of the features of dental records, a case base is constructed. A mixed case retrieval method (FAIES) is proposed for the knowledge reuse of dental records by adopting Fuzzy Mathematics, which improves similarity algorithm based on Euclidian-Lagrangian Distance, and PULL &amp; PUSH weight adjustment strategy. Finally, an intelligent system of dental cases generation (CBR-DENT) is constructed. The effectiveness of the system, the efficiency of the retrieval method, the extent of adaptation and the adaptation efficiency are tested using the constructed case base. It is demonstrated that FAIES is very effective in terms of reducing the time of writing medical records and improving the efficiency and quality. FAIES is also proven to be an effective aid for diagnoses and provides a new idea for the management of medical records and its applications.
20347072	A learning method for the class imbalance problem with medical data sets.
Comput. Biol. Med. 20100326 2010May
In medical data sets, data are predominately composed of "normal" samples with only a small percentage of "abnormal" ones, leading to the so-called class imbalance problems. In class imbalance problems, inputting all the data into the classifier to build up the learning model will usually lead a learning bias to the majority class. To deal with this, this paper uses a strategy which over-samples the minority class and under-samples the majority one to balance the data sets. For the majority class, this paper builds up the Gaussian type fuzzy membership function and alpha-cut to reduce the data size; for the minority class, we use the mega-trend diffusion membership function to generate virtual samples for the class. Furthermore, after balancing the data size of classes, this paper extends the data attribute dimension into a higher dimension space using classification related information to enhance the classification accuracy. Two medical data sets, Pima Indians' diabetes and the BUPA liver disorders, are employed to illustrate the approach presented in this paper. The results indicate that the proposed method has better classification performance than SVM, C4.5 decision tree and two other studies.
20434024	Summary-of-findings tables in Cochrane reviews improved understanding and rapid retrieval of key information.
J Clin Epidemiol  2010Jun
To measure the effects of a summary-of-findings (SoF) table on user satisfaction, understanding, and time spent finding key results in a Cochrane review. We randomized participants in an evidence-based practice workshop (randomized controlled trial [RCT] I) and a Cochrane Collaboration entities meeting (RCT II) to receive a Cochrane review with or without an SoF table. In RCT I, we measured user satisfaction. In RCT II, we measured correct comprehension and time spent finding key results. RCT I: Participants with the SoF table (n=47) were more likely to "agree" or "strongly agree" that it was easy to find results for important outcomes than (n=25) participants without the SoF table-68% vs. 40% (P=0.021). RCT II: Participants with the SoF table (n=18) were more likely to correctly answer two questions regarding results than (n=15) those without the SoF table: 93% vs. 44% (P=0.003) and 87% vs. 11% (P&lt;0.001). Participants with the SoF table spent an average of 90 seconds to find key information compared with 4 minutes for participants without the SoF table (P=0.002). In two small trials, we found that inclusion of an SoF table in a review improved understanding and rapid retrieval of key findings compared with reviews with no SoF table.
20439252	An international comparison of web-based reporting about health care quality: content analysis.
J. Med. Internet Res. 20100413 2010
On more and more websites, consumers are provided with public reports about health care. This move toward provision of more comparative information has resulted in different information types being published that often contain contradictory information. The objective was to assess the current state of the art in the presentation of online comparative health care information and to compare how the integration of different information types is dealt with on websites. The content analysis was performed in order to provide website managers and Internet researchers with a resource of knowledge about presentation formats being applied internationally. A Web search was used to identify websites that contained comparative health care information. The websites were systematically examined to assess how three different types of information (provider characteristics and services, performance indicators, and health care user experience) were presented to consumers. Furthermore, a short survey was disseminated to the reviewed websites to assess how the presentation formats were selected. We reviewed 42 websites from the following countries: Australia, Canada, Denmark, Germany, Ireland, the Netherlands, Norway, the United Kingdom, the United States, and Sweden. We found the most common ways to integrate different information types were the two extreme options: no integration at all (on 36% of the websites) and high levels of integration in single tables on 41% of the websites). Nearly 70% of the websites offered drill down paths to more detailed information. Diverse presentation approaches were used to display comparative health care information on the Internet. Numbers were used on the majority of websites (88%) to display comparative information. Currently, approaches to the presentation of comparative health care information do not seem to be systematically selected. It seems important, however, that website managers become aware of the complexities inherent in comparative information when they release information on the Web. Important complexities to pay attention to are the use of numbers, the display of contradictory information, and the extent of variation among attributes and attribute levels. As for the integration of different information types, it remains unclear which presentation approaches are preferable. Our study provides a good starting point for Internet research to further address the question of how different types of information can be more effectively presented to consumers.
20443164	The importance of open-source integrative genomics to drug discovery.
Curr Opin Drug Discov Devel  2010May
Researchers investigating many areas of disease recognize the value of integrating large-scale genomic experiments across species and experimental methods. Analysis methods have been developed to make use of the breadth and depth of data from new technologies. Current paradigms of data storage, sharing and analysis are not yet ideal for these purposes. Open-access and analysis-enabled repositories are critical to progress, as they put the global integration of genomic data within reach of individual expert investigators. Current analytic approaches use the full scale and scope of data, but require data sharing, interoperability and community recognition of the value of shared information.
20442139	An overview of MetaMap: historical perspective and recent advances.
J Am Med Inform Assoc  2010 May-Jun
MetaMap is a widely available program providing access to the concepts in the unified medical language system (UMLS) Metathesaurus from biomedical text. This study reports on MetaMap's evolution over more than a decade, concentrating on those features arising out of the research needs of the biomedical informatics community both within and outside of the National Library of Medicine. Such features include the detection of author-defined acronyms/abbreviations, the ability to browse the Metathesaurus for concepts even tenuously related to input text, the detection of negation in situations in which the polarity of predications is important, word sense disambiguation (WSD), and various technical and algorithmic features. Near-term plans for MetaMap development include the incorporation of chemical name recognition and enhanced WSD.
20442145	An evaluation of medical knowledge contained in Wikipedia and its use in the LOINC database.
J Am Med Inform Assoc  2010 May-Jun
The logical observation identifiers names and codes (LOINC) database contains 55 000 terms consisting of more atomic components called parts. LOINC carries more than 18 000 distinct parts. It is necessary to have definitions/descriptions for each of these parts to assist users in mapping local laboratory codes to LOINC. It is believed that much of this information can be obtained from the internet; the first effort was with Wikipedia. This project focused on 1705 laboratory analytes (the first part in the LOINC laboratory name). Of the 1705 parts queried, 1314 matching articles were found in Wikipedia. Of these, 1299 (98.9%) were perfect matches that exactly described the LOINC part, 15 (1.14%) were partial matches (the description in Wikipedia was related to the LOINC part, but did not describe it fully), and 102 (7.76%) were mis-matches. The current release of RELMA and LOINC include Wikipedia descriptions of LOINC parts obtained as a direct result of this project.
20442153	Evaluation of family history information within clinical documents and adequacy of HL7 clinical statement and clinical genomics family history models for its representation: a case report.
J Am Med Inform Assoc  2010 May-Jun
Family history information has emerged as an increasingly important tool for clinical care and research. While recent standards provide for structured entry of family history, many clinicians record family history data in text. The authors sought to characterize family history information within clinical documents to assess the adequacy of existing models and create a more comprehensive model for its representation. Models were evaluated on 100 documents containing 238 sentences and 410 statements relevant to family history. Most statements were of family member plus disease or of disease only. Statement coverage was 91%, 77%, and 95% for HL7 Clinical Genomics Family History Model, HL7 Clinical Statement Model, and the newly created Merged Family History Model, respectively. Negation (18%) and inexact family member specification (9.5%) occurred commonly. Overall, both HL7 models could represent most family history statements in clinical reports; however, refinements are needed to represent the full breadth of family history data.
20445715	Integration of evidence-based practice in bedside teaching paediatrics supported by e-learning.
Biomed Pap Med Fac Univ Palacky Olomouc Czech Repub  2010Mar
Bedside teaching with evidence-based practice elements, supported by e-learning activities, can play an important role in modern medical education. Teachers have to incorporate evidence from the medical literature to increase student motivation and interactivity. An integral part of the medical curricula at Palacky University Olomouc (Czech Republic) are real paediatric scenarios supplemented with a review of current literature to enhance evidence-based bedside teaching &amp; learning. Searching for evidence is taught through librarian-guided interactive hands-on sessions and/or web-based tutorials followed by clinical case presentations and feedback. Innovated EBM paediatric clerkship demonstrated students' preferences towards web-based interactive bedside teaching &amp; learning. In two academic years (2007/2008, 2008/2009), learning-focused feedback from 106 and 131 students, resp. was obtained about their attitudes towards evidence-based bedside teaching. The assessment included among others the overall level of instruction, quality of practical evidence-based training, teacher willingness and impact of instruction on increased interest in the specialty. There was some criticism about excessive workload. A parallel survey was carried out on the perceived values of different forms of information skills training (i.e. demonstration, online tutorials, and librarian-guided interactive search sessions) and post-training self-reported level of search skills. The new teaching/learning paediatric portfolio is a challenge for further activities, including effective knowledge translation, continuing medical &amp; professional development of teachers, and didactic, clinically integrated teaching approaches.
20446610	[Searching articles and their management].
Kyobu Geka  2010May
The development of digitalizing technology and the Internet has enabled medical doctors and researchers in medicine to search and read the latest articles at their desk without visiting a library. As a result of the time and effort spent for searching articles can be extremely reduced by learning how to use effective tools in combination, the time for the research activity will certainly be greatly saved. It is promising that the advancement of science database, online journals, evaluating system of the journal impact will be great help for researchers.
20451873	The informatics core of the Alzheimer's Disease Neuroimaging Initiative.
Alzheimers Dement  2010May
The Alzheimer's Diseases Neuroimaging Initiative project has brought together geographically distributed investigators, each collecting data on the progression of Alzheimer's disease. The quantity and diversity of the imaging, clinical, cognitive, biochemical, and genetic data acquired and generated throughout the study necessitated sophisticated informatics systems to organize, manage, and disseminate data and results. We describe, here, a successful and comprehensive system that provides powerful mechanisms for processing, integrating, and disseminating these data not only to support the research needs of the investigators who make up the Alzheimer's Diseases Neuroimaging Initiative cores, but also to provide widespread data access to the greater scientific community for the study of Alzheimer's Disease.
20299044	Reporting of harm in randomized controlled trials published in the urological literature.
J. Urol. 20100317 2010May
Evidence-based decision making seeks to balance potential benefits and harms (adverse effects) of health care interventions for an individual patient. We determined the prevalence and completeness of harm reporting in randomized controlled trials in the urological literature. We performed a systematic literature search of all randomized controlled trials of therapeutic interventions published in The Journal of Urology, Urology, European Urology and BJU International in 1996 and 2004. Each article was reviewed by 2 independent investigators for 10 harm reporting criteria recommended by the CONSORT group. Discrepancies were settled by discussion and consensus. A total of 152 randomized controlled trials met the inclusion criteria, of which 109 (72%) reported adverse event outcomes. The median number of harm reporting criteria satisfied improved marginally from 1996 to 2004 (2.8 to 3.3, p = 0.36). A large proportion of studies failed to address harm in the abstract (55, 36%), introduction (71, 47%) and discussion (52, 34%). Few studies specified which adverse events were evaluated (21, 14%), when harm information was collected (32, 21%) or how the harm was attributed to the intervention (5, 3%). Only 48 (32%) articles provided reasons for patient withdrawal and 1 in 5 (33, 22%) reported the severity of adverse events. Randomized controlled trials published in the urological literature contain significant deficiencies in adverse event reporting. These findings suggest the need for reporting standards for harm in urological journals. Improvements in adverse event reporting would permit a more balanced assessment of interventions and would enhance evidence-based urological practice.
20383330	Searching PubMed during a pandemic.
PLoS ONE 20100407 2010
The 2009 influenza A(H1N1) pandemic has generated thousands of articles and news items. However, finding relevant scientific articles in such rapidly developing health crises is a major challenge which, in turn, can affect decision-makers' ability to utilise up-to-date findings and ultimately shape public health interventions. This study set out to show the impact that the inconsistent naming of the pandemic can have on retrieving relevant scientific articles in PubMed/MEDLINE. We first formulated a PubMed search algorithm covering different names of the influenza pandemic and simulated the results that it would have retrieved from weekly searches for relevant new records during the first 10 weeks of the pandemic. To assess the impact of failing to include every term in this search, we then conducted the same searches but omitted in turn "h1n1," "swine," "influenza" and "flu" from the search string, and compared the results to those for the full string. On average, our core search string identified 44.3 potentially relevant new records at the end of each week. Of these, we determined that an average of 27.8 records were relevant. When we excluded one term from the string, the percentage of records missed out of the total number of relevant records averaged 18.7% for omitting "h1n1," 13.6% for "swine," 17.5% for "influenza," and 20.6% for "flu." Due to inconsistent naming, while searching for scientific material about rapidly evolving situations such as the influenza A(H1N1) pandemic, there is a risk that one will miss relevant articles. To address this problem, the international scientific community should agree on nomenclature and the specific name to be used earlier, and the National Library of Medicine in the US could index potentially relevant materials faster and allow publishers to add alert tags to such materials.
20384889	Comparison of color representations for content-based image retrieval in dermatology.
Skin Res Technol  2010Feb
We compare the effectiveness of 10 different color representations in a content-based image retrieval task for dermatology. As features, we use the average colors of healthy and lesion skin in an image. The extracted features are used to retrieve similar images from a database using a k-nearest-neighbor search and Euclidean distance. The images in the database are divided into four different color categories. We measure the effectiveness of retrieval by the average percentage of retrieved images that belong to the same category as a query image. We found that the difference of the colors of lesion and healthy skin is a better color descriptor than the pair of these colors. We obtained the best results with the CIE-Lab color representation [75+/-3.8% (95% confidence interval) correct retrieval rate for k=11], followed by CIE-Luv and CIE-Lch. CIE-Lab is the most effective color space for content-based image retrieval of dermatological images. The difference of the colors of lesion and healthy skin in an image is a better color descriptor than the pair of these colors.
20390683	Classifying health questions asked by the public using the ICPC-2 classification and a taxonomy of generic clinical questions: an empirical exploration of the feasibility.
Health Commun  2010Mar
In case of an overload of information, structure is needed to make the content of the information accessible and the information flow well-ordered. If we wish to gain insight into the health information needs of the public, a specific research tool is needed. The aim of this study was to investigate the feasibility of using two professional classification structures for medical information to classify health questions asked by the public: one classification for the subject of the question, the International Classification of Primary Care (ICPC-2), and one classification for the nature and type of the question, the Taxonomy for Generic Clinical Questions (TGCQ). Health questions asked during online consultations with health care providers were retrieved (452 subjects for coding) and were given two codes: one code according to the ICPC-2 and one according to the TGCQ. The problems encountered during coding were recorded and analyzed. Nine different clusters of problems arose during classification with the ICPC-2, including issues regarding specificity, lay versus professional terminology, a combination of diverse complaints not complying with a clinical syndrome, and preclinical issues. Nine types of problems were encountered during the classification with the TGCQ: questions about preclinical issues, preventive procedures, name finding, health promotion, where to go for a diagnostic test or therapy, justification of the choice of a test or treatment, and common knowledge. The results of this study are promising, and further investigation of the validity, reliability, and use of these two classification systems to classify health questions asked by the public is desirable. The problems that were encountered should be solved before these professional systems can be used to classify the health information needs of the general public.
20163981	Optimal embedding for shape indexing in medical image databases.
Med Image Anal 20100120 2010Jun
This paper addresses the problem of indexing shapes in medical image databases. Shapes of organs are often indicative of disease, making shape similarity queries important in medical image databases. Mathematically, shapes with landmarks belong to shape spaces which are curved manifolds with a well defined metric. The challenge in shape indexing is to index data in such curved spaces. One natural indexing scheme is to use metric trees, but metric trees are prone to inefficiency. This paper proposes a more efficient alternative. We show that it is possible to optimally embed finite sets of shapes in shape space into a Euclidean space. After embedding, classical coordinate-based trees can be used for efficient shape retrieval. The embedding proposed in the paper is optimal in the sense that it least distorts the partial Procrustes shape distance. The proposed indexing technique is used to retrieve images by vertebral shape from the NHANES II database of cervical and lumbar spine X-ray images maintained at the National Library of Medicine. Vertebral shape strongly correlates with the presence of osteophytes, and shape similarity retrieval is proposed as a tool for retrieval by osteophyte presence and severity. Experimental results included in the paper evaluate (1) the usefulness of shape similarity as a proxy for osteophytes, (2) the computational and disk access efficiency of the new indexing scheme, (3) the relative performance of indexing with embedding to the performance of indexing without embedding, and (4) the computational cost of indexing using the proposed embedding versus the cost of an alternate embedding. The experimental results clearly show the relevance of shape indexing and the advantage of using the proposed embedding.
20189869	A framework for optimizing measurement weight maps to minimize the required sample size.
Med Image Anal 20100201 2010Jun
We propose a fully automatic statistical framework for identifying the non-negative, real-valued weight map that best discriminate between two groups of objects. Given measurements on a spatially defined grid, a numerical optimization scheme is used to find the weight map that minimizes the sample size required to discriminate the two groups. The weight map produced by the method reflects the relative importance of the different areas in the objects, and the resulting sample size reduction is an important end goal in situations where data collection is difficult or expensive. An example is in clinical studies where the cost and the patient burden are directly related to the number of participants needed for the study. In addition, inspection of the weight map might provide clues that can lead to a better clinical understanding of the objects and pathologies being studied. The method is evaluated on synthetic data and on clinical data from knee cartilage MRI. The clinical data contain a total of 159 subjects aged 21-81 years and ranked from zero to four on the Kellgren-Lawrence osteoarthritis severity scale. Compared to a uniform weight map, we achieve sample size reductions up to 58% for cartilage thickness measurements. Based on quantifications from both morphometric and textural based imaging features, we also identify the most pathological areas in the articular cartilage.
20391160	Have computers, will travel: providing on-site library instruction in rural health facilities using a portable computer lab.
Med Ref Serv Q  2010Jan
The Saskatchewan Health Information Resources Partnership (SHIRP) provides library instruction to Saskatchewan's health care practitioners and students on placement in health care facilities as part of its mission to provide province-wide access to evidence-based health library resources. A portable computer lab was assembled in 2007 to provide hands-on training in rural health facilities that do not have computer labs of their own. Aside from some minor inconveniences, the introduction and operation of the portable lab has gone smoothly. The lab has been well received by SHIRP patrons and continues to be an essential part of SHIRP outreach.
20391165	Science.gov: gateway to government science information.
Med Ref Serv Q  2010Jan
Science.gov is a portal to more than 40 scientific databases and 200 million pages of science information via a single query. It connects users to science information and research results from the U.S. government. This column will provide readers with an overview of the resource, as well as basic search hints.
20391167	Solo librarian and outreach to hospital staff using Web 2.0 technologies.
Med Ref Serv Q  2010Jan
The part-time librarian at Penn Presbyterian Medical Center (PPMC) serves physicians, staff, and students. Challenged by time constraints and the need for a physical presence in the library, the librarian sought methods requiring limited manpower and maintenance to reach out to users. The librarian utilized two Web 2.0 technologies, Delicious and Bloglines, to extend library services beyond the confines of the hospital intranet. This article details the process to implement these two technologies in the hospital setting. Informational resources about Web 2.0 technologies are included in the article.
20391168	Medical education and information literacy in the era of open access.
Med Ref Serv Q  2010Jan
The Open Access movement in scholarly communications poses new issues and concerns for medical education in general and information literacy education specifically. For medical educators, Open Access can affect the availability of new information, instructional materials, and scholarship in medical education. For students, Open Access materials continue to be available to them post-graduation, regardless of affiliation. Libraries and information literacy librarians are challenged in their responses to the Open Access publishing movement in how best to support Open Access endeavors within their own institutions, and how best to educate their user base about Open Access in general.
20389405	Efficiency of complex modulation methods in coherent free-space optical links.
Opt Express  2010Feb15
We study the performance of various binary and nonbinary modulation methods applied to coherent laser communication through the turbulent atmosphere. We compare the spectral efficiencies and SNR efficiencies of complex modulations, and consider options for atmospheric compensation, including phase correction and diversity combining techniques. Our analysis shows that high communication rates require receivers with good sensitivity along with some technique to mitigate the effect of atmospheric fading.
20389629	Improving the privacy of optical steganography with temporal phase masks.
Opt Express  2010Mar15
Temporal phase modulation of spread stealth signals is proposed and demonstrated to improve optical steganography transmission privacy. After phase modulation, the temporally spread stealth signal has a more complex spectral-phase-temporal relationship, such that the original temporal profile cannot be restored when only dispersion compensation is applied to the temporally spread stealth signals. Therefore, it increases the difficulty for the eavesdropper to detect and intercept the stealth channel that is hidden under a public transmission, even with a correct dispersion compensation device. The experimental results demonstrate the feasibility of this approach and display insignificant degradation in transmission performance, compared to the conventional stealth transmission without temporal phase modulation. The proposed system can also work without a clock transmission for signal synchronization. Our analysis and simulation results show that it is difficult for the adversary to detect the existence of the stealth transmission, or find the correct phase mask to recover the stealth signals.
20389656	Improved sensitivity of nonvolatile holographic storage in triply doped LiNbO(3):Zr,Cu,Ce.
Opt Express  2010Mar15
We have designed and grown triply doped LiNbO(3):Zr,Cu,Ce crystal and investigated its characteristics of nonvolatile holographic storage. It's observed that the photorefractive sensitivity of LiNbO(3):Zr,Cu,Ce has improved to 0.099 cm/J, which is about one order of magnitude larger than that of congruent LiNbO(3):Cu,Ce. And LiNbO(3):Zr,Cu,Ce also has high suppression to light-induced scattering. Our results indicated that triply doped LiNbO(3):Zr,Cu,Ce is an excellent candidate for nonvolatile holographic data storage.
20360561	Bayesian approach to transforming public gene expression repositories into disease diagnosis databases.
Proc. Natl. Acad. Sci. U.S.A. 20100401 2010Apr13
The rapid accumulation of gene expression data has offered unprecedented opportunities to study human diseases. The National Center for Biotechnology Information Gene Expression Omnibus is currently the largest database that systematically documents the genome-wide molecular basis of diseases. However, thus far, this resource has been far from fully utilized. This paper describes the first study to transform public gene expression repositories into an automated disease diagnosis database. Particularly, we have developed a systematic framework, including a two-stage Bayesian learning approach, to achieve the diagnosis of one or multiple diseases for a query expression profile along a hierarchical disease taxonomy. Our approach, including standardizing cross-platform gene expression data and heterogeneous disease annotations, allows analyzing both sources of information in a unified probabilistic system. A high level of overall diagnostic accuracy was shown by cross validation. It was also demonstrated that the power of our method can increase significantly with the continued growth of public gene expression repositories. Finally, we showed how our disease diagnosis system can be used to characterize complex phenotypes and to construct a disease-drug connectivity map.
20397479	The master student presenter: Peer teaching in the simulation laboratory.
Nurs Educ Perspect  2010 Jan-Feb
The professional nurse needs to be able demonstrate the roles of teacher, manager, lifelong learner, research consumer, and provider of care. A teaching strategy that fosters the integration of these roles by the student is the Master Student Presenter (MSP) assignment, whereby students teach peers a fundamental skill in the simulation laboratory setting. The objectives of this assignment are for students to: (a) synthesize current literature to establish an evidence-based practice, (b) deliver an oral presentation, (c) demonstrate a skill to fellow students, and (d) evaluate peers on skill performance. Using the MSP assignment in the simulation laboratory contributes to the process of integrating these roles, which students build upon throughout their nursing education.
20401951	Croatian Medical Journal citation score in Web of Science, Scopus, and Google Scholar.
Croat. Med. J.  2010Apr
To analyze the 2007 citation count of articles published by the Croatian Medical Journal in 2005-2006 based on data from the Web of Science, Scopus, and Google Scholar. Web of Science and Scopus were searched for the articles published in 2005-2006. As all articles returned by Scopus were included in Web of Science, the latter list was the sample for further analysis. Total citation counts for each article on the list were retrieved from Web of Science, Scopus, and Google Scholar. The overlap and unique citations were compared and analyzed. Proportions were compared using chi(2)-test. Google Scholar returned the greatest proportion of articles with citations (45%), followed by Scopus (42%), and Web of Science (38%). Almost a half (49%) of articles had no citations and 11% had an equal number of identical citations in all 3 databases. The greatest overlap was found between Web of Science and Scopus (54%), followed by Scopus and Google Scholar (51%), and Web of Science and Google Scholar (44%). The greatest number of unique citations was found by Google Scholar (n=86). The majority of these citations (64%) came from journals, followed by books and PhD theses. Approximately 55% of all citing documents were full-text resources in open access. The language of citing documents was mostly English, but as many as 25 citing documents (29%) were in Chinese. Google Scholar shares a total of 42% citations returned by two others, more influential, bibliographic resources. The list of unique citations in Google Scholar is predominantly journal based, but these journals are mainly of local character. Citations received by internationally recognized medical journals are crucial for increasing the visibility of small medical journals but Google Scholar may serve as an alternative bibliometric tool for an orientational citation insight.
20129646	Creation of a fully digital pathology slide archive by high-volume tissue slide scanning.
Hum. Pathol. 20100204 2010May
Digital slide scanners for scanning glass slides are becoming increasingly popular because current scanners are fast enough and produce good enough images for diagnostic purposes, education, and research. Also, the price for storing vast amounts of data has decreased over the last years, and this trend is expected to continue. Where most laboratories use their scanners mainly for education and research with limited financial and technical implications, we decided to face the huge challenges of prospectively setting up a fully digital pathology slide archive, primarily aiming to optimize the preparation and running of clinicopathological conferences. In this article, we describe the setup of our digital archiving solution and discuss the technical challenges we had to overcome. To give insight in the performance of our digital archive, we provide some statistics as well. We also present our thoughts on future developments in the area of digital slide scanning.
20401946	Querying metabolism under different physiological constraints.
J Bioinform Comput Biol  2010Apr
Metabolism is a representation of the biochemical principles that govern the production, consumption, degradation, and biosynthesis of metabolites in living cells. Organisms respond to changes in their physiological conditions or environmental perturbations (i.e. constraints) via cooperative implementation of such principles. Querying inner working principles of metabolism under different constraints provides invaluable insights for both researchers and educators. In this paper, we propose a metabolism query language (MQL) and discuss its query processing. MQL enables researchers to explore the behavior of the metabolism with a wide-range of predicates including dietary and physiological condition specifications. The query results of MQL are enriched with both textual and visual representations, and its query processing is completely tailored based on the underlying metabolic principles.
20402799	Internet-based information-seeking behaviour amongst doctors and nurses: a short review of the literature.
Health Info Libr J  2010Mar
Reviews of how doctors and nurses search for online information are relatively rare, particularly where research examines how they decide whether to use Internet-based resources. Original research into their online searching behaviour is also rare, particularly in real world clinical settings. as is original research into their online searching behaviour. This review collates some of the existing evidence, from 1995 to 2009. To establish whether there are any significant differences in the ways and reasons why doctors and nurses seek out online information; to establish how nurses and doctors locate information online; to establish whether any conclusions can be drawn from the existing evidence that might assist health and medical libraries in supporting users. An initial scoping literature search was carried out on PubMed and CINAHL to identify existing reviews of the subject area and relevant original research between 1995 and 2009. Following refinement, further searches were carried out on Embase (Ovid), LISA and LISTA. Following the initial scoping search, two journals were identified as particularly relevant for further table of contents searching. Articles were exclused where the main focus was on patients searching for information or where the focus was the evaluation of online-based educational software or tutorials. Articles were included if they were review or meta-analysis articles, where they reported original research, and where the primary focus of the online search was for participants' ongoing Continuing Professional Development (CPD). The relevant articles are outlined, with details of numbers of participants, response rates, and the user groups. There appear to be no significant differences between the reasons why doctors and nurses seek online Internet-based evidence, or the ways in which they locate that evidence. Reasons for searching for information online are broadly the same: primarily patient care and CPD (Continuing Professional Development). The perceived barriers to accessing online information are the same in both groups. There is a lack of awareness of the library as a potential online information enabler. Libraries need to examine their policy and practice to ensure that they facilitate access to online evidence-based information, particularly where users are geographically remote or based in the community rather than in a hospital setting. Librarians also need to take into account the fact that medical professionals on duty may not be able to take advantage of the academic model of online information research. Further research is recommended into the difference between the idealised academic model of searching and real world practicalities; and how other user groups search, for example patients.
20402801	Semi-automating the manual literature search for systematic reviews increases efficiency.
Health Info Libr J  2010Mar
To minimise retrieval bias, manual literature searches are a key part of the search process of any systematic review. Considering the need to have accurate information, valid results of the manual literature search are essential to ensure scientific standards; likewise efficient approaches that minimise the amount of personnel time required to conduct a manual literature search are of great interest. The objective of this project was to determine the validity and efficiency of a new manual search method that utilises the scopus database. We used the traditional manual search approach as the gold standard to determine the validity and efficiency of the proposed scopus method. Outcome measures included completeness of article detection and personnel time involved. Using both methods independently, we compared the results based on accuracy of the results, validity and time spent conducting the search, efficiency. Regarding accuracy, the scopus method identified the same studies as the traditional approach indicating its validity. In terms of efficiency, using scopus led to a time saving of 62.5% compared with the traditional approach (3 h versus 8 h). The scopus method can significantly improve the efficiency of manual searches and thus of systematic reviews.
20371752	Use of information technology in medication reconciliation: a scoping review.
Ann Pharmacother 20100406 2010May
To identify studies involving information technology (IT) in medication reconciliation (MedRec) and determine how IT is used to facilitate the MedRec process. The search strategy included a database search of MEDLINE and Cumulative Index of Nursing and Allied Health Literature (CINAHL), hand-searching of collected material, and references from articles retrieved. The database search was limited to English-language papers. MEDLINE includes publications dating back to 1950 and CINAHL includes those dating back to 1982. The search included articles in both databases up to March 2009. Boolean queries were constructed using combinations of search terms for medication reconciliation, IT, and electronic records. Three inclusion criteria were used. The study had to (1) involve the MedRec process, (2) be a primary study, and (3) involve the use of IT. Selection was performed by 2 reviewers through consensus. Data related to study characteristics, focus, and IT use were extracted. The included studies described a range of IT used throughout the MedRec process, from basic email and databases to specialized MedRec tools. A generic MedRec workflow was created and types of IT found in the studies were mapped to the workflow activities as well as to a set of functionalities based on the Institute of Medicine's Key Capabilities of an Electronic Health Record System. In the studies reviewed, IT was mainly used to obtain medication information. Although there were only a few MedRec tools in the studies, those that did exist supported the central activities for MedRec: comparison of medications and clarification of discrepancies. MedRec is an important process to ensure patient medication safety. Evidence was found that IT can and has been used to facilitate some MedRec activities and new applications are being developed to support the entire MedRec process.
20406124	A comparison of two data storage strategies for implementing e-health in China.
Telemed J E Health  2010Apr
Data sharing and information exchange among medical institutions is a requirement for convenient and effective data availability for both healthcare professionals and patients. In this paper, the characteristics of medical data are studied; two mainstream technologies of data storage for medical information are compared, and three strategies of medical documents storage are described with detailed advantages and disadvantages. Semi-structured storage technology is easier to deploy and much more promising to promote in a wider range than all-structured methods. The combination of central and distributed data storage is more practical for regional data sharing. This analysis suggests that semi-structural data storage technology and the combination of central and distributed data storage are efficient and fit well the current situation in China.
20377909	An overview of the CellML API and its implementation.
BMC Bioinformatics 20100408 2010
CellML is an XML based language for representing mathematical models, in a machine-independent form which is suitable for their exchange between different authors, and for archival in a model repository. Allowing for the exchange and archival of models in a computer readable form is a key strategic goal in bioinformatics, because of the associated improvements in scientific record accuracy, the faster iterative process of scientific development, and the ability to combine models into large integrative models.However, for CellML models to be useful, tools which can process them correctly are needed. Due to some of the more complex features present in CellML models, such as imports, developing code ab initio to correctly process models can be an onerous task. For this reason, there is a clear and pressing need for an application programming interface (API), and a good implementation of that API, upon which tools can base their support for CellML. We developed an API which allows the information in CellML models to be retrieved and/or modified. We also developed a series of optional extension APIs, for tasks such as simplifying the handling of connections between variables, dealing with physical units, validating models, and translating models into different procedural languages.We have also provided a Free/Open Source implementation of this application programming interface, optimised to achieve good performance. Tools have been developed using the API which are mature enough for widespread use. The API has the potential to accelerate the development of additional tools capable of processing CellML, and ultimately lead to an increased level of sharing of mathematical model descriptions.
20410754	The faculty and information specialist partnership: stimulating student interest and experiential learning.
Nurse Educ  2010 May-Jun
Faculty are challenged to find innovative ways to teach the skills for evidenced-based practice and information literacy. Librarians are natural partners with nurse educators because of their information literacy expertise. Literature shows that an experiential approach is an effective strategy for teaching undergraduate nursing research. The authors describe the development and implementation of simple research projects in an undergraduate nursing research course and the collaboration among course faculty, nursing students, and the information literacy specialist.
20058248	ms_lims, a simple yet powerful open source laboratory information management system for MS-driven proteomics.
Proteomics  2010Mar
MS-based proteomics produces large amounts of mass spectra that require processing, identification and possibly quantification before interpretation can be undertaken. High-throughput studies require automation of these various steps, and management of the data in association with the results obtained. We here present ms_lims (http://genesis.UGent.be/ms_lims), a freely available, open-source system based on a central database to automate data management and processing in MS-driven proteomics analyses.
20077413	Using Laboratory Information Management Systems as central part of a proteomics data workflow.
Proteomics  2010Mar
The organization and storage of proteomics data are challenging issues today and even more for the rising amount of information in the future. This review article describes the advantages of using Laboratory Information Management Systems (LIMS) in proteomics laboratories. Seven typical LIMS are explored in detail to describe their role in an even bigger interrelation. They are a central part of the proteomics data workflow, starting with data generation and ending with the publication in journals and repositories. Therefore, they enable community-wide data utilization and further Systems Biology discoveries.
20101611	A guided tour of the Trans-Proteomic Pipeline.
Proteomics  2010Mar
The Trans-Proteomic Pipeline (TPP) is a suite of software tools for the analysis of MS/MS data sets. The tools encompass most of the steps in a proteomic data analysis workflow in a single, integrated software system. Specifically, the TPP supports all steps from spectrometer output file conversion to protein-level statistical validation, including quantification by stable isotope ratios. We describe here the full workflow of the TPP and the tools therein, along with an example on a sample data set, demonstrating that the setup and use of the tools are straightforward and well supported and do not require specialized informatic resources or knowledge.
20116280	Automated data reduction for hydrogen/deuterium exchange experiments, enabled by high-resolution Fourier transform ion cyclotron resonance mass spectrometry.
J. Am. Soc. Mass Spectrom. 20100104 2010Apr
Mass analysis of proteolytic fragment peptides following hydrogen/deuterium exchange offers a general measure of solvent accessibility/hydrogen bonding (and thus conformation) of solution-phase proteins and their complexes. The primary problem in such mass analyses is reliable and rapid assignment of mass spectral peaks to the correct charge state and degree of deuteration of each fragment peptide, in the presence of substantial overlap between isotopic distributions of target peptides, autolysis products, and other interferant species. Here, we show that at sufficiently high mass resolving power (m/Delta m(50%) &gt; or = 100,000), it becomes possible to resolve enough of those overlaps so that automated data reduction becomes possible, based on the actual elemental composition of each peptide without the need to deconvolve isotopic distributions. We demonstrate automated, rapid, reliable assignment of peptide masses from H/D exchange experiments, based on electrospray ionization FT-ICR mass spectra from H/D exchange of solution-phase myoglobin. Combined with previously demonstrated automated data acquisition for such experiments, the present data reduction algorithm enhances automation (and thus expands generality and applicability) for high-resolution mass spectrometry-based analysis of H/D exchange of solution-phase proteins.
20351464	Online resources on 2009 pandemic influenza for clinicians.
J Infect Dev Ctries 20100329 2010Mar
Numerous resources relevant to the 2009 pandemic influenza are available on-line . A casual search in google using the phrase "swine flu resources" yields 9,180,000 items making a time-bound search less fruitful for an average health care professional. This review has summarized the contents useful to practicing clinician available in 12 websites which were selected based on their relevance to clinical care.
20175896	Unifying generative and discriminative learning principles.
BMC Bioinformatics 20100222 2010
The recognition of functional binding sites in genomic DNA remains one of the fundamental challenges of genome research. During the last decades, a plethora of different and well-adapted models has been developed, but only little attention has been payed to the development of different and similarly well-adapted learning principles. Only recently it was noticed that discriminative learning principles can be superior over generative ones in diverse bioinformatics applications, too. Here, we propose a generalization of generative and discriminative learning principles containing the maximum likelihood, maximum a posteriori, maximum conditional likelihood, maximum supervised posterior, generative-discriminative trade-off, and penalized generative-discriminative trade-off learning principles as special cases, and we illustrate its efficacy for the recognition of vertebrate transcription factor binding sites. We find that the proposed learning principle helps to improve the recognition of transcription factor binding sites, enabling better computational approaches for extracting as much information as possible from valuable wet-lab data. We make all implementations available in the open-source library Jstacs so that this learning principle can be easily applied to other classification problems in the field of genome and epigenome analysis.
20150320	Accurate detection and genotyping of SNPs utilizing population sequencing data.
Genome Res. 20100211 2010Apr
Next-generation sequencing technologies have made it possible to sequence targeted regions of the human genome in hundreds of individuals. Deep sequencing represents a powerful approach for the discovery of the complete spectrum of DNA sequence variants in functionally important genomic intervals. Current methods for single nucleotide polymorphism (SNP) detection are designed to detect SNPs from single individual sequence data sets. Here, we describe a novel method SNIP-Seq (single nucleotide polymorphism identification from population sequence data) that leverages sequence data from a population of individuals to detect SNPs and assign genotypes to individuals. To evaluate our method, we utilized sequence data from a 200-kilobase (kb) region on chromosome 9p21 of the human genome. This region was sequenced in 48 individuals (five sequenced in duplicate) using the Illumina GA platform. Using this data set, we demonstrate that our method is highly accurate for detecting variants and can filter out false SNPs that are attributable to sequencing errors. The concordance of sequencing-based genotype assignments between duplicate samples was 98.8%. The 200-kb region was independently sequenced to a high depth of coverage using two sequence pools containing the 48 individuals. Many of the novel SNPs identified by SNIP-Seq from the individual sequencing were validated by the pooled sequencing data and were subsequently confirmed by Sanger sequencing. We estimate that SNIP-Seq achieves a low false-positive rate of approximately 2%, improving upon the higher false-positive rate for existing methods that do not utilize population sequence data. Collectively, these results suggest that analysis of population sequencing data is a powerful approach for the accurate detection of SNPs and the assignment of genotypes to individual samples.
20361040	Using pre-existing microarray datasets to increase experimental power: application to insulin resistance.
PLoS Comput. Biol. 20100326 2010Mar
Although they have become a widely used experimental technique for identifying differentially expressed (DE) genes, DNA microarrays are notorious for generating noisy data. A common strategy for mitigating the effects of noise is to perform many experimental replicates. This approach is often costly and sometimes impossible given limited resources; thus, analytical methods are needed which increase accuracy at no additional cost. One inexpensive source of microarray replicates comes from prior work: to date, data from hundreds of thousands of microarray experiments are in the public domain. Although these data assay a wide range of conditions, they cannot be used directly to inform any particular experiment and are thus ignored by most DE gene methods. We present the SVD Augmented Gene expression Analysis Tool (SAGAT), a mathematically principled, data-driven approach for identifying DE genes. SAGAT increases the power of a microarray experiment by using observed coexpression relationships from publicly available microarray datasets to reduce uncertainty in individual genes' expression measurements. We tested the method on three well-replicated human microarray datasets and demonstrate that use of SAGAT increased effective sample sizes by as many as 2.72 arrays. We applied SAGAT to unpublished data from a microarray study investigating transcriptional responses to insulin resistance, resulting in a 50% increase in the number of significant genes detected. We evaluated 11 (58%) of these genes experimentally using qPCR, confirming the directions of expression change for all 11 and statistical significance for three. Use of SAGAT revealed coherent biological changes in three pathways: inflammation, differentiation, and fatty acid synthesis, furthering our molecular understanding of a type 2 diabetes risk factor. We envision SAGAT as a means to maximize the potential for biological discovery from subtle transcriptional responses, and we provide it as a freely available software package that is immediately applicable to any human microarray study.
20359409	Health information system linkage and coordination are critical for increasing access to secondary prevention in Aboriginal health: a qualitative study.
Qual Prim Care  2010
Aboriginal Australians have low rates of participation in cardiac rehabilitation (CR), despite having high rates of cardiovascular disease. Barriers to CR participation reflect multiple patient-related issues. However, an examination of the broader context of health service delivery design and implementation is needed. To identify health professionals' perspectives of systems related barriers to implementation of the National Health and Medical Research Council (NHMRC) guidelines Strengthening Cardiac Rehabilitation and Secondary Prevention for Aboriginal and Torres Strait Islander Peoples. Semi-structured interviews were conducted with health professionals involved in CR within mainstream and Aboriginal Community Controlled Health Services in Western Australia (WA). Thirty-eight health professionals from 17 services (ten rural, seven metropolitan) listed in the WA Directory of CR services and seven Aboriginal Medical Services in WA were interviewed. Respondents reported barriers encountered in health information management and the impact of access to CR services for Aboriginal people. Crucial issues identified by participants were: poor communication across the health care sector and between providers, inconsistent and insufficient data collection processes (particularly relating to Aboriginal ethnicity identification), and challenges resulting from multiple clinical information systems and incompatible technologies. This study has demonstrated that inadequate information systems and communication strategies, particularly those representing the interface between primary and secondary care, contribute to the low participation rates of Aboriginal Australians in CR. Although these challenges are shared by non-Aboriginal Australians, the needs are greater for Aboriginal Australians and innovative solutions are required.
20170516	SING: subgraph search in non-homogeneous graphs.
BMC Bioinformatics 20100219 2010
Finding the subgraphs of a graph database that are isomorphic to a given query graph has practical applications in several fields, from cheminformatics to image understanding. Since subgraph isomorphism is a computationally hard problem, indexing techniques have been intensively exploited to speed up the process. Such systems filter out those graphs which cannot contain the query, and apply a subgraph isomorphism algorithm to each residual candidate graph. The applicability of such systems is limited to databases of small graphs, because their filtering power degrades on large graphs. In this paper, SING (Subgraph search In Non-homogeneous Graphs), a novel indexing system able to cope with large graphs, is presented. The method uses the notion of feature, which can be a small subgraph, subtree or path. Each graph in the database is annotated with the set of all its features. The key point is to make use of feature locality information. This idea is used to both improve the filtering performance and speed up the subgraph isomorphism task. Extensive tests on chemical compounds, biological networks and synthetic graphs show that the proposed system outperforms the most popular systems in query time over databases of medium and large graphs. Other specific tests show that the proposed system is effective for single large graphs.
20331850	CoBaltDB: Complete bacterial and archaeal orfeomes subcellular localization database and associated resources.
BMC Microbiol. 20100323 2010
The functions of proteins are strongly related to their localization in cell compartments (for example the cytoplasm or membranes) but the experimental determination of the sub-cellular localization of proteomes is laborious and expensive. A fast and low-cost alternative approach is in silico prediction, based on features of the protein primary sequences. However, biologists are confronted with a very large number of computational tools that use different methods that address various localization features with diverse specificities and sensitivities. As a result, exploiting these computer resources to predict protein localization accurately involves querying all tools and comparing every prediction output; this is a painstaking task. Therefore, we developed a comprehensive database, called CoBaltDB, that gathers all prediction outputs concerning complete prokaryotic proteomes. The current version of CoBaltDB integrates the results of 43 localization predictors for 784 complete bacterial and archaeal proteomes (2.548.292 proteins in total). CoBaltDB supplies a simple user-friendly interface for retrieving and exploring relevant information about predicted features (such as signal peptide cleavage sites and transmembrane segments). Data are organized into three work-sets ("specialized tools", "meta-tools" and "additional tools"). The database can be queried using the organism name, a locus tag or a list of locus tags and may be browsed using numerous graphical and text displays. With its new functionalities, CoBaltDB is a novel powerful platform that provides easy access to the results of multiple localization tools and support for predicting prokaryotic protein localizations with higher confidence than previously possible. CoBaltDB is available at http://www.umr6026.univ-rennes1.fr/english/home/research/basic/software/cobalten.
20346105	Moara: a Java library for extracting and normalizing gene and protein mentions.
BMC Bioinformatics 20100326 2010
Gene/protein recognition and normalization are important preliminary steps for many biological text mining tasks, such as information retrieval, protein-protein interactions, and extraction of semantic information, among others. Despite dedication to these problems and effective solutions being reported, easily integrated tools to perform these tasks are not readily available. This study proposes a versatile and trainable Java library that implements gene/protein tagger and normalization steps based on machine learning approaches. The system has been trained for several model organisms and corpora but can be expanded to support new organisms and documents. Moara is a flexible, trainable and open-source system that is not specifically orientated to any organism and therefore does not requires specific tuning in the algorithms or dictionaries utilized. Moara can be used as a stand-alone application or can be incorporated in the workflow of a more general text mining system.
20377446	Storage and retrieval of highly repetitive sequence collections.
J. Comput. Biol.  2010Mar
A repetitive sequence collection is a set of sequences which are small variations of each other. A prominent example are genome sequences of individuals of the same or close species, where the differences can be expressed by short lists of basic edit operations. Flexible and efficient data analysis on such a typically huge collection is plausible using suffix trees. However, the suffix tree occupies much space, which very soon inhibits in-memory analyses. Recent advances in full-text indexing reduce the space of the suffix tree to, essentially, that of the compressed sequences, while retaining its functionality with only a polylogarithmic slowdown. However, the underlying compression model considers only the predictability of the next sequence symbol given the k previous ones, where k is a small integer. This is unable to capture longer-term repetitiveness. For example, r identical copies of an incompressible sequence will be incompressible under this model. We develop new static and dynamic full-text indexes that are able of capturing the fact that a collection is highly repetitive, and require space basically proportional to the length of one typical sequence plus the total number of edit operations. The new indexes can be plugged into a recent dynamic fully-compressed suffix tree, achieving full functionality for sequence analysis, while retaining the reduced space and the polylogarithmic slowdown. Our experimental results confirm the practicality of our proposal.
20231818	V3D enables real-time 3D visualization and quantitative analysis of large-scale biological image data sets.
Nat. Biotechnol. 20100314 2010Apr
The V3D system provides three-dimensional (3D) visualization of gigabyte-sized microscopy image stacks in real time on current laptops and desktops. V3D streamlines the online analysis, measurement and proofreading of complicated image patterns by combining ergonomic functions for selecting a location in an image directly in 3D space and for displaying biological measurements, such as from fluorescent probes, using the overlaid surface objects. V3D runs on all major computer platforms and can be enhanced by software plug-ins to address specific biological problems. To demonstrate this extensibility, we built a V3D-based application, V3D-Neuron, to reconstruct complex 3D neuronal structures from high-resolution brain images. V3D-Neuron can precisely digitize the morphology of a single neuron in a fruitfly brain in minutes, with about a 17-fold improvement in reliability and tenfold savings in time compared with other neuron reconstruction tools. Using V3D-Neuron, we demonstrate the feasibility of building a 3D digital atlas of neurite tracts in the fruitfly brain.
20377764	Asking the right question and finding the right answers.
Nephrology (Carlton)  2010Feb
Clinical consultations generate questions that can be informed by published (and unpublished) evidence. This is the basis for evidence-based practice. Finding answers involves searching available electronic databases. We describe a method for rephrasing or 'framing' clinical questions into population, intervention, comparator and outcome terms that helps to determine the best type of study to search for, and aids in the design of search strategies.
20377765	How to get the most from the medical literature: searching the medical literature effectively.
Nephrology (Carlton)  2010Feb
Different clinical questions are best answered using different study designs. This paper describes the best methods for finding relevant studies for well-framed clinical questions. We focus on which database is best to search to answer your question, describe the structure of effective search strategies and explore ways to develop appropriate search terms. We illustrate these with sensitive and specific search strategies to answer different clinical questions arising from a hypothetical clinical scenario typical of a nephrologist's everyday practice.
20223684	Antiangiogenic tumor treatment: noninvasive monitoring with contrast pulse sequence imaging for contrast-enhanced grayscale ultrasound.
Acad Radiol 20100311 2010May
This study was designed to test the feasibility of contrast pulse sequencing imaging for contrast-enhanced grayscale ultrasound in assessing the effects of antiangiogenic therapy. Mice with subcutaneously implanted H22 mouse hepatoma were treated with thalidomide or placebo by oral gavage over 7 days, starting at 24 hours after implantation. Contrast pulse sequencing ultrasound imaging was performed on day 8 to evaluate maximal cross-sectional area and nonenhanced area. Immediately after imaging, mice were euthanized, and tumor tissue was removed for fixation in a 10% formalin solution. The section equivalent to the ultrasound imaging plane was stained with hematoxylin and eosin to allow for the assessment of the largest cross-sectional area and necrotic area. There was no significant difference in tumor volume between the two groups. The difference of largest cross-sectional area determined by the two methods was not significant between control and treated tumors (P &gt; .05). The nonenhanced area and its percentage evaluated by ultrasound were significantly larger in treated tumors than in control tumors (P &lt; .05). The necrotic area and its percentage estimated by pathology slice were also significantly larger in treated tumors than in control tumors (P &lt; .05). The largest cross-sectional area determined by the two methods was well correlated (r = 0.815, P &lt; .001). There was good correlation between the nonenhanced area on ultrasound and the necrotic area on pathology slides (r = 0.909, P &lt; .001). The percentage of nonenhanced area was well correlated with the percentage of necrotic area (r = 0.910, P &lt; .001). Contrast-enhanced grayscale ultrasound with contrast pulse sequencing imaging provides a tool for early monitoring of antiangiogenic treatment of tumors, before apparent change in tumor size.
20221258	Designing focused chemical libraries enriched in protein-protein interaction inhibitors using machine-learning methods.
PLoS Comput. Biol. 20100305 2010Mar
Protein-protein interactions (PPIs) may represent one of the next major classes of therapeutic targets. So far, only a minute fraction of the estimated 650,000 PPIs that comprise the human interactome are known with a tiny number of complexes being drugged. Such intricate biological systems cannot be cost-efficiently tackled using conventional high-throughput screening methods. Rather, time has come for designing new strategies that will maximize the chance for hit identification through a rationalization of the PPI inhibitor chemical space and the design of PPI-focused compound libraries (global or target-specific). Here, we train machine-learning-based models, mainly decision trees, using a dataset of known PPI inhibitors and of regular drugs in order to determine a global physico-chemical profile for putative PPI inhibitors. This statistical analysis unravels two important molecular descriptors for PPI inhibitors characterizing specific molecular shapes and the presence of a privileged number of aromatic bonds. The best model has been transposed into a computer program, PPI-HitProfiler, that can output from any drug-like compound collection a focused chemical library enriched in putative PPI inhibitors. Our PPI inhibitor profiler is challenged on the experimental screening results of 11 different PPIs among which the p53/MDM2 interaction screened within our own CDithem platform, that in addition to the validation of our concept led to the identification of 4 novel p53/MDM2 inhibitors. Collectively, our tool shows a robust behavior on the 11 experimental datasets by correctly profiling 70% of the experimentally identified hits while removing 52% of the inactive compounds from the initial compound collections. We strongly believe that this new tool can be used as a global PPI inhibitor profiler prior to screening assays to reduce the size of the compound collections to be experimentally screened while keeping most of the true PPI inhibitors. PPI-HitProfiler is freely available on request from our CDithem platform website, www.CDithem.com.
20181245	Word add-in for ontology recognition: semantic enrichment of scientific literature.
BMC Bioinformatics 20100224 2010
In the current era of scientific research, efficient communication of information is paramount. As such, the nature of scholarly and scientific communication is changing; cyberinfrastructure is now absolutely necessary and new media are allowing information and knowledge to be more interactive and immediate. One approach to making knowledge more accessible is the addition of machine-readable semantic data to scholarly articles. The Word add-in presented here will assist authors in this effort by automatically recognizing and highlighting words or phrases that are likely information-rich, allowing authors to associate semantic data with those words or phrases, and to embed that data in the document as XML. The add-in and source code are publicly available at http://www.codeplex.com/UCSDBioLit. The Word add-in for ontology term recognition makes it possible for an author to add semantic data to a document as it is being written and it encodes these data using XML tags that are effectively a standard in life sciences literature. Allowing authors to mark-up their own work will help increase the amount and quality of machine-readable literature metadata.
20224132	Real-time detection and tracking for augmented reality on mobile phones.
IEEE Trans Vis Comput Graph  2010 May-Jun
In this paper, we present three techniques for 6DOF natural feature tracking in real time on mobile phones. We achieve interactive frame rates of up to 30 Hz for natural feature tracking from textured planar targets on current generation phones. We use an approach based on heavily modified state-of-the-art feature descriptors, namely SIFT and Ferns plus a template-matching-based tracker. While SIFT is known to be a strong, but computationally expensive feature descriptor, Ferns classification is fast, but requires large amounts of memory. This renders both original designs unsuitable for mobile phones. We give detailed descriptions on how we modified both approaches to make them suitable for mobile phones. The template-based tracker further increases the performance and robustness of the SIFT- and Ferns-based approaches. We present evaluations on robustness and performance and discuss their appropriateness for Augmented Reality applications.
20224139	Hierarchical aggregation for information visualization: overview, techniques, and design guidelines.
IEEE Trans Vis Comput Graph  2010 May-Jun
We present a model for building, visualizing, and interacting with multiscale representations of information visualization techniques using hierarchical aggregation. The motivation for this work is to make visual representations more visually scalable and less cluttered. The model allows for augmenting existing techniques with multiscale functionality, as well as for designing new visualization and interaction techniques that conform to this new class of visual representations. We give some examples of how to use the model for standard information visualization techniques such as scatterplots, parallel coordinates, and node-link diagrams, and discuss existing techniques that are based on hierarchical aggregation. This yields a set of design guidelines for aggregated visualizations. We also present a basic vocabulary of interaction techniques suitable for navigating these multiscale visualizations.
20224141	Mélange: space folding for visual exploration.
IEEE Trans Vis Comput Graph  2010 May-Jun
Navigating in large geometric spaces-such as maps, social networks, or long documents-typically requires a sequence of pan and zoom actions. However, this strategy is often ineffective and cumbersome, especially when trying to study and compare several distant objects. We propose a new distortion technique that folds the intervening space to guarantee visibility of multiple focus regions. The folds themselves show contextual information and support unfolding and paging interactions. We conducted a study comparing the space-folding technique to existing approaches and found that participants performed significantly better with the new technique. We also describe how to implement this distortion technique and give an in-depth case study on how to apply it to the visualization of large-scale 1D time-series data.
20136577	How long does it take for the scientific literature to purge itself of fraudulent material?: the Breuning case revisited.
Curr Med Res Opin  2010Apr
It has been proposed that the scientific literature purges itself of articles known to be fraudulent. To test this, an investigation was carried out of post-retraction citations over a 19-year period in the Breuning case. On 10 March 2008 a cited reference search was conducted (all languages, all document types) using the name 'Breuning SE*'. The time limit was 1989-2007 with an option to exclude self-citations. The search included the ISI Web of Science Database including the Science Citation Index Expanded, the Social Sciences Citations Index and the Arts &amp; Humanities Citation Index. To ascertain the citation context, citations of Breuning were classified by two raters as affirmative, negative or neutral. For the period 1989-2000 both negative and affirmative citations were found. For the period 2001-2006 only affirmative citations (even to retracted articles) were found, some in journals with higher impact factors than those citing the case as fraudulent. In spite of the small number of citations of Breuning's articles, it is alarming that the affirmative citing of fraudulent research has not completely ceased but continues 24 years post-retraction (retracted 1982, cited 2006). While the limitations of a single case study are conceded, the results challenge the belief of scientific literature purging itself of fraudulent material. Retraction databases and widespread availability of computer software to check lists of references free of charge in any database or the internet are called for. Moreover, if a paper is never formally retracted, software for searching author names in the internet for fully investigated and proven scientific misconduct might be developed. The ethical guidelines on duplicate publication for purposes of disseminating the information as widely as possible should be reviewed.
20187952	Biomedical informatics and translational medicine.
J Transl Med 20100226 2010
Biomedical informatics involves a core set of methodologies that can provide a foundation for crossing the "translational barriers" associated with translational medicine. To this end, the fundamental aspects of biomedical informatics (e.g., bioinformatics, imaging informatics, clinical informatics, and public health informatics) may be essential in helping improve the ability to bring basic research findings to the bedside, evaluate the efficacy of interventions across communities, and enable the assessment of the eventual impact of translational medicine innovations on health policies. Here, a brief description is provided for a selection of key biomedical informatics topics (Decision Support, Natural Language Processing, Standards, Information Retrieval, and Electronic Health Records) and their relevance to translational medicine. Based on contributions and advancements in each of these topic areas, the article proposes that biomedical informatics practitioners ("biomedical informaticians") can be essential members of translational medicine teams.
20226145	[Analysis of relationships among syndrome, therapeutic treatment, and Chinese herbal medicine in patients with coronary artery disease based on complex networks].
Zhong Xi Yi Jie He Xue Bao  2010Mar
To analyze the relationships among syndrome, therapeutic method and Chinese herbal medicine in patients with coronary artery disease (CAD). Using cross-sectional survey, we collected the clinical information of hospitalized CAD patients through individualized Information Acquisition Platform of CAD. The relationships among syndrome, therapeutic treatment and Chinese herbs were excavated by means of complex networks based on theory of correspondence between prescription and syndrome. The fundamental syndrome factors were blood stasis, qi deficiency, phlegm-turbid, yin deficiency, yang deficiency, qi stagnation, and blood deficiency. The therapeutic treatment mainly included activating blood circulation, clearing heat, invigorating qi, resolving turbid and phlegm, nourishing yin, warming yang qi, and dispersing obstruction. These methods constituted an association with major syndrome factors. The major syndrome factors constituted an association with the following Chinese herbal medicines: Huangqi (Radix Astragali Mongolici), Chenpi (Pericarpium Citri Reticulatae), Dihuang (Radix Rehmanniae), Chuanxiong (Rhizoma Chuanxiong), Baizhu (Rhizoma Atractylodis Macrocephalae), Taoren (Semen Persicae), Fuling (Poria), Gancao (Radix Glycyrrhizae), Banxia (Rhizoma Pinelliae), Zexie (Rhizoma Alismatis), Chishao (Radix Paeoniae Rubra), Danggui (Radix Angelicae Sinensis), Danshen (Radix Salviae Miltiorrhizae), Zhiqiao (Fructus Aurantii Submaturus.), Guizhi (Ramulus Cinnamomi) and Maidong (Radix Ophiopogonis Japonici). The efficacy of Chinese berbal medicines constituting association with syndrome factors mainly included alleviating pain, resolving turbid and phlegm, clearing heat, activating blood circulation, invigorating qi, cooling blood, promoting urination, resolving stagnation, removing toxic material, nourishing blood, regulating qi, quieting spirit, invigorating spleen, regulating menstruation, promoting defecation, moistening dryness, and resolving stasis. The therapeutic methods for CAD are based on consistency in theory, method, formula and medicines. Therapeutic methods for clearing heat and removing toxical material should be further studied.
20229880	A residual correction method for high-resolution PET reconstruction with application to on-the-fly Monte Carlo based model of positron range.
Med Phys  2010Feb
The quality of tomographic images is directly affected by the system model being used in image reconstruction. An accurate system matrix is desirable for high-resolution image reconstruction, but it often leads to high computation cost. In this work the authors present a maximum a posteriori reconstruction algorithm with residual correction to alleviate the tradeoff between the model accuracy and the computation efficiency in image reconstruction. Unlike conventional iterative methods that assume that the system matrix is accurate, the proposed method reconstructs an image with a simplified system matrix and then removes the reconstruction artifacts through residual correction. Since the time-consuming forward and back projection operations using the accurate system matrix are not required in every iteration, image reconstruction time can be greatly reduced. The authors apply the new algorithm to high-resolution positron emission tomography reconstruction with an on-the-fly Monte Carlo (MC) based positron range model. Computer simulations show that the new method is an order of magnitude faster than the traditional MC-based method, whereas the visual quality and quantitative accuracy of the reconstructed images are much better than that obtained by using the simplified system matrix alone. The residual correction method can reconstruct high-resolution images and is computationally efficient.
20229898	Nonlinear registration of serial coronary CT angiography (CCTA) for assessment of changes in atherosclerotic plaque.
Med Phys  2010Feb
Coronary CT angiography (CCTA) is a high-resolution three-dimensional imaging technique for the evaluation of coronary arteries in suspected or confirmed coronary artery disease (CAD). Coregistration of serial CCTA scans would allow precise superimposition of images obtained at two different points in time, which could aid in recognition of subtle changes and precise monitoring of coronary plaque progression or regression. To this end, the authors aimed at developing a fully automatic nonlinear volume coregistration for longitudinal CCTA scan pairs. The algorithm combines global displacement and local deformation using nonlinear volume coregistration with a volume-preserving constraint. Histogram matching of intensities between two serial scans is performed prior to nonlinear coregistration with dense nonparametric local deformation in which sum of squared differences is used as a similarity measure. The approximate segmentation of coronary arteries obtained from commercially available software provides initial anatomical landmarks for the coregistration algorithm that help localize and emphasize the structure of interest. To avoid possible bias caused by incorrect segmentation, the authors convolve the Gaussian kernel with the segmented binary coronary tree mask and define an extended weighted region of interest. A multiresolution approach is employed to represent coarse-to-fine details of both volumes and the energy function is optimized using a gradient descent method. The authors applied the algorithm in ten paired CCTA datasets (20 scans in total) obtained within 10.7 +/- 5.7 months from each other on a dual source CT scanner to monitor progression of CAD. Serial CCTA coregistration was successful in 9/10 cases as visually confirmed. The global displacement and local deformation of target registration error obtained from four anatomical landmarks were 2.22 +/- 1.15 and 1.56 +/- 0.74 mm, respectively, and the inverse consistency error of local deformation was 0.14 +/- 0.06 mm. The observer variability between two expert observers was 1.31 +/- 0.91 mm. The proposed coregistration algorithm demonstrates potential to accurately register serial CCTA scans, which may allow direct comparison of calcified and noncalcified atherosclerotic plaque changes between the two scans.
20199681	Web GIS in practice VIII: HTML5 and the canvas element for interactive online mapping.
Int J Health Geogr 20100303 2010
HTML5 is being developed as the next major revision of HTML (Hypertext Markup Language), the core markup language of the World Wide Web. It aims at reducing the need for proprietary, plug-in-based rich Internet application (RIA) technologies such as Adobe Flash. The canvas element is part of HTML5 and is used to draw graphics using scripting (e.g., JavaScript). This paper introduces Cartagen, an open-source, vector-based, client-side framework for rendering plug-in-free, offline-capable, interactive maps in native HTML5 on a wide range of Web browsers and mobile phones. Cartagen was developed at MIT Media Lab's Design Ecology group. Potential applications of the technology as an enabler for participatory online mapping include mapping real-time air pollution, citizen reporting, and disaster response, among many other possibilities.
20229964	Development and evaluation of a nursing portal.
J Contin Educ Nurs  2010Mar
Access to information has tripled in the last 5 years, and the average person's ability to use new knowledge is limited. Methods that support the organization and use of information are needed to facilitate access to this knowledge. To address this challenge within the authors' institution, a nursing portal was designed, implemented, and evaluated. The Technology Acceptance Model was used to guide this process. Staff nurses received education on the functions and use of the nursing portal. The nurses also completed an online survey about the ease of use and usefulness of the nursing portal. Nurses reported that the nursing portal facilitated timely retrieval of information and decision-making. Staff education has been a key component of the success of the nursing portal.
20135081	Semantic interoperability adheres to proper models and code systems. A detailed examination of different approaches for score systems.
Methods Inf Med 20100205 2010
Achieving semantic interoperability requires not only the use of communication standards like HL7 with its underlying models and specifications, but also to constrain those models to instances including permitted attributes, data types, values and code systems. Even the application of both strategies may lead to different modeling approaches and therefore incompatible results, however. This paper analyzes the different ways to create a model exemplified at score and assessment systems. The different approaches have advantages and disadvantages. The presented results allow for transmitting the same basic information facilitating HL7 v2.x and V3 in a way reducing implementation efforts. Establishing a generic approach to communicate the details of score systems driven by an appropriate set of codes is the best solution for implementers.
20135082	Ensuring the continuity of care of cardiorespiratory diseases at home. Monitoring equipment and medical data exchange over semantically annotated web services.
Methods Inf Med 20100205 2010
A significant portion of care related to cardiorespiratory diseases is provided at home, usually but not exclusively, after the discharge of a patient from hospital. It is the purpose of the present study to present the technical means which we have developed, in order to support the adaptation of the continuity of care of cardiorespiratory diseases at home. We have developed an integrated system that includes: first, a prototype laptop-based portable monitoring system that comprises low-cost commercially available components, which enable the periodical or continuous monitoring of vital signs at home; second, software supporting medical decision-making related to tachycardia and ventricular fibrillation, as well as fuzzy-rules-based software supporting home-ventilation optimization; third, a typical continuity of care record (CCR) adapted to support also the creation of a homecare plan; and finally, a prototype ontology, based upon the HL7 clinical document architecture (CDA), serving as basis for the development of semantically annotated web services that allow for the exchange and retrieval of homecare information. The flexible design and the adaptable data-exchange mechanism of the developed system result in a useful and standard-compliant tool, for cardiorespiratory disease-related homecare. The ongoing laboratory testing of the system shows that it is able to contribute to an effective and low-cost package solution, supporting patient supervision and treatment. Furthermore, semantic web technologies prove to be the perfect solution for both the conceptualization of a continuity of care data exchange procedure and for the integration of the structured medical data.
20177648	Subword-based semantic retrieval of clinical and bibliographic documents.
Methods Inf Med 20100222 2010
The increasing amount of electronically available documents in bibliographic databases and the clinical documentation requires user-friendly techniques for content retrieval. A domain-specific approach on semantic text indexing for document retrieval is presented. It is based on a subword thesaurus and maps the content of texts in different European languages to a common interlingual representation, which supports the search across multilingual document collections. Three use cases are presented where the semantic retrieval method has been implemented: a bibliographic database, a department EHR system, and a consumer-oriented Web portal. It could be shown that a semantic indexing and retrieval approach, the performance of which had already been empirically assessed in prior studies, proved useful in different prototypical and routine scenarios and was well accepted by several user groups.
20178611	Extracting causal relations on HIV drug resistance from literature.
BMC Bioinformatics 20100223 2010
In HIV treatment it is critical to have up-to-date resistance data of applicable drugs since HIV has a very high rate of mutation. These data are made available through scientific publications and must be extracted manually by experts in order to be used by virologists and medical doctors. Therefore there is an urgent need for a tool that partially automates this process and is able to retrieve relations between drugs and virus mutations from literature. In this work we present a novel method to extract and combine relationships between HIV drugs and mutations in viral genomes. Our extraction method is based on natural language processing (NLP) which produces grammatical relations and applies a set of rules to these relations. We applied our method to a relevant set of PubMed abstracts and obtained 2,434 extracted relations with an estimated performance of 84% for F-score. We then combined the extracted relations using logistic regression to generate resistance values for each &lt;drug, mutation&gt; pair. The results of this relation combination show more than 85% agreement with the Stanford HIVDB for the ten most frequently occurring mutations. The system is used in 5 hospitals from the Virolab project http://www.virolab.org to preselect the most relevant novel resistance data from literature and present those to virologists and medical doctors for further evaluation. The proposed relation extraction and combination method has a good performance on extracting HIV drug resistance data. It can be used in large-scale relation extraction experiments. The developed methods can also be applied to extract other type of relations such as gene-protein, gene-disease, and disease-mutation.
20127316	Detection and identification of 700 drugs by multi-target screening with a 3200 Q TRAP LC-MS/MS system and library searching.
Anal Bioanal Chem 20100203 2010Apr
The multi-target screening method described in this work allows the simultaneous detection and identification of 700 drugs and metabolites in biological fluids using a hybrid triple-quadrupole linear ion trap mass spectrometer in a single analytical run. After standardization of the method, the retention times of 700 compounds were determined and transitions for each compound were selected by a "scheduled" survey MRM scan, followed by an information-dependent acquisition using the sensitive enhanced product ion scan of a Q TRAP hybrid instrument. The identification of the compounds in the samples analyzed was accomplished by searching the tandem mass spectrometry (MS/MS) spectra against the library we developed, which contains electrospray ionization-MS/MS spectra of over 1,250 compounds. The multi-target screening method together with the library was included in a software program for routine screening and quantitation to achieve automated acquisition and library searching. With the help of this software application, the time for evaluation and interpretation of the results could be drastically reduced. This new multi-target screening method has been successfully applied for the analysis of postmortem and traffic offense samples as well as proficiency testing, and complements screening with immunoassays, gas chromatography-mass spectrometry, and liquid chromatography-diode-array detection. Other possible applications are analysis in clinical toxicology (for intoxication cases), in psychiatry (antidepressants and other psychoactive drugs), and in forensic toxicology (drugs and driving, workplace drug testing, oral fluid analysis, drug-facilitated sexual assault).
20299709	Kernel entropy component analysis.
IEEE Trans Pattern Anal Mach Intell  2010May
We introduce kernel entropy component analysis (kernel ECA) as a new method for data transformation and dimensionality reduction. Kernel ECA reveals structure relating to the Renyi entropy of the input space data set, estimated via a kernel matrix using Parzen windowing. This is achieved by projections onto a subset of entropy preserving kernel principal component analysis (kernel PCA) axes. This subset does not need, in general, to correspond to the top eigenvalues of the kernel matrix, in contrast to the dimensionality reduction using kernel PCA. We show that kernel ECA may produce strikingly different transformed data sets compared to kernel PCA, with a distinct angle-based structure. A new spectral clustering algorithm utilizing this structure is developed with positive results. Furthermore, kernel ECA is shown to be an useful alternative for pattern denoising.
20138751	Data integration and analysis of biological networks.
Curr. Opin. Biotechnol. 20100206 2010Feb
During the past decade, bottom-up and top-down approaches of network reconstruction have greatly facilitated integration and analysis of biological networks, including transcriptional, protein interaction, and metabolic networks. As increasing amounts of multidimensional high-throughput data become available, biological networks have also been upgraded, allowing more accurate understanding of whole cellular characteristics. The network size is constantly expanding as larger volume of information and omics data are further integrated into the biological networks previously built upon a single type of data. Such effort more recently led to the modeling of human metabolic network and prediction of its tissue-specific metabolism, reconstruction of consensus yeast metabolic network, and simulation of mutual interactions among multiple microorganisms. It is expected that this trend will continue, the outcomes of which will allow development of more sophisticated networks integrating diverse omics data, and enhance our understanding of biological systems.
20302435	Selecting healthcare information systems provided by third-party vendors: a mind map beyond the manuals.
Inform Health Soc Care  2010Jan
The selection of a new healthcare information system (HIS) has always been a daunting process for clinicians, health care providers and policy makers. The objective of this study is to present the lessons learned and the main findings from several relevant case studies to support this process. Data were collected by retrospectively reviewing the summative results of three well-established systems, acquiring feedback from two E.U. projects, and conducting semi-structured interviews with a number of collaborators involved in electronic healthcare interventions. Selection issues were identified and classified into the following five categories: (i) data creation, (ii) data management, (iii) data sharing, (iv) data presentation and (v) modules management. A mind map was also structured to provide a more manageable list of issues concerning the most common electronic clinical technologies (e-CT). The vendor manual is intended as an overview of the merchandise e-CT and therefore has limited potential in supporting effectively the selection process of a new HIS. The present classification and the mind map - based on lessons learned - provide a ready-to-use toolkit for supporting the HIS selection process when healthcare organisations are unable to employ research development groups to lay the groundwork for building a new HIS from scratch.
20305778	FragKB: structural and literature annotation resource of conserved peptide fragments and residues.
PLoS ONE 20100318 2010
FragKB (Fragment Knowledgebase) is a repository of clusters of structurally similar fragments from proteins. Fragments are annotated with information at the level of sequence, structure and function, integrating biological descriptions derived from multiple existing resources and text mining. FragKB contains approximately 400,000 conserved fragments from 4,800 representative proteins from PDB. Literature annotations are extracted from more than 1,700 articles and are available for over 12,000 fragments. The underlying systematic annotation workflow of FragKB ensures efficient update and maintenance of this database. The information in FragKB can be accessed through a web interface that facilitates sequence and structural visualization of fragments together with known literature information on the consequences of specific residue mutations and functional annotations of proteins and fragment clusters. FragKB is accessible online at http://ubio.bioinfo.cnio.es/biotools/fragkb/. The information presented in FragKB can be used for modeling protein structures, for designing novel proteins and for functional characterization of related fragments. The current release is focused on functional characterization of proteins through inspection of conservation of the fragments.
20308490	What radiologists want to see: stack mode display in PowerPoint.
AJR Am J Roentgenol  2010Apr
The purpose of this article is to describe a stack mode viewer that is fully embeddable in PowerPoint and does not require external files. We developed a Web-based service named "RadViewer" that creates a Flash-based stack mode viewer and that is automatically embedded into PowerPoint. Features include mouse wheel and keyboard scrolling, zoom, cine loop, and brightness and contrast controls similar to those of a PACS workstation. Stack mode viewer created by RadViewer can be used in most radiology society meetings and conferences that use PowerPoint.
20205728	BIAdb: a curated database of benzylisoquinoline alkaloids.
BMC Pharmacol. 20100305 2010
Benzylisoquinoline is the structural backbone of many alkaloids with a wide variety of structures including papaverine, noscapine, codeine, morphine, apomorphine, berberine, protopine and tubocurarine. Many benzylisoquinoline alkaloids have been reported to show therapeutic properties and to act as novel medicines. Thus it is important to collect and compile benzylisoquinoline alkaloids in order to explore their usage in medicine. We extract information about benzylisoquinoline alkaloids from various sources like PubChem, KEGG, KNApSAcK and manual curation from literature. This information was processed and compiled in order to create a comprehensive database of benzylisoquinoline alkaloids, called BIAdb. The current version of BIAdb contains information about 846 unique benzylisoquinoline alkaloids, with multiple entries in term of source, function leads to total number of 2504 records. One of the major features of this database is that it provides data about 627 different plant species as a source of benzylisoquinoline and 114 different types of function performed by these compounds. A large number of online tools have been integrated, which facilitate user in exploring full potential of BIAdb. In order to provide additional information, we give external links to other resources/databases. One of the important features of this database is that it is tightly integrated with Drugpedia, which allows managing data in fixed/flexible format. A database of benzylisoquinoline compounds has been created, which provides comprehensive information about benzylisoquinoline alkaloids. This database will be very useful for those who are working in the field of drug discovery based on natural products. This database will also serve researchers working in the field of synthetic biology, as developing medicinally important alkaloids using synthetic process are one of important challenges. This database is available from http://crdd.osdd.net/raghava/biadb/.
20179075	Accelerated similarity searching and clustering of large compound sets by geometric embedding and locality sensitive hashing.
Bioinformatics 20100223 2010Apr1
Similarity searching and clustering of chemical compounds by structural similarities are important computational approaches for identifying drug-like small molecules. Most algorithms available for these tasks are limited by their speed and scalability, and cannot handle today's large compound databases with several million entries. In this article, we introduce a new algorithm for accelerated similarity searching and clustering of very large compound sets using embedding and indexing (EI) techniques. First, we present EI-Search as a general purpose similarity search method for finding objects with similar features in large databases and apply it here to searching and clustering of large compound sets. The method embeds the compounds in a high-dimensional Euclidean space and searches this space using an efficient index-aware nearest neighbor search method based on locality sensitive hashing (LSH). Second, to cluster large compound sets, we introduce the EI-Clustering algorithm that combines the EI-Search method with Jarvis-Patrick clustering. Both methods were tested on three large datasets with sizes ranging from about 260 000 to over 19 million compounds. In comparison to sequential search methods, the EI-Search method was 40-200 times faster, while maintaining comparable recall rates. The EI-Clustering method allowed us to significantly reduce the CPU time required to cluster these large compound libraries from several months to only a few days. Software implementations and online services have been developed based on the methods introduced in this study. The online services provide access to the generated clustering results and ultra-fast similarity searching of the PubChem Compound database with subsecond response time.
20200010	PaperMaker: validation of biomedical scientific publications.
Bioinformatics 20100303 2010Apr1
The automatic analysis of scientific literature can support authors in writing their manuscripts. PaperMaker is a novel IT solution that receives a scientific manuscript via a Web interface, automatically analyses the publication, evaluates consistency parameters and interactively delivers feedback to the author. It analyses the proper use of acronyms and their definitions, and the use of specialized terminology. It provides Gene Ontology (GO) and Medline Subject Headings (MeSH) categorization of text passages, the retrieval of relevant publications from public scientific literature repositories, and the identification of missing or unused references. The author receives a summary of findings, the manuscript in its corrected form and a digital abstract containing the GO and MeSH annotations in the NLM/PubMed format. http://www.ebi.ac.uk/Rebholz-srv/PaperMaker.
20338898	EBImage--an R package for image processing with applications to cellular phenotypes.
Bioinformatics  2010Apr1
EBImage provides general purpose functionality for reading, writing, processing and analysis of images. Furthermore, in the context of microscopy-based cellular assays, EBImage offers tools to segment cells and extract quantitative cellular descriptors. This allows the automation of such tasks using the R programming language and use of existing tools in the R environment for signal processing, statistical modeling, machine learning and data visualization. EBImage is free and open source, released under the LGPL license and available from the Bioconductor project (http://www.bioconductor.org/packages/release/bioc/html/EBImage.html).
20053842	Xper2: introducing e-taxonomy.
Bioinformatics 20100106 2010Mar1
Computer Aided Identification systems provide users with the resources to relate morpho-anatomic observations with taxa names and to subsequently access other knowledge about the organisms. They have the ability to manage descriptive data and make identifications through interactive keys. They are essential for both authors and users of biodiversity information. Xper(2) version 2.0 is one of the most user-friendly tools in its category and provides a complete environment dedicated to taxonomic management. Xper(2) software can be freely downloaded at http://lis-upmc.snv.jussieu.fr/lis/?q=en/resources/softwares/xper2
20180691	Complementary and alternative medicine and supportive care at leading cancer centers: a systematic analysis of websites.
J Altern Complement Med  2010Feb
With increasing frequency, patients with cancer and their family members are turning to the Internet to educate themselves about their disease and treatment options, including complementary and alternative medicine (CAM) and supportive care. However, very little is known about how national leading cancer centers represent these therapies via their websites. Simulating the perspective of an information-seeking patient or family member, we performed a systematic analysis of the websites of 41 National Cancer Institute designated comprehensive cancer centers. Two researchers independently evaluated websites, recorded CAM information, and rated quality of the websites using a 4-item Likert scale (overall, information, presentation, and navigation) with Cronbach's alpha = 0.97. Rating was adequately correlated between the two raters (correlation coefficient 0.8). Of 41 centers, 12 (29%) did not have functional websites with regard to information related to CAM. The most common CAM approaches mentioned were: acupuncture (59%), meditation/nutrition/spiritual support/yoga (56% for each), massage therapy (54%), and music therapy (51%). Twenty-three (23; 56%) presented information on support groups, 19 (46%) on patient seminars, 18 (44%) on survivorship effort, and 17 (41%) on symptom management clinics. Twenty-nine (29) (71%) of these websites had a telephone number available, 22 (54%) mentioned at least one ongoing research opportunity, and 19 (46%) provided links to the National Center for Complementary and Alternative Medicine website. Median rating of the quality of websites was 50 of 100, with only 7 (17%) of centers receiving a composite score 80 (excellent) or better. While a growing number of leading cancer centers provide information about CAM and supportive oncology information for patients via their websites, the quality and ease of navigation of these sites remain highly variable. Effective development and redesign of many of the websites is needed to better inform and empower patients and families seeking CAM and supportive care information.
20183880	The value of an in-domain lexicon in genomics QA.
J Bioinform Comput Biol  2010Feb
This paper demonstrates that a large-scale lexicon tailored for the biology domain is effective in improving question analysis for genomics Question Answering (QA). We use the TREC Genomics Track data to evaluate the performance of different question analysis methods. It is hard to process textual information in biology, especially in molecular biology, due to a huge number of technical terms which rarely appear in general English documents and dictionaries. To support biological Text Mining, we have developed a domain-specific resource, the BioLexicon. Started in 2006 from scratch, this lexicon currently includes more than four million biomedical terms consisting of newly curated terms and terms collected from existing biomedical databases. While conventional genomics QA systems provide query expansion based on thesauri and dictionaries, it is not clear to what extent a biology-oriented lexical resource is effective for question pre-processing for genomics QA. Experiments on the genomics QA data set show that question analysis using the BioLexicon performs slightly better than that using n-grams and the UMLS Specialist Lexicon.
20190053	Serving the enterprise and beyond with informatics for integrating biology and the bedside (i2b2).
J Am Med Inform Assoc  2010 Mar-Apr
Informatics for Integrating Biology and the Bedside (i2b2) is one of seven projects sponsored by the NIH Roadmap National Centers for Biomedical Computing (http://www.ncbcs.org). Its mission is to provide clinical investigators with the tools necessary to integrate medical record and clinical research data in the genomics age, a software suite to construct and integrate the modern clinical research chart. i2b2 software may be used by an enterprise's research community to find sets of interesting patients from electronic patient medical record data, while preserving patient privacy through a query tool interface. Project-specific mini-databases ("data marts") can be created from these sets to make highly detailed data available on these specific patients to the investigators on the i2b2 platform, as reviewed and restricted by the Institutional Review Board. The current version of this software has been released into the public domain and is available at the URL: http://www.i2b2.org/software.
20190054	The Enterprise Data Trust at Mayo Clinic: a semantically integrated warehouse of biomedical data.
J Am Med Inform Assoc  2010 Mar-Apr
Mayo Clinic's Enterprise Data Trust is a collection of data from patient care, education, research, and administrative transactional systems, organized to support information retrieval, business intelligence, and high-level decision making. Structurally it is a top-down, subject-oriented, integrated, time-variant, and non-volatile collection of data in support of Mayo Clinic's analytic and decision-making processes. It is an interconnected piece of Mayo Clinic's Enterprise Information Management initiative, which also includes Data Governance, Enterprise Data Modeling, the Enterprise Vocabulary System, and Metadata Management. These resources enable unprecedented organization of enterprise information about patient, genomic, and research data. While facile access for cohort definition or aggregate retrieval is supported, a high level of security, retrieval audit, and user authentication ensures privacy, confidentiality, and respect for the trust imparted by our patients for the respectful use of information about their conditions.
20190056	OIDs: how can I express you? Let me count the ways.
J Am Med Inform Assoc  2010 Mar-Apr
An object identifier (OID) has a central utility in providing a traceable source for the meaning of an identifier appearing in a cross-system communication. The views in this paper illustrate the problems with using the present OID registration system as a reliable source for the identifier, the confusion that the use of an OID introduces in messages, and the redundancy that the OID introduces at the expense of increased message size and no new content. In promoting clearly defined cross-system communication identifiers, Health Level 7 developed a standard that required use of OIDs outside of network addressing. This standard and its propagation by others may have paradoxically added more confusion than clarity.
20190057	The inadvertent disclosure of personal health information through peer-to-peer file sharing programs.
J Am Med Inform Assoc  2010 Mar-Apr
There has been a consistent concern about the inadvertent disclosure of personal information through peer-to-peer file sharing applications, such as Limewire and Morpheus. Examples of personal health and financial information being exposed have been published. We wanted to estimate the extent to which personal health information (PHI) is being disclosed in this way, and compare that to the extent of disclosure of personal financial information (PFI). After careful review and approval of our protocol by our institutional research ethics board, files were downloaded from peer-to-peer file sharing networks and manually analyzed for the presence of PHI and PFI. The geographic region of the IP addresses was determined, and classified as either USA or Canada. We estimated the proportion of files that contain personal health and financial information for each region. We also estimated the proportion of search terms that return files with personal health and financial information. We ascertained and discuss the ethical issues related to this study. Approximately 0.4% of Canadian IP addresses had PHI, as did 0.5% of US IP addresses. There was more disclosure of financial information, at 1.7% of Canadian IP addresses and 4.7% of US IP addresses. An analysis of search terms used in these file sharing networks showed that a small percentage of the terms would return PHI and PFI files (ie, there are people successfully searching for PFI and PHI on the peer-to-peer file sharing networks). There is a real risk of inadvertent disclosure of PHI through peer-to-peer file sharing networks, although the risk is not as large as for PFI. Anyone keeping PHI on their computers should avoid installing file sharing applications on their computers, or if they have to use such tools, actively manage the risks of inadvertent disclosure of their, their family's, their clients', or patients' PHI.
20190058	Effects of personal identifier resynthesis on clinical text de-identification.
J Am Med Inform Assoc  2010 Mar-Apr
De-identified medical records are critical to biomedical research. Text de-identification software exists, including "resynthesis" components that replace real identifiers with synthetic identifiers. The goal of this research is to evaluate the effectiveness and examine possible bias introduced by resynthesis on de-identification software. We evaluated the open-source MITRE Identification Scrubber Toolkit, which includes a resynthesis capability, with clinical text from Vanderbilt University Medical Center patient records. We investigated four record classes from over 500 patients' files, including laboratory reports, medication orders, discharge summaries and clinical notes. We trained and tested the de-identification tool on real and resynthesized records. We measured performance in terms of precision, recall, F-measure and accuracy for the detection of protected health identifiers as designated by the HIPAA Safe Harbor Rule. The de-identification tool was trained and tested on a collection of real and resynthesized Vanderbilt records. Results for training and testing on the real records were 0.990 accuracy and 0.960 F-measure. The results improved when trained and tested on resynthesized records with 0.998 accuracy and 0.980 F-measure but deteriorated moderately when trained on real records and tested on resynthesized records with 0.989 accuracy 0.862 F-measure. Moreover, the results declined significantly when trained on resynthesized records and tested on real records with 0.942 accuracy and 0.728 F-measure. The de-identification tool achieves high accuracy when training and test sets are homogeneous (ie, both real or resynthesized records). The resynthesis component regularizes the data to make them less "realistic," resulting in loss of performance particularly when training on resynthesized data and testing on real data.
20190060	Evaluating the decision accuracy and speed of clinical data visualizations.
J Am Med Inform Assoc  2010 Mar-Apr
Clinicians face an increasing volume of biomedical data. Assessing the efficacy of systems that enable accurate and timely clinical decision making merits corresponding attention. This paper discusses the multiple-reader multiple-case (MRMC) experimental design and linear mixed models as means of assessing and comparing decision accuracy and latency (time) for decision tasks in which clinician readers must interpret visual displays of data. These tools can assess and compare decision accuracy and latency (time). These experimental and statistical techniques, used extensively in radiology imaging studies, offer a number of practical and analytic advantages over more traditional quantitative methods such as percent-correct measurements and ANOVAs, and are recommended for their statistical efficiency and generalizability. An example analysis using readily available, free, and commercial statistical software is provided as an appendix. While these techniques are not appropriate for all evaluation questions, they can provide a valuable addition to the evaluative toolkit of medical informatics research.
20190062	The impact of electronic medical records data sources on an adverse drug event quality measure.
J Am Med Inform Assoc  2010 Mar-Apr
To examine the impact of billing and clinical data extracted from an electronic medical record system on the calculation of an adverse drug event (ADE) quality measure approved for use in The Joint Commission's ORYX program, a mandatory national hospital quality reporting system. The Child Health Corporation of America's "Use of Rescue Agents-ADE Trigger" quality measure uses medication billing data contained in the Pediatric Health Information Systems (PHIS) data warehouse to create The Joint Commission-approved quality measure. Using a similar query, we calculated the quality measure using PHIS plus four data sources extracted from our electronic medical record (EMR) system: medications charged, medication orders placed, medication orders with associated charges (orders charged), and medications administered. Inclusion and exclusion criteria were identical for all queries. Denominators and numerators were calculated using the five data sets. The reported quality measure is the ADE rate (numerator/denominator). Significant differences in denominators, numerators, and rates were calculated from different data sources within a single institution's EMR. Differences were due to both common clinical practices that may be similar across institutions and unique workflow practices not likely to be present at any other institution. The magnitude of the differences would significantly alter the national comparative ranking of our institution compared to other PHIS institutions. More detailed clinical information may result in quality measures that are not comparable across institutions due institution-specific workflow, differences that are exposed using EMR-derived data.
20100674	Objective image quality assessment based on support vector regression.
IEEE Trans Neural Netw 20100122 2010Mar
Objective image quality estimation is useful in many visual processing systems, and is difficult to perform in line with the human perception. The challenge lies in formulating effective features and fusing them into a single number to predict the quality score. In this brief, we propose a new approach to address the problem, with the use of singular vectors out of singular value decomposition (SVD) as features for quantifying major structural information in images and then support vector regression (SVR) for automatic prediction of image quality. The feature selection with singular vectors is novel and general for gauging structural changes in images as a good representative of visual quality variations. The use of SVR exploits the advantages of machine learning with the ability to learn complex data patterns for an effective and generalized mapping of features into a desired score, in contrast with the oft-utilized feature pooling process in the existing image quality estimators; this is to overcome the difficulty of model parameter determination for such a system to emulate the related, complex human visual system (HVS) characteristics. Experiments conducted with three independent databases confirm the effectiveness of the proposed system in predicting image quality with better alignment with the HVS's perception than the relevant existing work. The tests with untrained distortions and databases further demonstrate the robustness of the system and the importance of the feature selection.
20123570	An improved algorithm for the solution of the regularization path of support vector machine.
IEEE Trans Neural Netw 20100129 2010Mar
This paper describes an improved algorithm for the numerical solution to the support vector machine (SVM) classification problem for all values of the regularization parameter C . The algorithm is motivated by the work of Hastie and follows the main idea of tracking the optimality conditions of the SVM solution for ascending value of C . It differs from Hastie's approach in that the tracked path is not assumed to be 1-D. Instead, a multidimensional feasible space for the optimality condition is used to solve the tracking problem. Such a treatment allows the algorithm to properly handle data sets which Hastie's approach fails. These data sets are characterized by the presence of linearly dependent points (in the kernel space), duplicate points, or nearly duplicate points. Such data sets are quite common among many real-world data, especially those with nominal features. Other contributions of this paper include a unifying formulation of the tracking process in the form of a linear programming problem, update formula for the linear programs, considerations that guard against accumulation of errors resulting from the use of incremental updates, and routines to speed up the algorithm. The algorithm is implemented under the Matlab environment and is available for download. Experiments with several data sets including data set having up to several thousand data points are reported.
20123571	Computation of synchronized periodic solution in a BAM network with two delays.
IEEE Trans Neural Netw 20100129 2010Mar
A bidirectional associative memory (BAM) neural network with four neurons and two discrete delays is considered to represent an analytical method, namely, perturbation-incremental scheme (PIS). The expressions for the periodic solutions derived from Hopf bifurcation are given by using the PIS. The result shows that the PIS has higher accuracy than the center manifold reduction (CMR) with normal form for the values of time delay not far away from the Hopf bifurcation point. In terms of the PIS, the necessary and sufficient conditions of synchronized periodic solution arising from a Hopf bifurcation are obtained and the synchronized periodic solution is expressed in an analytical form. It can be seen that theoretical analysis is in good agreement with numerical simulation. It implies that the provided method is valid and the obtained result is correct. To the best of our knowledge, the paper is the first one to introduce the PIS to study the periodic solution derived from Hopf bifurcation for a 4-D delayed system quantitatively.
20187203	Comparison of ferucarbotran-enhanced fluid-attenuated inversion-recovery echo-planar, T2-weighted turbo spin-echo, T2*-weighted gradient-echo, and diffusion-weighted echo-planar imaging for detection of malignant liver lesions.
J Magn Reson Imaging  2010Mar
To compare the diagnostic accuracy of superparamagnetic iron oxide (SPIO)-enhanced fluid-attenuated inversion-recovery echo-planar imaging (FLAIR EPI) for malignant liver tumors with that of T2-weighted turbo spin-echo (TSE), T2*-weighted gradient-echo (GRE), and diffusion-weighted echo-planar imaging (DW EPI). SPIO-enhanced magnetic resonance imaging (MRI) that included FLAIR EPI, T2-weighted TSE, T2*-weighted GRE, and DW EPI sequences was performed using a 3 T system in 54 consecutive patients who underwent surgical exploration with intraoperative ultrasonography. A total of 88 malignant liver tumors were evaluated. Images were reviewed independently by two blinded observers who used a 5-point confidence scale to identify lesions. Results were correlated with results of histopathologic findings and surgical exploration with intraoperative ultrasonography. The accuracy of each MRI sequence was measured with jackknife alternative free-response receiver operating characteristic analysis. The sensitivity of each observer with each MRI sequence was compared with McNemar's test. Accuracy values were significantly higher with FLAIR EPI sequence (0.93) than with T2*-weighted GRE (0.80) or DW EPI sequences (0.80) (P &lt; 0.05). Sensitivity was significantly higher with the FLAIR EPI sequence than with any of the other sequences. SPIO-enhanced FLAIR EPI sequence was more accurate in the diagnosis of malignant liver tumors than T2*-weighted GRE and DW EPI sequences. SPIO-enhanced FLAIR EPI sequence is helpful for the detection of malignant liver tumors.
20187187	The influence of temporal resolution in determining pharmacokinetic parameters from DCE-MRI data.
Magn Reson Med  2010Mar
We investigated the influence of the temporal resolution of dynamic contrast-enhanced MRI data on pharmacokinetic parameter estimation. Dynamic Gd-DTPA (Gadolinium-diethylene triamine pentaacetic acid) enhanced MRI data of implanted prostate tumors on rat hind limb were acquired at 4.7 T, with a temporal resolution of approximately 5 sec. The data were subsequently downsampled to temporal resolutions in the range of 15 sec to 85 sec, using a strategy that involves a recombination of k-space data. A basic two-compartment model was fit to the contrast agent uptake curves. The results demonstrated that as temporal resolution decreases, the volume transfer constant (K(trans)) is progressively underestimated (approximately 4% to approximately 25%), and the fractional extravascular extracellular space (v(e)) is progressively overestimated (approximately 1% to approximately 10%). The proposed downsampling strategy simulates the influence of temporal resolution more realistically than simply downsampling by removing samples.
20137077	Extracting consistent knowledge from highly inconsistent cancer gene data sources.
BMC Bioinformatics 20100205 2010
Hundreds of genes that are causally implicated in oncogenesis have been found and collected in various databases. For efficient application of these abundant but diverse data sources, it is of fundamental importance to evaluate their consistency. First, we showed that the lists of cancer genes from some major data sources were highly inconsistent in terms of overlapping genes. In particular, most cancer genes accumulated in previous small-scale studies could not be rediscovered in current high-throughput genome screening studies. Then, based on a metric proposed in this study, we showed that most cancer gene lists from different data sources were highly functionally consistent. Finally, we extracted functionally consistent cancer genes from various data sources and collected them in our database F-Census. Although they have very low gene overlapping, most cancer gene data sources are highly consistent at the functional level, which indicates that they can separately capture partial genes in a few key pathways associated with cancer. Our results suggest that the sample sizes currently used for cancer studies might be inadequate for consistently capturing individual cancer genes, but could be sufficient for finding a number of cancer genes that could represent functionally most cancer genes. The F-Census database provides biologists with a useful tool for browsing and extracting functionally consistent cancer genes from various data sources.
20205186	Using the NCBI map viewer to browse genomic sequence data.
Curr Protoc Bioinformatics  2010Mar
This unit includes a Basic Protocol with an introduction to the Map Viewer, describing how to perform a simple text-based search of genome annotations to view the genomic context of a gene, navigate along a chromosome, zoom in and out, and change the displayed maps to hide and show information. It also describes some of NCBI's sequence-analysis tools, which are provided as links from the Map Viewer. The Alternate Protocols describe different ways to query the genome sequence, and also illustrate additional features of the Map Viewer. Alternate Protocol 1 shows how to perform and interpret the results of a BLAST search against the human genome. Alternate Protocol 2 demonstrates how to retrieve a list of all genes between two STS markers. Finally, Alternate Protocol 3 shows how to find all annotated members of a gene family.
20205187	Using the DFCI gene index databases for biological discovery.
Curr Protoc Bioinformatics  2010Mar
The DFCI Gene Index Web pages provide access to analyses of ESTs and gene sequences for nearly 114 species, as well as a number of resources derived from these. Each species-specific database is presented using a common format with a home page. A variety of methods exist that allow users to search each species-specific database. Methods implemented currently include nucleotide or protein sequence queries using WU-BLAST, text-based searches using various sequence identifiers, searches by gene, tissue and library name, and searches using functional classes through Gene Ontology assignments. This protocol provides guidance for using the Gene Index Databases to extract information.
20158899	Electronic search strategies to identify reports of cluster randomized trials in MEDLINE: low precision will improve with adherence to reporting standards.
BMC Med Res Methodol 20100216 2010
Cluster randomized trials (CRTs) present unique methodological and ethical challenges. Researchers conducting systematic reviews of CRTs (e.g., addressing methodological or ethical issues) require efficient electronic search strategies (filters or hedges) to identify trials in electronic databases such as MEDLINE. According to the CONSORT statement extension to CRTs, the clustered design should be clearly identified in titles or abstracts; however, variability in terminology may make electronic identification challenging. Our objectives were to (a) evaluate sensitivity ("recall") and precision of a well-known electronic search strategy ("randomized controlled trial" as publication type) with respect to identifying CRTs, (b) evaluate the feasibility of new search strategies targeted specifically at CRTs, and (c) determine whether CRTs are appropriately identified in titles or abstracts of reports and whether there has been improvement over time. We manually examined a wide range of health journals to identify a gold standard set of CRTs. Search strategies were evaluated against the gold standard set, as well as an independent set of CRTs included in previous systematic reviews. The existing strategy (randomized controlled trial.pt) is sensitive (93.8%) for identifying CRTs, but has relatively low precision (9%, number needed to read 11); the number needed to read can be halved to 5 (precision 18.4%) by combining with cluster design-related terms using the Boolean operator AND; combining with the Boolean operator OR maximizes sensitivity (99.4%) but would require 28.6 citations read to identify one CRT. Only about 50% of CRTs are clearly identified as cluster randomized in titles or abstracts; approximately 25% can be identified based on the reported units of randomization but are not amenable to electronic searching; the remaining 25% cannot be identified except through manual inspection of the full-text article. The proportion of trials clearly identified has increased from 28% between the years 2000-2003, to 60% between 2004-2007 (absolute increase 32%, 95% CI 17 to 47%). CRTs should include the phrase "cluster randomized trial" in titles or abstracts; this will facilitate more accurate indexing of the publication type by reviewers at the National Library of Medicine, and efficient textword retrieval of the subset employing cluster randomization.
20097331	Relabeling algorithm for retrieval of noisy instances and improving prediction quality.
Comput. Biol. Med. 20100125 2010Mar
A relabeling algorithm for retrieval of noisy instances with binary outcomes is presented. The relabeling algorithm iteratively retrieves, selects, and re-labels data instances (i.e., transforms a decision space) to improve prediction quality. It emphasizes knowledge generalization and confidence rather than classification accuracy. A confidence index incorporating classification accuracy, prediction error, impurities in the relabeled dataset, and cluster purities was designed. The proposed approach is illustrated with a binary outcome dataset and was successfully tested on the standard benchmark four UCI repository dataset as well as bladder cancer immunotherapy data. A subset of the most stable instances (i.e., 7% to 51% of the sample) with high confidence (i.e., between 64%-99.44%) was identified for each application along with most noisy instances. The domain experts and the extracted knowledge validated the relabeled instances and corresponding confidence indexes. The relabeling algorithm with some modifications can be applied to other medical, industrial, and service domains.
20209084	Zsyntax: a formal language for molecular biology with projected applications in text mining and biological prediction.
PLoS ONE 20100303 2010
We propose a formal language that allows for transposing biological information precisely and rigorously into machine-readable information. This language, which we call Zsyntax (where Z stands for the Greek word zetaomegaeta, life), is grounded on a particular type of non-classical logic, and it can be used to write algorithms and computer programs. We present it as a first step towards a comprehensive formal language for molecular biology in which any biological process can be written and analyzed as a sort of logical "deduction". Moreover, we illustrate the potential value of this language, both in the field of text mining and in that of biological prediction.
20122157	LAITOR--Literature Assistant for Identification of Terms co-Occurrences and Relationships.
BMC Bioinformatics 20100201 2010
Biological knowledge is represented in scientific literature that often describes the function of genes/proteins (bioentities) in terms of their interactions (biointeractions). Such bioentities are often related to biological concepts of interest that are specific of a determined research field. Therefore, the study of the current literature about a selected topic deposited in public databases, facilitates the generation of novel hypotheses associating a set of bioentities to a common context. We created a text mining system (LAITOR: Literature Assistant for Identification of Terms co-Occurrences and Relationships) that analyses co-occurrences of bioentities, biointeractions, and other biological terms in MEDLINE abstracts. The method accounts for the position of the co-occurring terms within sentences or abstracts. The system detected abstracts mentioning protein-protein interactions in a standard test (BioCreative II IAS test data) with a precision of 0.82-0.89 and a recall of 0.48-0.70. We illustrate the application of LAITOR to the detection of plant response genes in a dataset of 1000 abstracts relevant to the topic. Text mining tools combining the extraction of interacting bioentities and biological concepts with network displays can be helpful in developing reasonable hypotheses in different scientific backgrounds.
20215101	How breast cancer patients want to search for and retrieve information from stories of other patients on the internet: an online randomized controlled experiment.
J. Med. Internet Res. 20100309 2010
Other patients' stories on the Internet can give patients information, support, reassurance, and practical advice. We examined which search facility for online stories resulted in patients' satisfaction and search success. This study was a randomized controlled experiment with a 2x2 factorial design conducted online. We facilitated access to 170 stories of breast cancer patients in four ways based on two factors: (1) no versus yes search by story topic, and (2) no versus yes search by writer profile. Dutch speaking women with breast cancer were recruited. Women who gave informed consent were randomly assigned to one of four groups. After searching for stories, women were offered a questionnaire relating to satisfaction with the search facility, the stories retrieved, and impact of the stories on coping with breast cancer. Of 353 enrolled women, 182 (51.6%) completed the questionnaire: control group (n = 37), story topics group (n = 49), writer profile group (n = 51), and combination group (n = 45). Questionnaire completers were evenly distributed over the four groups (chi(2) (3) = 3.7, P = .30). Women who had access to the story topics search facility (yes vs no): were more positive about (mean scores 4.0 vs 3.6, P = .001) and more satisfied with the search facility (mean scores 7.3 vs 6.3, P &lt; .001); were more positive about the number of search options (mean scores 2.3 vs 2.1, P = .04); were better enabled to find desired information (mean scores 3.3 vs 2.8, P = .001); were more likely to recommend the search facility to others or intend to use it themselves (mean scores 4.1 vs 3.5, P &lt; .001); were more positive about how retrieved stories were displayed (mean scores 3.6 vs 3.2, P = .001); retrieved stories that better covered their information needs (mean scores 3.0 vs 2.6, P = .02); were more satisfied with the stories retrieved (mean scores 7.1 vs 6.4, P = .002); and were more likely to report an impact of the stories on coping with breast cancer (mean scores 3.2 vs 2.9, P =. 02). Three main effects were associated with use of the writer profile search (yes vs no): being more positive about (mean scores 3.9 vs 3.6, P = .005) and more satisfied with the search facility (mean scores 7.1 vs 6.5, P =. 01), and being more positive about how retrieved stories were displayed (mean scores 3.8 vs 2.9, P &lt; .001). For satisfaction with the search facility, an interaction effect was found (P = .03): at least one of the two search facilities was needed for satisfaction. Having access to the story topics search facility clearly had the most positive effect on patient satisfaction and search success.
20176968	Solving the apparent diversity-accuracy dilemma of recommender systems.
Proc. Natl. Acad. Sci. U.S.A. 20100222 2010Mar9
Recommender systems use data on past user preferences to predict possible future likes and interests. A key challenge is that while the most useful individual recommendations are to be found among diverse niche objects, the most reliably accurate results are obtained by methods that recommend objects based on user or object similarity. In this paper we introduce a new algorithm specifically to address the challenge of diversity and show how it can be used to resolve this apparent dilemma when combined in an elegant hybrid with an accuracy-focused algorithm. By tuning the hybrid appropriately we are able to obtain, without relying on any semantic or context-specific information, simultaneous gains in both accuracy and diversity of recommendations.
20149233	LINNAEUS: a species name identification system for biomedical literature.
BMC Bioinformatics 20100211 2010
The task of recognizing and identifying species names in biomedical literature has recently been regarded as critical for a number of applications in text and data mining, including gene name recognition, species-specific document retrieval, and semantic enrichment of biomedical articles. In this paper we describe an open-source species name recognition and normalization software system, LINNAEUS, and evaluate its performance relative to several automatically generated biomedical corpora, as well as a novel corpus of full-text documents manually annotated for species mentions. LINNAEUS uses a dictionary-based approach (implemented as an efficient deterministic finite-state automaton) to identify species names and a set of heuristics to resolve ambiguous mentions. When compared against our manually annotated corpus, LINNAEUS performs with 94% recall and 97% precision at the mention level, and 98% recall and 90% precision at the document level. Our system successfully solves the problem of disambiguating uncertain species mentions, with 97% of all mentions in PubMed Central full-text documents resolved to unambiguous NCBI taxonomy identifiers. LINNAEUS is an open source, stand-alone software system capable of recognizing and normalizing species name mentions with speed and accuracy, and can therefore be integrated into a range of bioinformatics and text-mining applications. The software and manually annotated corpus can be downloaded freely at http://linnaeus.sourceforge.net/.
19703805	Effect of implementing a computerized system for bone mineral density storage and report preparation on result turnaround time and savings in cost, time, and space.
Endocr Pract  2010 Jan-Feb
To evaluate whether introduction of a densitometry workflow, data-storage, and reporting software system would result in streamlined workflow with fewer expenses and quicker result turnaround time. BoneStation was implemented March 30, 2009, in a large, urban, tertiary referral center performing more than 6000 bone mineral density studies annually at 3 different geographic sites. The times of scan acquisition, report preparation, and final signature in the online medical record were recorded, and the delays from scan to report and from scan to final signature in the online medical record were calculated for each patient during 2 representative weeks before (n = 274) and 2 weeks after (n = 235) implementation of BoneStation. Use of BoneStation reduced time from scan to report from 2.11 +/- 0.16 days to 0.46 +/- 0.05 days (P&lt;.001). BoneStation saved our practice $8.94 per scan, while costing only $3 per scan, resulting in net savings. Considering that the total reimbursement from Medicare in 2010 for dual-energy x-ray absorptiometry is projected to be $55.44, this constitutes cost savings of 10.7% of the total reimbursement. The introduction of a specialized electronic medical system for data storage and reporting reduced costs and improved result turnaround time in a densitometry practice.
20150664	Automatic detection of large dense-core vesicles in secretory cells and statistical analysis of their intracellular distribution.
IEEE/ACM Trans Comput Biol Bioinform  2010 Jan-Mar
Analyzing the morphological appearance and the spatial distribution of large dense-core vesicles (granules) in the cell cytoplasm is central to the understanding of regulated exocytosis. This paper is concerned with the automatic detection of granules and the statistical analysis of their spatial locations in different cell groups. We model the locations of granules of a given cell as a realization of a finite spatial point process and the point patterns associated with the cell groups as replicated point patterns of different spatial point processes. First, an algorithm to segment the granules using electron microscopy images is proposed. Second, the relative locations of the granules with respect to the plasma membrane are characterized by two functional descriptors: the empirical cumulative distribution function of the distances from the granules to the plasma membrane and the density of granules within a given distance to the plasma membrane. The descriptors of the different cells for each group are compared using bootstrap procedures. Our results show that these descriptors and the testing procedure allow discriminating between control and treated cells. The application of these novel tools to studies of secretion should help in the analysis of diseases associated with dysfunctional secretion, such as diabetes.
20154757	High resolution imaging of patterned model biological membranes by localized surface plasmon microscopy.
Appl Opt  2010Feb10
We report on microscopic imaging of phospholipid membranes. To achieve nonlabel, noncontact, and high spatial resolution imaging of the membranes, we use optically excited localized surface plasmons as a virtual measurement probe to obtain the local refractive index. This enables significantly higher lateral resolution of approximately 170 nm. We reveal that the developed microscope has the capability of observing lipid bilayers with thickness of 3.0 nm deposited into the gaps in a patterned lipid bilayer with thickness of 4.6 nm. We find that the thickness resolution against the deposited lipid bilayer is approximately 0.33 nm.
20056198	Uterine electromyogram database and processing function interface: An open standard analysis platform for electrohysterogram signals.
Comput. Biol. Med. 20100106 2010Feb
The uterine electromyogram or electrohysterogram (EHG) is one of the most promising biophysical markers of preterm labor. At this time no recording parameter standard exists for EHG recordings which can be a problem for the establishment of international multicentric trials. In this paper, we present a management and processing system dedicated to storing and processing EHG signals. This system can process EHG signals recorded in different experimental conditions i.e. different sampling frequencies. The signal management is performed using an easy to use graphical user interface. Other available functions include visualization, preprocessing and analysis of EHG signals. The proposed processing functions provide temporal, spectral and time-scale parameters obtained from the EHG bibliography. The obtained results from real signals recorded in two different hospitals in two different countries are in accordance with the literature and demonstrate the potential of the proposed system. The incorporation of new functions is easy, due to a standardization of the EHG data formats.
20078892	OLS dialog: an open-source front end to the ontology lookup service.
BMC Bioinformatics 20100117 2010
With the growing amount of biomedical data available in public databases it has become increasingly important to annotate data in a consistent way in order to allow easy access to this rich source of information. Annotating the data using controlled vocabulary terms and ontologies makes it much easier to compare and analyze data from different sources. However, finding the correct controlled vocabulary terms can sometimes be a difficult task for the end user annotating these data. In order to facilitate the location of the correct term in the correct controlled vocabulary or ontology, the Ontology Lookup Service was created. However, using the Ontology Lookup Service as a web service is not always feasible, especially for researchers without bioinformatics support. We have therefore created a Java front end to the Ontology Lookup Service, called the OLS Dialog, which can be plugged into any application requiring the annotation of data using controlled vocabulary terms, making it possible to find and use controlled vocabulary terms without requiring any additional knowledge about web services or ontology formats. As a user-friendly open source front end to the Ontology Lookup Service, the OLS Dialog makes it straightforward to include controlled vocabulary support in third-party tools, which ultimately makes the data even more valuable to the biomedical community.
20165697	The effect of providing a USB syllabus on resident reading of landmark articles.
Med Educ Online 20100129 2010
The acquisition of new knowledge is a primary goal of residency training. Retrieving and retaining influential primary and secondary medical literature can be challenging for house officers. We set out to investigate the effect of a Universal Serial Bus (USB) drive loaded with landmark scientific articles on housestaff education in a pilot study. We created a USB syllabus that contains 187 primary scientific research articles. The electronic syllabus had links to the full-text articles and was organized using an html webpage with a table of contents according to medical subspecialties. We performed a prospective cohort study of 53 house officers in the internal medicine residency program who received the USB syllabus. We evaluated the impact of the USB syllabus on resident education with surveys at the beginning and conclusion of the nine-month study period. All 50 respondents (100%) reported to have used the USB syllabus. The self-reported number of original articles read each month was higher at the end of the nine-month study period compared to baseline. Housestaff rated original articles as being a more valuable educational resource after the intervention. An electronic syllabus with landmark scientific articles placed on a USB drive was widely utilized by housestaff, increased the self-reported reading of original scientific articles and seemed to have positively influenced residents' attitude toward original medical literature.
20047655	The BridgeDb framework: standardized access to gene, protein and metabolite identifier mapping services.
BMC Bioinformatics 20100104 2010
Many complementary solutions are available for the identifier mapping problem. This creates an opportunity for bioinformatics tool developers. Tools can be made to flexibly support multiple mapping services or mapping services could be combined to get broader coverage. This approach requires an interface layer between tools and mapping services. Here we present BridgeDb, a software framework for gene, protein and metabolite identifier mapping. This framework provides a standardized interface layer through which bioinformatics tools can be connected to different identifier mapping services. This approach makes it easier for tool developers to support identifier mapping. Mapping services can be combined or merged to support multi-omics experiments or to integrate custom microarray annotations. BridgeDb provides its own ready-to-go mapping services, both in webservice and local database forms. However, the framework is intended for customization and adaptation to any identifier mapping service. BridgeDb has already been integrated into several bioinformatics applications. By uncoupling bioinformatics tools from mapping services, BridgeDb improves capability and flexibility of those tools. All described software is open source and available at http://www.bridgedb.org.
20102628	Semi-automated screening of biomedical citations for systematic reviews.
BMC Bioinformatics 20100126 2010
Systematic reviews address a specific clinical question by unbiasedly assessing and analyzing the pertinent literature. Citation screening is a time-consuming and critical step in systematic reviews. Typically, reviewers must evaluate thousands of citations to identify articles eligible for a given review. We explore the application of machine learning techniques to semi-automate citation screening, thereby reducing the reviewers' workload. We present a novel online classification strategy for citation screening to automatically discriminate "relevant" from "irrelevant" citations. We use an ensemble of Support Vector Machines (SVMs) built over different feature-spaces (e.g., abstract and title text), and trained interactively by the reviewer(s). Semi-automating the citation screening process is difficult because any such strategy must identify all citations eligible for the systematic review. This requirement is made harder still due to class imbalance; there are far fewer "relevant" than "irrelevant" citations for any given systematic review. To address these challenges we employ a custom active-learning strategy developed specifically for imbalanced datasets. Further, we introduce a novel undersampling technique. We provide experimental results over three real-world systematic review datasets, and demonstrate that our algorithm is able to reduce the number of citations that must be screened manually by nearly half in two of these, and by around 40% in the third, without excluding any of the citations eligible for the systematic review. We have developed a semi-automated citation screening algorithm for systematic reviews that has the potential to substantially reduce the number of citations reviewers have to manually screen, without compromising the quality and comprehensiveness of the review.
20167826	Building capacity through the Internet: lessons learnt from the Reviews of Health Promotion &amp; Education Online.
Health Promot Int  2010Mar
From 2000 to 2008, the International Union for Health Promotion and Health Education transformed the Internet Journal of Health Promotion it had inherited into an innovative electronic multilingual capacity building experiment, the Reviews of Health Promotion &amp; Education Online &lt;http://rhpeo.net/index2.html&gt;. Using a variety of sources (content analysis of the papers, site consultation statistics, users' survey), this paper analyzes reflexively the strengths and weaknesses of this experiment that was replaced in October 2009 by an Internet forum: Views on Health Promotion Online &lt;http://vhpo.net&gt;.
20173902	An explanation for the non-uniform grating effects during recording of diffraction gratings in photopolymers.
Opt Express  2010Jan18
The recent results reported in reference 1 have produced an increased interest in explaining deviations from the ideal behavior of the energetic variation of the diffraction efficiency of holographic gratings. This ideal behavior occurs when uniform gratings are recorded, and the index modulation is proportional to the energetic exposure. As a result, a typical sin(2) curve is obtained reaching a maximum diffraction efficiency and saturation at or below this value. However, linear deviations are experimentally observed when the first maximum on the curve is lower than the second. This effect does not correspond to overmodulation and recently in PVA/acrylamide photopolymers of high thickness it has been explained by the dye concentration in the layer and the resulting molecular weight of the polymer chains generated in the polymerization process. In this work, new insights into these deviations are gained from the analysis of the non-uniform gratings recorded. Therefore, we show that deviations from the linear response can be explained by taking into account the energetic evolution of the index modulation as well as the fringe bending in the grating.
20174560	Application description and policy model in collaborative environment for sharing of information on epidemiological and clinical research data sets.
PLoS ONE 20100219 2010
Sharing of epidemiological and clinical data sets among researchers is poor at best, in detriment of science and community at large. The purpose of this paper is therefore to (1) describe a novel Web application designed to share information on study data sets focusing on epidemiological clinical research in a collaborative environment and (2) create a policy model placing this collaborative environment into the current scientific social context. The Database of Databases application was developed based on feedback from epidemiologists and clinical researchers requiring a Web-based platform that would allow for sharing of information about epidemiological and clinical study data sets in a collaborative environment. This platform should ensure that researchers can modify the information. A Model-based predictions of number of publications and funding resulting from combinations of different policy implementation strategies (for metadata and data sharing) were generated using System Dynamics modeling. The application allows researchers to easily upload information about clinical study data sets, which is searchable and modifiable by other users in a wiki environment. All modifications are filtered by the database principal investigator in order to maintain quality control. The application has been extensively tested and currently contains 130 clinical study data sets from the United States, Australia, China and Singapore. Model results indicated that any policy implementation would be better than the current strategy, that metadata sharing is better than data-sharing, and that combined policies achieve the best results in terms of publications. Based on our empirical observations and resulting model, the social network environment surrounding the application can assist epidemiologists and clinical researchers contribute and search for metadata in a collaborative environment, thus potentially facilitating collaboration efforts among research communities distributed around the globe.
20175472	Monte Carlo based, patient-specific RapidArc QA using Linac log files.
Med Phys  2010Jan
A Monte Carlo (MC) based QA process to validate the dynamic beam delivery accuracy for Varian RapidArc (Varian Medical Systems, Palo Alto, CA) using Linac delivery log files (DynaLog) is presented. Using DynaLog file analysis and MC simulations, the goal of this article is to (a) confirm that adequate sampling is used in the RapidArc optimization algorithm (177 static gantry angles) and (b) to assess the physical machine performance [gantry angle and monitor unit (MU) delivery accuracy]. Ten clinically acceptable RapidArc treatment plans were generated for various tumor sites and delivered to a water-equivalent cylindrical phantom on the treatment unit. Three Monte Carlo simulations were performed to calculate dose to the CT phantom image set: (a) One using a series of static gantry angles defined by 177 control points with treatment planning system (TPS) MLC control files (planning files), (b) one using continuous gantry rotation with TPS generated MLC control files, and (c) one using continuous gantry rotation with actual Linac delivery log files. Monte Carlo simulated dose distributions are compared to both ionization chamber point measurements and with RapidArc TPS calculated doses. The 3D dose distributions were compared using a 3D gamma-factor analysis, employing a 3%/3 mm distance-to-agreement criterion. The dose difference between MC simulations, TPS, and ionization chamber point measurements was less than 2.1%. For all plans, the MC calculated 3D dose distributions agreed well with the TPS calculated doses (gamma-factor values were less than 1 for more than 95% of the points considered). Machine performance QA was supplemented with an extensive DynaLog file analysis. A DynaLog file analysis showed that leaf position errors were less than 1 mm for 94% of the time and there were no leaf errors greater than 2.5 mm. The mean standard deviation in MU and gantry angle were 0.052 MU and 0.355 degrees, respectively, for the ten cases analyzed. The accuracy and flexibility of the Monte Carlo based RapidArc QA system were demonstrated. Good machine performance and accurate dose distribution delivery of RapidArc plans were observed. The sampling used in the TPS optimization algorithm was found to be adequate.
20176318	Online consumer search strategies for smoking-cessation information.
Am J Prev Med  2010Mar
For many Americans, the Internet has become a primary mechanism for locating information on healthcare and treatment options, including tobacco addiction. Detailed information on this behavior could inform design decisions for next-generation cessation interventions, but very little is known about how consumers search or what resources they locate. A subset of a publicly available, anonymized record of the search behavior of 650,000 individuals over 3 months in 2006 was analyzed. Smoking cessation-related queries were extracted and coded via manual identification of terms and by back-identifying terms by matching them to the websites ultimately visited. Destination sites were coded as to whether or not they originated from a professional source based on the literature and known healthcare organizations. A total of 628 individuals (0.10%) made 1106 cessation-related searches during the observation period. Of these, 76% resulted in the individual reaching a website; professional sites were reached by only 34% of searchers. Complementary or alternative therapies were popular, with 10% of individuals searching for "laser" therapy. A concerning disconnect exists between consumer demand (as demonstrated by search behavior) and the sites produced by researchers and health professionals. This "demand gap" may contribute to low overall participation rates and hamper the potential impact of such systems. Further research is needed to link online consumer preferences to intervention design decisions.
20074336	Gene prioritization and clustering by multi-view text mining.
BMC Bioinformatics 20100114 2010
Text mining has become a useful tool for biologists trying to understand the genetics of diseases. In particular, it can help identify the most interesting candidate genes for a disease for further experimental analysis. Many text mining approaches have been introduced, but the effect of disease-gene identification varies in different text mining models. Thus, the idea of incorporating more text mining models may be beneficial to obtain more refined and accurate knowledge. However, how to effectively combine these models still remains a challenging question in machine learning. In particular, it is a non-trivial issue to guarantee that the integrated model performs better than the best individual model. We present a multi-view approach to retrieve biomedical knowledge using different controlled vocabularies. These controlled vocabularies are selected on the basis of nine well-known bio-ontologies and are applied to index the vast amounts of gene-based free-text information available in the MEDLINE repository. The text mining result specified by a vocabulary is considered as a view and the obtained multiple views are integrated by multi-source learning algorithms. We investigate the effect of integration in two fundamental computational disease gene identification tasks: gene prioritization and gene clustering. The performance of the proposed approach is systematically evaluated and compared on real benchmark data sets. In both tasks, the multi-view approach demonstrates significantly better performance than other comparing methods. In practical research, the relevance of specific vocabulary pertaining to the task is usually unknown. In such case, multi-view text mining is a superior and promising strategy for text-based disease gene identification.
20179464	Evidence-based practice, step by step: asking the clinical question: a key step in evidence-based practice.
Am J Nurs  2010Mar
This is the third article in a series from the Arizona State University College of Nursing and Health Innovation's Center for the Advancement of Evidence-Based Practice. Evidence-based practice (EBP) is a problem-solving approach to the delivery of health care that integrates the best evidence from studies and patient care data with clinician expertise and patient preferences and values. When delivered in a context of caring and in a supportive organizational culture, the highest quality of care and best patient outcomes can be achieved.The purpose of this series is to give nurses the knowledge and skills they need to implement EBP consistently, one step at a time. Articles will appear every two months to allow you time to incorporate information as you work toward implementing EBP at your institution. Also, we've scheduled "Ask the Authors" call-ins every few months to provide a direct line to the experts to help you resolve questions. Details about how to participate in the next call will be published with May's Evidence-Based Practice, Step by Step.
19859951	IIR GRAPPA for parallel MR image reconstruction.
Magn Reson Med  2010Feb
Accelerated parallel MRI has advantage in imaging speed, and its image quality has been improved continuously in recent years. This paper introduces a two-dimensional infinite impulse response model of inverse filter to replace the finite impulse response model currently used in generalized autocalibrating partially parallel acquisitions class image reconstruction methods. The infinite impulse response model better characterizes the correlation of k-space data points and better approximates the perfect inversion of parallel imaging process, resulting in a novel generalized image reconstruction method for accelerated parallel MRI. This k-space-based reconstruction method includes the conventional generalized autocalibrating partially parallel acquisitions class methods as special cases and has a new infinite impulse response data estimation mechanism for effective improvement of image quality. The experiments on in vivo MRI data show that the proposed method significantly reduces reconstruction errors compared with the conventional two-dimensional generalized autocalibrating partially parallel acquisitions method, particularly at the high acceleration rates.
19918896	Phase-unwrapping algorithm for translation extraction from spherical navigator echoes.
Magn Reson Med  2010Feb
Spherical navigator echoes have been shown to determine rigid-body rotation and translation simultaneously. Following the determination of rotation, translations are determined from the phase change between the baseline and transformed spherical navigator echoes. Because the measured phase change is limited in the interval (-pi, pi), a phase-unwrapping algorithm is required to recover the true phase change in absolute values. The unwrapping algorithm presented in this article is based on a priori information about the true translation-induced phase-change function. The algorithm is verified using simulation and in vivo experiments, and the accuracy and precision of translation determination are evaluated. Specifically, the effects of background and off-resonance-induced phase noise are explored. When the proposed phase-unwrapping algorithm was used, translations up to 15 mm were measured, with accuracy better than 5%; for translations up to 40 mm, an error of approximately 10% was observed.
20122512	Primary angiitis of the central nervous system: apparent diffusion coefficient lesion analysis.
Clin Imaging  2010 Jan-Feb
Apparent diffusion coefficients (ADCs) of the brain lesions in primary angiitis of the central nervous system (PACNS) patients were analyzed in this study. The mean ADC ratios for acute/subacute phase lesions were significantly lower than that for chronic phase lesions. However, some acute/subacute phase lesions had elevated ADCs and these lesions disappeared overtime, implicating a nonischemic mechanism in PACNS.
20126236	Recovery of spectral reflectances of imaged objects by the use of features of spectral reflectances.
J Opt Soc Am A Opt Image Sci Vis  2010Feb1
Recovery of spectral reflectances of objects being imaged through the use of sensor responses is important to reproduce color images under various illuminations. Although the Wiener estimation is usually used for the recovery, the recovery performance of the estimation depends on the autocorrelation matrix of the spectral reflectances and the noise present in an image acquisition system. The purpose of the present paper is to show that the Wiener estimation with the noise variance estimated by the previous proposal [IEEE Trans. Image Process. 16, 1848 (2006)] and with the autocorrelation matrix that uses the features of the spectral reflectances recovered by the previous method is very effective in greatly improving the performance.
20134080	The LAILAPS search engine: relevance ranking in life science databases.
J Integr Bioinform 20100115 2010
Search engines and retrieval systems are popular tools at a life science desktop. The manual inspection of hundreds of database entries, that reflect a life science concept or fact, is a time intensive daily work. Hereby, not the number of query results matters, but the relevance does. In this paper, we present the LAILAPS search engine for life science databases. The concept is to combine a novel feature model for relevance ranking, a machine learning approach to model user relevance profiles, ranking improvement by user feedback tracking and an intuitive and slim web user interface, that estimates relevance rank by tracking user interactions. Queries are formulated as simple keyword lists and will be expanded by synonyms. Supporting a flexible text index and a simple data import format, LAILAPS can easily be used both as search engine for comprehensive integrated life science databases and for small in-house project databases. With a set of features, extracted from each database hit in combination with user relevance preferences, a neural network predicts user specific relevance scores. Using expert knowledge as training data for a predefined neural network or using users own relevance training sets, a reliable relevance ranking of database hits has been implemented. In this paper, we present the LAILAPS system, the concepts, benchmarks and use cases. LAILAPS is public available for SWISSPROT data at http://lailaps.ipk-gatersleben.de.
19941383	Characterization of the human plasma phosphoproteome using linear ion trap mass spectrometry and multiple search engines.
J. Proteome Res.  2010Feb5
Major plasma protein families play different roles in blood physiology and hemostasis and in immunodefense. Other proteins in plasma can be involved in signaling as chemical messengers or constitute biological markers of the status of distant tissues. In this respect, the plasma phosphoproteome holds potentially relevant information on the mechanisms modulating these processes through the regulation of protein activity. In this work we describe for the first time a collection of phosphopeptides identified in human plasma using immunoaffinity separation of the seven major serum protein families from other plasma proteins, SCX fractionation, and TiO(2) purification prior to LC-MS/MS analysis. One-hundred and twenty-seven phosphosites in 138 phosphopeptides mapping 70 phosphoproteins were identified with FDR &lt; 1%. A high-confidence collection of phosphosites was obtained using a combined search with the OMSSA, SEQUEST, and Phenyx search engines.
20141365	Work flow with digital intraoral radiography: a systematic review.
Acta Odontol. Scand.  2010Mar
This systematic review evaluates the six most frequently emphasized advantages of working with digital radiography: less working time, lower radiation dose to the patient, fewer retakes and errors, wider dynamic range, easier access to patient information and easier image storage and communication. Moreover, some clinical aspects and possible disadvantages of digital imaging that were not foreseen at the beginning of the digital era, such as patient discomfort, damage to the receptor, degradation of the image, cross-contamination and viewing conditions, were assessed. The literature search used the PubMed database with no limits and was performed during June to August 2009. Search strategies are described in the text for each of the mentioned tasks. A hand search of task-specific journals supplemented the search strategies. Time seems to be saved when switching from film to digital imaging in dental practice, a dose reduction may not be obtained, retakes and errors may be increased, the dynamic range may be wider with photostimulable storage phosphor (PSP) plates but not with sensors, the effect on patient information has not been well studied and storage and communication create new challenges with regard to handling large files and image compression. In addition, patient discomfort seems to be pronounced with sensors compared with PSP plates and film, the PSP plate may be scratched in clinical use and a two-layer barrier seems to be needed to prevent contamination of the receptor. The type of monitor may not be of major concern if the image is viewed in a room with subdued light. Not all of the predicted advantages with digital compared to film-based radiography hold true in daily clinical work. Of particular interest is the relationship between number of images, retakes and the dose given to the patient.
20064801	Quantifying clinical narrative redundancy in an electronic health record.
J Am Med Inform Assoc  2010 Jan-Feb
Although electronic notes have advantages compared to handwritten notes, they take longer to write and promote information redundancy in electronic health records (EHRs). We sought to quantify redundancy in clinical documentation by studying collections of physician notes in an EHR. We implemented a retrospective design to gather all electronic admission, progress, resident signout and discharge summary notes written during 100 randomly selected patient admissions within a 6 month period. We modified and applied a Levenshtein edit-distance algorithm to align and compare the documents written for each of the 100 admissions. We then identified and measured the amount of text duplicated from previous notes. Finally, we manually reviewed the content that was conserved between note types in a subsample of notes. We measured the amount of new information in a document, which was calculated as the number of words that did not match with previous documents divided by the length, in words, of the document. Results are reported as the percentage of information in a document that had been duplicated from previously written documents. Signout and progress notes proved to be particularly redundant, with an average of 78% and 54% information duplicated from previous documents respectively. There was also significant information duplication between document types (eg, from an admission note to a progress note). The study established the feasibility of exploring redundancy in the narrative record with a known sequence alignment algorithm used frequently in the field of bioinformatics. The findings provide a foundation for studying the usefulness and risks of redundancy in the EHR.
20064811	Transition from paper to electronic inpatient physician notes.
J Am Med Inform Assoc  2010 Jan-Feb
UW Medicine teaching hospitals have seen a move from paper to electronic physician inpatient notes, after improving the availability of workstations, and wireless laptops and the technical infrastructure supporting the electronic medical record (EMR). The primary driver for the transition was to unify the medical record for all disciplines in one location. The main barrier faced was the time required to enter notes, which was addressed with data-rich templates tailored to rounding workflow, simplified login and other measures. After a 2-year transition, nearly all physician notes for hospitalized patients are now entered electronically, approximately 1500 physician notes per day. Remaining challenges include time for note entry, and the perception that notes may be more difficult to understand and to find within the EMR. In general, the transition from paper to electronic notes has been regarded as valuable to patient care and hospital operations.
20051126	TreeGraph 2: combining and visualizing evidence from different phylogenetic analyses.
BMC Bioinformatics 20100105 2010
Today it is common to apply multiple potentially conflicting data sources to a given phylogenetic problem. At the same time, several different inference techniques are routinely employed instead of relying on just one. In view of both trends it is becoming increasingly important to be able to efficiently compare different sets of statistical values supporting (or conflicting with) the nodes of a given tree topology, and merging this into a meaningful representation. A tree editor supporting this should also allow for flexible editing operations and be able to produce ready-to-publish figures. We developed TreeGraph 2, a GUI-based graphical editor for phylogenetic trees (available from http://treegraph.bioinfweb.info). It allows automatically combining information from different phylogenetic analyses of a given dataset (or from different subsets of the dataset), and helps to identify and graphically present incongruences. The program features versatile editing and formatting options, such as automatically setting line widths or colors according to the value of any of the unlimited number of variables that can be assigned to each node or branch. These node/branch data can be imported from spread sheets or other trees, be calculated from each other by specified mathematical expressions, filtered, copied from and to other internal variables, be kept invisible or set visible and then be freely formatted (individually or across the whole tree). Beyond typical editing operations such as tree rerooting and ladderizing or moving and collapsing of nodes, whole clades can be copied from other files and be inserted (along with all node/branch data and legends), but can also be manually added and, thus, whole trees can quickly be manually constructed de novo. TreeGraph 2 outputs various graphic formats such as SVG, PDF, or PNG, useful for tree figures in both publications and presentations. TreeGraph 2 is a user-friendly, fully documented application to produce ready-to-publish trees. It can display any number of annotations in several ways, and permits easily importing and combining them. Additionally, a great number of editing- and formatting-operations is available.
20070799	Dentistry students' perceptions of learning management systems.
Eur J Dent Educ  2010Feb
This paper reports an exploratory survey study about students' perceptions of learning management systems (LMS) at the Faculty of Dentistry, University of Sydney. Two hundred and fifty-four students enrolled in the Bachelor of Dentistry and the Bachelor of Oral Health programmes participated in an online survey aimed at exploring their beliefs and attitudes as well as their preferences for eLearning tools. Results indicated a strong preference of students for using LMSs as resource repositories rather than for higher-order learning activities such as online discussion forums. This finding holds importance for consideration of the development of the educational resources modalities that support development of essential graduate attributes such as information literacy and collaborative learning.
20075480	Principles and tools for collaborative entity-based intelligence analysis.
IEEE Trans Vis Comput Graph  2010 Mar-Apr
Software tools that make it easier for analysts to collaborate as a natural part of their work will lead to better analysis that is informed by more perspectives. We are interested to know if software tools can be designed that support collaboration even as they allow analysts to find documents and organize information (including evidence, schemas, and hypotheses). We have modified the Entity Workspace system, described previously, to test such designs. We have evaluated the resulting design in both a laboratory study and a study where it is situated with an analysis team. In both cases, effects on collaboration appear to be positive. Key aspects of the design include an evidence notebook optimized for organizing entities (rather than text characters), information structures that can be collapsed and expanded, visualization of evidence that emphasizes events and documents (rather than emphasizing the entity graph), and a notification system that finds entities of mutual interest to multiple analysts. Long-term tests suggest that this approach can support both top-down and bottom-up styles of analysis.
20075481	Cross-filtered views for multidimensional visual analysis.
IEEE Trans Vis Comput Graph  2010 Mar-Apr
Analysis of multidimensional data often requires careful examination of relationships across dimensions. Coordinated multiple view approaches have become commonplace in visual analysis tools because they directly support expression of complex multidimensional queries using simple interactions. However, generating such tools remains difficult because of the need to map domain-specific data structures and semantics into the idiosyncratic combinations of interdependent data and visual abstractions needed to reveal particular patterns and distributions in cross-dimensional relationships. This paper describes: 1) a method for interactively expressing sequences of multidimensional set queries by cross-filtering data values across pairs of views and 2) design strategies for constructing coordinated multiple view interfaces for cross-filtered visual analysis of multidimensional data sets. Using examples of cross-filtered visualizations of data from several different domains, we describe how cross-filtering can be modularized and reused across designs, flexibly customized with respect to data types across multiple dimensions, and incorporated into more wide-ranging multiple view designs. We also identify several important limitations of the approach. The demonstrated analytic utility of these examples suggests that cross-filtering is a suitable design pattern for instantiation in a wide variety of visual analysis tools.
20075482	A visual analytics approach to understanding spatiotemporal hotspots.
IEEE Trans Vis Comput Graph  2010 Mar-Apr
As data sources become larger and more complex, the ability to effectively explore and analyze patterns among varying sources becomes a critical bottleneck in analytic reasoning. Incoming data contain multiple variables, high signal-to-noise ratio, and a degree of uncertainty, all of which hinder exploration, hypothesis generation/exploration, and decision making. To facilitate the exploration of such data, advanced tool sets are needed that allow the user to interact with their data in a visual environment that provides direct analytic capability for finding data aberrations or hotspots. In this paper, we present a suite of tools designed to facilitate the exploration of spatiotemporal data sets. Our system allows users to search for hotspots in both space and time, combining linked views and interactive filtering to provide users with contextual information about their data and allow the user to develop and explore their hypotheses. Statistical data models and alert detection algorithms are provided to help draw user attention to critical areas. Demographic filtering can then be further applied as hypotheses generated become fine tuned. This paper demonstrates the use of such tools on multiple geospatiotemporal data sets.
20075484	Route visualization using detail lenses.
IEEE Trans Vis Comput Graph  2010 Mar-Apr
We present a method designed to address some limitations of typical route map displays of driving directions. The main goal of our system is to generate a printable version of a route map that shows the overview and detail views of the route within a single, consistent visual frame. Our proposed visualization provides a more intuitive spatial context than a simple list of turns. We present a novel multifocus technique to achieve this goal, where the foci are defined by points of interest (POI) along the route. A detail lens that encapsulates the POI at a finer geospatial scale is created for each focus. The lenses are laid out on the map to avoid occlusion with the route and each other, and to optimally utilize the free space around the route. We define a set of layout metrics to evaluate the quality of a lens layout for a given route map visualization. We compare standard lens layout methods to our proposed method and demonstrate the effectiveness of our method in generating aesthetically pleasing layouts. Finally, we perform a user study to evaluate the effectiveness of our layout choices.
20075485	Analyzing and tracking burning structures in lean premixed hydrogen flames.
IEEE Trans Vis Comput Graph  2010 Mar-Apr
This paper presents topology-based methods to robustly extract, analyze, and track features defined as subsets of isosurfaces. First, we demonstrate how features identified by thresholding isosurfaces can be defined in terms of the Morse complex. Second, we present a specialized hierarchy that encodes the feature segmentation independent of the threshold while still providing a flexible multiresolution representation. Third, for a given parameter selection, we create detailed tracking graphs representing the complete evolution of all features in a combustion simulation over several hundred time steps. Finally, we discuss a user interface that correlates the tracking information with interactive rendering of the segmented isosurfaces enabling an in-depth analysis of the temporal behavior. We demonstrate our approach by analyzing three numerical simulations of lean hydrogen flames subject to different levels of turbulence. Due to their unstable nature, lean flames burn in cells separated by locally extinguished regions. The number, area, and evolution over time of these cells provide important insights into the impact of turbulence on the combustion process. Utilizing the hierarchy, we can perform an extensive parameter study without reprocessing the data for each set of parameters. The resulting statistics enable scientists to select appropriate parameters and provide insight into the sensitivity of the results with respect to the choice of parameters. Our method allows for the first time to quantitatively correlate the turbulence of the burning process with the distribution of burning regions, properly segmented and selected. In particular, our analysis shows that counterintuitively stronger turbulence leads to larger cell structures, which burn more intensely than expected. This behavior suggests that flames could be stabilized under much leaner conditions than previously anticipated.
20077926	Medical librarians supporting information systems project lifecycles toward improved patient safety. Medical librarians possess expertise to navigate various search resources and can investigate inquiries during IS project lifecycles.
J Healthc Inf Manag  2010Winter
Health information systems (HIS) have progressed from being used to manage billing to impacting patient safety and health professionals' job satisfaction. Many decisions are made during project management and the information system lifecycle of a HIS. Medical librarians are underutilized in HIS lifecycles; it may not be clear to stakeholders what they can provide and where their services fit. Medical librarians possess expertise to navigate various search resources and can investigate inquiries during information systems project lifecycles. Librarians can market specific skills to project lifecycle teams such as those involved in computerized provider order entry (CPOE), electronic medication administration record (eMAR) and root cause analysis (RCA). HIS project personnel, including patient safety team members, should make use of medical librarians in phases of health information systems project management. This will help them meet institutional and global objectives for evidence-based use of technology towards improved patient safety.
20027117	Field strength and diffusion encoding technique affect the apparent diffusion coefficient measurements in diffusion-weighted imaging of the abdomen.
Invest Radiol  2010Feb
The purpose of this study is to determine what effects a variety of diffusion encoding techniques at 1.5 T and 3 T have on measured abdominal apparent diffusion coefficient (ADC) values obtained in a healthy population. Sixteen healthy male volunteers were enrolled in this prospective Institutional Review Board-approved study following written informed consent. Imaging was performed on a 1.5 T and a 3 T magnetic resonance system (Siemens, Erlangen) with several abdominal axial diffusion weighted imaging (DWI) acquisitions: an orthogonal diffusion encoding with b-values of 0/400 seconds/mm, and a series of four 3-scan trace weighted acquisitions with b-values of 0/50, 0/400, 0/800, 0/50/400/800 seconds/mm, respectively. The mean ADC values were calculated for 3 regions of interest (ROI) in 5 locations (right hepatic lobe, spleen, pancreatic head, body, and tail). The ADC data were analyzed using a repeated-measures analysis of variance. There was a significant difference between measured ADC values at 1.5 T and 3 T for liver (P &lt; 0.001), but not for pancreas (P = 0.427) or spleen (P = 0.167). There was no significant difference (P &gt; 0.999) in the measured ADC values between the orthogonal encodings and the 3-scan trace weighted encoding with the same b-value. There were significant differences (P &lt; 0.001) between all 4 weighting schemes for the 3-scan trace with the measured ADC decreasing with increasing b-value. Measured abdominal ADC values depend on the exact selection of b-value used for encoding for liver, pancreas, and spleen. In addition, the measured ADC values depend on the field strength of the scanner for liver.
20098647	Beyond citation analysis: a model for assessment of research impact.
J Med Libr Assoc  2010Jan
Is there a means of assessing research impact beyond citation analysis? The case study took place at the Washington University School of Medicine Becker Medical Library. This case study analyzed the research study process to identify indicators beyond citation count that demonstrate research impact. The authors discovered a number of indicators that can be documented for assessment of research impact, as well as resources to locate evidence of impact. As a result of the project, the authors developed a model for assessment of research impact, the Becker Medical Library Model for Assessment of Research. Assessment of research impact using traditional citation analysis alone is not a sufficient tool for assessing the impact of research findings, and it is not predictive of subsequent clinical applications resulting in meaningful health outcomes. The Becker Model can be used by both researchers and librarians to document research impact to supplement citation analysis.
19910657	Opportunistic tangible user interfaces for augmented reality.
IEEE Trans Vis Comput Graph  2010 Jan-Feb
Opportunistic Controls are a class of user interaction techniques that we have developed for augmented reality (AR) applications to support gesturing on, and receiving feedback from, otherwise unused affordances already present in the domain environment. By leveraging characteristics of these affordances to provide passive haptics that ease gesture input, Opportunistic Controls simplify gesture recognition, and provide tangible feedback to the user. In this approach, 3D widgets are tightly coupled with affordances to provide visual feedback and hints about the functionality of the control. For example, a set of buttons can be mapped to existing tactile features on domain objects. We describe examples of Opportunistic Controls that we have designed and implemented using optical marker tracking, combined with appearance-based gesture recognition. We present the results of two user studies. In the first, participants performed a simulated maintenance inspection of an aircraft engine using a set of virtual buttons implemented both as Opportunistic Controls and using simpler passive haptics. Opportunistic Controls allowed participants to complete their tasks significantly faster and were preferred over the baseline technique. In the second, participants proposed and demonstrated user interfaces incorporating Opportunistic Controls for two domains, allowing us to gain additional insights into how user interfaces featuring Opportunistic Controls might be designed.
19926897	A boosting framework for visuality-preserving distance metric learning and its application to medical image retrieval.
IEEE Trans Pattern Anal Mach Intell  2010Jan
Similarity measurement is a critical component in content-based image retrieval systems, and learning a good distance metric can significantly improve retrieval performance. However, despite extensive study, there are several major shortcomings with the existing approaches for distance metric learning that can significantly affect their application to medical image retrieval. In particular, "similarity" can mean very different things in image retrieval: resemblance in visual appearance (e.g., two images that look like one another) or similarity in semantic annotation (e.g., two images of tumors that look quite different yet are both malignant). Current approaches for distance metric learning typically address only one goal without consideration of the other. This is problematic for medical image retrieval where the goal is to assist doctors in decision making. In these applications, given a query image, the goal is to retrieve similar images from a reference library whose semantic annotations could provide the medical professional with greater insight into the possible interpretations of the query image. If the system were to retrieve images that did not look like the query, then users would be less likely to trust the system; on the other hand, retrieving images that appear superficially similar to the query but are semantically unrelated is undesirable because that could lead users toward an incorrect diagnosis. Hence, learning a distance metric that preserves both visual resemblance and semantic similarity is important. We emphasize that, although our study is focused on medical image retrieval, the problem addressed in this work is critical to many image retrieval systems. We present a boosting framework for distance metric learning that aims to preserve both visual and semantic similarities. The boosting framework first learns a binary representation using side information, in the form of labeled pairs, and then computes the distance as a weighted Hamming distance using the learned binary representation. A boosting algorithm is presented to efficiently learn the distance function. We evaluate the proposed algorithm on a mammographic image reference library with an Interactive Search-Assisted Decision Support (ISADS) system and on the medical image data set from ImageCLEF. Our results show that the boosting framework compares favorably to state-of-the-art approaches for distance metric learning in retrieval accuracy, with much lower computational cost. Additional evaluation with the COREL collection shows that our algorithm works well for regular image data sets.
19534588	Cumulative knowledge and progress in human factors.
Annu Rev Psychol  2010
This review provides a cumulative perspective on current human factors research by first briefly acknowledging previous Annual Review articles. We show that several recent conceptual advances are an outgrowth of the information-processing approach adopted by the field and present several areas of current research that are built directly on prior work. Topic areas that provide fundamental tools for human factors analyses are summarized, and several current application areas are reviewed. We end by considering alternatives to the information-processing approach that have been proposed and placing those alternatives in context. We argue that the information-processing language provides the foundation that has enabled much of the growth in human factors. This growth reflects a cumulative development of concepts and methods that continues today.
19957291	Rapid screening and confirmation of drugs and toxic compounds in biological specimens using liquid chromatography/ion trap tandem mass spectrometry and automated library search.
Rapid Commun. Mass Spectrom.  2010Jan
Recent advances in liquid chromatography/tandem mass spectrometry (LC/MS/MS) technology have provided an opportunity for the development of more specific approaches to achieve the 'screen' and 'confirmation' goals in a single analytical step. For this purpose, this study adapts the electrospray ionization ion trap LC/MS/MS instrumentation (LC/ESI-MS/MS) for the screening and confirmation of over 800 drugs and toxic compounds in biological specimens. Liquid-liquid and solid-phase extraction protocols were coupled to LC/ESI-MS/MS using a 1.8-microm particle size analytical column operated at 50 degrees C. Gradient elution of the analytes was conducted using a solvent system composed of methanol and water containing 0.1% formic acid. Positive-ion ESI-MS/MS spectra and retention times for each of the 800 drugs and toxic compounds were first established using 1-10 microg/mL standard solutions. This spectra and retention time information was then transferred to the library and searched by the identification algorithm for the confirmation of compounds found in test specimens - based on retention time matches and scores of fit, reverse fit, and purity resulting from the searching process. The established method was found highly effective when applied to the analyses of postmortem specimens (blood, urine, and hair) and external proficiency test samples provided by the College of American Pathology (CAP). The development of this approach has significantly improved the efficiency of our routine laboratory operation that was based on a two-step (immunoassay and GC/MS) approach in the past.
19610077	Disability in inflammatory bowel diseases: developing ICF Core Sets for patients with inflammatory bowel diseases based on the International Classification of Functioning, Disability, and Health.
Inflamm. Bowel Dis.  2010Jan
The inflammatory bowel diseases (IBDs) are associated with a reduced quality of life. The impact of IBD on disability remains largely unknown. With the International Classification of Functioning, Disability, and Health (ICF) of the World Health Organization (WHO), we can now rely on a globally agreed-upon framework and system for classifying the typical spectrum of problems in the functioning of persons with a specific disease given the environmental context in which they live. The aim of this article is to outline the methods to be utilized to develop ICF Core Sets for IBD. The project is a cooperation between the ICF Research Branch of the WHO, the IPNIC group, the International Society of Physical Rehabilitation Medicine (ISPRM), and the International Organization on Inflammatory Bowel Disease (IOIBD). Four worldwide studies will be conducted from 2009 to 2010. ICF categories relevant for IBD will be identified by systematic literature review of outcomes and measures used in IBD research, semistructured patient interviews, Internet-based expert survey, and cross-sectional study for clinical applicability. The final definition of ICF Core Sets for IBD will be determined at a Consensus Conference. Field testing will then be used to validate the ICF Core Sets. ICF Core Sets are being designed to provide useful standards for research and clinical practice. This tool will enable research that improves understanding of functioning, disability, and health in IBD, and may lead to interventions to improve and maintain functioning and minimize disability among IBD patients throughout the world.
20032731	Online resources for pediatric home care clinicians.
Home Healthc Nurse  2010Jan
Pediatric home care clinicians dealing with premature infants can encounter any number of unusual conditions related to their patient's prematurity. Finding reliable information on the condition can be difficult for a clinician who does not have immediate access to a health sciences library. There are, however, many useful, reliable websites that can be consulted with a laptop that has wireless access. This article reviews a variety of useful websites with a wealth of evidence-based information. The author also provides information on Health on the Net (HON), a nongovernmental, nonprofit organization that has established a code of conduct that websites must meet in order to display their seal.
20027579	Measuring the heart in pulmonary arterial hypertension (PAH): implications for trial study size.
J Magn Reson Imaging  2010Jan
To calculate the sample size for a theoretical pulmonary arterial hypertension (PAH) randomized controlled trial (RCT) by using cardiovascular magnetic resonance (CMR) imaging to determine the repeatability of measures between two scans. Two same-day examinations from 10 PAH patients were analyzed manually and semiautomatically. Study size was calculated from the standard deviation (SD) of repeatability. Different approaches to right-ventricle (RV) mass were investigated, agreement between methods tested and interobserver reproducibility measured by Bland-Altman analysis to explore how the PAH heart might be best measured. Repeatability was good for almost all manually-measured indices but poor for semiautomated measurement of RV mass and left-ventricle (LV) end-diastolic volume (EDV). Thus, for an RCT (power, 80%; significance level, 5%) analyzing "outcome" indices (RVEDV, LVEDV, RV ejection fraction, and RV mass; anticipated change: 10 mL, 10 mL, 3%, and 10 g, respectively) manually, 34 patients are required compared to 78 if analysis is semiautomated. RV mass was repeatable if the interventricular septum was divided between ventricles or if wholly apportioned to the LV. Limits of agreement between manual and semiautomated analyses were unsatisfactory for RV measures and interobserver reproducibility was worse for semiautomated than manual analysis. Manual is more robust than semiautomated analysis and at present should be favored in RCTs in PAH as it leads to lower sample size requirements.